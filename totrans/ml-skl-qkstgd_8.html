<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performance Evaluation Methods</h1>
                </header>
            
            <article>
                
<p class="mce-root">Your method of performance evaluation will vary by the type of machine learning algorithm that you choose to implement. In general, there are different metrics that can potentially determine how well your model is performing at its given task for classification, regression, and unsupervised machine learning algorithms. </p>
<p>In this chapter, we will explore how the different performance evaluation methods can help you to better understand your model. The chapter will be split into three sections, as follows:</p>
<ul>
<li>Performance evaluation for classification algorithms</li>
<li>Performance evaluation for regression algorithms </li>
<li>Performance evaluation for unsupervised algorithms </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You will be required to have <span class="fontstyle0">Python 3.6 or greater, </span><span class="fontstyle0">Pandas ≥ 0.23.4, </span><span class="fontstyle0">Scikit-learn ≥ 0.20.0, </span><span class="fontstyle0">NumPy ≥ 1.15.1, </span><span class="fontstyle0">Matplotlib ≥ 3.0.0, and </span><span class="fontstyle0">Scikit-plot ≥ 0.3.7</span></span> <span class="fontstyle0">installed on your system.</span></p>
<p class="mce-root">The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb</a></p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2EY4nJU">http://bit.ly/2EY4nJU</a></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why is performance evaluation critical?</h1>
                </header>
            
            <article>
                
<p>It is key for you to understand why we need to evaluate the performance of a model in the first place. Some of the potential reasons why performance evaluation is critical are as follows:</p>
<ul>
<li><strong>It prevents overfitting</strong>:<strong> </strong>Overfitting occurs when your algorithm hugs the data too tightly and makes predictions that are specific to only one dataset. In other words, your model cannot generalize its predictions outside of the data that it was trained on.</li>
<li><strong>It prevents underfitting</strong>:<strong> </strong>This is the exact opposite of overfitting. In this case, the model is very generic in nature.</li>
<li><strong>Understanding predictions</strong>:<strong> </strong>Performance evaluation methods will help you to understand, in greater detail, how your model makes predictions, along with the nature of those predictions and other useful information, such as the accuracy of your model. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance evaluation for classification algorithms</h1>
                </header>
            
            <article>
                
<p>In order to evaluate the performance of classification, let's consider the two classification algorithms that we have built in this book: k-nearest neighbors and logistic regression. </p>
<p>The first step will be to implement both of these algorithms in the fraud detection dataset. We can do this by using the following code:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn import linear_model<br/><br/>#Reading in the fraud detection dataset <br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Creating the features <br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>#Splitting the data into training and test sets <br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)<br/><br/># Building the K-NN Classifier <br/><br/>knn_classifier = KNeighborsClassifier(n_neighbors=3)<br/><br/>knn_classifier.fit(X_train, y_train)<br/><br/>#Initializing an logistic regression object<br/><br/>logistic_regression = linear_model.LogisticRegression()<br/><br/>#Fitting the model to the training and test sets<br/><br/>logistic_regression.fit(X_train, y_train)</pre>
<p>In the preceding code, we read the fraud detection dataset into our notebook and split the data into the features and target variables, as usual. We then split the data into training and test sets, and build the k-nearest neighbors and logistic regression models in the training data.</p>
<p>In this section, you will learn how to evaluate the performance of a single model: k-nearest neighbors. You will also learn how to compare and contrast multiple models. Therefore, you will learn about the following things:</p>
<ul>
<li>Confusion matrix</li>
<li>Normalized confusion matrix</li>
<li>Area under the curve (<kbd>auc</kbd> score)</li>
<li>Cumulative gains curve </li>
<li>Lift curve </li>
<li>K-S statistic plot</li>
<li>Calibration plot</li>
<li>Learning curve</li>
<li>Cross-validated box plot</li>
</ul>
<p>Some of the visualizations in this section will require a package titled <kbd>scikit-plot</kbd>. The <kbd>scikit-plot</kbd> package is very effective, and it is used to visualize the various performance measures of machine learning models. It was specifically made for models that are built using scikit-learn. </p>
<p class="mce-root"/>
<p>In order to install <kbd>scikit-plot</kbd> on your local machine, using <kbd>pip</kbd> in Terminal, we use the following code:</p>
<pre><strong>pip3 install scikit-plot</strong></pre>
<p>If you are using the Anaconda distribution to manage your Python packages, you can install <kbd>scikit-plot</kbd> by using the following code:</p>
<pre><strong><span>conda install -c conda-forge scikit-plot</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The confusion matrix</h1>
                </header>
            
            <article>
                
<p>Until now, we have used the accuracy as the sole measure of model performance. That was fine, because we have a balanced dataset. A balanced dataset is a dataset in which there are almost the same numbers of labels for each category. In the dataset that we are working with, 8,000 labels belong to the fraudulent transactions, while 12,000 belong to the non-fraudulent transactions. </p>
<p>Imagine a situation in which 90% of our data had non-fraudulent transactions, while only 10% of the transactions had fraudulent cases. If the classifier reported an accuracy of 90%, it wouldn't make sense, because most of the data that it has seen thus far were the non-fraudulent cases and it has seen very little of the fraudulent cases. So, even if it classified 90% of the cases accurately, it would mean that most of the cases that it classified would belong to the non-fraudulent cases. That would provide no value to us. </p>
<p>A <strong>confusion matrix</strong> is a performance evaluation technique that can be used in such cases, which do not involve a balanced dataset. The confusion matrix for our dataset would look as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5fb4aee3-f8bc-4ef3-810d-e6102797028e.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Confusion matrix for fraudulent transactions </div>
<p>The goal of the confusion matrix is to maximize the number of true positives and true negatives, as this gives the correct predictions; it also minimizes the number of false negatives and false positives, as they give us the wrong predictions. </p>
<p>Depending on your problem, the false positives might be more problematic than the false negatives (and vice versa), and thus, the goal of building the right classifier should be to solve your problem in the best possible way. </p>
<p>In order to implement the confusion matrix in scikit-learn, we use the following code:</p>
<pre class="mce-root">from sklearn.metrics import confusion_matrix<br/><br/>#Creating predictions on the test set <br/><br/>prediction = knn_classifier.predict(X_test)<br/><br/>#Creating the confusion matrix <br/><br/>print(confusion_matrix(y_test, prediction))</pre>
<p>This produces the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f160e07-2447-4a3f-a8e2-a6c2076082f2.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The confusion matrix output from our classifier for fraudulent transactions </div>
<p>In the preceding code, we create a set of predictions using the<span> </span><kbd>.predict()</kbd><em> </em>method on the test training data, and then we use the <kbd>confusion_matrix()</kbd><em> </em>function on the test set of the target variable and the predictions that were created earlier. </p>
<p class="mce-root">The preceding confusion matrix looks almost perfect, as most cases are classified into the true positive and true negative categories, along the main diagonal. Only 46 cases are classified incorrectly, and this number is almost equal. This means that the numbers of false positives and false negatives are minimal and balanced, and one does not outweigh the other. This is an example of the ideal classifier. </p>
<p>Three other metrics that can be derived from the confusion matrix are<span> </span><strong>precision</strong>, <strong>recall,</strong><span> </span>and <strong>F1-score</strong>. A high value of precision indicates that not many non-fraudulent transactions are classified as fraudulent, while a high value of recall indicates that most of the fraudulent cases were predicted correctly. </p>
<p>The F1-score is the weighted average of the precision and recall.</p>
<p class="mce-root"/>
<p>We can compute the precision and recall by using the following code:</p>
<pre>from sklearn.metrics import classification_report<br/><br/>#Creating the classification report <br/><br/>print(classification_report(y_test, prediction))</pre>
<p>This produces the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a13ae6ab-fe62-495e-896f-a5130759e7f4.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Classification report</div>
<p>In the preceding code, we use the <kbd>classificiation_report()</kbd><span> </span>function with two arguments: the test set of the target variable and the prediction variable that we created for the confusion matrix earlier.</p>
<p>In the output, the precision, recall, and F1-score are all high, because we have built the ideal machine learning model. These values range from 0 to 1, with 1 being the highest. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The normalized confusion matrix</h1>
                </header>
            
            <article>
                
<p>A <strong>normalized confusion matrix</strong> makes it easier for the data scientist to visually interpret how the labels are being predicted. In order to construct a normalized confusion matrix, we use the following code:</p>
<pre>import matplotlib.pyplot as plt<br/>import scikitplot as skplt<br/><br/>#Normalized confusion matrix for the K-NN model<br/><br/>prediction_labels = knn_classifier.predict(X_test)<br/>skplt.metrics.plot_confusion_matrix(y_test, prediction_labels, normalize=True)<br/>plt.show()</pre>
<p>This results in the following normalized confusion matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c3c87197-7a32-4095-a4f8-7ea43559918d.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Normalized confusion matrix for the K-NN model</div>
<p>In the preceding plot, the predicted labels are along the <em>x </em>axis, while the true (or actual) labels are along the <em>y </em>axis. We can see that the model has 0.01, or 1%, of the predictions for the fraudulent transactions incorrect, while 0.99, or 99%, of the fraudulent transactions have been predicted correctly. We can also see that the K-NN model predicted 100% of the non-fraudulent transactions correctly. </p>
<p>Now, we can compare the performance of the logistic regression model by using a normalized confusion matrix, as follows:</p>
<pre>#Normalized confusion matrix for the logistic regression model<br/><br/>prediction_labels = logistic_regression.predict(X_test)<br/>skplt.metrics.plot_confusion_matrix(y_test, prediction_labels, normalize=True)<br/>plt.show()</pre>
<p>This results in the following normalized confusion matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/125d0f78-4a87-415d-a3d0-f383eac558fb.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Normalized confusion matrix for the logistic regression model</div>
<p>In the preceding confusion matrix, it is clear that the logistic regression model only predicted 42% of the non-fraudulent transactions correctly. This indicates, almost instantly, that the k-nearest neighbor model performed better.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Area under the curve</h1>
                </header>
            
            <article>
                
<p>The curve, in this case, is the <strong>receiver operator characteristics</strong> (<strong>ROC</strong>) curve. This is a plot between the true positive rate and the false positive rate. We can plot this curve as follows:</p>
<pre>from sklearn.metrics import roc_curve<br/>from sklearn.metrics import roc_auc_score<br/>import matplotlib.pyplot as plt<br/><br/>#Probabilities for each prediction output <br/><br/>target_prob = knn_classifier.predict_proba(X_test)[:,1]<br/><br/>#Plotting the ROC curve <br/><br/>fpr, tpr, thresholds = roc_curve(y_test, target_prob)<br/><br/>plt.plot([0,1], [0,1], 'k--')<br/><br/>plt.plot(fpr, tpr)<br/><br/>plt.xlabel('False Positive Rate')<br/><br/>plt.ylabel('True Positive Rate')<br/><br/>plt.title('ROC Curve')<br/><br/>plt.show()</pre>
<p>This produces the following curve:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7bfbb951-cb4e-4052-b9fe-4ca056717412.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">ROC curve </div>
<p class="mce-root">In the preceding code, first, we create a set of probabilities for each of the predicted labels. For instance, the predicted label of <span class="packt_screen">1</span> would have a certain set of probabilities associated with it, while the label <span class="packt_screen">0</span> would have a certain set of probabilities associated with it. Using these probabilities, we use the <kbd>roc_curve()</kbd><em> </em>function, along with the target test set, to generate the ROC curve.</p>
<p>The preceding curve is an example of a perfect ROC curve. The preceding curve has a true positive rate of 1.0, which indicates accurate predictions, while it has a false positive rate of 0, which indicates a lack of wrong predictions. </p>
<p>Such a curve also has the most area under the curve, as compared to the curves of models that have a lower accuracy. In order to compute the area under the curve score, we use the following code:</p>
<pre>#Computing the auc score <br/><br/>roc_auc_score(y_test, target_prob)</pre>
<p>This produces a score of 0.99. A higher <kbd>auc</kbd> score indicates a better performing model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cumulative gains curve</h1>
                </header>
            
            <article>
                
<p>When building multiple machine learning models, it is important to understand which of the models in question produces the type of predictions that you want it to generate. The <strong>cumulative gains curve</strong> helps you with the process of model comparison, by telling you about the percentage of a category/class that appears within a percentage of the sample population for a particular model. </p>
<p>In simple terms, in the fraud detection dataset, we might want to pick a model that can predict a larger number of fraudulent transactions, as opposed to a model that cannot. In order to construct the cumulative gains plot for the k-nearest neighbors model, we use the following code:</p>
<pre>import scikitplot as skplt<br/><br/>target_prob = knn_classifier.predict_proba(X_test)<br/>skplt.metrics.plot_cumulative_gain(y_test, target_prob)<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8db348f7-5c94-4fe9-9382-216276efc920.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Cumulative gains plot for the k-nearest neighbors model</div>
<p>In the preceding code, the following applies:</p>
<ul>
<li>First, we import the <kbd>scikit-plot</kbd> package, which generates the preceding plot. We then compute the probabilities for the target variable, which, in this case, are the probabilities if a particular mobile transaction is fraudulent or not on the test data.</li>
<li>Finally, we use the <kbd>plot_cumulative_gain()</kbd><em> </em><span>function on these probabilities and the test data target labels, in order to generate the preceding plot.</span></li>
</ul>
<p>How do we interpret the preceding plot? We simply look for the point at which a certain percentage of the data contains 100% of the target class. This is illustrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6f432ac3-7149-40a5-9ed4-d7fb0c139012.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Point at which 100% of the target class exists </div>
<p>The point defined in the preceding diagram corresponds to a value of 0.3 on the <em>x </em>axis and 1.0 on the <em>y </em>axis. This means that 0.3 to 1.0 (or 30% to 100%) of the data will consist of the target class, 1, which are the fraudulent transactions. </p>
<p>This can also be interpreted as follows: 70% of the total data will contain 100% of the fraudulent transaction predictions if you use the k-nearest neighbors model.</p>
<p>Now, let's compute the cumulative gains curve for the logistic regression model, and see if it is different. In order to do this, we use the following code:</p>
<pre>#Cumulative gains plot for the logistic regression model<br/><br/>target_prob = logistic_regression.predict_proba(X_test)<br/>skplt.metrics.plot_cumulative_gain(y_test, target_prob)<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/438c000a-eafd-4c9e-b1c2-b604b65f299e.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Cumulative gains plot for the logistic regression model</div>
<p class="mce-root">The preceding plot is similar to the cumulative gains plot that was previously produced by the K-NN model, in that 70% of the data contains 100% of the target class. Therefore, using either the K-NN or the logistic regression model will yield similar results. </p>
<p>However, it is a good practice to compare how different models behave by using the cumulative gains chart, in order to gain a fundamental understanding of how your model makes predictions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Lift curve</h1>
                </header>
            
            <article>
                
<p>A <strong>lift curve</strong> gives you information about how well you can make predictions by using a machine learning model, as opposed to when you are not using one. In order to construct a lift curve for the k-nearest neighbor model, we use the following code:</p>
<pre># Lift curve for the K-NN model<br/><br/>target_prob = knn_classifier.predict_proba(X_test)<br/>skplt.metrics.plot_lift_curve(y_test, target_prob)<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cba26353-d293-4a22-9cae-f6e6315a92bd.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Lift curve for the K-NN model</div>
<p>How do we interpret the preceding lift curve? We have to look for the point at which the curve dips. This is illustrated for you in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/94c581f3-70aa-49f6-8d32-298e88da3e6c.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Point of interest in the lift curve</div>
<p>In the preceding plot, the point that is highlighted is the point that we want to look for in any lift curve. The point tells us that 0.3, or 30%, of our total data will perform 3.5 times better when using the K-NN predictive model, as opposed to when we do not use any model at all to predict the fraudulent transactions. </p>
<p>Now, we can construct the lift curve for the logistic regression model, in order to compare and contrast the performance of the two models. We can do this by using the following code:</p>
<pre>#Cumulative gains plot for the logistic regression model<br/><br/>target_prob = logistic_regression.predict_proba(X_test)<br/>skplt.metrics.plot_lift_curve(y_test, target_prob)<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a99e0447-e440-49ac-9f06-98e3c4779cce.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Lift curve for the logistic regression model</div>
<p>Although the plot tells us that 30% of the data will see an improved performance (similar to that of the K-NN model that we built earlier in order to predict the fraudulent transactions), there is a difference when it comes to predicting the non-fraudulent transactions (the blue line). </p>
<p>For a small percentage of the data, the lift curve for the non-fraudulent transactions is actually lower than the baseline (the dotted line). This means that the logistic regression model does worse than not using a predictive model for a small percentage of the data when it comes to predicting the non-fraudulent transactions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-S statistic plot</h1>
                </header>
            
            <article>
                
<p>The <strong>K-S statistic plot</strong>, or the <strong>Kolmogorov Smirnov</strong> statistic plot, is a plot that tells you whether the model gets confused when it comes to predicting the different labels in your dataset. In order to illustrate what the term <em>confused</em> means in this case, we will construct the K-S statistic plot for the K-NN model by using the following code:</p>
<p class="mce-root"/>
<pre class="mce-root">#KS plot for the K-NN model<br/><br/>target_proba = knn_classifier.predict_proba(X_test)<br/>skplt.metrics.plot_ks_statistic(y_test, target_proba)<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0d914ee1-1c8e-406d-8137-0e1885305642.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">K-S statistic plot for the K-NN model</div>
<p>In the preceding plot, the following applies:</p>
<ul>
<li>The dotted line is the distance between the predictions for the fraudulent transactions (the yellow line at the bottom) and the non-fraudulent transactions (the blue line at the top). This distance is 0.985, as indicated by the plot.</li>
<li>A K-S statistic score that is close to 1 is usually a good indication that the model does not get confused between predicting the two different target labels, and can make a clear distinction when it comes to predicting the labels. </li>
<li>In the preceding plot, the score of 0.985 can be observed as the difference between the two classes of predictions, for up to 70% (0.7) of the data. This can be observed along the <em>x </em>axis, as a threshold of 0.7 still has the maximum separation distance. </li>
</ul>
<p>We can now compute the K-S statistic plot for the logistic regression model, in order to compare which of the two models provides a better distinction in predictions between the two class labels. We can do this by using the following code:</p>
<pre>#KS plot for the logistic regression model<br/><br/>target_proba = logistic_regression.predict_proba(X_test)<br/>skplt.metrics.plot_ks_statistic(y_test, target_proba)<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/acbae8fc-3ed1-4f58-b477-86449a66fcf2.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">K-S statistic plot for the logistic regression model</div>
<p>Although the two models have the same separation score of 0.985, the threshold at which the separation occurs is quite different. In the case of logistic regression, this distance only occurs for the bottom 43% of the data, since the maximum separation starts at a threshold of 0.57, along the <em>x </em>axis. </p>
<p>This means that the k-nearest neighbors model, which has a large distance for about 70% of the total data, is much better at making predictions about fraudulent transactions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calibration plot</h1>
                </header>
            
            <article>
                
<p>A <strong>calibration plot</strong>, as the name suggests, tells you how well calibrated your model is. A well-calibrated model will have a prediction score equal to the fraction of the positive class (in this case, the fraudulent transactions). In order to plot a calibration plot, we use the following code:</p>
<pre>#Extracting the probabilites that the positive class will be predicted<br/><br/>knn_proba = knn_classifier.predict_proba(X_test)<br/>log_proba = logistic_regression.predict_proba(X_test)<br/><br/>#Storing probabilities in a list<br/><br/>probas = [knn_proba, log_proba]<br/><br/># Storing the model names in a list <br/><br/>model_names = ["k_nn", "Logistic Regression"]<br/><br/>#Creating the calibration plot<br/><br/>skplt.metrics.plot_calibration_curve(y_test, probas, model_names)<br/><br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This results in the following calibration plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1ce52083-1671-4729-b9f4-f13b2696bca9.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Calibration plot for the two models </div>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we compute the probability that the positive class (fraudulent transactions) will be predicted for each model. </li>
<li>Then, we store these probabilities and the model names in a list. </li>
<li>Finally, we use the <kbd>plot_calibration_curve()</kbd><em> </em>function from the <kbd>scikit-plot</kbd> package with these probabilities, the test labels, and the model names, in order to create the calibration plot. </li>
</ol>
<p>This results in the preceding calibration plot, which can be explained as follows:</p>
<ul>
<li>The dotted line represents the perfect calibration plot. This is because the mean prediction value has the exact value of the fraction of the positive class at each and every point. </li>
<li>From the plot, it is clear that the k-nearest neighbors model is much better calibrated than the calibration plot of the logistic regression model.</li>
<li>This is because the calibration plot of the k-nearest neighbors model follows that of the ideal calibration plot much more closely than the calibration plot of the logistic regression model. </li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning curve</h1>
                </header>
            
            <article>
                
<p>A <strong>learning curve</strong> is a plot that compares how the training accuracy scores and the test accuracy scores vary as the number of samples/rows added to the data increases. In order to construct the learning curve for the k-nearest neighbors model, we use the following code:</p>
<pre>skplt.estimators.plot_learning_curve(knn_classifier, features, target)<br/><br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/07066563-bdff-4d65-b444-a13ba26b829f.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Learning curve for the K-NN model</div>
<p>In the preceding curve, the following applies:</p>
<ol>
<li>The training score and the test score are only the highest when the number of samples is 15,000. This suggests that even if we had only 15,000 samples (instead of the 17,500), we would still get the best possible results. </li>
<li>Anything under the 15,000 samples will mean that the test cross-validated scores will be much lower than the training scores, suggesting that the model is overfit.</li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cross-validated box plot</h1>
                </header>
            
            <article>
                
<p>In this plot, we compare the cross-validated accuracy scores of multiple models by making use of box plots. In order to do so, we use the following code:</p>
<pre>from sklearn import model_selection<br/><br/>#List of models<br/><br/>models = [('k-NN', knn_classifier), ('LR', logistic_regression)]<br/><br/>#Initializing empty lists in order to store the results<br/>cv_scores = []<br/>model_name_list = []<br/><br/>for name, model in models:<br/>    <br/>    #5-fold cross validation<br/>    cv_5 = model_selection.KFold(n_splits= 5, random_state= 50)<br/>    # Evaluating the accuracy scores<br/>    cv_score = model_selection.cross_val_score(model, X_test, y_test, cv = cv_5, scoring= 'accuracy')<br/>    cv_scores.append(cv_score)<br/>    model_name_list.append(name)<br/>    <br/># Plotting the cross-validated box plot <br/><br/>fig = plt.figure()<br/>fig.suptitle('Boxplot of 5-fold cross validated scores for all the models')<br/>ax = fig.add_subplot(111)<br/>plt.boxplot(cv_scores)<br/>ax.set_xticklabels(model_name_list)<br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1c850faa-94c3-4070-90a1-5d19b28b7899.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Cross-validated box plot</div>
<p>In the preceding code, the following applies:</p>
<ol>
<li>First, we store the models that we want to compare in a list. </li>
<li>Then, we initialize two empty lists, in order to store the results of the cross-validated accuracy scores and the names of the models, so that we can use them later, in order to create the box plots. </li>
<li>We then iterate over each model in the list of models, and use the <kbd>model_selection.KFold()</kbd><em> </em>function in order to split the data into a five-fold cross-validated set.</li>
<li>Next, we extract the five-fold cross-validated scores by using the <br/>
<kbd>model_selection.cross_val_scores()</kbd><em> </em>function and append the scores, along with the model names, into the lists that we initialized at the beginning of the code. </li>
<li>Finally, a box plot is created, displaying the cross-validated scores in a box plot. </li>
</ol>
<p>The list that we created consists of the five cross-validated scores, along with the model names. A box plot takes these five scores for each model and computes the min, max, median, first, and third quartiles, in the form of a box plot. </p>
<p class="mce-root"/>
<p>In the preceding plot, the following applies:</p>
<ol>
<li>It is clear that the K-NN model has the highest value of accuracy, with the lowest difference between the minimum and maximum values.</li>
<li>The logistic regression model, on the other hand, has the greatest difference between the minimum and maximum values, and has an outlier in its accuracy score, as well.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance evaluation for regression algorithms</h1>
                </header>
            
            <article>
                
<p>There are three main metrics that you can use to evaluate the performance of the regression algorithm that you built, as follows:</p>
<ul>
<li><strong>Mean absolute error</strong> (<strong>MAE</strong>)</li>
<li><strong>Mean squared error</strong> (<strong>MSE</strong>)</li>
<li><strong>Root mean squared error</strong> (<strong>RMSE</strong>)</li>
</ul>
<p>In this section, you will learn what the three metrics are, how they work, and how you can implement them using scikit-learn. The first step is to build the linear regression algorithm. We can do this by using the following code:</p>
<pre>## Building a simple linear regression model<br/><br/>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Define the feature and target arrays<br/><br/>feature = df['oldbalanceOrg'].values<br/>target = df['amount'].values<br/><br/>#Initializing a linear regression model <br/><br/>linear_reg = linear_model.LinearRegression()<br/><br/>#Reshaping the array since we only have a single feature<br/><br/>feature = feature.reshape(-1, 1)<br/>target = target.reshape(-1, 1)<br/><br/>#Fitting the model on the data<br/><br/>linear_reg.fit(feature, target)<br/><br/>predictions = linear_reg.predict(feature)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean absolute error</h1>
                </header>
            
            <article>
                
<p>The mean absolute error is given by the following formula:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8406d412-d72f-4741-886f-84610b68299e.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">MAE formula</div>
<p>In the preceding formula, <img class="fm-editor-equation" src="assets/6b9d56b7-d0be-4784-bc16-c611b85cc303.png" style="width:1.33em;height:1.33em;"/> represents the true (or actual) value of the output, while the <img class="fm-editor-equation" src="assets/bfcb7fbb-92c2-4a74-b7d8-a723cfe02682.png" style="width:1.33em;height:1.33em;"/> hat represents the predicted output values. Therefore, by computing the summation of the difference between the true value and the predicted value of the output for each row in your data, and then dividing it by the total number of observations, you get the mean value of the absolute error. </p>
<p>In order to implement the MAE in scikit-learn, we use the following code:</p>
<pre>from sklearn import metrics<br/><br/>metrics.mean_absolute_error(target, predictions)</pre>
<p>In the preceding code, the <kbd>mean_absolute_error()</kbd><em> </em>function from the <kbd>metrics</kbd> module in scikit-learn is used to compute the MAE. It takes in two arguments: the real/true output, which is the target, and the predictions, which are the predicted outputs. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean squared error</h1>
                </header>
            
            <article>
                
<p>The mean squared error is given by the following formula:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0c21cb85-d073-4e73-8dc4-bee43b5bc301.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">MSE formula</div>
<p class="mce-root">The preceding formula is similar to the formula that we saw for the mean absolute error, except that instead of computing the absolute difference between the true and predicted output values, we compute the square of the difference. </p>
<p class="mce-root">In order to implement the MSE in scikit-learn, we use the following code:</p>
<pre class="mce-root">metrics.mean_squared_error(target, predictions)</pre>
<p>We use the <kbd>mean_squared_error()</kbd><em> </em>function from the <kbd>metrics</kbd> module, with the real/true output values and the predictions as arguments. The mean squared error is better at detecting larger errors, because we square the errors, instead of depending on only the difference. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Root mean squared error</h1>
                </header>
            
            <article>
                
<p>The root mean squared error is given by the following formula:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1960f41c-0b17-4d04-86de-f8af9e2b6b77.png" style=""/></div>
<p>The preceding formula is very similar to that of the mean squared error, except for the fact that we take the square root of the MSE formula. </p>
<p>In order to compute the RMSE in scikit-learn, we use the following code:</p>
<pre>import numpy as np<br/><br/>np.sqrt(metrics.mean_squared_error(target, predictions))</pre>
<p>In the preceding code, we use the <kbd>mean_squared_error()</kbd><em> </em>function with the true/real output and the predictions, and then we take the square root of this answer by using the <kbd>np.sqrt()</kbd><em> </em>function from the <kbd>numpy</kbd> package. </p>
<p>Compared to the MAE and the MSE, the RMSE is the best possible metric that you can use in order to evaluate the linear regression model, since this detects large errors and gives you the value in terms of the output units. The key takeaway from using any one of the three metrics is that the value that these <kbd>metrics</kbd> gives you should be as low as possible, indicating that the model has relatively low error values. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance evaluation for unsupervised algorithms</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how to evaluate the performance of an unsupervised machine learning algorithm, such as the k-means algorithm. The first step is to build a simple k-means model. We can do so by using the following code:</p>
<pre>#Reading in the dataset<br/><br/>df = pd.read_csv('fraud_prediction.csv')<br/><br/>#Dropping the target feature &amp; the index<br/><br/>df = df.drop(['Unnamed: 0', 'isFraud'], axis = 1)<br/><br/>#Initializing K-means with 2 clusters<br/><br/>k_means = KMeans(n_clusters = 2)<br/><br/></pre>
<p>Now that we have a simple k-means model with two clusters, we can proceed to evaluate the model's performance. The different visual performance charts that can be deployed are as follows:</p>
<ul>
<li>Elbow plot</li>
<li>Silhouette analysis plot</li>
</ul>
<p>In this section, you will learn how to create and interpret each of the preceding plots. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elbow plot</h1>
                </header>
            
            <article>
                
<p>In order to construct an elbow plot, we use the following code:</p>
<pre>skplt.cluster.plot_elbow_curve(k_means, df, cluster_ranges=range(1, 20))<br/>plt.show()</pre>
<p>This results in the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8acf8d3f-2174-46de-a8e1-d9ab51207753.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Elbow plot</div>
<div>The elbow plot is a plot between the number of clusters that the model takes into consideration along the <em>x </em>axis and the sum of the squared errors along the <em>y </em>axis.</div>
<p>In the preceding code, the following applies:</p>
<ul>
<li>We use the <kbd>plot_elbow_curve()</kbd><em> </em>function with the k-means model, the data, and the number of clusters that we want to evaluate</li>
<li>In this case, we define a range of 1 to 19 clusters</li>
</ul>
<p>In the preceding plot, the following applies:</p>
<ul>
<li>It is clear that the elbow point, or the point at which the sum of the squared errors (<em>y </em>axis) starts decreasing very slowly, is where the number of clusters is 4.</li>
<li>The plot also gives you another interesting metric on the <em>y </em>axis (right-hand side), which is the clustering duration (in seconds). This indicates the amount of time it took for the algorithm to create the clusters, in seconds.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned how to evaluate the performances of the three different types of machine learning algorithms: classification, regression, and unsupervised. </p>
<p>For the classification algorithms, you learned how to evaluate the performance of a model by using a series of visual techniques, such as the confusion matrix, normalized confusion matrix, area under the curve, K-S statistic plot, cumulative gains plot, lift curve, calibration plot, learning curve, and cross-validated box plot. </p>
<p>For the regression algorithms, you learned how to evaluate the performance of a model by using three metrics: the mean squared error, mean absolute error, and root mean squared error.</p>
<p>Finally, for the unsupervised machine learning algorithms, you learned how to evaluate the performance of a model by using the elbow plot. </p>
<p><span>Congratulations! You have now made it to the end of your machine learning journey with scikit-learn. You've made your way through eight chapters, which gave you the quickest entry point into the wonderful world of machine learning with one of the world's most popular machine learning frameworks: scikit-learn. </span></p>
<p>In this book, you learned about the following topics:</p>
<ul>
<li>What machine learning is (in a nutshell) and the different types and applications of machine learning</li>
<li>Supervised machine learning algorithms, such as K-NN, logistic regression, Naive Bayes, support vector machines, and linear regression</li>
<li>Unsupervised machine learning algorithms, such as the k-means algorithm</li>
<li>Algorithms that can perform both classification and regression, such as decision trees, random forests, and gradient-boosted trees</li>
</ul>
<p>I hope that you can make the best possible use of the application based on the knowledge that this book has given you, allowing you to solve many real-world problems by using machine learning as your tool!</p>


            </article>

            
        </section>
    </body></html>
- en: Predicting Categories with Naive Bayes and SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about two popular classification machine learning
    algorithms: the Naive Bayes algorithm and the linear support vector machine. The
    Naive Bayes algorithm is a probabilistic model that predicts classes and categories,
    while the linear support vector machine uses a linear decision boundary to predict
    classes and categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical concept behind the Naive Bayes algorithm, explained in mathematical
    terms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Naive Bayes algorithm by using scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the linear support vector machine algorithm works under the hood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphically optimizing the hyperparameters of the linear support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, and Matplotlib ≥ 3.0.0 installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_04.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_04.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2COBMUj](http://bit.ly/2COBMUj)'
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm makes use of the Bayes theorem, in order to classify
    classes and categories. The word **naive** was given to the algorithm because
    the algorithm assumes that all attributes are independent of one another. This
    is not actually possible, as every attribute/feature in a dataset is related to
    another attribute, in one way or another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite being naive, the algorithm does well in actual practice. The formula
    for the Bayes theorem is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b16aa8f-172a-40bb-8fcd-29c05d95d1b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes theorem formula
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split the preceding algorithm into the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**p(h|D)**:This is the probability of a hypothesis taking place, provided that
    we have a dataset. An example of this would be the probability of a fraudulent
    transaction taking place, provided that we had a dataset that consisted of fraudulent
    and non-fraudulent transactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p(D|h)**:This is the probability of having the data, given a hypothesis.
    An example of this would be the probability of having a dataset that contained
    fraudulent transactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p(h)**:This is the probability of a hypothesis taking place, in general.
    An example of this would be a statement that the average probability of fraudulent
    transactions taking place in the mobile industry is 2%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p(D)**:This is the probability of having the data before knowing any hypothesis.
    An example of this would be the probability that a dataset of mobile transactions
    could be found without knowing what we wanted to do with it (for example, predict
    fraudulent mobile transactions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding formula, the *p(D)*can be rewritten in terms of *p(h)* and
    *p(D|h),*as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6afb98e-348a-465e-9789-3cdf54bf90c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at how we can implement this with the method of predicting
    classes, in the case of the mobile transaction example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **p(D&#124;h)** | **p(h)** | **p(D&#124;-h)** | **(1 - p(h))** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.8 | 0.08 | 0.02 | 0.92 |'
  prefs: []
  type: TYPE_TB
- en: Substituting the values in the preceding table into the Bayes theorem formula
    produces a result of 0.77\. This means that the classifier predicts that there
    is a 77% probability that a transaction will be predicted as fraudulent, using
    the data that was given previously.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Naive Bayes algorithm in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have learned how the Naive Bayes algorithm generates predictions,
    we will implement the same classifier using scikit-learn, in order to predict
    whether a particular transaction is fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to import the data, create the feature and target arrays,
    and split the data into training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the Naive Bayes classifier. We can do this by using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the `GaussianNB`module from scikit-learn
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we initialize a Naive Bayes classifier and store it in the variable `nb_classifier`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we fit the classifier to the training data and evaluate its accuracy on
    the test data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Naive Bayes classifier has only one hyperparameter, which is the prior
    probability of the hypothesis, *p(h)***.** However, keep the following in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: The prior probability will not be available to us in most problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if it is, the value is usually fixed as a statistical fact, and therefore,
    hyperparameter optimization is not performed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn about **support vector machines (SVMs),** or,
    to be more specific, **linear support vector machines**. In order to understand
    support vector machines, you will need to know what support vectors are. They
    are illustrated for you in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a99c06e-4d59-437e-8c66-e6c83ccbe883.png)'
  prefs: []
  type: TYPE_IMG
- en: The concept of support vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: The linear support vector machine is a form of linear classifier. A linear decision
    tree boundary is constructed, and the observations on one side of the boundary
    (the circles) belong to one class, while the observations on the other side of
    the boundary (the squares) belong to another class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The support vectors are the observations that have a triangle on them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the observations that are either very close to the linear decision
    boundary or have been incorrectly classified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can define which observations we want to make support vectors by defining
    how close to the decision boundary they should be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is controlled by the hyperparameter known as the **inverse regularization
    strength****.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to understand how the linear support vector machines work, consider
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bbc50eb-d97f-40c3-9b0a-a27f872768ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Concept of max-margins
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: The line between the support vectors and the linear decision boundary is known
    as the **margin**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of the support vector machines is to maximize this margin, so that
    a new data point will be correctly classified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A low value of inverse regularization strength ensures that this margin is as
    big as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the linear support vector machine algorithm in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to implement the linear support vector
    machines in scikit-learn. The first step is to import the data and split it into
    training and testing sets. We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to build the linear support vector machine classifier. We
    can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the `LinearSVC`module from scikit-learn
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we initialize a linear support vector machine object with a random state
    of 50, so that the model produces the same result every time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we fit the model to the training data and evaluate its accuracy on
    the test data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have built the model, we can find and optimize the most ideal value
    for the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization for the linear SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to optimize the hyperparameters for the
    linear support vector machines. In particular, there is one hyperparameter of
    interest: the **inverse regularization strength**.'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore how to optimize this hyperparameter, both graphically and algorithmically.
  prefs: []
  type: TYPE_NORMAL
- en: Graphical hyperparameter optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to optimize the inverse regularization strength, we will plot the
    accuracy scores for the training and testing sets, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we initialize two empty lists, in order to store the accuracy scores
    for both the training and testing datasets
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to create a list of values of the hyperparameter, which, in
    this case, is the inverse regularization strength
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then loop over each value in the hyperparameter list and build a linear support
    vector machine classifier with each inverse regularization strength value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The accuracy scores for the training and testing datasets are then appended
    to the empty lists
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `matplotlib`, we then create a plot between the inverse regularization
    strength (along the *x* axis) and the accuracy scores for both the training and
    test sets (along the *y* axis)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will produce a plot as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f919f6a8-5f0d-4d0d-87f2-6bc652d2ca54.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphical hyperparameter optimization
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that the accuracy score is highest for the training and testing
    sets for an inverse regularization strength of 10^(-2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to pick a value that has a high value of accuracy for both the
    training and testing sets, and not just either one of the datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will help you to prevent both overfitting and underfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization using GridSearchCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to optimize the inverse regularization
    strength using the `GridSearchCV` algorithm. We can do this using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the `GridSearchCV`module from scikit-learn
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to initialize a linear support vector machine model with a
    random state of 50, in order to ensure that we obtain the same results every time
    we build the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then initialize a grid of possible hyperparameter values for the inverse
    regularization strength
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we fit the grid of hyperparameter values to the training set, so that
    we can build multiple linear SVM models with the different values of the inverse
    regularization strength
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `GridSearchCV` algorithm then evaluates the model that produces the fewest
    generalization errors and returns the optimal value of the hyperparameter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It's a good practice to compare and contrast the results of the graphical method
    of hyperparameter optimization with that of `GridSearchCV`, in order to validate
    your results.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the data for performance improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn about how scaling and standardizing the data
    can lead to an improvement in the overall performance of the linear support vector
    machines. The concept of scaling remains the same as in the case of the previous
    chapters, and it will not be discussed here. In order to scale the data, we use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the `StandardScaler` the `Pipeline`modules from scikit-learn,
    in order to build a scaling pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then set up the order of the pipeline, which specifies that we use the `StandardScaler()`function
    first, in order to scale the data and build the linear support vector machine
    on that scaled data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `Pipeline()`function is applied to the order of the pipeline which sets
    up the pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then fit this pipeline to the training data and extract the scaled accuracy
    scores from the test data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter introduced you to two fundamental supervised machine learning
    algorithms: the Naive Bayes algorithm and linear support vector machines. More
    specifically, you learned about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How the Bayes theorem is used to produce a probability, to indicate whether
    a data point belongs to a particular class or category
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Naive Bayes classifier in scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the linear support vector machines work under the hood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the linear support vector machines in scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the inverse regularization strength, both graphically and by using
    the `GridSearchCV` algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to scale your data for a potential improvement in performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next chapter, you will learn about the other type of supervised machine
    learning algorithm, which is used to predict numeric values, rather than classes
    and categories: linear regression!'
  prefs: []
  type: TYPE_NORMAL

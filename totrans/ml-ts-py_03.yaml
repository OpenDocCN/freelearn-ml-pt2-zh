- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Machine Learning for Time-Series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we've talked about time-series, time-series analysis,
    and preprocessing. In this chapter, we'll talk about machine learning for time-series.
    **Machine learning** is the study of algorithms that improve through experience.
    These algorithms or models can make systematic, repeatable, validated decisions
    based on data. This chapter is meant to give an introduction given both the context
    and the technical background to much of what we'll use in the remainder of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: We'll go through different kinds of problems and applications of machine learning
    in time-series, and types of analyses relevant to machine learning and time-series
    analysis. We'll explain the main machine learning problems with time-series, such
    as forecasting, classification, regression, segmentation, and anomaly detection.
    We'll then review the basics of machine learning as relevant to time-series. Then,
    we'll look at the history and current uses of machine learning for time-series.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning with time-series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised, unsupervised, and reinforcement learning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: History of Machine Learning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Error metrics for time-series
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing time-series
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning algorithms for time-series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll start with a general introduction to machine learning with time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning with time-series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I'll give an introduction to applications and the main categories
    of machine learning with time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning approaches for time-series are crucial in domains such as economics,
    medicine, meteorology, demography, and many others. Time-Series datasets are ubiquitous
    and occur in domains as diverse as healthcare, economics, social sciences, Internet-of-Things
    applications, operations management, digital marketing, cloud infrastructure,
    the simulation of robotic systems, and others. These datasets are of immense practical
    importance, as they can be leveraged to forecast and predict the detection of
    anomalies more effectively, thereby supporting decision making.
  prefs: []
  type: TYPE_NORMAL
- en: 'The technical applications within machine learning for time-series abound in
    techniques. A few applications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Curve fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation/clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will examine these technical applications in this book. These different applications
    have different statistical methods and models behind them that can overlap.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go briefly through some of these applications for an overview of what
    to expect in the chapters to come.
  prefs: []
  type: TYPE_NORMAL
- en: '**Curve fitting** is the task of fitting a mathematical function (a curve)
    to a series of points. The mathematical function is defined by parameters, and
    the parameters are adapted to fit the time-series through optimization. Curve
    fitting can be employed as a visual aid on graphs or for inference (extrapolation).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression** is an umbrella term for statistical approaches for finding relationships
    between independent variables (features) and independent variables (targets).
    For instance, we could be predicting the exact temperature based on the release
    of carbon dioxide and methane. If there''s more than one outcome variable, this
    is called multi-target (or multi-output). An example of this could be predicting
    the temperature for different locations at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: When the problem is assigning labels to a time-series (or a part of it), this
    is called **classification**. The main difference to regression is that the prediction
    is categorical rather than continuous. The model that's used for classification
    is often referred to as a classifier. Classification can be binary, when there
    are precisely two classes, or multi-class, when there are more categories. An
    example would be detecting eye movements or epilepsy in EEG signals.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions about the future is called **forecasting**. Forecasting can
    be based only on the time-series values itself or on other variables. The techniques
    can range from curve fitting to extrapolating, from analysis of the current trends
    and variability to complex machine learning techniques. For example, we could
    be forecasting global temperatures based on the data of the last 100 years, or
    we could be forecasting the economic wellbeing of a nation. The antonym of forecasting
    is **backcasting**, where we make predictions about the past. We could be backcasting
    temperatures backward in time from before we have data available.
  prefs: []
  type: TYPE_NORMAL
- en: '**Segmentation**, or **clustering**, is the process of grouping parts of the
    time-series into clusters (or segments) of different regimes, behaviors, or baselines.
    An example would be different activity levels in brain waves.'
  prefs: []
  type: TYPE_NORMAL
- en: Within the context of time-series, **anomaly detection**, also known as **outlier
    detection**, is the task of identifying events that are rare or outside the norm.
    These could be novel, changes of regime, noise, or just exceptions. A rather crude
    example would be an outage of the electricity grid detectable in a sudden drop
    of the voltage. More subtle perhaps, by way of an example, would be an increase
    in the number of calls to a call center within a certain period. In both cases,
    anomaly detection could provide actionable business insights. Techniques in anomaly
    detection can range from simple thresholding or statistics to a set of rules,
    to pattern-based approaches based on the time-series distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, **reinforcement learning** is the practice of learning based on maximizing
    expected rewards from a series of decisions. Reinforcement learning algorithms
    are employed in environments where there's a high level of uncertainty. This could
    mean that the conditions are unstable (high variation) or there's a general lack
    of information. Applications are bidding and pricing algorithms in stock trading
    or general auctions, and control tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive into a bit more detail about what these terms mean.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised, unsupervised, and reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning can be broadly categorized into supervised, unsupervised,
    and reinforcement learning, as this diagram shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![taxonomy.png](img/B17577_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Dividing machine learning into categories'
  prefs: []
  type: TYPE_NORMAL
- en: In **supervised** learning, the features are mapped to outcomes ![](img/B17577_04_001.png)in
    a process called **prediction** (sometimes **inference**).
  prefs: []
  type: TYPE_NORMAL
- en: In the supervised case, parameters are estimated from labeled observations.
    We need to have the outcome available for each observation as the **target** column
    (or columns, in the plural).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the machine learning algorithm finds a mapping from X to Y.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_002.png)'
  prefs: []
  type: TYPE_IMG
- en: The function ![](img/B17577_04_003.png) is just one possible mapping or model
    of the input distribution X to the output distribution Y.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning can be categorized into classification and regression.
    In regression, our targets are continuous, and the goal is to predict the value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_004.png)'
  prefs: []
  type: TYPE_IMG
- en: The target Y can be real-valued, either a single value or a higher dimensionality
    (multioutput).
  prefs: []
  type: TYPE_NORMAL
- en: The labels match the dataset in length, but there can be several labels for
    each observation as well (multi-output).
  prefs: []
  type: TYPE_NORMAL
- en: An example would be the number of products sold in a shop on a specific day
    or the amount of oil coming through a pipeline over the next month. The features
    could include current sales, demand, or day of the week.
  prefs: []
  type: TYPE_NORMAL
- en: In **classification**, the goal is to predict the class of the observation.
    In this case, ![](img/B17577_04_005.png) could be drawn from a categorical distribution,
    a distribution consisting of ordinal values, for example, integers as in ![](img/B17577_04_006.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we want to find a function that would give us probabilities or scores
    for given observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_007.png)'
  prefs: []
  type: TYPE_IMG
- en: In practical terms, regression and classification are very similar problems,
    and often regression and classification algorithms can be applied interchangeably.
    However, understanding the distinction is crucial to dealing with any specific
    problem in an appropriate manner.
  prefs: []
  type: TYPE_NORMAL
- en: In time-series **forecasting**, the historical values are extrapolated into
    the future. The only features are the past values. For example, we could be estimating
    the calls into a call center over the next month based on calls during the last
    2 years. The forecasting task could be univariate, relying on and extrapolating
    a single feature, or multivariate, where multiple features are projected into
    the future.
  prefs: []
  type: TYPE_NORMAL
- en: In **unsupervised** learning, the algorithm's task is to categorize observations
    based on their features. Examples of unsupervised learning are clustering or recommender
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In most of the book, we'll be talking about supervised algorithms, although
    we'll also talk about unsupervised tasks such as change detection and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The mapping function, f, predicts an outcome ![](img/B17577_04_008.png)Each
    function is specified by a set of parameters, and the optimization results in
    a set of parameters that minimizes the mismatch between Y and ![](img/B17577_04_009.png).
    Usually, this is done heuristically.
  prefs: []
  type: TYPE_NORMAL
- en: 'The match (mismatch) between Y and ![](img/B17577_04_010.png) is measured by
    an error function, ![](img/B17577_04_011.png), so the optimization consists of
    estimating parameters ![](img/B17577_04_012.png) for the function f that minimizes the
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_013.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, since the error function is part of the optimization, ![](img/B17577_04_014.png)
    is called the **objective function**.
  prefs: []
  type: TYPE_NORMAL
- en: In **reinforcement learning**, an agent is interacting with the environment
    through actions and gets feedback in the shape of rewards. You can find out more
    about reinforcement learning for time-series in *Chapter 11*, *Reinforcement Learning
    for Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to the situation in supervised learning, no labeled data is available,
    but rather the environment is explored and exploited based on the expectation
    of cumulative rewards.
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine learning**, the study of algorithms that improve with experience,
    can be traced back to the 1960s when statistical methods (discussed in *Chapter
    1*, *Time-Series with Python*) were discovered. Let''s start with a brief history
    of machine learning to give some context. This will provide some more terminology
    and a basic idea of the principal directions in machine learning. We''ll give
    some more detailed context in the appropriate chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: History of machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Biological neural networks were conceptualized as a mathematical model by Warren
    McCulloch and Walter Pitts in 1943 in what was the foundation of artificial neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Frank Rosenblatt developed the so-called **perceptron** in 1958, which is a
    **fully connected feed-forward neural network** in today''s terms. This schematic
    shows a perceptron with two input neurons and a single output neuron (based on
    an image on Wikimedia Commons):'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_SIwGeX/Screenshot
    2021-08-15 at 18.23.06.png](img/B17577_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: The connections to the output neuron *y* have weights *w*[1] and *w*[2]. This
    is a simple linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Another important step was how errors can be propagated backward through the
    network. The basics of **backpropagation** were published by Henry J. Kelley shortly
    afterward (1960) as a mechanism for training these networks.
  prefs: []
  type: TYPE_NORMAL
- en: This research was dealt a severe blow, however, when, in 1969, Marvin Minsky
    and Seymour Papert published the book "*Perceptrons*", which included a simple
    proof that linear functions (as in the 2-layer perceptron) could not model non-linear
    functions. According to the authors, this meant that perceptrons, wouldn't be
    useful or interesting in practice. The fact that perceptrons could have more than
    two layers, parameters of which could be learned via backpropagation, was glossed
    over in the book. Research in artificial neural networks only picked up again
    in the 1980s.
  prefs: []
  type: TYPE_NORMAL
- en: The **nearest neighbor algorithm** was described by Evelyn Fix and Joseph Hodges
    in 1951, and then expanded in 1967 by Thomas Cover and Peter E. Hart. The nearest
    neighbor algorithm can be applied to both classification and regression. It works
    by retrieving the k most similar instances between a new data point, and all known
    instances in the dataset (k is a parameter). In the case of classification, the
    algorithm votes for the most frequent label; in the case of regression, it averages
    the labels.
  prefs: []
  type: TYPE_NORMAL
- en: Another important milestone was the development of the decision tree algorithm.
    The ID3 decision tree algorithm (Iterative Dichotomiser 3) was published by Ross
    Quinlan in a 1979 paper and is the precursor to decision trees used today.
  prefs: []
  type: TYPE_NORMAL
- en: The **CART** algorithm (**Classification And Regression Tree**) was published
    by Leo Breiman in 1984\. The **C4.5** algorithm, a descendant of ID3, came out
    in 1992 (Ross Quinlan) and is regarded today as a landmark in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'What was revolutionary about the decision tree is that it consists of step
    functions that partition the feature space of the data points into pockets that
    have a similar outcome. While many machine learning algorithms struggle when there
    are many interactions to consider, decision trees thrive in these situations.
    The following diagram illustrates what a decision tree looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![../../../../Desktop/Screenshot%202021-04-26%20at%2000.09](img/B17577_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: Each node or split in the tree is a single question based on the value of a
    feature. At each iteration during the tree construction, a statistical function
    called a split criterion is applied to decide on the best feature to query. Typical
    choices for a split criterion are the Gini impurity or information entropy, which
    both minimize the variability of the targets within branches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees, in turn, form the basis of ensemble techniques such as the
    random forest or gradient boosted trees. There are two main ensemble techniques:
    boosting and bagging. **Boosting** was invented by Robert Schapire in 1990, and
    consists of incrementally adding base learners in a cascade. A **base learner**
    (also **weak learner**) is a very simple model that in itself is only weakly correlated
    to the targets. Each time when adding a new base learner to the existing ones,
    the importance (weights) of data points in the training set are rebalanced.'
  prefs: []
  type: TYPE_NORMAL
- en: This means that in each iteration, the algorithm comes to grips with more and
    more samples that it struggles with, leading to higher precision with each new
    addition of a base learner.
  prefs: []
  type: TYPE_NORMAL
- en: This formed the basis for **AdaBoost**, an adaptive boosting algorithm, which
    won its inventors, Yoav Freund and Robert Schapire, the Gödel Prize, a prestigious
    recognition for outstanding papers in the area of theoretical computer science.
  prefs: []
  type: TYPE_NORMAL
- en: 'This illustration (from Wikipedia) shows how each base classifier is trained
    subsequently on different subsets of the dataset, where weights are changed for
    each new training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1024px-Ensemble_Boosting.svg.png](img/B17577_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Boosting'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging** is the basis for the random forest, and was invented in 1994 by
    Leo Breiman. Bagging consists of two parts, the bootstrap and aggregation. **Bootstrapping**
    is sampling with replacements from the training set. A separate model can be trained
    on each sample in isolation. These models together form an ensemble. The predictions
    from the individual models can then be aggregated into a combined decision, for
    example, by taking the mean.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram (source: Wikipedia) shows how a bagged ensemble is trained
    and used for prediction. This is how a **random forest** (Leo Breiman, 2001) learns
    with the decision tree as a base learner.'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://upload.wikimedia.org/wikipedia/commons/b/bd/Bagging_for_Classification_with_descripitons.png](img/B17577_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Bagging'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the main differences between bagging and boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Bagging | Boosting |'
  prefs: []
  type: TYPE_TB
- en: '| Base learners are trained: | Independently (can be learned in parallel) |
    Sequentially |'
  prefs: []
  type: TYPE_TB
- en: '| Weights are: | Left unchanged | Changed after every iteration |'
  prefs: []
  type: TYPE_TB
- en: '| Base learners are weighted: | Equally | According to training performance
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.7: Differences between bagging and boosting'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient boosting** (developed by Friedman and others) is a further extension
    of boosting with Unhyphenate. In gradient boosting, new weak learners are added
    in a fashion that they are maximally correlated with the negative gradient of
    the loss function. These are some popular implementations for gradient boosted
    trees:'
  prefs: []
  type: TYPE_NORMAL
- en: CatBoost (by Andrey Gulin and others at Yandex)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light Gradient Boosting Machine (LightGBM, at Microsoft)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation was rediscovered in 1986 by David Rumelhart, Geoffrey Hinton,
    and Ronald J. Williams. Shortly after, deeper networks were developed that could
    be applied to more interesting problems that attracted attention.
  prefs: []
  type: TYPE_NORMAL
- en: Between 1995 and 1997, Sepp Hochreiter and Jürgen Schmidhuber proposed a recurrent
    neural network architecture, the **long short-term memory** (**LSTM**). For many
    years, LSTMs constituted the state-of-the-art for many applications in voice recognition,
    translation, and more. Today, recurrent neural networks, have been largely replaced
    by transformers or ConvNets, even for sequence modeling tasks. With LSTM's high
    demands on computing resources, some people go as far as regarding the LSTM as
    obsolete given the alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVMs**) were developed in the early 1990s at
    AT&T Bell Laboratories by Vladimir Vapnik and colleagues based on statistical
    learning frameworks described by Vapnik and Chervonenkis. In classification, SVMs
    maximize the distance between the two categories in a projected space. As part
    of the training, a hyperplane, called a support vector, is constructed that separates
    positive and negative examples.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll go through the basics of machine learning modeling
    and scientific practices in model validation.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next section, we'll go through the basics of time-series and machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mostly deals with numerical data that is in tabular form as
    a matrix of size ![](img/B17577_04_015.png). The layout is generally in a way
    that each row ![](img/B17577_04_016.png) represents an observation, and each column
    ![](img/B17577_04_017.png) represents a feature.
  prefs: []
  type: TYPE_NORMAL
- en: In time-series problems, the column related to time doesn't necessarily serve
    as a feature, but rather as an index to slice and order the dataset. Time columns
    can, however, be transformed into features, as we'll see in *Chapter 3*, *Preprocessing
    time-series*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each observation is described by a vector of M features. Although a few machine
    learning algorithms can deal with non-numerical data internally, typically, each
    feature is either numerical or gets converted to numbers before feeding it into
    a machine learning algorithm. An example of a conversion is representing Male
    as 0 and Female as 1\. Put simply, each feature can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_018.png)'
  prefs: []
  type: TYPE_IMG
- en: The machine learning workflow can be separated into three processes, as shown
    in the following diagram. I've added data loading and time-series analysis, which
    informs machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![machine_learning_workflow_cropped.png](img/B17577_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Machine learning workflow'
  prefs: []
  type: TYPE_NORMAL
- en: We first must transform (or preprocess) our data, train or fit a model, and
    then we can apply the trained model to new data. This diagram, very simplistic
    perhaps, puts the focus on the three different stages of the machine learning
    process. Each stage comes with its own challenges and particularities for time-series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: This can also help to think about the data flow from input to transform to training
    to prediction. We should keep in mind the available historical data and its limitations,
    as well as the future data points that are to be used for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss the general principles of cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a well-known saying in machine learning attributed to George Box, whom
    we''ve encountered several times already in this book: "All models are wrong,
    but some are useful."'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms make repeatable decisions and, given the correct
    controls, these decisions can be free from the cognitive biases that underlie
    much of human decision making. The point is to make sure that our model is useful
    by validating performance. In machine learning, the process of testing a model
    on data it hasn't seen in training is called cross-validation (sometimes, **out-of-sample
    testing**).
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that parameters estimated on a dataset of limited size are still
    valid for more data, we must go through a validation that makes sure that the
    quality holds up. For validation, we usually split the dataset into at least two
    parts, the training set and the test set. We estimate parameters on a training
    set, and then run the model on the test set to get an idea of the quality of the
    model on unseen data points. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, in machine learning we would shuffle points randomly before splitting
    between training and test. However, in time-series, we would take older data points
    for training and newer points for testing. For instance, having 1 year of data
    available on the email opening propensities of customers, we would train a model
    on 9 months' worth of data, validate our model on 2 months' worth of data, and
    test the final performance on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The use of validation and test can be seen as a nested process in the sense
    that the test set checks the main testing process that involves the validation
    dataset. Often, the separation of validation and test sets is omitted, so the
    dataset is split only into training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note about terminology: while a **loss function** is part of the optimization
    for training your model, a **metric** is used to evaluate your model. The evaluation
    can be post-hoc, after training, or during training as additional information.
    In this section, we''ll discuss both metrics and loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: It is good practice to start a project with an assessment of how to measure
    performance. We need to choose how to measure performance to translate the business
    problem into a metric or a loss. Some algorithms allow flexibility in the choice
    of objective functions, others don't, but we can measure performance with a different
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll be discussing error and loss metrics for regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Error metrics for time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time-series data is defined as a set of data points containing details about
    different points in time. Generally, time-series data contains data points sampled
    or observed at an equal interval of time.
  prefs: []
  type: TYPE_NORMAL
- en: For the different applications that we discussed earlier, we need to be able
    to quantify the performance of the model, be it a regression, classification,
    or another type of model, and choose a metric that captures the performance we
    want to achieve. Once we have chosen a metric for our model, we can then build
    and train models to improve them. Often, we'd start with a simpler model and then
    try to improve on the performance of this simpler model as a baseline. In the
    end, we want to find the model that is best according to our metric.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll discuss commonly used performance measures and their
    properties. Generally, for an error measure, the smaller the values, the better
    the prediction (or the forecast). In changing the parameters of our model, we
    want to reduce the error.
  prefs: []
  type: TYPE_NORMAL
- en: There's not just a single metric that's apt for the purpose of any arbitrary
    application or dataset. Depending on the dataset, you might have to search and
    try different error metrics and see which one best captures your objective. In
    some circumstances, you might even want to define your own metric.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time-Series regression is the task of identifying patterns and signals in the
    features in relation to the behavior of time-series, for example, how skill improves
    with the time invested in practice.
  prefs: []
  type: TYPE_NORMAL
- en: During training, when your regression model gives a result on the training set,
    we can utilize a metric that compares the model output to the training set values,
    and during validation, we can calculate the same measure to know how good our
    regression predictions line up to the validation set targets. The error metric
    summarizes the difference between the values predicted by your machine learning
    model and the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: 'If ![](img/B17577_04_019.png) is a prediction of the model for time step *t*,
    and the actual target value is *y*[t], intuitively, for a particular point, *t*,
    of our dataset, the **forecast error** (also **prediction error** or **residual**)
    is the difference between the actual values of the target and the values our model
    predicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This compares the actual target Y to the predicted targets ![](img/B17577_04_021.png).
    According to this formula, the error is negative if the prediction is higher than
    the actual target value. The **sum of squares of the residuals** (SS, also **residual
    sum of squares**) ignores the direction of the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_022.png)'
  prefs: []
  type: TYPE_IMG
- en: While both the residual and the squared residual could already be used to measure
    the performance of predictions over a time-series, they are not commonly used
    as a regression metric or loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the most commonly used metric for regression: the **coefficient
    of determination**. This is a relatively simple formula based on a ratio of the
    sum of the squares of the residuals, SS, and the total sum of squares, TSS, a
    measure of the variability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_023.png)'
  prefs: []
  type: TYPE_IMG
- en: In this fraction, the nominator is the sum of the squares of the residuals,
    SS, the unexplained variance.
  prefs: []
  type: TYPE_NORMAL
- en: The denominator is TSS, the total sum of squares. This is defined as ![](img/B17577_04_024.png),
    where ![](img/B17577_04_025.png) is the mean of the series, ![](img/B17577_04_026.png).
    The total sum of squares represents the explained variance of the time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, we are measuring the summed squares of residuals in relation to the
    total variance of the time-series. This fraction, between 0 and 1, where 0 is
    best – no error at all, is inverted by subtracting from 1, so that finally 0 is
    worst and 1 is best.
  prefs: []
  type: TYPE_NORMAL
- en: 'When expanded, this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_027.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B17577_04_028.png) expresses the proportion of the variance in the
    dependent variable that is predictable from the independent variable. As mentioned,
    it is bounded between 0 and 1, where 1 means there''s a perfect relationship,
    and 0 means there''s no relationship at all.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The coefficient of determination, ![](img/B17577_04_029.png), is not an error
    measure since an error measure expresses the distribution of residuals so that
    high is bad and low is good. We could, however, express an error measure, let''s
    call it the **r-error (RE)**, very similar to the above, as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_030.png)'
  prefs: []
  type: TYPE_IMG
- en: This is rarely used in practice. An error measure very similar to RE is the
    **mean relative absolute error** (**MRAE**), which we'll discuss further ahead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naively, we could take the average error, where we just take the mean over
    the forecast error – the mean error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_031.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* is the number of points (or the number of discrete time steps). We
    calculate the error for each point and then take the mean over all these errors.
  prefs: []
  type: TYPE_NORMAL
- en: If the ME is positive, the model systematically underestimates the targets,
    if it's positive, it overestimates the targets on the whole. While this can be
    useful, it's a serious problem for an error metric, however, because the effects
    of positive and negative errors cancel each other out. Therefore, a low ME does
    not mean that predictions are good, rather that the average is close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, most regression models include a constant term that is equal to
    the mean of the target, so this value would be exactly 0\. In conclusion, our
    naïve measure is useless in practical settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve included the ME for discussion of why most measures that are commonly
    used discard the direction of the error and for highlighting the importance of
    the main components of the basic error metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: The residual operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of the ME, the residual operation is the identify function, which
    means the residual doesn't change. More often, the square or absolute functions
    are used. The integration of the errors is often the (arithmetic) mean, but sometimes
    the median; however, it can be a more complex operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, the most popular error metrics are the mean squared error (MSE),
    mean absolute error (MAE), and the root mean squared error (RMSE). These most
    important error metrics are defined in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric Name | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| Mean squared error | ![](img/B17577_04_032.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Mean absolute error | ![](img/B17577_04_033.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Root mean squared error | ![](img/B17577_04_034.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.10: Popular regression metrics'
  prefs: []
  type: TYPE_NORMAL
- en: With the **mean squared error (MSE)**, we calculate the residual for each point,
    then square them, so positive and negative errors don't cancel each other out.
    Then we take the mean over these squared errors. An MSE of 0 indicates perfect
    performance. This can happen with toy datasets that you can play around with for
    fun; however, in practice, this will only happen if you made a mistake in building
    your dataset or in validation, because real life is always more complex than you
    can capture with a model.
  prefs: []
  type: TYPE_NORMAL
- en: The **mean absolute error (MAE)** is very similar to the MSE, only instead of
    squaring the residuals, we take their absolute values. As opposed to the MSE,
    all errors contribute in linear proportion (rather than being squared).
  prefs: []
  type: TYPE_NORMAL
- en: A major difference between taking the absolute versus taking the square is in
    how outliers or extreme values are treated. The square function forces a higher
    weight on values that are very different. With the MSE, the error grows quadratically
    instead of linearly as is the case with the MAE. This means that the MSE punishes
    extreme values much more strongly and, as a result, it is less robust to outliers
    in the dataset than the MAE. The distribution of the errors is a major concern
    in choosing an error metric that's right for the job.
  prefs: []
  type: TYPE_NORMAL
- en: Another common metric is the **root mean squared error (RMSE)**, or **root mean
    square deviation (RMSD),** which, as the name suggests, is the square root of
    the MSE. In that sense, RMSE is a scaled version of the MSE. Which one to take
    between the two is a presentation choice – both of them would lead to the same
    models.
  prefs: []
  type: TYPE_NORMAL
- en: What makes the RMSE interesting as a choice is that it comes in the same units
    and scale as the predicted variable, which makes it more intuitive. Finally, the
    RMSE is equivalent to the standard deviation or the error. This connection between
    standard deviation and the distribution of errors is quite meaningful, and you
    can summarize the error distribution with other measures such as the standard
    error or confidence interval (both of which we've discussed in *Chapter 2*, *Time-Series
    Analysis with Python*).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many more metrics and they all have their purpose. The following
    table sums up a few more popular error metrics in time-series modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric Name | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| Median absolute error | ![](img/B17577_04_035.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Mean absolute percentage error | ![](img/B17577_04_036.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Symmetric mean absolute percentage error | ![](img/B17577_04_037.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Normalized mean squared error | ![](img/B17577_04_038.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.11: More metrics for regression'
  prefs: []
  type: TYPE_NORMAL
- en: The **median absolute error (MdAE)** is similar to the MAE. However, instead
    of the mean operation for integration, a different average, the median, is employed.
    Since the median is unaffected by values at the tails, this measure is even more
    robust than the MAE.
  prefs: []
  type: TYPE_NORMAL
- en: The **mean percentage error (MAPE)** is the mean average error normalized by
    the target. 0 represents a perfect model, and higher than 1 means the model's
    predictions are systematically higher than the targets. The MAPE doesn't have
    an upper bound. Additionally, since it deals with percentages in terms of the
    target (scaling or dividing by the targets), positive and negative residuals are
    treated differently. As a result, if the prediction is bigger than the target,
    the MAPE is higher than for the same error in the other direction. Therefore,
    depending on the sign of the residual, the MAPE is higher or lower!
  prefs: []
  type: TYPE_NORMAL
- en: The common choice for the denominator is the target; however, you can also scale
    by the mean of the prediction and target. This is called the **symmetric mean
    absolute percentage error (SMAPE)**. The SMAPE has not only a lower bound but
    also an upper bound, which makes the percentage much easier to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling can have different benefits as well. If you want to compare models validated
    on different datasets, the measures presented before wouldn't be helpful. The
    split between training, validation, and test sets is randomized, so when you compare
    model performances, any of these measures would confound the effects of dataset
    variance in the validation set and the effect of the model performance itself.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the **normalized mean squared error (NMSE)** is particularly intuitive
    as a presentation choice over the MSE because it scales the model's performance
    with the deviation. The NMSE normalizes the MSE obtained after dividing it by
    the target variance.
  prefs: []
  type: TYPE_NORMAL
- en: There are lots of other error measures. Some error measures compare predictions
    against predictions of a naïve model that returns the average target value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction performance of this naïve model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_039.png)'
  prefs: []
  type: TYPE_IMG
- en: We can normalize prediction errors by dividing the prediction error by this
    naïve prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way, we can define several other measures:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric Name** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| Mean relative absolute error | ![](img/B17577_04_040.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Median relative absolute error | ![](img/B17577_04_041.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Relative root mean squared error | ![](img/B17577_04_042.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.12: Normalized regression metrics'
  prefs: []
  type: TYPE_NORMAL
- en: All these measures should be intuitive if you have the idea of the naïve model
    in mind and you want to compare the performance of the naïve model with the same
    error metric by dividing.
  prefs: []
  type: TYPE_NORMAL
- en: The **mean relative absolute error** (**MRAE**) is very similar to the coefficient
    of determination, with the only difference being that the MRAE takes the averages
    rather than the sums.
  prefs: []
  type: TYPE_NORMAL
- en: Another error is the **root mean squared logarithmic error** (**RMSLE**).
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric Name** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| Root mean squared logarithmic error | ![](img/B17577_04_043.png) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.13: Root mean squared logarithmic error'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the RMSLE, you take the log of the residuals as the basic operation.
    This is to avoid penalizing large differences in the error when both predicted
    and true values are very high values. Because of the inflection point of the logarithm
    at 1, the RMLSE has the unique property that it penalizes the underestimation
    of the actual value more severely than it does for overestimation. This can be
    useful when the distributions of errors don't follow a normal distribution similar
    to the scaling operations that we've discussed in *Chapter 3*, *Preprocessing
    Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: We could extend the number of metrics if we take into account metrics based
    on entropy, such as Theil's Uncertainty. **Theil's U** is a normalized measure
    of the total prediction error. U is between 0 and 1, where 0 means a perfect fit.
    It is based on the concept of conditional entropy, and can also be used as a measure
    of uncertainty or even as a correlation measure in categorical-categorical cases.
  prefs: []
  type: TYPE_NORMAL
- en: As these headers say, the first two concentrate on quantifying the performance
    of models. The last section is useful for distance-based models, which are often
    used as a solid baseline for performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let's switch over to error metrics for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many metrics are specific to more binary classification (where there are exactly
    two classes), although some of them can be extended to the case of multi-class
    classification, where the number of classes is bigger than two.
  prefs: []
  type: TYPE_NORMAL
- en: 'In binary classification, we can contrast the prediction against the actual
    outcome in a **confusion matrix**, where predictions and actual outcomes are cross-tabulated
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | **Actual outcome** |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | false | true |'
  prefs: []
  type: TYPE_TB
- en: '| **Predicted outcome** | false | true negative (TN) | false negative (FN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| true | false positive (FP) | true positive (TP) |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.14: Confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: This is a crucial visualization for classification tasks, and many measures
    are based on summarizing this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two of the most important metrics for classification are precision and recall.
    **Recall** is the ratio of the number of correctly predicted positive instances
    across all positive instances. We can also state this in terms of the confusion
    matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall is also called the **true positive rate** or **sensitivity**. It focuses
    on the true predictions, ignoring the negative instances; however, we might also
    want to know how accurate the positive predictions are. This is **precision**
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can visualize these two metrics as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 12)/Screenshot 2021-04-26 at 00.05.38.png](img/B17577_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: False positives (FP), true positives (TP), and false negatives
    (FN)'
  prefs: []
  type: TYPE_NORMAL
- en: In this graph, **false positives** (**FP**), **true positives** (**TP**), and
    **false negatives** (**FN**) are shown. You can see instances that are actually
    true, instances that are classified as true by the model, and the intersection
    of the two – instances that are true and that the model classifies as true.
  prefs: []
  type: TYPE_NORMAL
- en: We can quickly count and calculate precision and recall. We have four true positives
    and six false positives. The precision for this example is therefore ![](img/B17577_04_046.png).
  prefs: []
  type: TYPE_NORMAL
- en: We have four false negatives. The recall is ![](img/B17577_04_047.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Both recall and precision are obviously important, so why not integrate them?
    The ![](img/B17577_04_048.png) score is the harmonic mean of precision and sensitivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also parametrize the relative importance of recall and precision. This
    is a generalized version of the ![](img/B17577_04_050.png) score, the ![](img/B17577_04_051.png)
    score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_052.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another very useful metric comes from the **receiver operator curve (ROC)**,
    which plots the **true positive rate** (**TPR**) against the **false positive
    rate** (**FPR**) at various threshold settings. The **false positive rate**, also
    called the **false alarm ratio**, is defined analogously to the true positive
    rate (recall) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_053.png)'
  prefs: []
  type: TYPE_IMG
- en: An ROC graph shows the relationship between sensitivity and specificity, with
    the general idea being that it is very hard to do both and find all positive instances
    (sensitivity) and to do them correctly. More often than not, you have to compromise
    between sensitivity and specificity. This plot illustrates how well your model
    maneuvers this issue. The **area under the curve** summarizes the plot and is
    a metric that's often used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another less common metric is the **Correlation Ratio**, which was introduced
    by Karl Pearson as a measure of categorical-continuous association:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_054.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![](img/B17577_04_055.png) is the number of observations in category
    *x*, and we define:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_056.png)'
  prefs: []
  type: TYPE_IMG
- en: The correlation ratio is based on the variance within individual categories
    and the variance across the whole population. ![](img/B17577_04_057.png) is in
    the range [0,1] where 0 means a category is not associated, and 1 means a category
    is associated with absolute certainty.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll examine similarity measures between time-series.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similarity measures have applications in time-series indexing for retrieval
    in search, clustering, forecasting, regression, and classification, but if we
    want to decide whether two temporal sequences are similar, how do we measure the
    similarity?
  prefs: []
  type: TYPE_NORMAL
- en: The simplest would be to use the Pearson correlation coefficient; however, other
    measures can be more informative.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll go through a series of measures to compare a pair of time-series:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic time warping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Granger causality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Euclidean distance**, a generic distance, is applicable to any pair of
    vectors, including time-series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_058.png)'
  prefs: []
  type: TYPE_IMG
- en: The Euclidean distance can be useful; however, in practice, for time-series
    you can do better. You can take the Euclidean distance over the time-series that
    has been transformed by the fast Fourier transformed to a frequency domain.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the exact time position and its duration of events in time-series
    can vary. **Dynamic time warping** (**DTW**) is one of the algorithms for measuring
    similarity between two temporal sequences, which may vary in speed. Intuitively,
    the exact time position and its duration of events in time-series can vary. A
    similarity measure between time-series should be able to deal with these kinds
    of shifts and elongations.
  prefs: []
  type: TYPE_NORMAL
- en: In general, DTW is a method that calculates an optimal match between two given
    time sequences with certain restrictions and rules according to a heuristic. Basically,
    it attempts to match indexes from the first sequence to indexes from the other
    sequence. DTW is an edit distance – it expresses the cost of transforming a sequence
    t1 into t2.
  prefs: []
  type: TYPE_NORMAL
- en: DTW has been applied to automatic speech recognition because of its ability
    to cope with different speeds. DTW, however, fails at quantifying dissimilarity
    between non-matching sequences.
  prefs: []
  type: TYPE_NORMAL
- en: DTW is applied to each feature dimension independently and then the distances
    can be summed up. Alternatively, the warping can cover all features simultaneously
    by calculating the distance between two points as the Euclidean distance across
    all dimensions. Thus, this Dependent Warping (![](img/B17577_04_059.png)) is a
    multivariate approach.
  prefs: []
  type: TYPE_NORMAL
- en: '**Granger causality** determines if a time-series can help to forecast another
    time-series. Although the question of true causality in the measure is debatable,
    the measure considers values of one series prior in time to values of the other,
    and it can be argued that the measure shows a temporal relationship or a relationship
    in the predictive sense.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Granger causality is quite intuitive in both its idea and its formulation.
    Its two principles are (simplified):'
  prefs: []
  type: TYPE_NORMAL
- en: The cause must precede the effect
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The cause has a unique effect on the result
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Therefore, if we can fit a model that shows that X and Y have a relationship
    in which Y systematically follows X, this is taken to mean that X Granger causes
    Y.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms for time-series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important distinction in machine learning for time-series is the one between
    univariate and multivariate, in which algorithms are univariate, which means that
    they can only work with a single feature, or multi-variate, which means that they
    work with many features.
  prefs: []
  type: TYPE_NORMAL
- en: In univariate datasets, each case has a single series and a class label. Earlier
    models (classical modeling) focused on univariate datasets and applications. This
    is also reflected in the availability of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important repositories for time-series datasets, the **UCR**
    (**University of California, Riverside**) archive, which was released first in
    2002, has provided a valuable resource for univariate time-series. It now contains
    about 120 datasets, but is lacking multivariate datasets. Furthermore, the M competitions
    (especially M3, 4, and 5) have a lot of available time-series datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate time-series are datasets that have multiple feature dimensions.
    Many real-life datasets are inherently multivariate – multivariate cases are much
    more frequent in practice than univariate. Examples include human activity recognition,
    diagnoses based on an **electrocardiogram** (**ECG**), **electroencephalogram**
    (**EEG**), **magnetoencephalography** (**MEG**), and systems monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Only recently (Anthony Bagnall and others, 2018) created the **UAE** (**University
    of East Anglia**) archive with 30 multivariate datasets. Another archive for multivariate
    datasets is the MTS archive.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll briefly discuss distance-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Distance-based approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the k-nearest-neighbor approaches (kNN for short), which we mentioned earlier,
    training examples are stored, and then, at inference time when a prediction for
    a new data point is required, the prediction is based on the closest k neighbors.
    This requires a distance measure between examples.
  prefs: []
  type: TYPE_NORMAL
- en: I've introduced two measures for time-series, **Dynamic Time Warping** (**DTW**)
    and Euclidean distances, earlier in this chapter. Many distance-based approaches
    take either of these as a distance measure.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach that has been tried is extracting features from time-series,
    and then storing these extracted features for retrieval with kNN. These features
    include shapelets or **scale-invariant features** (**SIFT**). SIFT features are
    extracted from time-series as shapes surrounding the extremum (Adeline Bailly
    and others, 2015).
  prefs: []
  type: TYPE_NORMAL
- en: We've discussed shapelets and ROCKET in separate sections in *Chapter 3*, *Preprocessing
    Time-Series*, so we'll keep their descriptions brief, but focus on their applications
    in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Shapelets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ve discussed shapelets in *Chapter 3*, *Preprocessing Time-Series*, so
    we''ll keep it brief here. Shapelets for time-series were presented in the research
    paper "*Time-Series Shapelets*: *a novel technique that allows accurate, interpretable
    and fast classification*" (Lexiang Ye and Eamonn Keogh, 2011). The basic idea
    of shapelets is decomposing the time-series into discriminative subsections (called
    **Shapelets**). A few methods have been presented that are based on shapelet features.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Shapelet Transform Classifier** (**STC**; Hills and others, 2014) consists
    of taking the shapelets as feature transformation and then feeding the shapelets
    into a machine learning algorithm. They tested the C4.5 decision tree, Naïve Bayes,
    1NN, SVM, and a rotation forest, but didn't find any significant differences between
    these methods in a classification setting.
  prefs: []
  type: TYPE_NORMAL
- en: The **Generalized random shapelet forest** (**gRFS**; Karlsson and others, 2016)
    follows the idea of the random forest. Each tree is built on a distinct set of
    shapelets of random length, which are extracted from one random dimension for
    each tree. A decision tree is trained on top of these shapelets. These random
    shapelet trees are then integrated as the ensemble model, which is the gRFS.
  prefs: []
  type: TYPE_NORMAL
- en: ROCKET
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've explained ROCKET in *Chapter 3*, *Preprocessing Time-Series*. Each input
    feature gets transformed separately by 10,000 random kernels (this number can
    be changed). In practice, this is a very fast process. These transformed features
    can be fed into a machine learning algorithm. Its inventors, Angus Dempster, François
    Petitjean, and Geoff Webb, recommended a linear model in the original publication
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, a new variant, MINIROCKET, was published that is about 75 times faster
    than ROCKET while maintaining roughly the same accuracy – *MINIROCKET: A Very
    Fast (Almost) Deterministic Transform for Time-Series Classification* (*Angus
    Dempster, Daniel F. Schmidt, and Geoff Webb, 2020*).'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning research, **critical difference (CD) diagrams** are a powerful
    visualization tool for comparing outcomes of multiple algorithms. The average
    ranks indicate how algorithms stack up in relation to each other (a lower rank
    is better). Algorithmic results are compared statistically – a horizontal line
    links algorithms, where the differences between them can't be statistically separated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a critical difference diagram that illustrates the comparative performances
    of MiniRocket with other algorithms (from the MiniRocket repo by Dempster and
    others):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: Mean rank of MiniRocket in terms of accuracy versus other state-of-the-art
    approaches on 109 datasets of the UCR archive'
  prefs: []
  type: TYPE_NORMAL
- en: The numbers show the rank of the algorithms across 109 datasets in the test.
    We can see that MiniRocket is better than Rocket, but worse than TS-CHIEF and
    HIVE-COTE, although the difference between them is not statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss InceptionTime in *Chapter 10*, *Deep Learning for Time-Series*.
    Some of the other methods mentioned will be introduced in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Time-Series Forest and Canonical Interval Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main innovation of the **Time-Series Forest** (**TSF**; by Houtao Deng and
    others, 2013) was the introduction of the entrance gain as a split criterion for
    the tree nodes. They showed that an ensemble classifier based on simple features
    such as mean, deviation, and slope outperforms 1NN classifiers with DTW while
    being computationally efficient (due to parallelism).
  prefs: []
  type: TYPE_NORMAL
- en: The **Proximity Forest (PF)**, introduced by a group of researchers lead by
    Geoff Webb, is a tree ensemble based on the similarity of each time-series to
    a set of reference time-series (distance-based features). They found that PF attains
    a classification performance comparable to BOSS and Shapelet Transforms.
  prefs: []
  type: TYPE_NORMAL
- en: '**TS-CHIEF**, short for **Time-Series Combination of Heterogeneous and Integrated
    Embedding Forest**, comes from the same group (Ahmed Shifaz, Charlotte Pelletier,
    François Petitjean, and Geoff Webb, 2020), and it extends PF with dictionary-based
    (BOSS) and interval-based (RISE) splitters while keeping the original features
    introduced with PF. The authors claim that depending on the dataset size, it can
    run between 900 times and 46,000 times faster than HIVE-COTE.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the **Canonical Interval Forest** (**CIF**; by Matthew Middlehurst,
    James Large, and Anthony Bagnall, 2020) was to extend the TSF with the catch22
    features. It is an ensemble of time-series trees based on the 22 Catch22 features
    and summary statistics extracted from phase-dependent intervals. They also used
    the entrance gain criterion for the trees.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we describe the evolution of symbolic approaches, from
    BOSS to the **Temporal Dictionary Ensemble** (**TDE**).
  prefs: []
  type: TYPE_NORMAL
- en: Symbolic approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Symbolic approaches are methods that transform a numeric time-series to symbols
    from an alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: '**Symbolic Aggregate ApproXimation** (**SAX**) was first published by Eamonn
    Keogh and Jessica Lin in 2002\. It extends **Piecewise Aggregate Approximation**
    (**PAA**), which calculates averages within equal segments of the time-series.
    In SAX, these averages are then quantized (binned), so the alphabet corresponds
    to intervals of the original numerical values. The two important parameters are
    the number of segments in PAA and the number of bins.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot below (from Thach Le Nguyen''s MrSEQL repository on GitHub) illustrates
    how SAX works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![https://github.com/lnthach/Mr-SEQL/raw/master/figs/sax_demo.png](img/B17577_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: SAX'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the segments as a grid along the *x* axis and the bins as a grid
    along the *y* axis. Each segment is then replaced with its mean value. The time-series
    is discretized by replacing it in each segment with the bin ID (letter in the
    plot).
  prefs: []
  type: TYPE_NORMAL
- en: '**Symbolic Fourier Approximation** (**SFA**; Patrick Schäfer and Mikael Högqvist,
    2012) also transforms a time-series to a symbolic representation, but using the
    frequency domain. The dimensionality of the dataset is first reduced by performing
    Discrete Fourier Transformation, low-pass filtered, and then quantized.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Bag of SFA Symbols (BOSS;** Patrick Schäfer, 2015 and 2016) is based on
    histograms of n-grams to form a **bag-of-patterns** (**BoP**) from SFA representations.
    BOSS has been extended as **BOSS in Vector Space** (**BOSS VS**). The BOSS VS
    classifier is one to four orders of magnitude faster than the state of the art
    and significantly more accurate than the 1-NN DTW.
  prefs: []
  type: TYPE_NORMAL
- en: Contract BOSS (cBOSS; Matthew Middlehurst, William Vickers, and Anthony Bagnall,
    2019) speeds up BOSS by introducing a new parameter limiting the number of base
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '**SEQL** (Thach Le Nguyen, Severin Gsponer, and Georgiana Ifrim, 2017) is a
    symbolic sequence learning algorithm that selects the most discriminative subsequences
    for a linear model using greedy gradient descent. This is illustrated here (from
    the MrSEQL GitHub repo):'
  prefs: []
  type: TYPE_NORMAL
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 11)/Screenshot 2021-04-25 at 23.48.05.png](img/B17577_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: SEQL'
  prefs: []
  type: TYPE_NORMAL
- en: The multiple representation sequence learner (**MrSEQL**; Thach Le Nguyen, Severin
    Gsponer, Iulia Ilie, and Georgiana Ifrim, 2019) is extending SEQL by selecting
    transformed features across multiple resolutions and multiple domains.
  prefs: []
  type: TYPE_NORMAL
- en: '**WEASEL+MUSE** (Patrick Schäfer and Ulf Leser, 2017 and 2018) consists of
    two stages. WEASEL stands for **Word Extraction for Time-Series Classification**,
    while MUSE stands for **Multivariate Unsupervised Symbols and Derivatives**. This
    deserves emphasizing – while WEASEL is a univariate method, MUSE extends the method
    for multivariate problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In a first step, WEASEL derives features from windows at multiple lengths from
    the truncated Fourier transform and discretization. This acts in a way similar
    to a low-pass filter, keeping only the first *l* coefficients. These coefficients
    are then discretized into an alphabet of fixed size and counted as **Bag-of-Patterns**
    (**BOP**) in histograms. This is done in isolation for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: In a second step (MUSE), the histogram features are concatenated across dimensions,
    and a statistical test, the χ2 test, is used for filter-based feature selection,
    resulting in a much smaller but more discriminative feature set.
  prefs: []
  type: TYPE_NORMAL
- en: These BOPs are then fed into a logistic regression algorithm for classification.
  prefs: []
  type: TYPE_NORMAL
- en: HIVE-COTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Hierarchical Vote Collective of Transformation-Based Ensembles** (**HIVE-COTE**)
    is the current state of the art in terms of classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Proposed in 2016 and adapted in 2020 (Anthony Bagnall and others, 2020), it''s
    an ensemble method that combines a heterogeneous collection of different methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shapelet Transform Classifier** (**STC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-Series Forest** (**TSF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contractable Bag of Symbolic-Fourier Approximation Symbols** (**CBOSS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Interval Spectral Ensemble** (**RISE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Interval Spectral Ensemble (RISE**) is a tree-based time-series classification
    algorithm, originally introduced as **Random Interval Features** (**RIF**) at
    the same time as HIVE-COTE (Jason Lines, Sarah Taylor, and Anthony Bagnall, 2016).
    At each iteration of RISE, a set of Fourier, autocorrelation, and partial autocorrelation
    features are extracted, and a decision tree is trained. RISE''s runtime complexity
    is quadratic to the series length, which can be a problem, and a new version has
    been released, **c-RISE** (*c* for *contract*), where the algorithm can be stopped earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: The runtime complexity of HIVE-COTE, the quadratic runtime to the length of
    the series, is one of the biggest obstacles to its adoption. STC and another model,
    the **elastic ensemble** (**EE**) were the two slowest base models in the original
    algorithm from 2016\. One of the main differences of the new version (1.0) includes
    dropping EE. They re-implemented STC and BOSS to make them more efficient, and
    they replaced RISE with c-RISE.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these base learners is trained separately. The base learners are weighted
    probabilistically based on a **Cross-Validation** **Accuracy Weighted Probabilistic
    Ensemble** (**CAWPE**) structure (James Large, Jason Lines, and Anthony Bagnall,
    2019).
  prefs: []
  type: TYPE_NORMAL
- en: In publications postdating HIVE-COTE 1.0, the group showed that the ensemble
    is even stronger when replacing the CIF with the TSF (2020) and when replacing
    BOSS with the **Temporal Dictionary Ensemble** (**TDE**, 2021).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the performance and trade-offs of different
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generally, there''s a trade-off between accuracy and prediction times, and
    in these methods, there''s a huge difference in time complexity and model accuracy.
    This chart illustrates this compromise (from Patrick Schäfer''s GitHub repository
    of SFA):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17577_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Machine learning algorithms: query time versus accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: Features could be the result of simple operations or themselves be the outcome
    of machine learning models. We could imagine second-order features as the combination
    of the original features, and third-order features as the combination of second-order
    features, and so on, a potentially large preprocessing pipeline, where features
    are combined and created.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can sum up the different algorithms in this table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Univariate | Multivariate |'
  prefs: []
  type: TYPE_TB
- en: '| Distance-based | DTW, Proximity Forest (PF) | DTW-D |'
  prefs: []
  type: TYPE_TB
- en: '| Dictionary-based/Symbolic | BOSS, CBOSS, S-BOSS, WEASEL, Temporal Dictionary
    Ensemble (TDE), SAX-VSM, BOSS | WEASEL+MUSE |'
  prefs: []
  type: TYPE_TB
- en: '| Shapelets | The Shapelet Transform Classifier (STC), MrSEQL |  |'
  prefs: []
  type: TYPE_TB
- en: '| Interval and Spectral-based | Time-Series Forest (TSF), Random Interval Spectral
    Ensemble (RISE) |  |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | ResNet, FCN, InceptionTime | TapNet |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble | The Hierarchical Vote Collective of Transformation-based Ensembles
    (HIVE-COTE), Time-Series Combination of Heterogeneous and Integrated Embeddings
    Forest (TS-CHIEF) |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.20: Detailed taxonomy of time-series machine learning algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: This classification is far from perfect, but hopefully useful. TDE is both an
    ensemble and a dictionary-based model. HIVE-COTE is based on BOSS features. Furthermore,
    the two featurization methods – Random Convolutional Kernel Transform (ROCKET)
    and Canonical Time-Series Characteristics (Catch22), operate on features individually;
    however, machine learning algorithms that train on and predict based on these
    features as inputs can therefore work in a multivariate setting. The ROCKET features
    together with a linear classifier were indeed found to be highly competitive with
    multivariate approaches. Because of the high dimensionality, the machine learning
    model can potentially take interactions between the original features into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'A review paper that I would highly recommend to readers is "*The great multivariate
    time-series classification bake-off*", by Alejandro Pasos Ruiz, Michael Flynn,
    and Anthony Bagnall (2020). It compares state-of-the-art algorithms (16 of which
    were included in the analysis) on 26 multivariate datasets from the UAE archive.
    The approaches included the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic time warping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MUSE+WEASEL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RISE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CBOSS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TSF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: gRSF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROCKET
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HIVE-COTE 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CIF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The critical difference diagram (as found on [timeseriesclassification.com](http://timeseriesclassification.com))
    shows the rank of the algorithms across 26 datasets in the test. Links between
    algorithms show that the differences between them can''t be statistically separated
    (based on the Wilcoxon rank-sum test):'
  prefs: []
  type: TYPE_NORMAL
- en: '![../../../../Desktop/Screenshot%202021-04-25%20at%2021.24](img/B17577_04_14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 Critical difference diagram of time-series classification algorithms
  prefs: []
  type: TYPE_NORMAL
- en: They found a clique of top-performing classifiers, with ROCKET at the top achieving
    a considerable improvement in at least an order of magnitude less time. ROCKET
    was followed by HIVE-COTE and CIF.
  prefs: []
  type: TYPE_NORMAL
- en: In a study from 2019, Hassan Fawaz and others compared deep learning algorithms
    for time-series across 12 multivariate datasets from the MTSC archive. The fully
    connected convolutional network (FCN) was best, followed by ResNet – on 85 univariate
    datasets from the UCR repository, ResNet beat FCN to the top spot (winning on
    50 out of 85 datasets). In a separate comparison involving just ResNet with some
    of the state-of-the-art non-deep learning methods on both univariate and multivariate
    datasets, they found that ResNet's performance was behind HIVE-COTE, although
    not significantly worse across datasets, while beating other approaches such as
    BOSS and 1NN with DTW (the latter to a statistically significant degree). We'll
    talk more about this paper in *Chapter 10*, *Deep Learning for Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: In another comparison study on multivariate time-series classification on 20
    datasets from the MTSC archive (Bhaskar Dhariyal, Thach Le Nguyen, Severin Gsponer,
    and Georgiana Ifrim, 2020), it was established that ROCKET won on 14 datasets
    and was much better than most deep learning algorithms, while at the same time
    being the fastest method – ROCKET's runtime on the 20 datasets was 34 minutes,
    while the DTW ran for days.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the critical diagram created with Hassan Fawaz''s Python script from
    their results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![../../../../Downloads/Machine-Learning%20for%20Time-Series%20with%20Python/mtsc_cd-di](img/B17577_04_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Critical difference diagram of multivariate time-series classification'
  prefs: []
  type: TYPE_NORMAL
- en: Many different feature sets were tried, the best of which (`9_stat_MulPAA`)
    didn't end up far off.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Great algorithms would be worth much less in practice without software that
    provides them in a way that makes them easy to use and reliable to use in a production
    setting of a company. Alternatively, implementing algorithms from scratch can
    take time, and is not without complications. Therefore, it's a boon that there
    are many reliable, available implementations in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes implementations of supervised algorithms for
    regression and classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | sktime | Pyts |'
  prefs: []
  type: TYPE_TB
- en: '| Autoregressive Integrated Moving Average (ARIMA) | X |  |'
  prefs: []
  type: TYPE_TB
- en: '| DTW | X | X |'
  prefs: []
  type: TYPE_TB
- en: '| BATS | X |  |'
  prefs: []
  type: TYPE_TB
- en: '| MUSE+WEASEL | X | X |'
  prefs: []
  type: TYPE_TB
- en: '| MrSEQL | X |  |'
  prefs: []
  type: TYPE_TB
- en: '| ROCKET | X | X |'
  prefs: []
  type: TYPE_TB
- en: '| BOSS | X | X |'
  prefs: []
  type: TYPE_TB
- en: '| Bag-of-SFA Symbols in Vector Space (BOSSVS) |  | X |'
  prefs: []
  type: TYPE_TB
- en: '| CBOSS | X |  |'
  prefs: []
  type: TYPE_TB
- en: '| SAX-VSM |  | X |'
  prefs: []
  type: TYPE_TB
- en: '| RISE | X |  |'
  prefs: []
  type: TYPE_TB
- en: '| HIVE-COTE | X |  |'
  prefs: []
  type: TYPE_TB
- en: '| Time-Series Forest | X |  |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4.23: Pyts versus SkTime implementations of machine learning algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: It's not an accident that sktime has so many implementations. It is actively
    used in research activity by the group around Anthony Bagnall at the University
    of East Anglia. Pyts is being maintained by Johann Faouzi and Hicham Janati, postdoctoral
    fellows at the Paris Brain Institute and the **Centre de Mathématiques Appliquées**
    (**CMAP**) in Rémy. Johann Faouzi is also behind the tslearn library that implements
    time-series analysis and feature extraction algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: I've omitted deep learning algorithms from the table, which are often implemented
    as part of different libraries. Please note that sktime allows use of the prophet
    forecaster through the same interface. For example, the sktime-DL library implements
    ResNet, InceptionTime, and TapNet algorithms, and dl-4-tsc implements more than
    a dozen deep learning models. We'll come to deep learning model implementations
    in *Chapter 10*, *Deep Learning for Time-Series*.
  prefs: []
  type: TYPE_NORMAL
- en: Facebook's Prophet contains a single model, a special case of the **Generalized
    Additive Model** (**GAM**). The Statsmodels library contains a GAM as well as
    linear regression models and a **Generalized Linear Model** (**GLM**), **moving
    average** (**MA**), **Autoregressive Integrated Moving Average** (**ARIMA**),
    and **Vector Autoregressions** (**VAR**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Darts library provides a consistent interface to several models for time-series
    processing and forecasting. It includes both classical and deep learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Exponential smoothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARIMA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temporal convolutional network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-BEATS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our overview of time-series machine learning libraries in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've talked about both the context and the technical background
    of machine learning with time-series. Machine learning algorithms or models can
    make systematic, repeatable, validated decisions based on data. We explained the
    main machine learning problems with time-series such as forecasting, classification,
    regression, segmentation, and anomaly detection. We then reviewed the basics of
    machine learning as relevant to time-series, and we looked at the history and
    current uses of machine learning for time-series.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed different types of methods based on the approach and features used.
    Furthermore, we discussed many algorithms, concentrating on state-of-the-art machine
    learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: I will discuss approaches including deep learning or classical models, such
    as autoregressive and moving averages, in chapters dedicated to them (for example,
    in *chapter 5*, *Time-Series Forecasting with Moving Averages and Autoregressive
    Models*, and *chapter 10*, *Deep Learning for Time-Series*).
  prefs: []
  type: TYPE_NORMAL

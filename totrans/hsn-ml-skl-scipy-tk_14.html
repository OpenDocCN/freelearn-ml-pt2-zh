<html><head></head><body>
    <section>
      <header class="header-title chapter-title">
                    Clustering – Making Sense of Unlabeled Data
                </header>
      <article>
        <p>Clustering is the poster child of unsupervised learning methods. It is usually our first choice when we need to add meaning to unlabeled data. In an e-commerce website, the marketing team may ask you to put your users into a few buckets so that they can tailor the messages they send to each group of them. If no one has labeled those millions of users for you, then clustering is your only way to put these users into buckets. When dealing with a large number of documents, videos, or web pages, and there are no categories assigned to this content, and you are not willing to ask <em>Marie Kondo</em> for help, then clustering is your only way to declutter this mess.</p>
        <p>Since this is our first chapter about supervised learning algorithms, we will start with some theoretical background about clustering. Then, we will have a look at three commonly used clustering algorithms, in addition to the methods used for evaluating them.</p>
        <p>In this chapter, we are going to cover the following topics:</p>
        <ul>
          <li>Understanding clustering</li>
          <li>K-means clustering</li>
          <li>Agglomerative clustering</li>
          <li>DBSCAN </li>
        </ul>
        <p>Let's get started!</p>
        <h1 id="uuid-8d103a35-3ac6-4a37-ba7a-1ffeaef98372">Understanding clustering</h1>
        <p>Machine learning algorithms can be seen as optimization problems. They take data samples, and an objective function, and try to optimize this function. In the case of supervised learning, the objective function is based on the labels given to it. We try to minimize the differences between our predictions and the actual labels. In the case of unsupervised learning, things are different due to the lack of labels. Clustering algorithms, in essence, try to put the data samples into clusters so that it minimizes the intracluster distances and maximizes the intercluster distances. In other words, we want samples that are in the same cluster to be as similar as possible, and samples from different clusters to be as different as possible.</p>
        <p>Nevertheless, there is one trivial solution to this optimization problem. If we treat each sample as its own cluster, then the intracluster distances are all zeros and the intercluster distances are at their maximum. Obviously, this is not what we want from our clustering algorithm. Thus, to avoid this trivial solution, we usually add a constraint to our optimization function. For example, we may predefine the number of clusters we need to make sure the aforementioned trivial solution is avoided. One other possible constraint involves setting the minimum number of samples per cluster. We will see those constraints in practice when we discuss each of the different clustering algorithms in this chapter. </p>
        <p>The lack of labels also dictates the different metrics for evaluating how good the resulting clusters are. That's why I decided to emphasize the objective function of clustering algorithms here, since understanding the objective of an algorithm makes it easier to understand its evaluation metrics. We will come across a couple of evaluation metrics throughout this chapter.  </p>
        <div class="packt_infobox">One way to measure the intracluster distances is to calculate the distances between each point in the cluster and the cluster's centroid. The concept of the centroid should be familiar to you by now since we discussed the <strong>nearest centroid</strong> algorithm in <a href="b95b628d-5913-477e-8897-989ce2afb974.xhtml">Chapter 5</a>, <em>Image Processing with Nearest Neighbors</em>. The centroid is basically the mean of all the samples in the clusters. Furthermore, the average Euclidean distance between some samples and their mean has another name that we all learned about in primary school – <strong>standard deviation</strong>. The very same distance measure can be used to measure the dissimilarity between the clusters' centroids. </div>
        <p>At this point, we are ready to explore our first algorithm, known as <strong>K-means</strong>. However, we need to create some sample data first so that we can use it to demonstrate our algorithms. In the next section, after explaining the algorithm, we are going to create the needed data and use the K-means algorithm to cluster it.</p>
        <h1 id="uuid-372d1e71-1ef6-40f4-89a5-5ad241499c4b">K-means clustering</h1>
        <div class="packt_quote">"We all know we are unique individuals, but we tend to see others as representatives of groups."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">- Deborah Tannen</div>
        <p>In the previous section, we discussed the constraint we put on our objective function by specifying the number of clusters we need. This is what the <em>K</em> stands for: the number of clusters. We also discussed the cluster's centroid, hence the word means. The algorithm works as follows:</p>
        <ol>
          <li>It starts by picking <em>K</em> random points and setting them as the cluster centroids.</li>
          <li>Then, it assigns each data point to the nearest centroid to it to form <em>K</em> clusters.</li>
          <li>Then, it calculates a new centroid for the newly formed clusters. </li>
          <li>Since the centroids have been updated, we need to go back to <em>step 2</em> to reassign the samples to their new clusters based on the updated centroids. However, if the centroids didn't move much, we know that the algorithm has converged, and we stop.  </li>
        </ol>
        <p>As you can see, this is an iterative algorithm. It keeps iterating until it converges, but we can limit the number of iterations by setting its <kbd>max_iter</kbd>hyperparameter. Additionally, we may decide to tolerate bigger centroid movements and stop earlier by setting the <kbd>tol</kbd><strong><em/></strong>hyperparameter to a larger value. The different choices regarding the initial cluster centroids may lead to different results. Setting the algorithm's<kbd>init</kbd> hyperparameter to<kbd>k-means++</kbd>makes sure the initial centroids are distant from each other. This usually leads to better results than random initialization. The choice of <em>K</em> is also given using the<kbd>n_clusters</kbd> hyperparameter. To demonstrate the usage of this algorithm and its hyperparameters, let's start by creating a sample dataset. </p>
        <h2 id="uuid-c7f40be9-9804-4a0d-ada1-8e3b5125d238">Creating a blob-shaped dataset</h2>
        <p>We usually visualize clusters as rounded blobs of scattered data points. This sort of shape is also known as a convex cluster and is one of the easiest shapes for algorithms to deal with. Later on, we will generate harder-to-cluster datasets, but let's start with the easy blobs for now. </p>
        <p>The <kbd>make_blobs</kbd> function helps us create a blob-shaped dataset. Here, we set the number of samples to <kbd>100</kbd> and divide them into four clusters. Each data point only has two features. This will make it easier for us to visualize the data later on. The clusters have different standard deviations; that is, some clusters are more dispersed than the others. The function also returns labels. We will keep the labels aside to validate our algorithm later on. Finally, we put the <kbd>x</kbd>'s and the <kbd>y</kbd>'s into a DataFrame and call it <kbd>df_blobs</kbd>:</p>
        <pre>from sklearn.datasets import make_blobs<br/><br/>x, y = make_blobs(n_samples=100, centers=4, n_features=2, cluster_std=[1, 1.5, 2, 2], random_state=7)<br/><br/>df_blobs = pd.DataFrame(<br/>    {<br/>        'x1': x[:,0],<br/>        'x2': x[:,1],<br/>        'y': y<br/>    }<br/>)</pre>
        <p>To make sure you get the exact same data I did, set the <kbd>random_state</kbd> parameter of the data generating function to a specific random seed. Now that the data is ready, we need to create a function to visualize this data.</p>
        <h2 id="uuid-b021da31-1661-40ef-ba3f-2ca68ff1b9ec">Visualizing our sample data</h2>
        <p>We are going to use the following function throughout this chapter. It takes the 2D <em>x</em>'s and <em>y</em> labels and plots them into the given Matplotlib axis, <em>ax</em>. In real-life scenarios, no labels are given, but still, we can give this function the labels predicted by the clustering algorithm instead. The resulting plot gets a title, along with the number of clusters that have been deduced from the cardinality of the given <em>y</em>:</p>
        <pre>def plot_2d_clusters(x, y, ax):<br/><br/>    y_uniques = pd.Series(y).unique()<br/><br/>    for y_unique_item in y_uniques:<br/> x[<br/>            y == y_unique_item<br/>        ].plot(<br/>            title=f'{len(y_uniques)} Clusters',<br/>            kind='scatter',<br/>            x='x1', y='x2',<br/>            marker=f'${y_unique_item}$',<br/>            ax=ax,<br/>        )</pre>
        <p>We can use the new <kbd>plot_2d_clusters()</kbd> function as follows:</p>
        <pre>fig, ax = plt.subplots(1, 1, figsize=(10, 6))<br/>x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/>plot_2d_clusters(x, y, ax)</pre>
        <p>This will give us the following diagram:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/46e06437-e455-4edb-9f80-2a5f0e3fa3a8.png" style="width:41.83em;"/>
        </p>
        <p>Each data point is marked according to its given label. Now, we will pretend those labels haven't been given to us and see whether the K-means algorithm will be able to predict them or not.</p>
        <h2 id="uuid-9a0d8fa1-e5ac-4a63-b8af-b20c4666d801">Clustering with K-means</h2>
        <p>Now that we're pretending that no labels have been given, how can we tell what value to use for <em>K</em>, that is, the<kbd>n_clusters</kbd>hyperparameter? We can't. We will just pick any number for now; later on, we will learn how to find the best value for <kbd>n_clusters</kbd>. Let's set it to five for now. We will keep all the other hyperparameters at their default values. Once the algorithm is initialized, we can use its <kbd>fit_predict</kbd> method, as follows:</p>
        <pre>from sklearn.cluster import KMeans<br/>kmeans = KMeans(n_clusters=2, random_state=7)<br/>x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/>y_pred = kmeans.fit_predict(x)</pre>
        <div class="packt_infobox">Note that the concept of fitting on a training set and predicting a test date seldom makes sense here. We usually fit and predict on the same dataset. We also don't pass any labels to the <kbd>fit</kbd> or<kbd>fit_predict</kbd> methods.</div>
        <p>Now that we've predicted the new labels, we can use the <kbd>plot_2d_clusters()</kbd> function to compare our predictions to the original labels, as follows:</p>
        <pre>fig, axs = plt.subplots(1, 2, figsize=(14, 6))<br/><br/>x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/>plot_2d_clusters(x, y, axs[0])<br/>plot_2d_clusters(x, y_pred, axs[1])<br/><br/>axs[0].set_title(f'Actuals: {axs[0].get_title()}')<br/>axs[1].set_title(f'KMeans: {axs[1].get_title()}')</pre>
        <p>I prepended the words <kbd>Actuals</kbd> and <kbd>KMeans</kbd> to their corresponding figure titles. The resulting clusters are shown in the following screenshot:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/5ee215f8-1d25-4616-9a9f-af712af33559.png" style="width:51.92em;"/>
        </p>
        <p>One of the original four clusters has been split into two since we set <em>K</em> to five. Other than that, the predictions for the other clusters make sense. The labels that have been given to the clusters are arbitrary. The original cluster with label one was called three by the algorithm. This should not bother us at all, as long as the clusters have the exact same members. This should not bother the clustering evaluation metrics either. They usually take this fact into account and ignore the label names when evaluating a clustering algorithm. </p>
        <p>Still, how do we determine the value of <em>K</em>? We have no other choice but to run the algorithm multiple times with different numbers of clusters and pick the best one. In the following code snippet, we're looping over three different values for <kbd>n_clusters</kbd>. We also have access to the final centroids, which are calculated for each cluster after the algorithm converges. Seeing these centroids clarifies how the algorithm assigned each data point to its own cluster. The last line in our code snippet uses a triangular marker to plot the centroids in each of the three graphs:</p>
        <pre>from sklearn.cluster import KMeans<br/><br/>n_clusters_options = [2, 4, 6]<br/><br/>fig, axs = plt.subplots(1, len(n_clusters_options), figsize=(16, 6))<br/><br/>for i, n_clusters in enumerate(n_clusters_options):<br/><br/>    x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/><br/>    kmeans = KMeans(n_clusters=n_clusters, random_state=7)<br/>    y_pred = kmeans.fit_predict(x)<br/><br/>    plot_2d_clusters(x, y_pred, axs[i])<br/><br/>    axs[i].plot(<br/>        kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], <br/>        'k^', ms=12, alpha=0.75<br/>    )<br/><br/></pre>
        <p>Here are the results of the three choices, side by side:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/0b987078-8fc4-4e29-aaab-d2ad7f166a07.png" style="width:54.42em;"/>
        </p>
        <p>A visual investigation of the three graphs tells us that the choice of four clusters was the right choice. Nevertheless, we have to remember that we are dealing with 2D data points here. The same visual investigation would have been much harder if our data samples contained more than two features. In the next section, we are going to learn about the silhouette score and use it to pick the optimum number of clusters, without the need for visual aid.</p>
        <h2 id="uuid-2682ced6-8f2a-4091-aac1-5274d907c8d6">The silhouette score</h2>
        <p>The <strong>silhouette score</strong> is a measure of how similar a sample is to its own cluster compared to the samples in the other clusters. For each sample, we will calculate the average distance between this sample and all the other samples in the same cluster. Let's call this mean distance <em>A</em>. Then, we calculate the average distance between the same sample and all the other samples in the nearest cluster. Let's call this other mean distance <em>B</em>. Now, we can define the silhouette score, as follows:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img class="fm-editor-equation" src="assets/31c4243e-df21-4e17-adc4-552606e7e508.png" style="width:9.92em;"/>
        </p>
        <p>Now, rather than performing a visual investigation of the clusters, we are going to loop over multiple values for <kbd>n_clusters</kbd> and store the silhouette score after each iteration. As you can see, <kbd>silhouette_score</kbd> takes two parameters – the data points (<kbd>x</kbd>) and the predicted cluster labels (<kbd>y_pred</kbd>):</p>
        <pre>from sklearn.cluster import KMeans<br/>from sklearn.metrics import silhouette_score<br/><br/>n_clusters_options = [2, 3, 4, 5, 6, 7, 8]<br/>silhouette_scores = []<br/><br/>for i, n_clusters in enumerate(n_clusters_options):<br/><br/>    x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/>    kmeans = KMeans(n_clusters=n_clusters, random_state=7)<br/>    y_pred = kmeans.fit_predict(x)<br/><br/>    silhouette_scores.append(silhouette_score(x, y_pred))</pre>
        <p>We can just pick the <kbd>n_clusters</kbd> value that gives the best score. Here, we put the calculated scores into a DataFrame and use a bar chart to compare them:</p>
        <pre>fig, ax = plt.subplots(1, 1, figsize=(12, 6), sharey=False)<br/><br/>pd.DataFrame(<br/>    {<br/>        'n_clusters': n_clusters_options,<br/>        'silhouette_score': silhouette_scores,<br/>    }<br/>).set_index('n_clusters').plot(<br/>    title='KMeans: Silhouette Score vs # Clusters chosen',<br/>    kind='bar',<br/>    ax=ax<br/>)</pre>
        <p>The resulting scores confirm our initial decision that four is the best choice for the number of clusters:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/6e3a0b1b-2b2c-4241-b818-5c0dc80941ec.png" style="width:48.67em;"/>
        </p>
        <p class="mce-root">In addition to picking the number of clusters, the choice of the algorithm's initial centroid also affects its accuracy. A bad choice may lead the K-means algorithm to converge at an undesirable local minimum. In the next section, we are going to witness how the initial centroids may affect the algorithm's final decision. </p>
        <h2 id="uuid-db55c5ee-c9ec-46f9-85c9-bb332cc95077">Choosing the initial centroids</h2>
        <p class="mce-root">By default, the K-means implementation of scikit-learn picks random initial centroids that are further apart from each other. It also tries multiple initial centroids and picks the one that gives the best results. Having said that, we can also set the initial centroids by hand. In the following code snippet, we will compare two initial settings to see their effect on the final results. We will then print the two outcomes side by side:</p>
        <pre>from sklearn.cluster import KMeans<br/><br/>initial_centroid_options = np.array([<br/>    [(-10,5), (0, 5), (10, 0), (-10, 0)],<br/>    [(0,0), (0.1, 0.1), (0, 0), (0.1, 0.1)],<br/>])<br/><br/>fig, axs = plt.subplots(1, 2, figsize=(16, 6))<br/><br/>for i, initial_centroids in enumerate(initial_centroid_options):<br/><br/>    x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/>    kmeans = KMeans(<br/>       init=initial_centroids, max_iter=500, n_clusters=4, random_state=7<br/>    )<br/>    y_pred = kmeans.fit_predict(x)<br/>    plot_2d_clusters(x, y_pred, axs[i])<br/><br/>    axs[i].plot(<br/>       kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'k^'<br/>    )</pre>
        <p class="mce-root">The following graphs show the resulting clusters after the algorithm converges. Parts of the styling code were omitted for brevity: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/aba5995c-2526-458d-adcf-3f7a3eb00ffa.png" style="width:56.00em;"/>
        </p>
        <p>Clearly, the first initial setup helped the algorithm, while the second one led it to bad results. Thus, we have to be aware of the algorithm's initialization since its results are nondeterministic. </p>
        <div class="packt_tip">In the field of machine learning, the term transfer learning refers to the set of problems where we need to repurpose the knowledge gained while solving one problem and apply it to a slightly different problem. Humans also need transfer learning. The K-means algorithm has a <kbd>fit_transform</kbd> method. If our data (<em>x</em>) is made of <em>N</em> samples and <em>M</em> features, the method will transform it into <em>N</em> samples and <em>K</em> columns instead. The values in the columns are based on the predicted clusters. Usually, <em>K</em> is much smaller than <em>N</em>. Thus, you can repurpose your K-means clustering<em><strong/></em>algorithm so that it can be used as a dimensionality reduction step, before feeding its transformed output to a simple classifier or regressor. Similarly, in a<strong>multi-class</strong> classification problem, a clustering algorithm can be used to reduce the cardinality of the targets.</div>
        <p>In contrast to the K-means algorithm, <strong>agglomerative clustering</strong> is another algorithm whose results are deterministic. It doesn't rely on any initial choices since it approaches the clustering problem from a different angle. Agglomerative clustering is the topic of the next section. </p>
        <h1 id="uuid-72048cb6-1c1b-4407-93a0-417a1bfe1764">Agglomerative clustering</h1>
        <div class="packt_quote">"The most populous city is but an agglomeration of wildernesses."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">- Aldous Huxley</div>
        <p class="mce-root">In the K-means clustering algorithm, we had our <em>K</em> cluster from day one. With each iteration, some samples may change their allegiances and some clusters may change their centroids, but in the end, the clusters are defined from the very beginning. Conversely, in agglomerative clustering, no clusters exist at the beginning. Initially, each sample belongs to its own cluster. We have as many clusters in the beginning as there are data samples. Then, we find the two closest samples and aggregate them into one cluster. After that, we keep iterating by combining the next closest two samples, two clusters, or the next closest sample and a cluster. As you can see, with each iteration, the number of clusters decreases by one until all our samples join a single cluster. Putting all the samples into one cluster sounds unintuitive. Thus, we have the option to stop the algorithm at any iteration, depending on the final number of clusters we need.</p>
        <p class="mce-root">With that, let's learn how to use the agglomerative clustering algorithm. All you have to do for the algorithm to prematurely abort its agglomeration mission is to let it know the final number of clusters we need via its <kbd>n_clusters</kbd>hyperparameter. Obviously, since I mentioned that the algorithm combines the closed clusters, we need to dive into how intercluster distances are being calculated, but let's ignore this for now – we will get to it in a bit. Here is how the algorithm is used when the number of clusters has been set to <kbd>4</kbd>:</p>
        <pre>from sklearn.cluster import AgglomerativeClustering<br/><br/>x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/><br/>agglo = AgglomerativeClustering(n_clusters=4)<br/>y_pred = agglo.fit_predict(x)<br/><br/></pre>
        <p>Since we set the number of clusters to <kbd>4</kbd>, the predicted <kbd>y_pred</kbd> will have values from zero to three. </p>
        <p>In fact, the agglomerative clustering algorithm did not stop when the number of clusters was four. It continued to aggregate the clusters and kept track of which clusters are members of which bigger clusters using an internal tree structure. When we specified that we just needed four clusters, it revisited this internal tree and inferred the clusters' labels accordingly. In the next section, we are going to learn how to access the algorithm's internal hierarchy and trace the tree it builds. </p>
        <h2 id="uuid-1ebba002-7874-4ed7-8970-28d844bae23a">Tracing the agglomerative clustering's children</h2>
        <p>As we mentioned previously, each sample or cluster becomes a member of another cluster, which, in turn, becomes a member of a bigger cluster, and so forth. This hierarchy is stored in the algorithm's <kbd>children_</kbd> attribute. This attribute is in the form of a list of lists. The outer list has as many members as the number of data samples, minus one. Each of the member lists is made up of two numbers. We can list the last five members of the <kbd>children_</kbd> attribute as follows:</p>
        <pre>agglo.children_[-5:]</pre>
        <p>This will give us the following list:  </p>
        <pre>array([[182, 193],
       [188, 192],
       [189, 191],
       [194, 195],
       [196, 197]])</pre>
        <p>The very last element of the list is the root of the tree. It has two children, <kbd>196</kbd> and <kbd>197</kbd>. Those are the IDs of the children of this root node. An ID that is greater than or equal to the number of data samples is a cluster ID, while the smaller IDs refer to individual samples. If you subtract the number of data samples from a cluster ID, it will give you the location in the children list where you can get the members of this cluster. From this information, we can build the following recursive function, which takes a list of children and the number of data samples and returns the nested tree of all the clusters and their members, as follows:</p>
        <pre>def get_children(node, n_samples):<br/>    if node[0] &gt;= n_samples:<br/>        child_cluster_id = node[0] - n_samples<br/>        left = get_children(<br/>            agglo.children_[child_cluster_id], <br/>            n_samples<br/>        )<br/>    else:<br/>        left = node[0]<br/><br/>    if node[1] &gt;= n_samples:<br/>        child_cluster_id = node[1] - n_samples<br/>        right = get_children(<br/>            agglo.children_[child_cluster_id], <br/>            n_samples<br/>        )<br/>    else:<br/>        right = node[1]<br/><br/>    return [left, right]</pre>
        <p>We can call the function we've just created like so:</p>
        <pre>root = agglo.children_[-1]<br/>n_samples = df_blobs.shape[0]<br/>tree = get_children(root, n_samples)</pre>
        <p>At this point, <kbd>tree[0]</kbd> and <kbd>tree[1]</kbd> contain the IDs of the samples in the left-hand side and right-hand side of the tree – these are the members of the two biggest clusters. If our aim is to divide our samples into four clusters instead of two, we can use <kbd>tree[0][0]</kbd>, <kbd>tree[0][1]</kbd>, <kbd>tree[1][0]</kbd>, and <kbd>tree[1][1]</kbd>. Here is what <kbd>tree[0][0]</kbd> looks like:</p>
        <pre>[[[46, [[25, 73], [21, 66]]], [87, 88]],
 [[[22, 64], [4, [49, 98]]],
  [[19, [55, 72]], [[37, 70], [[[47, 82], [13, [39, 92]]], [2, [8, 35]]]]]]]</pre>
        <p>This nestedness allows us to set how deep we want our clusters to be and retrieve their members accordingly. Nevertheless, we can flatten this list using the following code:</p>
        <pre>def flatten(sub_tree, flat_list):<br/>    if type(sub_tree) is not list:<br/>        flat_list.append(sub_tree)<br/>    else:<br/>        r, l = sub_tree<br/>        flatten(r, flat_list)<br/>        flatten(l, flat_list)</pre>
        <p>Now, we can get a member of<kbd>tree[0][0]</kbd>, as follows:</p>
        <pre>flat_list = []<br/>flatten(tree[0][0], flat_list)<br/>print(flat_list)</pre>
        <p>We can also mimic the output of <kbd>fit_predict</kbd> and build our own predicted labels using the following code snippet. It will assign the labels from zero to three to the members of the different branches of the tree we built. Let's call our predicted labels <kbd>y_pred_dash</kbd>:</p>
        <pre>n_samples = x.shape[0]<br/>y_pred_dash = np.zeros(n_samples)<br/>for i, j, label in [(0,0,0), (0,1,1), (1,0,2), (1,1,3)]:<br/>    flat_list = []<br/>    flatten(tree[i][j], flat_list)<br/>    for sample_index in flat_list:<br/>        y_pred_dash[sample_index] = label</pre>
        <p>To make sure our code works as expected, the values in <kbd>y_pred_dash</kbd> should match those in <kbd>y_pred</kbd> from the previous section. Nonetheless, nothing says whether the<kbd>tree[0][0]</kbd>part of the tree should be given the label <kbd>0</kbd>, <kbd>1</kbd>, <kbd>2</kbd>, or <kbd>3</kbd>. Our choice of labels is arbitrary. Therefore, we need a scoring function that compares the two predictions while taking into account that the label names may vary. That's the job of the adjusted Rand index, which is going to be the topic of the next section.</p>
        <h2 id="uuid-fda78ac4-ef62-4bab-93a1-21068da02b1a">The adjusted Rand index</h2>
        <p>The <strong>adjusted Rand index</strong> is very similar to the accuracy score in terms of its classification. It calculates the level of agreement between two lists of labels, yet it accounts for the following issues that the accuracy score cannot deal with:</p>
        <ul>
          <li>The adjusted rand index doesn't care much about the actual labels, as long as the members of one cluster here are the same members of the cluster there.</li>
          <li>Unlike in classification, we may end up having too many clusters. In the extreme case of having each sample as its own cluster, any two lists of clusters will agree with each other if we ignore the names of the labels. Thus, the adjusted rand index discounts the possibility of the two clusters agreeing by chance. </li>
        </ul>
        <p>The best-adjusted rand index<span class="mw-headline"> is</span><kbd>1</kbd><span class="mw-headline"> when the two predictions match. Thus, we can use it to compare</span><kbd>y_pred</kbd> with our <kbd>y_pred_dash</kbd>. The score is symmetric, so the order of its parameters doesn't matter when calling the scoring function, as follows:</p>
        <pre>from sklearn.metrics import adjusted_rand_score<br/>adjusted_rand_score(y_pred, y_pred_dash)</pre>
        <p>Since we get an adjusted rand index of <kbd>1</kbd>, we can rest assured that our code for inferring the cluster memberships from the children tree is correct. </p>
        <p>I quickly mentioned that, in each iteration, the algorithm combines the two closest clusters. It is easy to imagine how distances are calculated between the two samples. They are basically two points and we have already used different distance measures, such as the Euclidean distance and Manhattan distance, before. However, a cluster is not a point. Where exactly should we measure the distances from? Shall we use the cluster's centroid? Shall we pick a specific data point within each cluster to calculate the distance from it? All these choices can be specified using the <strong>linkage</strong> hyperparameter. In the next section, we are going to see its different options. </p>
        <h2 id="uuid-bb74cf1c-8e61-4f91-8d78-e0b7887d5d66">Choosing the cluster linkage </h2>
        <p>By default, the <strong>Euclidean</strong> distance is used to decide which cluster pairs are closest to each other. This default metric can be changed using the <strong>affinity</strong> hyperparameter. Please refer to <a href="b95b628d-5913-477e-8897-989ce2afb974.xhtml">Chapter 5</a><em>, Image Processing with Nearest Neighbors</em>, if you want to know more about the different distance metrics, such as the <strong>c</strong><strong>osine</strong> and<strong>Manhattan</strong> distance. When calculating the distance between two clusters, the <strong>linkage</strong> criterion decides how the distances can be measured, given the fact that a cluster usually contains more than one data point. In a <em>complete</em> linkage, the maximum distance between all the data points in the two clusters is used. Conversely, in a <em>single</em> linkage, the minimum distance is used. Clearly, the <em>average</em> linkage takes the average of all the distances between all sample pairs. In a <em>ward</em> linkage, two clusters are merged if the average Euclidean distances between each data point in the two clusters and the centroid of the merging cluster are at their minimum. Only Euclidean distances can be used with ward linkage. </p>
        <p>To be able to compare the aforementioned linkage methods, we need to create a new dataset. The data points will be arranged in the form of two concentric circles. The smaller circle is enclaved into the bigger one, like Lesotho and South Africa. The <kbd>make_circles</kbd> function specifies the number of samples to generate (<kbd>n_samples</kbd>), how far apart the two circles are (<kbd>factor</kbd>), and how noisy the data is (<kbd>noise</kbd>): </p>
        <pre>from sklearn.datasets import make_circles<br/>x, y = make_circles(n_samples=150, factor=0.5, noise=0.05, random_state=7)<br/>df_circles = pd.DataFrame({'x1': x[:,0], 'x2': x[:,1], 'y': y})</pre>
        <p>I will display the resulting dataset in a bit, but first, let's use the agglomerative algorithm to cluster the new data samples. I will run the algorithm twice: first with a complete linkage and then with a single linkage. I will be using Manhattan distance this time:</p>
        <pre>from sklearn.cluster import AgglomerativeClustering<br/><br/>linkage_options = ['complete', 'single']<br/><br/>fig, axs = plt.subplots(1, len(linkage_options) + 1, figsize=(14, 6))<br/><br/>x, y = df_circles[['x1', 'x2']], df_circles['y']<br/><br/>plot_2d_clusters(x, y, axs[0])<br/>axs[0].set_title(f'{axs[0].get_title()}\nActuals')<br/><br/>for i, linkage in enumerate(linkage_options, 1):<br/><br/>    y_pred = AgglomerativeClustering(<br/>        n_clusters=2, affinity='manhattan', linkage=linkage<br/>    ).fit_predict(x)<br/><br/>    plot_2d_clusters(x, y_pred, axs[i])<br/><br/>    axs[i].set_title(f'{axs[i].get_title()}\nAgglomerative\nLinkage= {linkage}')</pre>
        <p>Here are the results of the two linkage methods side by side:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/c84230e2-77e0-4616-b3a2-40c162c83714.png" style="width:61.58em;"/>
        </p>
        <p>When a single linkage is used, the shortest distance between each cluster pair is considered. This allows it to identify the circular strip where the data points have been arranged. The compete linkage considers the longest distances between the clusters. This resulted in more biased results. Clearly, the single linkage had the best results here. Nevertheless, it is subject to noise due to its variance. To demonstrate this, we can regenerate the circular samples once more after increasing the noise from <kbd>0.05</kbd> to <kbd>0.08</kbd>, as follows:</p>
        <pre>from sklearn.datasets import make_circles<br/>x, y = make_circles(n_samples=150, factor=0.5, noise=0.08, random_state=7)<br/>df_circles = pd.DataFrame({'x1': x[:,0], 'x2': x[:,1], 'y': y})</pre>
        <p>Running the same clustering algorithm on the new samples will give us the following results: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/805f7f0e-cd92-42a4-bb30-501e56fa5dd2.png" style="width:59.25em;"/>
        </p>
        <p>The noisy data confused our single linkage this time, while the outcome of the complete linkage did not vary much. In the single linkage, a noisy point that falls between two clusters may cause them to merge. The average linkage can be seen as a middle ground between the single and the complete linkage criteria. Due to the iterative nature of these algorithms, the three linkage methods cause the bigger clusters to grow even bigger. This may result in uneven cluster sizes. If having imbalanced clusters must be avoided, then the ward linkage should be favored over the other three linkage methods. </p>
        <p>So far, the desired number of clusters had to be predefined for the K-means and agglomerative clustering algorithms. Agglomerative<em><strong/></em>clustering<em><strong/></em>is computationally expensive compared to the K-means algorithm, while the K-means algorithm cannot deal with non-convex data. In the next section, we are going to see a third algorithm that doesn't require the number of clusters to be predefined. </p>
        <h1 id="uuid-bf4eda64-4817-44ba-8c2d-2506aabe07b8">DBSCAN</h1>
        <div class="packt_quote">"You never really understand a person until you consider things from his point of view."              </div>
        <div class="packt_quote CDPAlignRight CDPAlign">- Harper Lee</div>
        <p>The acronym <strong>DBSCAN</strong> stands for <strong>density-based spatial clustering of applications with noise</strong>. It sees clusters as areas of high density separated by areas of low density. This allows it to deal with clusters of any shape. This is in contrast to the K-means algorithm, which assumes clusters to be convex; that is, data blobs with centroids. The DBSCAN<em><strong/></em>algorithm starts by identifying the core samples. These are points that have at least <kbd>min_samples</kbd> around them within a distance of <kbd>eps</kbd> (<em><strong>ε</strong></em>). Initially, a cluster is built out of its core samples. Once a core sample has been identified, its neighbors are also examined and added to the cluster if they meet the core sample criteria. Then, the cluster is expanded so that we can add non-core samples to it. These are samples that can be reached directly from the core samples within a distance of <kbd>eps</kbd> but are not core samples themselves. Once all the clusters have been identified, along with their core and non-core samples, the remaining samples are considered noise. </p>
        <p>It is clear that the <kbd>min_samples</kbd> and <kbd>eps</kbd> hyperparameters play a big role in the final predictions. Here, we're setting <kbd>min_samples</kbd> to <kbd>3</kbd> and trying a different setting for <kbd>eps</kbd><em><strong>:</strong></em></p>
        <pre>from sklearn.cluster import DBSCAN<br/><br/>eps_options = [0.1, 1.0, 2.0, 5.0]<br/><br/>fig, axs = plt.subplots(1, len(eps_options) + 1, figsize=(14, 6))<br/><br/>x, y = df_blobs[['x1', 'x2']], df_blobs['y']<br/><br/>plot_2d_clusters(x, y, axs[0])<br/>axs[0].set_title(f'{axs[0].get_title()}\nActuals')<br/><br/>for i, eps in enumerate(eps_options, 1):<br/><br/>    y_pred = DBSCAN(eps=eps, min_samples=3, metric='euclidean').fit_predict(x)<br/><br/>    plot_2d_clusters(x, y_pred, axs[i])<br/>    axs[i].set_title(f'{axs[i].get_title()}\nDBSCAN\neps = {eps}')</pre>
        <p>The resulting clusters for the blobs dataset help us identify the effect of the <kbd>eps</kbd><em><strong/></em>hyperparameter:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/df4122fc-30e5-4c42-a0f8-bbda069507d7.png" style="width:44.92em;"/>
        </p>
        <p>A very small <kbd>eps</kbd> does not allow any core samples to form. When <kbd>eps</kbd> was set to <kbd>0.1</kbd>, almost all the points were treated as noise. The core points started to form as we increased the value of <kbd>eps</kbd>. However, at some point, when <kbd>eps</kbd><em><strong/></em>was set to <kbd>0.5</kbd>, two clusters were mistakenly merged. </p>
        <p>Similarly, the value of <kbd>min_samples</kbd> can make or break our clustering algorithm. Here, we're going to try different values of <kbd>min_samples</kbd><em><strong/></em>for our concentric data points:</p>
        <pre>from sklearn.cluster import DBSCAN<br/><br/>min_samples_options = [3, 5, 10]<br/><br/>fig, axs = plt.subplots(1, len(min_samples_options) + 1, figsize=(14, 6))<br/><br/>x, y = df_circles[['x1', 'x2']], df_circles['y']<br/><br/>plot_2d_clusters(x, y, axs[0])<br/>axs[0].set_title(f'{axs[0].get_title()}\nActuals')<br/><br/>for i, min_samples in enumerate(min_samples_options, 1):<br/><br/>    y_pred = DBSCAN(<br/>        eps=0.25, min_samples=min_samples, metric='euclidean', n_jobs=-1<br/>    ).fit_predict(x)<br/><br/>    plot_2d_clusters(x, y_pred, axs[i])<br/><br/>    axs[i].set_title(f'{axs[i].get_title()}\nDBSCAN\nmin_samples = {min_samples}')</pre>
        <p>Here, we can see the effect of <kbd>min_samples</kbd> on our clustering results:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/00022b95-11a4-44a4-9c8e-4de20b8a90a9.png" style="width:56.83em;"/>
        </p>
        <p>Once more, a careful choice of <kbd>min_samples</kbd> gave the best results. In contrast to <kbd>eps</kbd>, the bigger the value of <kbd>min_samples</kbd>, the harder it is for the core samples to form. </p>
        <p>In addition to the aforementioned hyperparameters, we can also change the distance metric used by the algorithm. Usually, <kbd>min_samples</kbd> takes values above three. Setting <kbd>min_samples</kbd> to one means that each sample will be its own cluster, while setting it to two will give similar results to the agglomerative clustering algorithm, but with a single linkage. You may start by setting the <kbd>min_samples</kbd> value to double the dimensionality of your data; that is, twice the number of features. Then, you may increase it if your data is known to be noisy and decrease it otherwise. As for <kbd>eps</kbd>, we can use the following <strong>k-distance graph</strong>.</p>
        <p>In the concentric dataset, we set <kbd>min_samples</kbd> to three. Now, for each sample, we want to see how far its two neighbors are. The following code snippet calculates the distance between each point and its closest two neighbors:</p>
        <pre>from sklearn.neighbors import NearestNeighbors<br/><br/>x = df_circles[['x1', 'x2']]<br/>distances, _ = NearestNeighbors(n_neighbors=2).fit(x).kneighbors()</pre>
        <p>If <kbd>min_samples</kbd> was set to any other number, we would have wanted to get as many neighbors as that number, minus one. Now, we can focus on the farthest neighbor of the two for each sample and plot all the resulting distances, as follows:</p>
        <pre>pd.Series(distances[:,-1]).sort_values().reset_index(drop=True).plot() </pre>
        <p>The resulting graph will look as follows:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/6264955d-c1b2-4dbc-b366-674508dc0573.png" style="width:56.67em;"/>
        </p>
        <p>The point where the graph changes its slope dramatically gives us a rough estimate for our <kbd>eps</kbd> value. Here, when<kbd>min_samples</kbd> was set to three, an <kbd>eps</kbd> value of <kbd>0.2</kbd> sounded quite right. Furthermore, we can try different values for these two numbers and use the silhouette score or any other clustering metric to fine-tune our hyperparameters. </p>
        <h1 id="uuid-dcbca4c6-8824-4e87-87c5-fdcef7cb5123">Summary</h1>
        <p>The British historian Arnold Toynbee once said, "<em>n</em><em>o tool is omnicompetent"</em>. In this chapter, we used three tools for clustering. Each of the three algorithms we discussed here approaches the problem from a different angle. The K-means clustering algorithm tries to find points that summarize the clusters and the centroids and builds its clusters around them. The agglomerative clustering approach is more of a bottom-up approach, while the DBSCAN clustering algorithm introduces new concepts such as core points and density. This chapter is the first of three chapters to deal with unsupervised learning problems. The lack of labels here forced us to learn about newer evaluation metrics, such as the adjusted rand index and the silhouette score. </p>
        <p>In the next chapter, we are going to deal with our second unsupervised learning problem: <strong>anomaly detection</strong>. Luckily, the concepts discussed here, as well as the ones from<a href="b95b628d-5913-477e-8897-989ce2afb974.xhtml">Chapter 5</a>, <em>Image Processing with Nearest Neighbors,</em>about nearest neighbors and nearest centroid algorithms will help us in the next chapter. Once more, we will be given unlabeled data samples, and we will be tasked with picking the odd samples out.</p>
      </article>
    </section>
  </body></html>
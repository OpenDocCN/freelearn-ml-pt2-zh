<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 3. Clustering – Finding Related Posts"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Clustering – Finding Related Posts</h1></div></div></div><p>In the previous chapter, you learned how to find the classes or categories of individual datapoints. With a handful of training data items that were paired with their respective classes, you learned a model, which we can now use to classify future data items. We called this supervised learning because the learning was guided by a teacher; in our case, the teacher had the form of correct classifications.</p><p>Let's now imagine that we do not possess those labels by which we can learn the classification model. This could be, for example, because they were too expensive to collect. Just imagine the cost if the only way to obtain millions of labels will be to ask humans to classify those manually. What could we have done in that case?</p><p>Well, of course, we will not be able to learn a classification model. Still, we could find some pattern within the data itself. That is, let the data describe itself. This is what we will do in this chapter, where we consider the challenge of a question and answer website. When a user is browsing our site, perhaps because they were searching for particular information, the search engine will most likely point them to a specific answer. If the presented answers are not what they were looking for, the website should present (at least) the related answers so that they can quickly see what other answers are available and hopefully stay on our site.</p><p>The naïve approach will be to simply take the post, calculate its similarity to all other posts and display the top <span class="emphasis"><em>n</em></span> most similar posts as links on the page. Quickly, this will become very costly. Instead, we need a method that quickly finds all the related posts.</p><p>We will achieve this goal in this chapter using clustering. This is a method of arranging items so that similar items are in one cluster and dissimilar items are in distinct ones. The tricky thing that we have to tackle first is how to turn text into something on which we can calculate similarity. With such a similarity measurement, we will then proceed to investigate how we can leverage that to quickly arrive at a cluster that contains similar posts. Once there, we will only have to check out those documents that also belong to that cluster. To achieve this, we will introduce you to the marvelous <a id="id147" class="indexterm"/>SciKit library, which comes with diverse machine learning methods that we will also use in the following chapters.</p><div class="section" title="Measuring the relatedness of posts"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Measuring the relatedness of posts</h1></div></div></div><p>From the machine learning point of view, raw text is useless. Only if we manage to transform it into meaningful numbers, can we then feed it into our machine learning algorithms, such as <a id="id148" class="indexterm"/>clustering. This is true for more mundane operations on text such as similarity measurement.</p><div class="section" title="How not to do it"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec21"/>How not to do it</h2></div></div></div><p>One text similarity measure is the Levenshtein distance, which also goes by the name Edit Distance. Let's say we have two words, "machine" and "mchiene". The similarity between them can be expressed as the minimum set of edits that are necessary to turn one word into the other. In this case, the edit distance will be 2, as we have to add an "a" after the "m" and delete the first "e". This algorithm is, however, quite costly as it is bound by the length of the first word times the length of the second word.</p><p>Looking at our posts, we could cheat by treating whole words as characters and performing the edit distance calculation on the word level. Let's say we have two posts (let's concentrate on the following title, for simplicity's sake) called "How to format my hard disk" and "Hard disk format problems", we will need an edit distance of 5 because of removing "how", "to", "format", "my" and then adding "format" and "problems" in the end. Thus, one could express the difference between two posts as the number of words that have to be added or deleted so that one text morphs into the other. Although we could speed up the overall approach quite a bit, the time complexity remains the same.</p><p>But even if it would have been fast enough, there is another problem. In the earlier post, the word "format" accounts for an edit distance of 2, due to deleting it first, then adding it. So, our distance seems to be not robust enough to take word reordering into account.</p></div><div class="section" title="How to do it"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"/>How to do it</h2></div></div></div><p>More robust than edit distance is the so-called <a id="id149" class="indexterm"/>
<span class="strong"><strong>bag of word</strong></span> approach. It totally ignores the order of words and simply uses word counts as their basis. For each word in the post, its occurrence is counted and noted in a vector. Not surprisingly, this step is also called vectorization. The vector is typically huge as it contains as many elements as words occur in the whole dataset. Take, for instance, two example posts with the following word counts:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Word</p>
</th><th style="text-align: left" valign="bottom">
<p>Occurrences in post 1</p>
</th><th style="text-align: left" valign="bottom">
<p>Occurrences in post 2</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>disk</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>format</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>how</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>hard</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>my</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>problems</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>to</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr></tbody></table></div><p>The columns Occurrences in post 1 and Occurrences in post 2 can now be treated as simple vectors. We <a id="id150" class="indexterm"/>can simply calculate the Euclidean distance between the vectors of all posts and take the nearest one (too slow, as we have found out earlier). And as such, we can use them later as our feature vectors in the clustering steps according to the following procedure:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Extract salient features from each post and store it as a vector per post.</li><li class="listitem">Then compute clustering on the vectors.</li><li class="listitem">Determine the cluster for the post in question.</li><li class="listitem">From this cluster, fetch a handful of posts having a different similarity to the post in question. This will increase diversity.</li></ol></div><p>But there is some more work to be done before we get there. Before we can do that work, we need some data to work on.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Preprocessing – similarity measured as a similar number of common words"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Preprocessing – similarity measured as a similar number of common words</h1></div></div></div><p>As we have seen <a id="id151" class="indexterm"/>earlier, the bag of word approach is both fast and robust. It is, though, not without challenges. Let's dive directly into them.</p><div class="section" title="Converting raw text into a bag of words"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Converting raw text into a bag of words</h2></div></div></div><p>We do<a id="id152" class="indexterm"/> not have to write custom code for counting words and representing those counts as a vector. SciKit's <code class="literal">CountVectorizer</code> method does the job not only efficiently but also has a very convenient interface. SciKit's functions and classes are imported via the <code class="literal">sklearn</code> package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; vectorizer = CountVectorizer(min_df=1)</strong></span>
</pre></div><p>The <code class="literal">min_df</code> parameter determines how <code class="literal">CountVectorizer</code> treats seldom words (minimum document frequency). If it is set to an integer, all words occurring less than that value will be dropped. If it <a id="id153" class="indexterm"/>is a fraction, all words that occur in less than that fraction of the overall dataset will be dropped. The <code class="literal">max_df</code> parameter works in a similar manner. If we print the instance, we see what other parameters SciKit provides together with their default values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(vectorizer)CountVectorizer(analyzer='word', binary=False, charset=None,</strong></span>
<span class="strong"><strong>        charset_error=None, decode_error='strict',</strong></span>
<span class="strong"><strong>        dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',</strong></span>
<span class="strong"><strong>        lowercase=True, max_df=1.0, max_features=None, min_df=1,</strong></span>
<span class="strong"><strong>        ngram_range=(1, 1), preprocessor=None, stop_words=None,</strong></span>
<span class="strong"><strong>        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',</strong></span>
<span class="strong"><strong>        tokenizer=None, vocabulary=None)</strong></span>
</pre></div><p>We see that, as expected, the counting is done at word level (<code class="literal">analyzer=word</code>) and words are determined by the regular expression pattern <code class="literal">token_pattern</code>. It will, for example, tokenize "cross-validated" into "cross" and "validated". Let's ignore the other parameters for now and consider the following two example subject lines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; content = ["How to format my hard disk", " Hard disk format problems "]</strong></span>
</pre></div><p>We can now put this list of subject lines into the <code class="literal">fit_transform()</code> function of our vectorizer, which does all the hard vectorization work.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; X = vectorizer.fit_transform(content)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; vectorizer.get_feature_names()[u'disk', u'format', u'hard', u'how', u'my', u'problems', u'to']</strong></span>
</pre></div><p>The vectorizer has detected seven words for which we can fetch the counts individually:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(X.toarray().transpose())</strong></span>
<span class="strong"><strong>[[1 1]</strong></span>
<span class="strong"><strong> [1 1]</strong></span>
<span class="strong"><strong> [1 1]</strong></span>
<span class="strong"><strong> [1 0]</strong></span>
<span class="strong"><strong> [1 0]</strong></span>
<span class="strong"><strong> [0 1]</strong></span>
<span class="strong"><strong> [1 0]]</strong></span>
</pre></div><p>This means that the first sentence contains all the words except "problems", while the second contains all but "how", "my", and "to". In fact, these are exactly the same columns as we have seen in the preceding table. From <code class="literal">X</code>, we can extract a feature vector that we will use to compare two documents with each other.</p><p>We will start with a naïve approach first, to point out some preprocessing peculiarities we have to account for. So<a id="id154" class="indexterm"/> let's pick a random post, for which we then create the count vector. We will then compare its distance to all the count vectors and fetch the post with the smallest one.</p><div class="section" title="Counting words"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec11"/>Counting words</h3></div></div></div><p>Let's play <a id="id155" class="indexterm"/>with the toy dataset consisting of the following posts:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Post filename</p>
</th><th style="text-align: left" valign="bottom">
<p>Post content</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">01.txt</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is a toy post about machine learning. Actually, it contains not much interesting stuff.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">02.txt</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Imaging databases can get huge.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">03.txt</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Most imaging databases save images permanently.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">04.txt</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Imaging databases store images.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">05.txt</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Imaging databases store images. Imaging databases store images. Imaging databases store images.</p>
</td></tr></tbody></table></div><p>In this post dataset, we want to find the most similar post for the short post "imaging databases".</p><p>Assuming that the posts are located in the directory <code class="literal">DIR</code>, we can feed <code class="literal">CountVectorizer</code> with it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; vectorizer = CountVectorizer(min_df=1)</strong></span>
</pre></div><p>We have to notify the vectorizer about the full dataset so that it knows upfront what words are to be expected:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; X_train = vectorizer.fit_transform(posts)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; num_samples, num_features = X_train.shape</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print("#samples: %d, #features: %d" % (num_samples, num_features))</strong></span>
<span class="strong"><strong>#samples: 5, #features: 25</strong></span>
</pre></div><p>Unsurprisingly, we have five posts with a total of 25 different words. The following words that have been tokenized will be counted:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(vectorizer.get_feature_names())
[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'this', u'toy']</strong></span>
</pre></div><p>Now we can vectorize our new post.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; new_post = "imaging databases"</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; new_post_vec = vectorizer.transform([new_post])</strong></span>
</pre></div><p>Note that the <a id="id156" class="indexterm"/>count vectors returned by the <code class="literal">transform</code> method are sparse. That is, each vector does not store one count value for each word, as most of those counts will be zero (the post does not contain the word). Instead, it uses the more memory-efficient implementation <code class="literal">coo_matrix</code> (for "COOrdinate"). Our new post, for instance, actually contains only two elements:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(new_post_vec)</strong></span>
<span class="strong"><strong>  (0, 7)  1</strong></span>
<span class="strong"><strong>  (0, 5)  1</strong></span>
</pre></div><p>Via its <code class="literal">toarray()</code> member, we can once again access the full <code class="literal">ndarray</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(new_post_vec.toarray())</strong></span>
<span class="strong"><strong>[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</strong></span>
</pre></div><p>We need to use the full array, if we want to use it as a vector for similarity calculations. For the similarity measurement (the naïve one), we calculate the Euclidean distance between the count vectors of the new post and all the old posts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import scipy as sp</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; def dist_raw(v1, v2):</strong></span>
<span class="strong"><strong>...     delta = v1-v2</strong></span>
<span class="strong"><strong>...     return sp.linalg.norm(delta.toarray())</strong></span>
</pre></div><p>The <code class="literal">norm()</code> function calculates the Euclidean norm (shortest distance). This is just one obvious first pick and there are many more interesting ways to calculate the distance. Just take a look at the paper <span class="emphasis"><em>Distance Coefficients between Two Lists or Sets</em></span> in The Python Papers Source Codes, in which Maurice Ling nicely presents 35 different ones.</p><p>With <code class="literal">dist_raw</code>, we just need to iterate over all the posts and remember the nearest one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import sys</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; best_doc = None</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; best_dist = sys.maxint</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; best_i = None</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for i, post in enumerate(num_samples):</strong></span>
<span class="strong"><strong>...     if post == new_post:</strong></span>
<span class="strong"><strong>...         continue</strong></span>
<span class="strong"><strong>...     post_vec = X_train.getrow(i)</strong></span>
<span class="strong"><strong>...     d = dist_raw(post_vec, new_post_vec)</strong></span>
<span class="strong"><strong>...     print("=== Post %i with dist=%.2f: %s"%(i, d, post))</strong></span>
<span class="strong"><strong>...     if d&lt;best_dist:</strong></span>
<span class="strong"><strong>...         best_dist = d</strong></span>
<span class="strong"><strong>...         best_i = i</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print("Best post is %i with dist=%.2f"%(best_i, best_dist))</strong></span>

<span class="strong"><strong>=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.</strong></span>
<span class="strong"><strong>=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.</strong></span>
<span class="strong"><strong>=== Post 2 with dist=2.00: Most imaging databases save images permanently.</strong></span>
<span class="strong"><strong>=== Post 3 with dist=1.41: Imaging databases store data.</strong></span>
<span class="strong"><strong>=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.</strong></span>
<span class="strong"><strong>Best post is 3 with dist=1.41</strong></span>
</pre></div><p>Congratulations, we<a id="id157" class="indexterm"/> have our first similarity measurement. Post 0 is most dissimilar from our new post. Quite understandably, it does not have a single word in common with the new post. We can also understand that Post 1 is very similar to the new post, but not the winner, as it contains one word more than Post 3, which is not contained in the new post.</p><p>Looking at Post 3 and Post 4, however, the picture is not so clear any more. Post 4 is the same as Post 3 duplicated three times. So, it should also be of the same similarity to the new post as Post 3.</p><p>Printing the corresponding feature vectors explains why:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(X_train.getrow(3).toarray())</strong></span>
<span class="strong"><strong>[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(X_train.getrow(4).toarray())</strong></span>
<span class="strong"><strong>[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]</strong></span>
</pre></div><p>Obviously, using only the counts of the raw words is too simple. We will have to normalize them to get vectors of unit length.</p></div><div class="section" title="Normalizing word count vectors"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec12"/>Normalizing word count vectors</h3></div></div></div><p>We will <a id="id158" class="indexterm"/>have to extend <code class="literal">dist_raw</code> to calculate the vector distance not on the raw vectors but on the normalized instead:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def dist_norm(v1, v2):</strong></span>
<span class="strong"><strong>...    v1_normalized = v1/sp.linalg.norm(v1.toarray())</strong></span>
<span class="strong"><strong>...    v2_normalized = v2/sp.linalg.norm(v2.toarray())</strong></span>
<span class="strong"><strong>...    delta = v1_normalized - v2_normalized</strong></span>
<span class="strong"><strong>...    return sp.linalg.norm(delta.toarray())</strong></span>
</pre></div><p>This leads to the following similarity measurement:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.</strong></span>
<span class="strong"><strong>=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.</strong></span>
<span class="strong"><strong>=== Post 2 with dist=0.92: Most imaging databases save images permanently.</strong></span>
<span class="strong"><strong>=== Post 3 with dist=0.77: Imaging databases store data.</strong></span>
<span class="strong"><strong>=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.</strong></span>
<span class="strong"><strong>Best post is 3 with dist=0.77</strong></span>
</pre></div><p>This looks a <a id="id159" class="indexterm"/>bit better now. Post 3 and Post 4 are calculated as being equally similar. One could argue whether that much repetition would be a delight to the reader, but from the point of counting the words in the posts this seems to be right.</p></div><div class="section" title="Removing less important words"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec13"/>Removing less important words</h3></div></div></div><p>Let's have<a id="id160" class="indexterm"/> another look at Post 2. Of its words that are not in the new post, we have "most", "save", "images", and "permanently". They are actually quite different in the overall importance to the post. Words such as "most" appear very often in all sorts of different contexts and are called stop words. They do not carry as much information and thus should not be weighed as much as words such as "images", which doesn't occur often in different contexts. The best option would be to remove all the words that are so frequent that they do not help to distinguish between different texts. These words are called stop words.</p><p>As this is such a common step in text processing, there is a simple parameter in <code class="literal">CountVectorizer</code> to achieve that:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; vectorizer = CountVectorizer(min_df=1, stop_words='english')</strong></span>
</pre></div><p>If you have a clear picture of what kind of stop words you would want to remove, you can also pass a list of them. Setting <code class="literal">stop_words</code> to <code class="literal">english</code> will use a set of 318 English stop words. To find out which ones, you can use <code class="literal">get_stop_words()</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; sorted(vectorizer.get_stop_words())[0:20]</strong></span>
<span class="strong"><strong>['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']</strong></span>
</pre></div><p>The new word list is seven words lighter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'learning', u'machine', u'permanently', u'post', u'provide', u'save', u'storage', u'store', u'stuff', u'toy']</strong></span>
</pre></div><p>Without stop words, we arrive at the following similarity measurement:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.</strong></span>
<span class="strong"><strong>=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.</strong></span>
<span class="strong"><strong>=== Post 2 with dist=0.86: Most imaging databases save images permanently.</strong></span>
<span class="strong"><strong>=== Post 3 with dist=0.77: Imaging databases store data.</strong></span>
<span class="strong"><strong>=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.</strong></span>
<span class="strong"><strong>Best post is 3 with dist=0.77</strong></span>
</pre></div><p>Post 2 is now<a id="id161" class="indexterm"/> on par with Post 1. It has, however, changed not much overall since our posts are kept short for demonstration purposes. It will become vital when we look at real-world data.</p></div><div class="section" title="Stemming"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec14"/>Stemming</h3></div></div></div><p>One <a id="id162" class="indexterm"/>thing is<a id="id163" class="indexterm"/> still missing. We count similar words in different variants as different words. Post 2, for instance, contains "imaging" and "images". It will make sense to count them together. After all, it is the same concept they are referring to.</p><p>We need a function that reduces words to their specific word stem. SciKit does not contain a stemmer by default. With the <a id="id164" class="indexterm"/>
<span class="strong"><strong>Natural Language Toolkit</strong></span> (<span class="strong"><strong>NLTK</strong></span>), we can download a free software toolkit, which provides a stemmer that we can easily plug into <code class="literal">CountVectorizer</code>.</p><div class="section" title="Installing and using NLTK"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec01"/>Installing and using NLTK</h4></div></div></div><p>How to<a id="id165" class="indexterm"/> install NLTK on your operating <a id="id166" class="indexterm"/>system is described in detail at <a class="ulink" href="http://nltk.org/install.html">http://nltk.org/install.html</a>. Unfortunately, it is not yet officially supported for Python 3, which means that also pip install will not work. We can, however, download the package from <a class="ulink" href="http://www.nltk.org/nltk3-alpha/">http://www.nltk.org/nltk3-alpha/</a> and install it manually after uncompressing using Python's <code class="literal">setup.py</code> install.</p><p>To check whether your installation was successful, open a Python interpreter and type:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import nltk</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>You will find a very nice tutorial to NLTK in the book <span class="emphasis"><em>Python 3 Text Processing with NLTK 3 Cookbook</em></span>, <span class="emphasis"><em>Jacob Perkins</em></span>, <span class="emphasis"><em>Packt Publishing</em></span>. To play a little bit with a stemmer, you can visit the web page <a class="ulink" href="http://text-processing.com/demo/stem/">http://text-processing.com/demo/stem/</a>.</p></div></div><p>NLTK comes with different stemmers. This is necessary, because every language has a different set of rules for stemming. For English, we can take <code class="literal">SnowballStemmer</code>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import nltk.stem</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; s = nltk.stem.SnowballStemmer('english')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; s.stem("graphics")</strong></span>
<span class="strong"><strong>u'graphic'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; s.stem("imaging")</strong></span>
<span class="strong"><strong>u'imag'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; s.stem("image")</strong></span>
<span class="strong"><strong>u'imag'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; s.stem("imagination")</strong></span>
<span class="strong"><strong>u'imagin'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; s.stem("imagine")</strong></span>
<span class="strong"><strong>u'imagin'</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>Note that stemming does not necessarily have to result in valid English words.</p></div></div><p>It also <a id="id167" class="indexterm"/>works with verbs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; s.stem("buys")</strong></span>
<span class="strong"><strong>u'buy'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; s.stem("buying")</strong></span>
<span class="strong"><strong>u'buy'</strong></span>
</pre></div><p>This means, it works most of the time:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; s.stem("bought")</strong></span>
<span class="strong"><strong>u'bought'</strong></span>
</pre></div></div><div class="section" title="Extending the vectorizer with NLTK's stemmer"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec02"/>Extending the vectorizer with NLTK's stemmer</h4></div></div></div><p>We need to stem<a id="id168" class="indexterm"/> the posts before we feed them into <code class="literal">CountVectorizer</code>. The class provides several hooks with which we can customize the stage's preprocessing and tokenization. The preprocessor and tokenizer can be set as parameters in the constructor. We do not want to place the stemmer into any of them, because we will then have to do the tokenization and normalization by ourselves. Instead, we overwrite the <code class="literal">build_analyzer</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import nltk.stem</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; english_stemmer = nltk.stem.SnowballStemmer('english'))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; class StemmedCountVectorizer(CountVectorizer):</strong></span>
<span class="strong"><strong>...     def build_analyzer(self):</strong></span>
<span class="strong"><strong>...         analyzer = super(StemmedCountVectorizer, self).build_analyzer()</strong></span>
<span class="strong"><strong>...         return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')</strong></span>
</pre></div><p>This will do the following process for each post:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is lower casing the raw post in the preprocessing step (done in the parent class).</li><li class="listitem">Extracting all individual words in the tokenization step (done in the parent class).</li><li class="listitem">This concludes with converting each word into its stemmed version.</li></ol></div><p>As a result, we <a id="id169" class="indexterm"/>now have one feature less, because "images" and "imaging" collapsed to one. Now, the set of feature names is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']</strong></span>
</pre></div><p>Running our new stemmed vectorizer over our posts, we see that collapsing "imaging" and "images", revealed that actually Post 2 is the most similar post to our new post, as it contains the concept "imag" twice:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.</strong></span>
<span class="strong"><strong>=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.</strong></span>
</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>=== Post 2 with dist=0.63: Most imaging databases save images permanently.</strong></span>
<span class="strong"><strong>=== Post 3 with dist=0.77: Imaging databases store data.</strong></span>
<span class="strong"><strong>=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.</strong></span>
<span class="strong"><strong>Best post is 2 with dist=0.63</strong></span>
</pre></div></div></div><div class="section" title="Stop words on steroids"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec15"/>Stop words on steroids</h3></div></div></div><p>Now that we <a id="id170" class="indexterm"/>have a reasonable way to extract a compact vector from a noisy textual post, let's step back for a while to think about what the feature values actually mean.</p><p>The feature values simply count occurrences of terms in a post. We silently assumed that higher values for a term also mean that the term is of greater importance to the given post. But what about, for instance, the word "subject", which naturally occurs in each and every single post? Alright, we can tell <code class="literal">CountVectorizer</code> to remove it as well by means of its <code class="literal">max_df</code> parameter. We can, for instance, set it to <code class="literal">0.9</code> so that all words that occur in more than 90 percent of all posts will always be ignored. But, what about words that appear in 89 percent of all posts? How low will we be willing to set <code class="literal">max_df</code>? The problem is that however we set it, there will always be the problem that some terms are just more discriminative than others.</p><p>This can only be solved by counting term frequencies for every post and in addition discount those that appear in many posts. In other words, we want a high value for a given term in a given value, if that term occurs often in that particular post and very seldom anywhere else.</p><p>This is exactly what <a id="id171" class="indexterm"/>
<span class="strong"><strong>term frequency – inverse document frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>) does. TF stands for the counting part, while IDF factors in the discounting. A naïve implementation will look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import scipy as sp</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; def tfidf(term, doc, corpus):</strong></span>
<span class="strong"><strong>...     tf = doc.count(term) / len(doc)</strong></span>
<span class="strong"><strong>...     num_docs_with_term = len([d for d in corpus if term in d])</strong></span>
<span class="strong"><strong>...     idf = sp.log(len(corpus) / num_docs_with_term)</strong></span>
<span class="strong"><strong>...     return tf * idf</strong></span>
</pre></div><p>You see that we<a id="id172" class="indexterm"/> did not simply count the terms, but also normalize the counts by the document length. This way, longer documents do not have an unfair advantage over shorter ones.</p><p>For the following documents, <code class="literal">D</code>, consisting of three already tokenized documents, we can see how the terms are treated differently, although all appear equally often per document:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; a, abb, abc = ["a"], ["a", "b", "b"], ["a", "b", "c"]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; D = [a, abb, abc]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(tfidf("a", a, D))</strong></span>
<span class="strong"><strong>0.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(tfidf("a", abb, D))</strong></span>
<span class="strong"><strong>0.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(tfidf("a", abc, D))</strong></span>
<span class="strong"><strong>0.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(tfidf("b", abb, D))</strong></span>
<span class="strong"><strong>0.270310072072</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(tfidf("a", abc, D))</strong></span>
<span class="strong"><strong>0.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(tfidf("b", abc, D))</strong></span>
<span class="strong"><strong>0.135155036036</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(tfidf("c", abc, D))</strong></span>
<span class="strong"><strong>0.366204096223</strong></span>
</pre></div><p>We see that <code class="literal">a</code> carries no meaning for any document since it is contained everywhere. The <code class="literal">b</code> term is more important for the document <code class="literal">abb</code> than for <code class="literal">abc</code> as it occurs there twice.</p><p>In reality, there are more corner cases to handle than the preceding example does. Thanks to SciKit, we don't have to think of them as they are already nicely packaged in <code class="literal">TfidfVectorizer</code>, which is inherited from <code class="literal">CountVectorizer</code>. Sure enough, we don't want to miss our stemmer:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; class StemmedTfidfVectorizer(TfidfVectorizer):</strong></span>
<span class="strong"><strong>...     def build_analyzer(self):</strong></span>
<span class="strong"><strong>...         analyzer = super(TfidfVectorizer,</strong></span>
<span class="strong"><strong>                             self).build_analyzer()</strong></span>
<span class="strong"><strong>...         return lambda doc: (</strong></span>
<span class="strong"><strong>                english_stemmer.stem(w) for w in analyzer(doc))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; vectorizer = StemmedTfidfVectorizer(min_df=1,</strong></span>
<span class="strong"><strong>                    stop_words='english', decode_error='ignore')</strong></span>
</pre></div><p>The resulting<a id="id173" class="indexterm"/> document vectors will not contain counts any more. Instead they will contain the individual TF-IDF values per term.</p></div></div><div class="section" title="Our achievements and goals"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec24"/>Our achievements and goals</h2></div></div></div><p>Our current <a id="id174" class="indexterm"/>text pre-processing phase includes <a id="id175" class="indexterm"/>the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Firstly, tokenizing the text.</li><li class="listitem">This is followed by throwing away words that occur way too often to be of any help in detecting relevant posts.</li><li class="listitem">Throwing away words that occur way so seldom so that there is only little chance that they occur in future posts.</li><li class="listitem">Counting the remaining words.</li><li class="listitem">Finally, calculating TF-IDF values from the counts, considering the whole text corpus.</li></ol></div><p>Again, we can congratulate ourselves. With this process, we are able to convert a bunch of noisy text into a concise representation of feature values.</p><p>But, as simple and powerful the bag of words approach with its extensions is, it has some drawbacks, which <a id="id176" class="indexterm"/>we should be aware of:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>It does not cover word relations</strong></span>: With the aforementioned vectorization approach, the text "Car hits wall" and "Wall hits car" will both have the same feature vector.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>It does not capture negations correctly</strong></span>: For instance, the text "I will eat ice cream" and "I will not eat ice cream" will look very similar by means of their feature vectors although they contain quite the opposite meaning. This problem, however, can be easily changed by not only counting individual words, also called "unigrams", but instead also considering bigrams (pairs of words) or trigrams (three words in a row).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>It totally fails with misspelled words</strong></span>: Although it is clear to the human beings among us readers that "database" and "databas" convey the same meaning, our approach will treat them as totally different words.</li></ul></div><p>For brevity's sake, let's nevertheless stick with the current approach, which we can now use to efficiently build <a id="id177" class="indexterm"/>clusters from.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Clustering</h1></div></div></div><p>Finally, we <a id="id178" class="indexterm"/>have our vectors, which we believe capture the posts to a sufficient degree. Not surprisingly, there are many ways to group them together. Most clustering algorithms fall into one of the two methods: flat and hierarchical clustering.</p><p>Flat clustering divides the posts into a set of clusters without relating the clusters to each other. The goal is simply to come up with a partitioning such that all posts in one cluster are most similar to each other while being dissimilar from the posts in all other clusters. Many flat clustering algorithms require the number of clusters to be specified up front.</p><p>In <a id="id179" class="indexterm"/>hierarchical clustering, the<a id="id180" class="indexterm"/> number of clusters does not have to be specified. Instead, hierarchical clustering creates a hierarchy of clusters. While similar posts are grouped into one cluster, similar clusters are again grouped into one <span class="emphasis"><em>uber-cluster</em></span>. This is done recursively, until only one cluster is left that contains everything. In this hierarchy, one can then choose the desired number of clusters after the fact. However, this comes at the cost of lower efficiency.</p><p>SciKit provides a wide range of<a id="id181" class="indexterm"/> clustering approaches in the <code class="literal">sklearn.cluster</code> package. You can get a quick overview of advantages and drawbacks of each of them at <a class="ulink" href="http://scikit-learn.org/dev/modules/clustering.html">http://scikit-learn.org/dev/modules/clustering.html</a>.</p><p>In the following sections, we will use the flat clustering method K-means and play a bit with the desired number of clusters.</p><div class="section" title="K-means"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec25"/>K-means</h2></div></div></div><p>k-means<a id="id182" class="indexterm"/> is the<a id="id183" class="indexterm"/> most widely used flat clustering algorithm. After initializing it with the desired number of clusters, <code class="literal">num_clusters</code>, it maintains that number of so-called cluster centroids. Initially, it will pick any <code class="literal">num_clusters</code> posts and set the centroids to their feature vector. Then it will go through all other posts and assign them the nearest centroid as their current cluster. Following this, it will move each centroid into the middle of all the vectors of that particular class. This changes, of course, the cluster assignment. Some posts are now nearer to another cluster. So it will update the assignments for those changed posts. This is done as long as the centroids move considerably. After some iterations, the movements will fall below a threshold and we consider clustering to be converged.</p><p>Let's play this through with a toy example of posts containing only two words. Each point in the following<a id="id184" class="indexterm"/> chart represents one document:</p><div class="mediaobject"><img src="images/2772OS_03_01.jpg" alt="K-means"/></div><p>After<a id="id185" class="indexterm"/> running one iteration of K-means, that is, taking any two vectors as starting points, assigning the labels to the rest and updating the cluster centers to now be the center point of all points in that cluster, we get the following clustering:</p><div class="mediaobject"><img src="images/2772OS_03_02.jpg" alt="K-means"/></div><p>Because the <a id="id186" class="indexterm"/>cluster <a id="id187" class="indexterm"/>centers moved, we have to reassign the cluster labels and recalculate the cluster centers. After iteration 2, we get the following clustering:</p><div class="mediaobject"><img src="images/2772OS_03_03.jpg" alt="K-means"/></div><p>The<a id="id188" class="indexterm"/> arrows<a id="id189" class="indexterm"/> show the movements of the cluster centers. After five iterations in this example, the cluster centers don't move noticeably any more (SciKit's tolerance threshold is 0.0001 by default).</p><p>After the clustering has settled, we just need to note down the cluster centers and their identity. Each new document that comes in, we then have to vectorize and compare against all cluster centers. The cluster center with the smallest distance to our new post vector belongs to the cluster we will assign to the new post.</p></div><div class="section" title="Getting test data to evaluate our ideas on"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"/>Getting test data to evaluate our ideas on</h2></div></div></div><p>In order to <a id="id190" class="indexterm"/>test clustering, let's move away from the toy text examples and find a dataset that resembles the data we are expecting in the future so that we can test our approach. For our purpose, we need documents about technical topics that are already grouped together so that we can check whether our algorithm works as expected when we apply it later to the posts we hope to receive.</p><p>One standard dataset in machine learning is the <code class="literal">20newsgroup</code> dataset, which contains 18,826 posts from 20 different newsgroups. Among the groups' topics are technical ones such as <code class="literal">comp.sys.mac.hardware</code> or <code class="literal">sci.crypt</code>, as well as more politics- and religion-related ones such as <code class="literal">talk.politics.guns</code> or <code class="literal">soc.religion.christian</code>. We will restrict ourselves<a id="id191" class="indexterm"/> to the technical groups. If we assume each newsgroup as one cluster, we can nicely test whether our approach of finding related posts works.</p><p>The dataset can be downloaded from <a class="ulink" href="http://people.csail.mit.edu/jrennie/20Newsgroups">http://people.csail.mit.edu/jrennie/20Newsgroups</a>. Much more comfortable, however, is to download it from <a id="id192" class="indexterm"/>MLComp at <a class="ulink" href="http://mlcomp.org/datasets/379">http://mlcomp.org/datasets/379</a> (free registration required). SciKit already contains custom loaders for that dataset and rewards you with very convenient data loading options.</p><p>The dataset comes in the form of a ZIP file <code class="literal">dataset-379-20news-18828_WJQIG.zip</code>, which we have to unzip to get the directory <code class="literal">379</code>, which contains the datasets. We also have to notify SciKit about the path containing that data directory. It contains a metadata file and three directories <code class="literal">test</code>, <code class="literal">train</code>, and <code class="literal">raw</code>. The <code class="literal">test</code> and <code class="literal">train</code> directories split the whole dataset into 60 percent of training and 40 percent of testing posts. If you go this route, then you either need to set the environment variable <code class="literal">MLCOMP_DATASETS_HOME</code> or you specify the path directly with the <code class="literal">mlcomp_root</code> parameter when loading the dataset.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note07"/>Note</h3><p>
<a class="ulink" href="http://mlcomp.org">http://mlcomp.org</a> is a website for comparing machine learning programs on diverse datasets. It serves two purposes: finding the right dataset to tune your machine learning program, and exploring how other people use a particular dataset. For instance, you can see how well other people's algorithms performed on particular datasets and compare against them.</p></div></div><p>For convenience, the <code class="literal">sklearn.datasets</code> module also contains the <code class="literal">fetch_20newsgroups</code> function, which automatically downloads the data behind the scenes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import sklearn.datasets</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; all_data = sklearn.datasets.fetch_20newsgroups(subset='all')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(len(all_data.filenames))</strong></span>
<span class="strong"><strong>18846</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(all_data.target_names)</strong></span>
<span class="strong"><strong>['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']</strong></span>
</pre></div><p>We can choose between training and test sets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(len(train_data.filenames))</strong></span>
<span class="strong"><strong>11314</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; test_data = sklearn.datasets.fetch_20newsgroups(subset='test')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(len(test_data.filenames))</strong></span>
<span class="strong"><strong>7532</strong></span>
</pre></div><p>For<a id="id193" class="indexterm"/> simplicity's sake, we will restrict ourselves to only some newsgroups so that the overall experimentation cycle is shorter. We can achieve this with the <code class="literal">categories</code> parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(len(train_data.filenames))</strong></span>
<span class="strong"><strong>3529</strong></span>

<span class="strong"><strong>&gt;&gt;&gt; test_data = sklearn.datasets.fetch_20newsgroups(subset='test', categories=groups)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(len(test_data.filenames))</strong></span>
<span class="strong"><strong>2349</strong></span>
</pre></div></div><div class="section" title="Clustering posts"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec27"/>Clustering posts</h2></div></div></div><p>You <a id="id194" class="indexterm"/>would have already noticed one thing—real data is noisy. The newsgroup dataset is no exception. It even contains invalid characters that will result in <code class="literal">UnicodeDecodeError</code>.</p><p>We have to tell the vectorizer to ignore them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,</strong></span>
<span class="strong"><strong>...              stop_words='english', decode_error='ignore')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; vectorized = vectorizer.fit_transform(train_data.data)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; num_samples, num_features = vectorized.shape</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print("#samples: %d, #features: %d" % (num_samples, num_features))</strong></span>
<span class="strong"><strong>#samples: 3529, #features: 4712</strong></span>
</pre></div><p>We now have a pool of 3,529 posts and extracted for each of them a feature vector of 4,712 dimensions. That is what K-means takes as input. We will fix the cluster size to 50 for this chapter and hope you are curious enough to try out different values as an exercise.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; num_clusters = 50</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.cluster import KMeans</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; km = KMeans(n_clusters=num_clusters, init='random', n_init=1,</strong></span>
<span class="strong"><strong>verbose=1, random_state=3)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; km.fit(vectorized)</strong></span>
</pre></div><p>That's it. We <a id="id195" class="indexterm"/>provided a random state just so that you can get the same results. In real-world applications, you will not do this. After fitting, we can get the clustering information out of members of <code class="literal">km</code>. For every vectorized post that has been fit, there is a corresponding integer label in <code class="literal">km.labels_</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(km.labels_)</strong></span>
<span class="strong"><strong>[48 23 31 ...,  6  2 22]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(km.labels_.shape)</strong></span>
<span class="strong"><strong>3529</strong></span>
</pre></div><p>The cluster centers can be accessed via <code class="literal">km.cluster_centers_</code>.</p><p>In the next section, we will see how we can assign a cluster to a newly arriving post using <code class="literal">km.predict</code>.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Solving our initial challenge"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Solving our initial challenge</h1></div></div></div><p>We will <a id="id196" class="indexterm"/>now put everything together and demonstrate our system for the following new post that we assign to the <code class="literal">new_post</code> variable:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Disk drive problems. Hi, I have a problem with my hard disk.</em></span></p><p><span class="emphasis"><em>After 1 year it is working only sporadically now.</em></span></p><p><span class="emphasis"><em>I tried to format it, but now it doesn't boot any more.</em></span></p><p><span class="emphasis"><em>Any ideas? Thanks."</em></span></p></blockquote></div><p>As you learned earlier, you will first have to vectorize this post before you predict its label:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; new_post_vec = vectorizer.transform([new_post])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; new_post_label = km.predict(new_post_vec)[0]</strong></span>
</pre></div><p>Now that we have the clustering, we do not need to compare <code class="literal">new_post_vec</code> to all post vectors. Instead, we can focus only on the posts of the same cluster. Let's fetch their indices in the original data set:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; similar_indices = (km.labels_==new_post_label).nonzero()[0]</strong></span>
</pre></div><p>The<a id="id197" class="indexterm"/> comparison in the bracket results in a Boolean array, and <code class="literal">nonzero</code> converts that array into a smaller array containing the indices of the <code class="literal">True</code> elements.</p><p>Using <code class="literal">similar_indices</code>, we then simply have to build a list of posts together with their similarity scores:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; similar = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for i in similar_indices:</strong></span>
<span class="strong"><strong>...    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())</strong></span>
<span class="strong"><strong>...    similar.append((dist, dataset.data[i]))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; similar = sorted(similar)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(len(similar))</strong></span>
<span class="strong"><strong>131</strong></span>
</pre></div><p>We found 131 posts in the cluster of our post. To give the user a quick idea of what kind of similar posts are available, we can now present the most similar post (<code class="literal">show_at_1</code>), and two less similar but still related ones – all from the same cluster.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; show_at_1 = similar[0]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; show_at_2 = similar[int(len(similar)/10)]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; show_at_3 = similar[int(len(similar)/2)]</strong></span>
</pre></div><p>The following table shows the posts together with their similarity values:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Position</p>
</th><th style="text-align: left" valign="bottom">
<p>Similarity</p>
</th><th style="text-align: left" valign="bottom">
<p>Excerpt from post</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1.038</p>
</td><td style="text-align: left" valign="top">
<p>BOOT PROBLEM with IDE controller</p>
<p>Hi,</p>
<p>I've got a Multi I/O card (IDE controller + serial/parallel interface) and two floppy drives (5 1/4, 3 1/2) and a Quantum ProDrive 80AT connected to it. I was able to format the hard disk, but I could not boot from it. I can boot from drive A: (which disk drive does not matter) but if I remove the disk from drive A and press the reset switch, the LED of drive A: continues to glow, and the hard disk is not accessed at all. I guess this must be a problem of either the Multi I/o card or floppy disk drive settings (jumper configuration?) Does someone have any hint what could be the reason for it. […]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1.150</p>
</td><td style="text-align: left" valign="top">
<p>Booting from B drive</p>
<p>I have a 5 1/4" drive as drive A. How can I make the system boot from my 3 1/2" B drive? (Optimally, the computer would be able to boot: from either A or B, checking them in order for a bootable disk. But: if I have to switch cables around and simply switch the drives so that: it can't boot 5 1/4" disks, that's OK. Also, boot_b won't do the trick for me. […]</p>
<p> […]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>1.280</p>
</td><td style="text-align: left" valign="top">
<p>IBM PS/1 vs TEAC FD</p>
<p>Hello, I already tried our national news group without success. I tried to replace a friend s original IBM floppy disk in his PS/1-PC with a normal TEAC drive. I already identified the power supply on pins 3 (5V) and 6 (12V), shorted pin 6 (5.25"/3.5" switch) and inserted pullup resistors (2K2) on pins 8, 26, 28, 30, and 34. The computer doesn't complain about a missing FD, but the FD s light stays on all the time. The drive spins up o.k. when I insert a disk, but I can't access it. The TEAC works fine in a normal PC. Are there any points I missed? […]</p>
<p> […]</p>
</td></tr></tbody></table></div><p>It is interesting<a id="id198" class="indexterm"/> how the posts reflect the similarity measurement score. The first post contains all the salient words from our new post. The second also revolves around booting problems, but is about floppy disks and not hard disks. Finally, the third is neither about hard disks, nor about booting problems. Still, of all the posts, we would say that they belong to the same domain as the new post.</p><div class="section" title="Another look at noise"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"/>Another look at noise</h2></div></div></div><p>We should not expect a <a id="id199" class="indexterm"/>perfect clustering in the sense that posts from the same newsgroup (for example, <code class="literal">comp.graphics</code>) are also clustered together. An example will give us a quick impression of the noise that we have to expect. For the sake of simplicity, we will focus on one of the shorter posts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; post_group = zip(train_data.data, train_data.target)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; all = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; graphics = sorted([post for post in all if post[2]=='comp.graphics'])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(graphics[5])</strong></span>
<span class="strong"><strong>(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\nSubject: test....(sorry)\nOrganization: The University of Birmingham, United Kingdom\nLines: 1\nNNTP-Posting-Host: ibm3090.bham.ac.uk&lt;…snip…&gt;', 'comp.graphics')</strong></span>
</pre></div><p>For this post, there is no real indication that it belongs to <code class="literal">comp.graphics</code> considering only the wording that is left after the preprocessing step:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; noise_post = graphics[5][1]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; analyzer = vectorizer.build_analyzer()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(list(analyzer(noise_post)))</strong></span>
<span class="strong"><strong>['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']</strong></span>
</pre></div><p>This is <a id="id200" class="indexterm"/>only after tokenization, lowercasing, and stop word removal. If we also subtract those words that will be later filtered out via <code class="literal">min_df</code> and <code class="literal">max_df</code>, which will be done later in <code class="literal">fit_transform</code>, it gets even worse:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; useful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(sorted(useful))</strong></span>
<span class="strong"><strong>['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']</strong></span>
</pre></div><p>Even more, most of the words occur frequently in other posts as well, as we can check with the IDF scores. Remember that the higher TF-IDF, the more discriminative a term is for a given post. As IDF is a multiplicative factor here, a low value of it signals that it is not of great value in general.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; for term in sorted(useful):</strong></span>
<span class="strong"><strong>...     print('IDF(%s)=%.2f'%(term, vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))</strong></span>
<span class="strong"><strong>IDF(ac)=3.51</strong></span>
<span class="strong"><strong>IDF(birmingham)=6.77</strong></span>
<span class="strong"><strong>IDF(host)=1.74</strong></span>
<span class="strong"><strong>IDF(kingdom)=6.68</strong></span>
<span class="strong"><strong>IDF(nntp)=1.77</strong></span>
<span class="strong"><strong>IDF(sorri)=4.14</strong></span>
<span class="strong"><strong>IDF(test)=3.83</strong></span>
<span class="strong"><strong>IDF(uk)=3.70</strong></span>
<span class="strong"><strong>IDF(unit)=4.42</strong></span>
<span class="strong"><strong>IDF(univers)=1.91</strong></span>
</pre></div><p>So, the terms with the highest discriminative power, <code class="literal">birmingham</code> and <code class="literal">kingdom</code>, clearly are not that computer graphics related, the same is the case with the terms with lower IDF scores. Understandably, posts from different newsgroups will be clustered together.</p><p>For our goal, however, this is no big deal, as we are only interested in cutting down the number of posts that we have to compare a new post to. After all, the particular newsgroup from where our training data came from is of no special interest.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Tweaking the parameters"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Tweaking the parameters</h1></div></div></div><p>So what <a id="id201" class="indexterm"/>about all the other parameters? Can we tweak them to get better results?</p><p>Sure. We can, of course, tweak the number of clusters, or play with the vectorizer's <code class="literal">max_features</code> parameter (you should try that!). Also, we can play with different cluster center initializations. <a id="id202" class="indexterm"/>Then there are more exciting alternatives to K-means itself. There are, for example, clustering approaches that let you even use different similarity measurements, such as Cosine similarity, Pearson, or Jaccard. An exciting field for you to play.</p><p>But before you go there, you will have to define what you actually mean by "better". SciKit has a complete package dedicated only to this definition. The package is called <code class="literal">sklearn.metrics</code> and also contains a full range of different metrics to measure clustering quality. Maybe that should be the first place to go now. Right into the sources of the metrics package.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Summary</h1></div></div></div><p>That was a tough ride from pre-processing over clustering to a solution that can convert noisy text into a meaningful concise vector representation, which we can cluster. If we look at the efforts we had to do to finally being able to cluster, it was more than half of the overall task. But on the way, we learned quite a bit on text processing and how simple counting can get you very far in the noisy real-world data.</p><p>The ride has been made much smoother, though, because of SciKit and its powerful packages. And there is more to explore. In this chapter, we were scratching the surface of its capabilities. In the next chapters, we will see more of its power.</p></div></div>
</body></html>
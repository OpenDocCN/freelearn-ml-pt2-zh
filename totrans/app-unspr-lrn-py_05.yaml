- en: '*Chapter 5*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain where autoencoders can be applied and their use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how artificial neural networks are implemented and used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement an artificial neural network using the Keras framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how autoencoders are used in dimensionality reduction and denoising
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement an autoencoder using the Keras framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain and implement an autoencoder model using convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will take a look at autoencoders and their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter continues our discussion of dimensionality reduction techniques
    as we turn our attention to autoencoders. Autoencoders are a particularly interesting
    area of focus as they provide a means of using supervised learning based on artificial
    neural networks, but in an unsupervised context. Being based on artificial neural
    networks, autoencoders are an extremely effective means of dimensionality reduction,
    but also provide additional benefits. With recent increases in the availability
    of data, processing power, and network connectivity, autoencoders are experiencing
    a resurgence in usage and study from their origins in the late 1980s. This is
    also consistent with the study of artificial neural networks, which was first
    described and implemented as a concept in the 1960s. Presently, you would only
    need to conduct a cursory internet search to discover the popularity and power
    of neural nets.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders can be used for de-noising images and generating artificial data
    samples in combination with other methods, such as recurrent or **Long Short-Term
    Memory** (**LSTM**) architectures, to predict sequences of data. The flexibility
    and power that arises from the use of artificial neural networks also enables
    autoencoders to form very efficient representations of the data, which can then
    be used either directly as an extremely efficient search method, or as a feature
    vector for later processing.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the use of an autoencoder in an image de-noising application, where
    we are presented with the image on the left in [*Figure 5.1*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor109).
    We can see that the image is affected by the addition of some random noise. We
    can use a specially trained autoencoder to remove this noise, as represented by
    the image on the right in *Figure 5.1*. In learning how to remove this noise,
    the autoencoder has also learned to encode the important information that composes
    the image and how to reconstruct (or decode) this information into a clearer version
    of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: Autoencoder de-noising](img/C12626_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Autoencoder de-noising'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This image is modified from [http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/](http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/)
    under CC0.
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates one aspect of autoencoders that makes them useful
    for unsupervised learning (the encoding stage), and one that is useful in generating
    new images (decoding). Throughout this chapter, we will delve further into these
    two useful stages of autoencoders and apply the output of the autoencoder to clustering
    the CIFAR-10 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a representation of an encoder and decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Encoder/decoder representation](img/C12626_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Encoder/decoder representation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fundamentals of Artificial Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that autoencoders are based on artificial neural networks, an understanding
    of how neural networks is also critical for understanding autoencoders. This section
    of the chapter will briefly review the fundamentals of artificial neural networks.
    It is important to note that there are many aspects of neural nets that are outside
    of the scope of this book. The topic of neural networks could easily, and has,
    filled many books on its own, and this section is not to be considered an exhaustive
    discussion of the topic.
  prefs: []
  type: TYPE_NORMAL
- en: As described earlier, artificial neural networks are primarily used in supervised
    learning problems, where we have a set of input information, say a series of images,
    and we are training an algorithm to map the information to a desired output, such
    as a class or category. Consider the CIFAR-10 dataset ([*Figure 5.3*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor111))
    as an example, which contains images of 10 different categories (airplane, automobile,
    bird, cat, deer, dog, frog, horse, ship, and truck), with 6,000 images per category.
    When neural nets are used in a supervised learning context, the images are fed
    to the network with a representation of the corresponding category labels being
    the desired output of the network.
  prefs: []
  type: TYPE_NORMAL
- en: The network is then trained to maximize its ability to infer or predict the
    correct label for a given image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: CIFAR-10 dataset](img/C12626_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: CIFAR-10 dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This image is taken from [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    from Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.
  prefs: []
  type: TYPE_NORMAL
- en: The Neuron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The artificial neural network derives its name from the biological neural networks
    commonly found in the brain. While the accuracy of the analogy can certainly be
    questioned, it is a useful metaphor to break down the concept of artificial neural
    networks and facilitate understanding. As with their biological counterparts,
    the neuron is the building block on which all neural networks are constructed,
    connecting a number of neurons in different configurations to form more powerful
    structures. Each neuron ([*Figure 5.4*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor113))
    is composed of four individual parts: an input value, a tunable weight (theta),
    an activation function that operates on the input value, and the resulting output
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Anatomy of a neuron](img/C12626_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Anatomy of a neuron'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The activation function is specifically chosen depending upon the objective
    of the neural network being designed, and there are a number of common functions,
    including `tanh`, `sigmoid`, `linear`, `sigmoid`, and `ReLU` (rectified linear
    unit). Throughout this chapter, we will use both the `sigmoid` and `ReLU` activation
    functions, so let's look at them in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The sigmoid activation function is very commonly used as an output in the classification
    of neural networks due to its ability to shift the input values to approximate
    a binary output. The sigmoid function produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Output of the sigmoid function'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see in [*Figure 5.5*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor115)
    that the output of the sigmoid function asymptotes (approaches but never reaches)
    1 as *x* increases and asymptotes 0 as *x* moves further away from 0 in the negative
    direction. This function is used in classification tasks as it provides close
    to a binary output and is not a member of class (0) or is a member of the class
    (1).
  prefs: []
  type: TYPE_NORMAL
- en: Rectified Linear Unit (ReLU)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The rectified linear unit is a very useful activation function that's commonly
    used at intermediary stages of neural networks. Simply put, the value 0 is assigned
    to values less than 0, and the value is returned for greater than 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6: Output of ReLU](img/C12626_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Output of ReLU'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 18: Modeling the Neurons of an Artificial Neural Network'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will practically introduce a programmatic representation
    of the neuron in NumPy using the sigmoid function. We will keep the inputs fixed
    and adjust the tunable weights to investigate the effect on the neuron. Interestingly,
    this model is also very close to the supervised learning method of logistic regression.
    Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy` and matplotlib packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure matplotlib to enable the use of Latex to render mathematical symbols
    in the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `sigmoid` function as a Python function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here, we''re using the sigmoid function. You could also use the ReLU function.
    The ReLU activation function, while being powerful in artificial neural networks,
    is easy to define. It simply needs to return the input value if greater than 0;
    otherwise, it returns 0:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`def relu(x):`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`return np.max(0, x)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be 100 numbers linearly spaced between `-5` and
    `5`. Set `theta= 1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A section of the output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.7: Printing the inputs](img/C12626_05_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.7: Printing the inputs'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the outputs (`y`) of the neuron:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the output of the neuron versus the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8: Plot of neurons versus inputs](img/C12626_05_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.8: Plot of neurons versus inputs'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Set the tunable parameter, `theta`, to `5`, and recompute and store the output
    of the neuron:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the tunable parameter, `theta`, to `0.2`, and recompute and store the
    output of the neuron:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, `theta = 0.2`) on one graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.9: Output curves of neurons](img/C12626_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Output curves of neurons'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this exercise, we modeled the basic building block of an artificial neural
    network with a sigmoid activation function. We can see that using the sigmoid
    function increases the steepness of the gradient and means that only small values
    of x will push the output to either close to 1 or 0\. Similarly, reducing `theta`
    reduces the sensitivity of the neuron to non-zero values and results in much extreme
    input values being required to push the result of the output to either 0 or 1,
    tuning the output of the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Modeling Neurons with a ReLU Activation Function'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will investigate the ReLU activation function and the
    effect tunable weights have in modifying the output of ReLU units:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `numpy` and matplotlib.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the ReLU activation function as a Python function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be 100 numbers linearly spaced between `-5` and
    `5`. Set `theta = 1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output (`y`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the output of the neuron versus the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, set `theta = 5`, and recompute and store the output of the neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, set `theta = 0.2`, and recompute and store the output of the neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, and `theta = 0.2`) on one graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By the end of this activity, you will have developed a range of response curves
    for the ReLU activated neuron. You will also be able to describe the effect of
    changing the value of theta on the output of the neuron. The output will look
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10: Expected output curves](img/C12626_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Expected output curves'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 333.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Networks: Architecture Definition'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Individual neurons aren't particularly useful in isolation; they provide an
    activation function and a means of tuning the output, but a single neuron would
    have an limited learning ability. Neurons become much more powerful when many
    of them are combined and connected together in a network structure. By using a
    number of different neurons and combining the outputs of individual neurons, more
    complex relationships can be established and more powerful learning algorithms
    can be built. In this section, we will briefly discuss the structure of a neural
    network and implement a simple neural network using the Keras machine learning
    framework ([https://keras.io/](https://keras.io/)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11: Simplified representation of a neural network](img/C12626_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Simplified representation of a neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Figure 5.11* illustrates the structure of a two-layered, fully-connected neural
    network. One of the first observations we can make is that there is a lot of information
    contained within this structure, with a high degree of connectivity as represented
    by the arrows that point to and from each of the nodes. Working from the left-hand
    side of the image, we can see the input values to the neural network, as represented
    by the (*x*) values. In this example, we have *m* input values per sample, and
    only the first sample is being fed into the network, hence, values from ![A close
    up of a stool'
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12626_05_Formula_01.png) to ![A close
    up of a sign
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12626_05_Formula_02.png). These values
    are then multiplied by the corresponding weights of the first layer of the neural
    network (![](img/C12626_05_Formula_03.png)) before being passed into the activation
    function of the corresponding neuron. This is known as a **feedforward** neural
    network. The notation used in *Figure 5.11* to identify the weights is ![A close
    up of a logo
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12626_05_Formula_04.png), where *i*
    is the layer the weight belongs to, *j* is the input node number (starting with
    1 at the top), and *k* is the node in the subsequent layer that the weight feeds
    into to.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the inter-connectivity between the outputs of layer 1 (also known
    as the **hidden layer**) and the inputs to the output layer, we can see that there
    is a large number of trainable parameters (weights) that can be used to map the
    input to the desired output. The network of *Figure 5.11* represents an *n* class
    neural network classifier, where the output for each of the *n* nodes represents
    the probability of the input belonging to the corresponding class.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer is able to use a different activation function as described by ![](img/C12626_05_Formula_05.png)
    and ![](img/C12626_05_Formula_06.png), thus allowing different activation functions
    to be mixed, in which the first layer could use ReLU, the second could use tanh,
    and the third could use sigmoid, for example. The final output is calculated by
    taking the sum of the product of the output of the previous layer with the corresponding
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider the output of the first node of layer 1, it can be calculated
    by multiplying the inputs by the corresponding weights, adding the result, and
    passing it through the activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Calculating the output of the last node'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we increase the number of layers between the input and output of the network,
    we increase the depth of the network. An increase in the depth is also an increase
    in the number of trainable parameters, as well as the complexity of the relationships
    within the data, as described by the network. It is, typically, harder to train
    networks with increased depth because the types of features selected for the input
    become more critical. Additionally, as we add more neurons to each layer, we increase
    the height of the neural network. By adding more neurons, the ability of the network
    to describe the dataset increases as we add more trainable parameters. If too
    many neurons are added, the network can memorize the dataset but fails to generalize
    new samples. The trick in constructing neural networks is to find the balance
    between sufficient complexity to be able to describe the relationships within
    the data and not be so complicated as to memorize the training samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 19: Defining a Keras Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will define a neural network architecture (similar to *Figure
    5.11*) using the Keras machine learning framework to classify images for the CIFAR-10
    dataset. As each input image is 32 x 32 pixels in size, the input vector will
    comprise 32*32 = 1,024 values. With 10 individual classes in CIFAR-10, the output
    of the neural network will be composed of 10 individual values, with each value
    representing the probability of the input data belonging to the corresponding
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we will require the Keras machine learning framework. Keras
    is a high-level neural network API that is used on top of an existing library,
    such as TensorFlow or Theano. Keras makes it easy to switch between lower-level
    frameworks because the high-level interface it provides remains the same irrespective
    of the underlying library. In this book, we will be using TensorFlow as the underlying
    library. If you have yet to install Keras and TensorFlow, do so using `conda`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, you can install it using `pip`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will require the `Sequential` and `Dense` classes from `keras.models` and
    `keras.layers`, respectively. Import these classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As described earlier, the input layer will receive 1,024 values. The second
    layer (Layer 1) will have 500 units and, because the network is to classify one
    of 10 different classes, the output layer will have 10 units. In Keras, a model
    is defined by passing an ordered list of layers to the `Sequential` model class.
    This example uses the `Dense` layer class, which is a fully-connected neural network
    layer. The first layer will use a ReLU activation function, while the output will
    use the `softmax` function to determine the probability of each class. Define
    the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the model defined, we can use the `summary` method to confirm the structure
    and the number of trainable parameters (or weights) within the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.13: Structure and count of trainable parameters in the model](img/C12626_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: Structure and count of trainable parameters in the model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This table summarizes the structure of the neural network. We can see that
    there are the two layers that we specified, with 500 units in the first layer
    and 10 output units in the second layer. The `Param #` column tells us how many
    trainable weights are available in that specific layer. The table also tells us
    that there are 517,510 trainable weights in total within the network.'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we created a neural network model in Keras that contains a
    network of over 500,000 weights that can be used to classify the images of CIFAR-10\.
    In the next section, we will train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Networks: Training'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the neural network model defined, we can begin the training process; at
    this stage, we will be training the model in a supervised fashion to develop some
    familiarity with the Keras framework before moving on to training autoencoders.
    Supervised learning models are trained by providing the model with both the input
    information as well as the known output; the goal of training is to construct
    a network that takes the input information and returns the known output using
    only the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In a supervised classification example such as CIFAR-10, the input information
    is an image and the known output is the class that the image belongs to. During
    training, for each sample prediction, the errors in the feedforward network predictions
    are calculated using a specified error function. Each of the weights within the
    model is then tuned in an attempt to reduce the error. This tuning process is
    known as **back-propagation** because the error is propagated backward through
    the network from the output to the start of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'During back-propagation, each trainable weight is adjusted in proportion to
    its contribution to the overall error multiplied by a value known as the **learning
    rate**, which controls the rate of change in the trainable weights. Looking at
    *Figure 5.14*, we can see that increasing the value of the learning rate can increase
    the speed at which the error is reduced, but risks not converging on a minimum
    error as we step over the values. A learning rate that''s too small may lead to
    us running out of patience or simply not having sufficient time to find the global
    minimum. Thus, finding the correct learning rate is a trial and error process,
    though starting with a larger learning rate and reducing it can often be a productive
    method. The following figure represents the selection of the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14: Selecting the correct learning rate (one epoch is one learning
    step)](img/C12626_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Selecting the correct learning rate (one epoch is one learning
    step)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Training is repeated until the error in the predictions stop reducing or the
    developer runs out of patience waiting for a result. In order to complete the
    training process, we first need to make some design decisions, the first being
    the most appropriate error function. There are a range of error functions available
    for use, from a simple mean squared difference to more complex options. Categorical
    cross entropy (which is used in the following exercise) is a very useful error
    function for classifying more than one class.
  prefs: []
  type: TYPE_NORMAL
- en: With the error function defined, we need to choose the method of updating the
    trainable parameters using the error function. One of the most memory-efficient
    and effective update methods is stochastic gradient descent (SGD); there are a
    number of variants of SGD, all of which involve adjusting each of the weights
    in accordance with their individual contribution to the calculated error. The
    final training design decision to be made is the performance metric by which the
    model is evaluated and the best architecture selected; in a classification problem,
    this may be the classification accuracy of the model or perhaps the model that
    produces the lowest error score in a regression problem. These comparisons are
    generally made using a method of cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 20: Training a Keras Neural Network Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Thankfully, we don''t need to worry about manually programming the components
    of the neural network, such as backpropagation, because the Keras framework manages
    this for us. In this exercise, we will use Keras to train a neural network to
    classify a small subset of the CIFAR-10 dataset using the model architecture defined
    in the previous exercise. As with all machine learning problems, the first and
    the most important step is to understand as much as possible about the dataset,
    and this will be the initial focus of the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` and `batches.meta` files from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, `matplotlib` and the `Sequential` class from `keras.models`,
    and import `Dense` from `keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the sample of the CIFAR-10 dataset that is provided with the accompanying
    source code in the `data_batch_1` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data is loaded as a dictionary. Display the keys of the dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the keys are stored as binary strings as denoted by `b''`. We are
    interested in the contents of data and labels. Let''s look at labels first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.15: Displaying the labels'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can see that the labels are a list of values 0 – 9, indicating which class
    each sample belongs to. Now, look at the contents of the `data` key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.16: Content of the data key](img/C12626_05_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.16: Content of the data key'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The data key provides a NumPy array with all the image data stored within the
    array. What is the shape of the image data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that we have 1000 samples, but each sample is a single dimension
    of 3,072 samples. Aren''t the images supposed to be 32 x 32 pixels? Yes, they
    are, but because the images are color or RGB images, they contain three channels
    (red, green, and blue), which means the images are 32 x 32 x 3\. They are also
    flattened, providing 3,072 length vectors. So, we can reshape the array and then
    visualize a sample of images. According to the CIFAR-10 documentation, the first
    1,024 samples are red, the second 1,024 are green, and the third 1,024 are blue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the first 12 images, along with their labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.17: The first 12 images](img/C12626_05_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.17: The first 12 images'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'What is the actual meaning of the labels? To find out, load the `batches.meta`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.18: Meaning of the labels'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Decode the binary strings to get the actual labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.19: Printing the actual labels'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the labels for the first 12 images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.20: Labels of the first 12 images](img/C12626_05_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.20: Labels of the first 12 images'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we need to prepare the data for training the model. The first step is to
    prepare the output. Currently, the output is a list of numbers 0 – 9, but we need
    each sample to be represented as a vector of 10 units as per the previous model.
    The encoded output will be a NumPy array with a shape of 10000 x 10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the one hot encoding values for the first 12 samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.21: One hot encoding values for first 12 samples](img/C12626_05_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.21: One hot encoding values for first 12 samples'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model has 1,024 inputs because it expects a 32 x 32 grayscale image. Take
    the average of the three channels for each image to convert it to RGB:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the first 12 images again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.22: Displaying the first 12 images again.](img/C12626_05_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.22: Displaying the first 12 images again.'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, scale the images to be between 0 and 1, which is required for all
    inputs to a neural network. As the maximum value in an image is 255, we will simply
    divide by 255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need the images to be in the shape 10,000 x 1,024:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Redefine the model with the same architecture as *Exercise 19*, *Defining a
    Keras Model*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can train the model in Keras. We first need to compile the method to
    specify the training parameters. We will be using categorical cross-entropy, with
    stochastic gradient descent and a performance metric of classification accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model using back-propagation for 100 epochs and the `fit` method
    of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.23: Training the model](img/C12626_05_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.23: Training the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We achieved approximately 90% classification accuracy for the 1,000 samples
    using this network. Examine the predictions made for the first 12 samples again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.24: Printing the predictions](img/C12626_05_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.24: Printing the predictions'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can use the `argmax` method to determine the most likely class for each
    sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compare with the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The network made one error in these samples, that is, it classified the second
    last samples as a 2 (bird) instead of a 4 (deer). Congratulations! You have just
    successfully trained a neural network model in Keras. Complete the next activity
    to further reinforce your skills in training neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: MNIST Neural Network'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, you will train a neural network to identify images in the
    MNIST dataset and reinforce your skills in training neural networks. This activity
    forms the basis of many neural network architectures in different classification
    problems, particularly in computer vision. From object detection and identification
    to classification, this general structure is used in a variety of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will help you complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pickle`, `numpy`, `matplotlib`, and the `Sequential` and `Dense` classes
    from Keras.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `mnist.pkl` file that contains the first 10,000 images and the corresponding
    labels from the MNIST dataset that are available in the accompanying source code.
    The MNIST dataset is a series of 28 x 28 grayscale images of handwritten digits,
    0 through 9\. Extract the images and labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the `mnist.pkl` file at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot the first 10 samples along with the corresponding labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the labels using one hot encoding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a neural network model in Keras that accepts the prepared images and
    has a hidden layer of 600 units with a ReLU activation function and an output
    of the same number of units as classes. The output layer uses a `softmax` activation
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the model using multiclass cross-entropy, stochastic gradient descent,
    and an accuracy performance metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model. How many epochs are required to achieve at least 95% classification
    accuracy on the training data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By completing this activity, you have trained a simple neural network to identify
    handwritten digits 0 through 9\. You have also developed a general framework for
    building neural networks for classification problems. With this framework, you
    can extend upon and modify the network for a range of other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 335.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we are comfortable developing supervised neural network models in
    Keras, we can return our attention to unsupervised learning and the main subject
    of this chapter—autoencoders. Autoencoders are a specifically designed neural
    network architecture that aims to compress the input information into lower dimensional
    space in an efficient yet descriptive manner. Autoencoder networks can be decomposed
    into two individual sub-networks or stages: an **encoding** stage and a **decoding**
    stage. The first, or encoding, stage takes the input information and compresses
    it through a subsequent layer that has fewer units than the size of the input
    sample. The latter stage, that is, the decoding stage, then expands the compressed
    form of the image and aims to return the compressed data to its original form.
    As such, the inputs and desired outputs of the network are the same; the network
    takes, say, an image in the CIFAR-10 dataset and tries to return the same image.
    This network architecture is shown in *Figure 5.25*; in this image, we can see
    that the encoding stage of the autoencoder reduces the number of neurons to represent
    the information, while the decoding stage takes the compressed format and returns
    it to its original state. The use of the decoding stage helps to ensure that the
    encoder has correctly represented the information, because the compressed representation
    is all that is provided to restore the image in its original state. We will now
    work through a simplified autoencoder model using the CIFAR-10 dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25: Simple autoencoder network architecture](img/C12626_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.25: Simple autoencoder network architecture'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 21: Simple Autoencoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will construct a simple autoencoder for the sample of the
    CIFAR-10 dataset, compressing the information stored within the images for later
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the autoencoder model. As we need access to the output of the encoder
    stage, we will need to define the model using a slightly different method to that
    previously used. Define an input layer of `1024` units:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a subsequent `Dense` layer of `256` units (a compression ratio of 1024/256
    = 4) and a ReLU activation function as the encoding stage. Note that we have assigned
    the layer to a variable and passed the previous layer to a call method for the
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a subsequent decoder layer using the sigmoid function as an activation
    function and the same shape as the input layer. The sigmoid function has been
    selected because the input values to the network are only between 0 and 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the autoencoder using a binary cross-entropy loss function and adadelta
    gradient descent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '`adadelta` is a more sophisticated version of stochastic gradient descent where
    the learning rate is adjusted on the basis of a window of recent gradient updates.
    Compared to the other methods of modifying the learning rate, this prevents the
    gradient of very old epochs from influencing the learning rate.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.26: Training the model](img/C12626_05_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.26: Training the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the encoder output to 16 x 16 (16 x 16 = 256) pixels and multiply by
    255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the output of the decoder to 32 x 32 and multiply by 255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the original images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.27: Output of simple autoencoder'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In *Figure 5.27*, we can see three rows of images. The first row is the original
    grayscale image, the second row is the corresponding autoencoder output for the
    original image, and finally, the third row is the reconstruction of the original
    image from the encoded input. We can see that the decoded images in the third
    row contain information about the basic shape of the image; we can see the main
    body of the frog and the deer, as well as the outline of the trucks and cars in
    the sample. Given that we only trained the model for 100 samples, this exercise
    would also benefit from an increase in the number of training epochs to further
    improve the performance of both the encoder and decoder. Now that we have the
    output of the autoencoder stage trained, we can use it as the feature vector for
    other unsupervised algorithms, such as K-means or K nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Simple MNIST Autoencoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, you will create an autoencoder network for the MNIST dataset
    contained within the accompanying source code. An autoencoder network such as
    the one built in this activity can be an extremely useful in the pre-processing
    stage of unsupervised learning. The encoded information produced by the network
    can be used in clustering or segmentation analysis, such as image-based web searches:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pickle`, `numpy`, and `matplotlib`, and the `Model`, `Input`, and `Dense`
    classes from Keras.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the images from the supplied sample of the MNIST dataset that is provided
    with the accompanying source code (`mnist.pkl`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `mnist.pklP-code` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a simple autoencoder network that reduces the image size to 10 x 10
    after the encoding stage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the encoder model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and store the output of the encoding stage for the first five samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by
    255.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and store the output of the decoding stage for the first five samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the output of the decoder to 28 x 28 and multiply by 255.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the original image, the encoder output, and the decoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In completing this activity, you will have successfully trained an autoencoder
    network that extracts the critical information from the dataset, preparing it
    for later processing. The output will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_05_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.28: Expected plot of original image, the encoder output, and the decoder'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 338.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 22: Multi-Layer Autoencoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will construct a multi-layer autoencoder for the sample
    of the CIFAR-10 dataset, compressing the information stored within the images
    for later use:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the multi-layer autoencoder model. We will use the same shape input
    as the simple autoencoder model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will add another layer before the 256 autoencoder stage, this time with
    512 neurons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the same size autoencoder as the previous exercise, but the input to
    the layer is the `hidden_encoding` layer this time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a decoding hidden layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the same output stage as in the previous exercise, this time connected
    to the hidden decoding stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.29: Training the model](img/C12626_05_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.29: Training the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by
    255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape the output of the decoder to 28 x 28 and multiply by 255:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the original image, the encoder output, and the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.30: Output of multi-layer autoencoder](img/C12626_05_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.30: Output of multi-layer autoencoder'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By looking at the error score produced by both the simple and multilayer autoencoders
    and by comparing *Figure 5.27* and *Figure 5.30*, we can see that there is little
    difference between the output of the two encoder structures. The middle row of
    both figures show that the features learned by the two models are, in fact, different.
    There are a number of options we can use to improve both of these models, such
    as training for more epochs, using a different number of units or neurons in the
    layers, or using varying numbers of layers. This exercise was constructed to demonstrate
    how to build and use an autoencoder, but optimization is often a process of systematic
    trial and error. We encourage you to adjust some of the parameters of the model
    and investigate the different results for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In constructing all of our previous neural network models, you would have noticed
    that we removed all the color information when converting the image to grayscale,
    and then flattened each image into a single vector of length 1,024\. In doing
    so, we essentially threw out a lot of information that may be of use to us. The
    colors in the images may be specific to the class or objects in the image; additionally,
    we lost a lot of our spatial information about the image, for example, the position
    of the trailer in the truck image relative to the cab or the legs of the deer
    relative to the head. Convolutional neural networks do not suffer from this information
    loss. This is because rather than using a flat structure of trainable parameters,
    they store the weights in a grid or matrix, which means that each group of parameters
    can have many layers in their structure. By organizing the weights in a grid,
    we prevent the loss of spatial information because the weights are applied in
    a sliding fashion across the image. Also, by having many layers, we can retain
    the color channels associated with the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In developing convolutional neural-network-based autoencoders, the MaxPooling2D
    and Upsampling2D layers are very important. The MaxPooling 2D layer downsamples
    or reduces the size of an input matrix in two dimensions by selecting the maximum
    value within a window of the input. Say we had a 2 x 2 matrix, where three cells
    have a value of 1 and one single cell has a value of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31: Demonstration of sample matrix](img/C12626_05_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.31: Demonstration of sample matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If provided to the MaxPooling2D layer, this matrix would return a single value
    of 2, thus reducing the size of the input in both directions by one half.
  prefs: []
  type: TYPE_NORMAL
- en: The UpSampling2D layer has the opposite effect as that of the MaxPooling2D layer,
    increasing the size of the input rather than reducing it. The upsampling process
    repeats the rows and columns of the data, thus doubling the size of the input
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 23: Convolutional Autoencoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will develop a convolutional neural-network-based autoencoder
    and compare the performance to the previous fully-connected neural network autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we are using a convolutional network, we can use the images with only rescaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the convolutional autoencoder model. We will use the same shape input
    as an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a convolutional stage with 32 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using the same padding, which means the output
    has the same length as the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a max pooling layer to the encoder with a 2 x 2 kernel. `MaxPooling` looks
    at all the values in an image, scanning through with a 2 x 2 matrix. The maximum
    value in each 2 x 2 area is returned, thus reducing the size of the encoded layer
    by a half:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add a decoding convolutional layer (this layer should be identical to the previous
    convolutional layer):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to return the image to its original size, for which we will upsample
    by the same size as `MaxPooling2D`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the final convolutional stage using three layers for the RGB channels of
    the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the structure of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we have far fewer trainable parameters as compared to the previous
    autoencoder examples. This has been a specific design decision to ensure that
    the example runs on a wide variety of hardware. Convolutional networks typically
    require a lot more processing power and often special hardware such as Graphical
    Processing Units (GPUs).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 20 epochs, because convolutional networks take
    a lot longer to compute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.32: Training the model](img/C12626_05_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.32: Training the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the error was already less than in the previous autoencoder exercise
    after the second epoch, suggesting a better encoding/decoding model. This reduced
    error can be mostly attributed to the fact that the convolutional neural network
    did not discard a lot of data, and the encoded images are 16 x 16 x 32, which
    is significantly larger than the previous 16 x 16 size. Additionally, we have
    not compressed the images per se as they now contain fewer pixels (16 x 16 x 32
    = 8,192), but with more depth (32 x 32 x 3,072) than before. This information
    has been rearranged to allow more effective encoding/decoding processes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each encoded image has a shape of 16 x 16 x 32 due to the number of filters
    selected for the convolutional stage. As such, we cannot visualize them without
    modification. We will reshape them to be 256 x 32 in size for visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the output of the decoder for the first five images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the original image, the mean encoder output, and the decoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.33: The original image, the encoder output, and the decoder](img/C12626_05_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.33: The original image, the encoder output, and the decoder'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 11: MNIST Convolutional Autoencoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will reinforce our knowledge of convolutional autoencoders
    using the MNIST dataset. Convolutional autoencoders typically achieve significantly
    improved performance when working with image-based datasets of a reasonable size.
    This is particularly useful when using autoencoders to generate artificial image
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `mnist.pkl` file, which contains the first 10,000 images and corresponding
    labels from the MNIST dataset, which are available in the accompanying source
    code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `mnist.pkl` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rescale the images to have values between 0 and 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to reshape the images to add a single depth channel for use with convolutional
    stages. Reshape the images to have a shape of 28 x 28 x 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define an input layer. We will use the same shape input as an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a convolutional stage, with 16 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using same padding, which means the output has
    the same length as the input image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a max pooling layer to the encoder with a 2 x 2 kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a decoding convolutional layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an upsampling layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the final convolutional stage using 1 layer as per the initial image depth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the model by passing the first and last layers of the network to the
    `Model` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the structure of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's fit the model; again, we pass the images as the training data and
    as the desired output. Train for 20 epochs as convolutional networks take a lot
    longer to compute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and store the output of the encoding stage for the first five samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the encoder output for visualization, where each image is X*Y in size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the output of the decoder for the first five images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the decoder output to be 28 x 28 in size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the original images back to be 28 x 28 in size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the original image, the mean encoder output, and the decoder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the end of this activity, you will have developed an autoencoder comprising
    convolutional layers within the neural network. Note the improvements made in
    the decoder representations. The output will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_05_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: Expected original image, the encoder output, and the decoder'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 340.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we started with an introduction to artificial neural networks,
    how they are structured, and the processes by which they learn to complete a particular
    task. Starting with a supervised learning example, we built an artificial neural
    network classifier to identify objects within the CIFAR-10 dataset. We then progressed
    to the autoencoder architecture of neural networks and learned how we can use
    these networks to prepare a dataset for use in an unsupervised learning problem.
    Finally, we completed this investigation with autoencoders, looking at convolutional
    neural networks and the benefits these additional layers can provide. This chapter
    prepared us well for the final instalment in dimensionality reduction, as we look
    at using and visualizing the encoded data with t-distributed nearest neighbors
    (t-SNE). T-distributed nearest neighbors provides an extremely effective method
    of visualizing high-dimensional data even after applying reduction techniques
    such as PCA. T-SNE is particularly useful method for unsupervised learning.
  prefs: []
  type: TYPE_NORMAL

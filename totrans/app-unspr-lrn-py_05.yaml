- en: '*Chapter 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to do the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Explain where autoencoders can be applied and their use cases
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how artificial neural networks are implemented and used
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement an artificial neural network using the Keras framework
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain how autoencoders are used in dimensionality reduction and denoising
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement an autoencoder using the Keras framework
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain and implement an autoencoder model using convolutional neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will take a look at autoencoders and their applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter continues our discussion of dimensionality reduction techniques
    as we turn our attention to autoencoders. Autoencoders are a particularly interesting
    area of focus as they provide a means of using supervised learning based on artificial
    neural networks, but in an unsupervised context. Being based on artificial neural
    networks, autoencoders are an extremely effective means of dimensionality reduction,
    but also provide additional benefits. With recent increases in the availability
    of data, processing power, and network connectivity, autoencoders are experiencing
    a resurgence in usage and study from their origins in the late 1980s. This is
    also consistent with the study of artificial neural networks, which was first
    described and implemented as a concept in the 1960s. Presently, you would only
    need to conduct a cursory internet search to discover the popularity and power
    of neural nets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders can be used for de-noising images and generating artificial data
    samples in combination with other methods, such as recurrent or **Long Short-Term
    Memory** (**LSTM**) architectures, to predict sequences of data. The flexibility
    and power that arises from the use of artificial neural networks also enables
    autoencoders to form very efficient representations of the data, which can then
    be used either directly as an extremely efficient search method, or as a feature
    vector for later processing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Consider the use of an autoencoder in an image de-noising application, where
    we are presented with the image on the left in [*Figure 5.1*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor109).
    We can see that the image is affected by the addition of some random noise. We
    can use a specially trained autoencoder to remove this noise, as represented by
    the image on the right in *Figure 5.1*. In learning how to remove this noise,
    the autoencoder has also learned to encode the important information that composes
    the image and how to reconstruct (or decode) this information into a clearer version
    of the original image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: Autoencoder de-noising](img/C12626_05_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Autoencoder de-noising'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This image is modified from [http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/](http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/)
    under CC0.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates one aspect of autoencoders that makes them useful
    for unsupervised learning (the encoding stage), and one that is useful in generating
    new images (decoding). Throughout this chapter, we will delve further into these
    two useful stages of autoencoders and apply the output of the autoencoder to clustering
    the CIFAR-10 dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a representation of an encoder and decoder:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2: Encoder/decoder representation](img/C12626_05_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Encoder/decoder representation'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fundamentals of Artificial Neural Networks
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that autoencoders are based on artificial neural networks, an understanding
    of how neural networks is also critical for understanding autoencoders. This section
    of the chapter will briefly review the fundamentals of artificial neural networks.
    It is important to note that there are many aspects of neural nets that are outside
    of the scope of this book. The topic of neural networks could easily, and has,
    filled many books on its own, and this section is not to be considered an exhaustive
    discussion of the topic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: As described earlier, artificial neural networks are primarily used in supervised
    learning problems, where we have a set of input information, say a series of images,
    and we are training an algorithm to map the information to a desired output, such
    as a class or category. Consider the CIFAR-10 dataset ([*Figure 5.3*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor111))
    as an example, which contains images of 10 different categories (airplane, automobile,
    bird, cat, deer, dog, frog, horse, ship, and truck), with 6,000 images per category.
    When neural nets are used in a supervised learning context, the images are fed
    to the network with a representation of the corresponding category labels being
    the desired output of the network.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The network is then trained to maximize its ability to infer or predict the
    correct label for a given image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: CIFAR-10 dataset](img/C12626_05_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: CIFAR-10 dataset'
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This image is taken from [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    from Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The Neuron
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The artificial neural network derives its name from the biological neural networks
    commonly found in the brain. While the accuracy of the analogy can certainly be
    questioned, it is a useful metaphor to break down the concept of artificial neural
    networks and facilitate understanding. As with their biological counterparts,
    the neuron is the building block on which all neural networks are constructed,
    connecting a number of neurons in different configurations to form more powerful
    structures. Each neuron ([*Figure 5.4*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor113))
    is composed of four individual parts: an input value, a tunable weight (theta),
    an activation function that operates on the input value, and the resulting output
    value:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Anatomy of a neuron](img/C12626_05_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4：神经元的解剖结构](img/C12626_05_04.jpg)'
- en: 'Figure 5.4: Anatomy of a neuron'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：神经元的解剖结构
- en: The activation function is specifically chosen depending upon the objective
    of the neural network being designed, and there are a number of common functions,
    including `tanh`, `sigmoid`, `linear`, `sigmoid`, and `ReLU` (rectified linear
    unit). Throughout this chapter, we will use both the `sigmoid` and `ReLU` activation
    functions, so let's look at them in a little more detail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择是根据神经网络的目标来特定选择的，有许多常用的函数，包括`tanh`、`sigmoid`、`linear`、`sigmoid`和`ReLU`（修正线性单元）。在本章中，我们将同时使用`sigmoid`和`ReLU`激活函数，接下来我们将更详细地讨论它们。
- en: Sigmoid Function
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'The sigmoid activation function is very commonly used as an output in the classification
    of neural networks due to its ability to shift the input values to approximate
    a binary output. The sigmoid function produces the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数因其能将输入值转化为接近二进制的输出，因此在神经网络分类任务中非常常见。Sigmoid函数的输出如下：
- en: '![](img/C12626_05_05.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_05_05.jpg)'
- en: 'Figure 5.5: Output of the sigmoid function'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：Sigmoid函数的输出
- en: We can see in [*Figure 5.5*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor115)
    that the output of the sigmoid function asymptotes (approaches but never reaches)
    1 as *x* increases and asymptotes 0 as *x* moves further away from 0 in the negative
    direction. This function is used in classification tasks as it provides close
    to a binary output and is not a member of class (0) or is a member of the class
    (1).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[*图 5.5*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor115)中看到，sigmoid函数的输出随着*x*的增大渐近于1，而当*x*在负方向远离0时，输出渐近于0。这个函数常用于分类任务，因为它提供接近二进制的输出，表示是否属于类（0）或类（1）。
- en: Rectified Linear Unit (ReLU)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）
- en: The rectified linear unit is a very useful activation function that's commonly
    used at intermediary stages of neural networks. Simply put, the value 0 is assigned
    to values less than 0, and the value is returned for greater than 0.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元是一个非常有用的激活函数，通常在神经网络的中间阶段使用。简单来说，输入小于0时输出为0，大于0时输出为输入值本身。
- en: '![Figure 5.6: Output of ReLU](img/C12626_05_06.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6：ReLU的输出](img/C12626_05_06.jpg)'
- en: 'Figure 5.6: Output of ReLU'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.6：ReLU的输出
- en: 'Exercise 18: Modeling the Neurons of an Artificial Neural Network'
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 18：模拟人工神经网络的神经元
- en: 'In this exercise, we will practically introduce a programmatic representation
    of the neuron in NumPy using the sigmoid function. We will keep the inputs fixed
    and adjust the tunable weights to investigate the effect on the neuron. Interestingly,
    this model is also very close to the supervised learning method of logistic regression.
    Perform the following steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过使用sigmoid函数，实际介绍神经元在NumPy中的编程表示。我们将固定输入并调整可调权重，以研究其对神经元的影响。有趣的是，这个模型也非常接近于逻辑回归的监督学习方法。请执行以下步骤：
- en: 'Import the `numpy` and matplotlib packages:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`和matplotlib包：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Configure matplotlib to enable the use of Latex to render mathematical symbols
    in the images:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置matplotlib以启用使用Latex渲染图像中的数学符号：
- en: '[PRE1]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the `sigmoid` function as a Python function:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`sigmoid`函数定义为Python函数：
- en: '[PRE2]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Here, we''re using the sigmoid function. You could also use the ReLU function.
    The ReLU activation function, while being powerful in artificial neural networks,
    is easy to define. It simply needs to return the input value if greater than 0;
    otherwise, it returns 0:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用的是sigmoid函数。你也可以使用ReLU函数。尽管ReLU激活函数在人工神经网络中非常强大，但它的定义非常简单。它只需要在输入大于0时返回输入值，否则返回0：
- en: '`def relu(x):`'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`def relu(x):`'
- en: '`return np.max(0, x)`'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`return np.max(0, x)`'
- en: 'Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be 100 numbers linearly spaced between `-5` and
    `5`. Set `theta= 1`:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经元的输入（`x`）和可调权重（`theta`）。在这个示例中，输入（`x`）将是100个在`-5`和`5`之间线性分布的数字。设置`theta
    = 1`：
- en: '[PRE3]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A section of the output is as follows:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '![Figure 5.7: Printing the inputs](img/C12626_05_07.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.7：打印输入](img/C12626_05_07.jpg)'
- en: 'Figure 5.7: Printing the inputs'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：打印输入
- en: 'Compute the outputs (`y`) of the neuron:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算神经元的输出（`y`）：
- en: '[PRE4]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Plot the output of the neuron versus the input:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制神经元的输出与输入的关系图：
- en: '[PRE5]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.8: Plot of neurons versus inputs](img/C12626_05_08.jpg)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.8：神经元与输入的关系图](img/C12626_05_08.jpg)'
- en: 'Figure 5.8: Plot of neurons versus inputs'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：神经元与输入的关系图
- en: 'Set the tunable parameter, `theta`, to `5`, and recompute and store the output
    of the neuron:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可调参数 `theta` 设置为 5，重新计算并存储神经元的输出：
- en: '[PRE6]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Change the tunable parameter, `theta`, to `0.2`, and recompute and store the
    output of the neuron:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可调参数 `theta` 更改为 `0.2`，然后重新计算并存储神经元的输出：
- en: '[PRE7]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, `theta = 0.2`) on one graph:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一图表上绘制神经元的三条不同输出曲线（`theta = 1`，`theta = 5`，`theta = 0.2`）：
- en: '[PRE8]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 5.9: Output curves of neurons](img/C12626_05_09.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9: 神经元的输出曲线](img/C12626_05_09.jpg)'
- en: 'Figure 5.9: Output curves of neurons'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.9: 神经元的输出曲线'
- en: In this exercise, we modeled the basic building block of an artificial neural
    network with a sigmoid activation function. We can see that using the sigmoid
    function increases the steepness of the gradient and means that only small values
    of x will push the output to either close to 1 or 0\. Similarly, reducing `theta`
    reduces the sensitivity of the neuron to non-zero values and results in much extreme
    input values being required to push the result of the output to either 0 or 1,
    tuning the output of the neuron.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们使用了一个具有 Sigmoid 激活函数的人工神经网络基本构建模块。我们可以看到，使用 Sigmoid 函数会增加梯度的陡峭度，这意味着只有较小的
    x 值才能将输出推向接近 1 或 0。同样，减小 `theta` 会降低神经元对非零值的敏感度，导致需要更极端的输入值才能将输出推向 0 或 1，从而调节神经元的输出。
- en: 'Activity 8: Modeling Neurons with a ReLU Activation Function'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 8：使用 ReLU 激活函数建模神经元
- en: 'In this activity, we will investigate the ReLU activation function and the
    effect tunable weights have in modifying the output of ReLU units:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将研究 ReLU 激活函数以及可调权重对修改 ReLU 单元输出的影响：
- en: Import `numpy` and matplotlib.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `numpy` 和 matplotlib。
- en: Define the ReLU activation function as a Python function.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ReLU 激活函数定义为一个 Python 函数。
- en: Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be 100 numbers linearly spaced between `-5` and
    `5`. Set `theta = 1`.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经元的输入（`x`）和可调权重（`theta`）。在这个示例中，输入（`x`）将是100个在线性间隔内从`-5`到`5`的数字。设置`theta
    = 1`。
- en: Compute the output (`y`).
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出（`y`）。
- en: Plot the output of the neuron versus the input.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制神经元的输出与输入的关系图。
- en: Now, set `theta = 5`, and recompute and store the output of the neuron.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将 `theta` 设置为 5，重新计算并存储神经元的输出。
- en: Now, set `theta = 0.2`, and recompute and store the output of the neuron.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将 `theta` 设置为 `0.2`，然后重新计算并存储神经元的输出。
- en: Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, and `theta = 0.2`) on one graph.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一图表上绘制神经元的三条不同输出曲线（`theta = 1`，`theta = 5`，和 `theta = 0.2`）。
- en: 'By the end of this activity, you will have developed a range of response curves
    for the ReLU activated neuron. You will also be able to describe the effect of
    changing the value of theta on the output of the neuron. The output will look
    as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动结束时，您将为 ReLU 激活神经元开发一系列响应曲线。您还将能够描述改变 `theta` 值对神经元输出的影响。输出将如下所示：
- en: '![Figure 5.10: Expected output curves](img/C12626_05_10.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10: 预期的输出曲线](img/C12626_05_10.jpg)'
- en: 'Figure 5.10: Expected output curves'
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.10: 预期的输出曲线'
- en: Note
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 333.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第 333 页找到。
- en: 'Neural Networks: Architecture Definition'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络：架构定义
- en: Individual neurons aren't particularly useful in isolation; they provide an
    activation function and a means of tuning the output, but a single neuron would
    have an limited learning ability. Neurons become much more powerful when many
    of them are combined and connected together in a network structure. By using a
    number of different neurons and combining the outputs of individual neurons, more
    complex relationships can be established and more powerful learning algorithms
    can be built. In this section, we will briefly discuss the structure of a neural
    network and implement a simple neural network using the Keras machine learning
    framework ([https://keras.io/](https://keras.io/)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元在孤立状态下并不是特别有用；它提供一个激活函数和调节输出的手段，但单个神经元的学习能力有限。当多个神经元被组合并连接成网络结构时，它们的功能会更强大。通过使用多个不同的神经元并结合各个神经元的输出，可以建立更复杂的关系并构建更强大的学习算法。在本节中，我们将简要讨论神经网络的结构，并使用
    Keras 机器学习框架([https://keras.io/](https://keras.io/)) 实现一个简单的神经网络。
- en: '![Figure 5.11: Simplified representation of a neural network](img/C12626_05_11.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11: 神经网络的简化表示](img/C12626_05_11.jpg)'
- en: 'Figure 5.11: Simplified representation of a neural network'
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Figure 5.11* illustrates the structure of a two-layered, fully-connected neural
    network. One of the first observations we can make is that there is a lot of information
    contained within this structure, with a high degree of connectivity as represented
    by the arrows that point to and from each of the nodes. Working from the left-hand
    side of the image, we can see the input values to the neural network, as represented
    by the (*x*) values. In this example, we have *m* input values per sample, and
    only the first sample is being fed into the network, hence, values from ![A close
    up of a stool'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12626_05_Formula_01.png) to ![A close
    up of a sign
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12626_05_Formula_02.png). These values
    are then multiplied by the corresponding weights of the first layer of the neural
    network (![](img/C12626_05_Formula_03.png)) before being passed into the activation
    function of the corresponding neuron. This is known as a **feedforward** neural
    network. The notation used in *Figure 5.11* to identify the weights is ![A close
    up of a logo
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Description automatically generated](img/C12626_05_Formula_04.png), where *i*
    is the layer the weight belongs to, *j* is the input node number (starting with
    1 at the top), and *k* is the node in the subsequent layer that the weight feeds
    into to.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the inter-connectivity between the outputs of layer 1 (also known
    as the **hidden layer**) and the inputs to the output layer, we can see that there
    is a large number of trainable parameters (weights) that can be used to map the
    input to the desired output. The network of *Figure 5.11* represents an *n* class
    neural network classifier, where the output for each of the *n* nodes represents
    the probability of the input belonging to the corresponding class.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Each layer is able to use a different activation function as described by ![](img/C12626_05_Formula_05.png)
    and ![](img/C12626_05_Formula_06.png), thus allowing different activation functions
    to be mixed, in which the first layer could use ReLU, the second could use tanh,
    and the third could use sigmoid, for example. The final output is calculated by
    taking the sum of the product of the output of the previous layer with the corresponding
    weights.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider the output of the first node of layer 1, it can be calculated
    by multiplying the inputs by the corresponding weights, adding the result, and
    passing it through the activation function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_05_12.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Calculating the output of the last node'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we increase the number of layers between the input and output of the network,
    we increase the depth of the network. An increase in the depth is also an increase
    in the number of trainable parameters, as well as the complexity of the relationships
    within the data, as described by the network. It is, typically, harder to train
    networks with increased depth because the types of features selected for the input
    become more critical. Additionally, as we add more neurons to each layer, we increase
    the height of the neural network. By adding more neurons, the ability of the network
    to describe the dataset increases as we add more trainable parameters. If too
    many neurons are added, the network can memorize the dataset but fails to generalize
    new samples. The trick in constructing neural networks is to find the balance
    between sufficient complexity to be able to describe the relationships within
    the data and not be so complicated as to memorize the training samples.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随着输入和输出之间的层数增加，我们增加了网络的深度。深度的增加也意味着可训练参数的数量增多，同时数据中描述关系的复杂性增加。通常，随着深度的增加，训练网络会变得更加困难，因为选择用于输入的特征变得更加关键。此外，随着我们向每一层添加更多的神经元，我们也增加了神经网络的高度。通过增加更多的神经元，网络描述数据集的能力增强，同时可训练的参数也增多。如果添加了过多的神经元，网络可能会记住数据集中的样本，但无法对新样本进行泛化。构建神经网络的关键在于找到一个平衡点，使得模型既有足够的复杂性来描述数据中的关系，又不会复杂到只会记忆训练样本。
- en: 'Exercise 19: Defining a Keras Model'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习19：定义Keras模型
- en: In this exercise, we will define a neural network architecture (similar to *Figure
    5.11*) using the Keras machine learning framework to classify images for the CIFAR-10
    dataset. As each input image is 32 x 32 pixels in size, the input vector will
    comprise 32*32 = 1,024 values. With 10 individual classes in CIFAR-10, the output
    of the neural network will be composed of 10 individual values, with each value
    representing the probability of the input data belonging to the corresponding
    class.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用Keras机器学习框架定义一个神经网络架构（类似于*图5.11*），用于对CIFAR-10数据集中的图像进行分类。由于每个输入图像的大小为32
    x 32像素，输入向量将包含32*32 = 1,024个值。CIFAR-10有10个不同的类别，因此神经网络的输出将由10个独立的值组成，每个值表示输入数据属于相应类别的概率。
- en: 'For this exercise, we will require the Keras machine learning framework. Keras
    is a high-level neural network API that is used on top of an existing library,
    such as TensorFlow or Theano. Keras makes it easy to switch between lower-level
    frameworks because the high-level interface it provides remains the same irrespective
    of the underlying library. In this book, we will be using TensorFlow as the underlying
    library. If you have yet to install Keras and TensorFlow, do so using `conda`:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将需要Keras机器学习框架。Keras是一个高层神经网络API，通常用于现有库之上，如TensorFlow或Theano。Keras使得在底层框架之间切换变得更加容易，因为它提供的高层接口在不同的底层库中保持一致。在本书中，我们将使用TensorFlow作为底层库。如果你还没有安装Keras和TensorFlow，请使用`conda`安装：
- en: '[PRE9]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, you can install it using `pip`:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你也可以使用`pip`安装它：
- en: '[PRE10]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will require the `Sequential` and `Dense` classes from `keras.models` and
    `keras.layers`, respectively. Import these classes:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要从`keras.models`和`keras.layers`中导入`Sequential`和`Dense`类。导入这些类：
- en: '[PRE11]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As described earlier, the input layer will receive 1,024 values. The second
    layer (Layer 1) will have 500 units and, because the network is to classify one
    of 10 different classes, the output layer will have 10 units. In Keras, a model
    is defined by passing an ordered list of layers to the `Sequential` model class.
    This example uses the `Dense` layer class, which is a fully-connected neural network
    layer. The first layer will use a ReLU activation function, while the output will
    use the `softmax` function to determine the probability of each class. Define
    the model:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，输入层将接收1,024个值。第二层（层1）将包含500个单元，并且因为网络需要分类10个不同的类别，所以输出层将包含10个单元。在Keras中，模型是通过将有序的层列表传递给`Sequential`模型类来定义的。此示例使用了`Dense`层类，它是一个全连接神经网络层。第一层将使用ReLU激活函数，而输出层将使用`softmax`函数来确定每个类别的概率。定义模型：
- en: '[PRE12]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With the model defined, we can use the `summary` method to confirm the structure
    and the number of trainable parameters (or weights) within the model:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义了模型之后，我们可以使用`summary`方法来确认模型的结构以及其中可训练的参数（或权重）的数量：
- en: '[PRE13]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.13: Structure and count of trainable parameters in the model](img/C12626_05_13.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: Structure and count of trainable parameters in the model'
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This table summarizes the structure of the neural network. We can see that
    there are the two layers that we specified, with 500 units in the first layer
    and 10 output units in the second layer. The `Param #` column tells us how many
    trainable weights are available in that specific layer. The table also tells us
    that there are 517,510 trainable weights in total within the network.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we created a neural network model in Keras that contains a
    network of over 500,000 weights that can be used to classify the images of CIFAR-10\.
    In the next section, we will train the model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural Networks: Training'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the neural network model defined, we can begin the training process; at
    this stage, we will be training the model in a supervised fashion to develop some
    familiarity with the Keras framework before moving on to training autoencoders.
    Supervised learning models are trained by providing the model with both the input
    information as well as the known output; the goal of training is to construct
    a network that takes the input information and returns the known output using
    only the parameters of the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: In a supervised classification example such as CIFAR-10, the input information
    is an image and the known output is the class that the image belongs to. During
    training, for each sample prediction, the errors in the feedforward network predictions
    are calculated using a specified error function. Each of the weights within the
    model is then tuned in an attempt to reduce the error. This tuning process is
    known as **back-propagation** because the error is propagated backward through
    the network from the output to the start of the network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'During back-propagation, each trainable weight is adjusted in proportion to
    its contribution to the overall error multiplied by a value known as the **learning
    rate**, which controls the rate of change in the trainable weights. Looking at
    *Figure 5.14*, we can see that increasing the value of the learning rate can increase
    the speed at which the error is reduced, but risks not converging on a minimum
    error as we step over the values. A learning rate that''s too small may lead to
    us running out of patience or simply not having sufficient time to find the global
    minimum. Thus, finding the correct learning rate is a trial and error process,
    though starting with a larger learning rate and reducing it can often be a productive
    method. The following figure represents the selection of the learning rate:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14: Selecting the correct learning rate (one epoch is one learning
    step)](img/C12626_05_14.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Selecting the correct learning rate (one epoch is one learning
    step)'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Training is repeated until the error in the predictions stop reducing or the
    developer runs out of patience waiting for a result. In order to complete the
    training process, we first need to make some design decisions, the first being
    the most appropriate error function. There are a range of error functions available
    for use, from a simple mean squared difference to more complex options. Categorical
    cross entropy (which is used in the following exercise) is a very useful error
    function for classifying more than one class.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: With the error function defined, we need to choose the method of updating the
    trainable parameters using the error function. One of the most memory-efficient
    and effective update methods is stochastic gradient descent (SGD); there are a
    number of variants of SGD, all of which involve adjusting each of the weights
    in accordance with their individual contribution to the calculated error. The
    final training design decision to be made is the performance metric by which the
    model is evaluated and the best architecture selected; in a classification problem,
    this may be the classification accuracy of the model or perhaps the model that
    produces the lowest error score in a regression problem. These comparisons are
    generally made using a method of cross-validation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 20: Training a Keras Neural Network Model'
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Thankfully, we don''t need to worry about manually programming the components
    of the neural network, such as backpropagation, because the Keras framework manages
    this for us. In this exercise, we will use Keras to train a neural network to
    classify a small subset of the CIFAR-10 dataset using the model architecture defined
    in the previous exercise. As with all machine learning problems, the first and
    the most important step is to understand as much as possible about the dataset,
    and this will be the initial focus of the exercise:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` and `batches.meta` files from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, `matplotlib` and the `Sequential` class from `keras.models`,
    and import `Dense` from `keras.layers`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the sample of the CIFAR-10 dataset that is provided with the accompanying
    source code in the `data_batch_1` file:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The data is loaded as a dictionary. Display the keys of the dictionary:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that the keys are stored as binary strings as denoted by `b''`. We are
    interested in the contents of data and labels. Let''s look at labels first:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_15.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.15: Displaying the labels'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can see that the labels are a list of values 0 – 9, indicating which class
    each sample belongs to. Now, look at the contents of the `data` key:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.16: Content of the data key](img/C12626_05_16.jpg)'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.16: Content of the data key'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The data key provides a NumPy array with all the image data stored within the
    array. What is the shape of the image data?
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that we have 1000 samples, but each sample is a single dimension
    of 3,072 samples. Aren''t the images supposed to be 32 x 32 pixels? Yes, they
    are, but because the images are color or RGB images, they contain three channels
    (red, green, and blue), which means the images are 32 x 32 x 3\. They are also
    flattened, providing 3,072 length vectors. So, we can reshape the array and then
    visualize a sample of images. According to the CIFAR-10 documentation, the first
    1,024 samples are red, the second 1,024 are green, and the third 1,024 are blue:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Display the first 12 images, along with their labels:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.17: The first 12 images](img/C12626_05_17.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.17: The first 12 images'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'What is the actual meaning of the labels? To find out, load the `batches.meta`
    file:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_18.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.18: Meaning of the labels'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Decode the binary strings to get the actual labels:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_19.jpg)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.19: Printing the actual labels'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the labels for the first 12 images:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.20: Labels of the first 12 images](img/C12626_05_20.jpg)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.20: Labels of the first 12 images'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we need to prepare the data for training the model. The first step is to
    prepare the output. Currently, the output is a list of numbers 0 – 9, but we need
    each sample to be represented as a vector of 10 units as per the previous model.
    The encoded output will be a NumPy array with a shape of 10000 x 10:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Display the one hot encoding values for the first 12 samples:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.21: One hot encoding values for first 12 samples](img/C12626_05_21.jpg)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.21: One hot encoding values for first 12 samples'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model has 1,024 inputs because it expects a 32 x 32 grayscale image. Take
    the average of the three channels for each image to convert it to RGB:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Display the first 12 images again:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.22: Displaying the first 12 images again.](img/C12626_05_22.jpg)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.22: Displaying the first 12 images again.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, scale the images to be between 0 and 1, which is required for all
    inputs to a neural network. As the maximum value in an image is 255, we will simply
    divide by 255:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We also need the images to be in the shape 10,000 x 1,024:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Redefine the model with the same architecture as *Exercise 19*, *Defining a
    Keras Model*:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we can train the model in Keras. We first need to compile the method to
    specify the training parameters. We will be using categorical cross-entropy, with
    stochastic gradient descent and a performance metric of classification accuracy:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Train the model using back-propagation for 100 epochs and the `fit` method
    of the model:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.23: Training the model](img/C12626_05_23.jpg)'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.23: Training the model'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We achieved approximately 90% classification accuracy for the 1,000 samples
    using this network. Examine the predictions made for the first 12 samples again:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.24: Printing the predictions](img/C12626_05_24.jpg)'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.24: Printing the predictions'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can use the `argmax` method to determine the most likely class for each
    sample:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Compare with the labels:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The network made one error in these samples, that is, it classified the second
    last samples as a 2 (bird) instead of a 4 (deer). Congratulations! You have just
    successfully trained a neural network model in Keras. Complete the next activity
    to further reinforce your skills in training neural networks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: MNIST Neural Network'
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, you will train a neural network to identify images in the
    MNIST dataset and reinforce your skills in training neural networks. This activity
    forms the basis of many neural network architectures in different classification
    problems, particularly in computer vision. From object detection and identification
    to classification, this general structure is used in a variety of applications.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'These steps will help you complete the activity:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Import `pickle`, `numpy`, `matplotlib`, and the `Sequential` and `Dense` classes
    from Keras.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `mnist.pkl` file that contains the first 10,000 images and the corresponding
    labels from the MNIST dataset that are available in the accompanying source code.
    The MNIST dataset is a series of 28 x 28 grayscale images of handwritten digits,
    0 through 9\. Extract the images and labels.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the `mnist.pkl` file at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09).
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot the first 10 samples along with the corresponding labels.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the labels using one hot encoding.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a neural network model in Keras that accepts the prepared images and
    has a hidden layer of 600 units with a ReLU activation function and an output
    of the same number of units as classes. The output layer uses a `softmax` activation
    function.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the model using multiclass cross-entropy, stochastic gradient descent,
    and an accuracy performance metric.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model. How many epochs are required to achieve at least 95% classification
    accuracy on the training data?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By completing this activity, you have trained a simple neural network to identify
    handwritten digits 0 through 9\. You have also developed a general framework for
    building neural networks for classification problems. With this framework, you
    can extend upon and modify the network for a range of other tasks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 335.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we are comfortable developing supervised neural network models in
    Keras, we can return our attention to unsupervised learning and the main subject
    of this chapter—autoencoders. Autoencoders are a specifically designed neural
    network architecture that aims to compress the input information into lower dimensional
    space in an efficient yet descriptive manner. Autoencoder networks can be decomposed
    into two individual sub-networks or stages: an **encoding** stage and a **decoding**
    stage. The first, or encoding, stage takes the input information and compresses
    it through a subsequent layer that has fewer units than the size of the input
    sample. The latter stage, that is, the decoding stage, then expands the compressed
    form of the image and aims to return the compressed data to its original form.
    As such, the inputs and desired outputs of the network are the same; the network
    takes, say, an image in the CIFAR-10 dataset and tries to return the same image.
    This network architecture is shown in *Figure 5.25*; in this image, we can see
    that the encoding stage of the autoencoder reduces the number of neurons to represent
    the information, while the decoding stage takes the compressed format and returns
    it to its original state. The use of the decoding stage helps to ensure that the
    encoder has correctly represented the information, because the compressed representation
    is all that is provided to restore the image in its original state. We will now
    work through a simplified autoencoder model using the CIFAR-10 dataset:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25: Simple autoencoder network architecture](img/C12626_05_25.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.25: Simple autoencoder network architecture'
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 21: Simple Autoencoder'
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will construct a simple autoencoder for the sample of the
    CIFAR-10 dataset, compressing the information stored within the images for later
    use.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Load the data:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define the autoencoder model. As we need access to the output of the encoder
    stage, we will need to define the model using a slightly different method to that
    previously used. Define an input layer of `1024` units:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define a subsequent `Dense` layer of `256` units (a compression ratio of 1024/256
    = 4) and a ReLU activation function as the encoding stage. Note that we have assigned
    the layer to a variable and passed the previous layer to a call method for the
    class:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define a subsequent decoder layer using the sigmoid function as an activation
    function and the same shape as the input layer. The sigmoid function has been
    selected because the input values to the network are only between 0 and 1:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Compile the autoencoder using a binary cross-entropy loss function and adadelta
    gradient descent:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Note
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '`adadelta` is a more sophisticated version of stochastic gradient descent where
    the learning rate is adjusted on the basis of a window of recent gradient updates.
    Compared to the other methods of modifying the learning rate, this prevents the
    gradient of very old epochs from influencing the learning rate.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.26: Training the model](img/C12626_05_26.jpg)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.26: Training the model'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Reshape the encoder output to 16 x 16 (16 x 16 = 256) pixels and multiply by
    255:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Reshape the output of the decoder to 32 x 32 and multiply by 255:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Reshape the original images:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output is as follows:'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12626_05_27.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.27: Output of simple autoencoder'
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In *Figure 5.27*, we can see three rows of images. The first row is the original
    grayscale image, the second row is the corresponding autoencoder output for the
    original image, and finally, the third row is the reconstruction of the original
    image from the encoded input. We can see that the decoded images in the third
    row contain information about the basic shape of the image; we can see the main
    body of the frog and the deer, as well as the outline of the trucks and cars in
    the sample. Given that we only trained the model for 100 samples, this exercise
    would also benefit from an increase in the number of training epochs to further
    improve the performance of both the encoder and decoder. Now that we have the
    output of the autoencoder stage trained, we can use it as the feature vector for
    other unsupervised algorithms, such as K-means or K nearest neighbors.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Simple MNIST Autoencoder'
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, you will create an autoencoder network for the MNIST dataset
    contained within the accompanying source code. An autoencoder network such as
    the one built in this activity can be an extremely useful in the pre-processing
    stage of unsupervised learning. The encoded information produced by the network
    can be used in clustering or segmentation analysis, such as image-based web searches:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Import `pickle`, `numpy`, and `matplotlib`, and the `Model`, `Input`, and `Dense`
    classes from Keras.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the images from the supplied sample of the MNIST dataset that is provided
    with the accompanying source code (`mnist.pkl`).
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `mnist.pklP-code` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10).
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a simple autoencoder network that reduces the image size to 10 x 10
    after the encoding stage.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the encoder model.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and store the output of the encoding stage for the first five samples.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by
    255.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and store the output of the decoding stage for the first five samples.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the output of the decoder to 28 x 28 and multiply by 255.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the original image, the encoder output, and the decoder.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In completing this activity, you will have successfully trained an autoencoder
    network that extracts the critical information from the dataset, preparing it
    for later processing. The output will be similar to the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_05_28.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.28: Expected plot of original image, the encoder output, and the decoder'
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 338.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 22: Multi-Layer Autoencoder'
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will construct a multi-layer autoencoder for the sample
    of the CIFAR-10 dataset, compressing the information stored within the images
    for later use:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Load the data:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Define the multi-layer autoencoder model. We will use the same shape input
    as the simple autoencoder model:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We will add another layer before the 256 autoencoder stage, this time with
    512 neurons:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Using the same size autoencoder as the previous exercise, but the input to
    the layer is the `hidden_encoding` layer this time:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Add a decoding hidden layer:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Use the same output stage as in the previous exercise, this time connected
    to the hidden decoding stage:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output is as follows:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.29: Training the model](img/C12626_05_29.jpg)'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.29: Training the model'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by
    255:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Reshape the output of the decoder to 28 x 28 and multiply by 255:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Plot the original image, the encoder output, and the decoder:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output is as follows:'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.30: Output of multi-layer autoencoder](img/C12626_05_30.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.30: Output of multi-layer autoencoder'
  id: totrans-349
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By looking at the error score produced by both the simple and multilayer autoencoders
    and by comparing *Figure 5.27* and *Figure 5.30*, we can see that there is little
    difference between the output of the two encoder structures. The middle row of
    both figures show that the features learned by the two models are, in fact, different.
    There are a number of options we can use to improve both of these models, such
    as training for more epochs, using a different number of units or neurons in the
    layers, or using varying numbers of layers. This exercise was constructed to demonstrate
    how to build and use an autoencoder, but optimization is often a process of systematic
    trial and error. We encourage you to adjust some of the parameters of the model
    and investigate the different results for yourself.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In constructing all of our previous neural network models, you would have noticed
    that we removed all the color information when converting the image to grayscale,
    and then flattened each image into a single vector of length 1,024\. In doing
    so, we essentially threw out a lot of information that may be of use to us. The
    colors in the images may be specific to the class or objects in the image; additionally,
    we lost a lot of our spatial information about the image, for example, the position
    of the trailer in the truck image relative to the cab or the legs of the deer
    relative to the head. Convolutional neural networks do not suffer from this information
    loss. This is because rather than using a flat structure of trainable parameters,
    they store the weights in a grid or matrix, which means that each group of parameters
    can have many layers in their structure. By organizing the weights in a grid,
    we prevent the loss of spatial information because the weights are applied in
    a sliding fashion across the image. Also, by having many layers, we can retain
    the color channels associated with the image.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'In developing convolutional neural-network-based autoencoders, the MaxPooling2D
    and Upsampling2D layers are very important. The MaxPooling 2D layer downsamples
    or reduces the size of an input matrix in two dimensions by selecting the maximum
    value within a window of the input. Say we had a 2 x 2 matrix, where three cells
    have a value of 1 and one single cell has a value of 2:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31: Demonstration of sample matrix](img/C12626_05_31.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.31: Demonstration of sample matrix'
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If provided to the MaxPooling2D layer, this matrix would return a single value
    of 2, thus reducing the size of the input in both directions by one half.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: The UpSampling2D layer has the opposite effect as that of the MaxPooling2D layer,
    increasing the size of the input rather than reducing it. The upsampling process
    repeats the rows and columns of the data, thus doubling the size of the input
    matrix.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 23: Convolutional Autoencoder'
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will develop a convolutional neural-network-based autoencoder
    and compare the performance to the previous fully-connected neural network autoencoder:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Load the data:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As we are using a convolutional network, we can use the images with only rescaling:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define the convolutional autoencoder model. We will use the same shape input
    as an image:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Add a convolutional stage with 32 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using the same padding, which means the output
    has the same length as the input image:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Add a max pooling layer to the encoder with a 2 x 2 kernel. `MaxPooling` looks
    at all the values in an image, scanning through with a 2 x 2 matrix. The maximum
    value in each 2 x 2 area is returned, thus reducing the size of the encoded layer
    by a half:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Add a decoding convolutional layer (this layer should be identical to the previous
    convolutional layer):'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now we need to return the image to its original size, for which we will upsample
    by the same size as `MaxPooling2D`:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Add the final convolutional stage using three layers for the RGB channels of
    the images:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Display the structure of the model:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Note that we have far fewer trainable parameters as compared to the previous
    autoencoder examples. This has been a specific design decision to ensure that
    the example runs on a wide variety of hardware. Convolutional networks typically
    require a lot more processing power and often special hardware such as Graphical
    Processing Units (GPUs).
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 20 epochs, because convolutional networks take
    a lot longer to compute:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.32: Training the model](img/C12626_05_32.jpg)'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.32: Training the model'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the error was already less than in the previous autoencoder exercise
    after the second epoch, suggesting a better encoding/decoding model. This reduced
    error can be mostly attributed to the fact that the convolutional neural network
    did not discard a lot of data, and the encoded images are 16 x 16 x 32, which
    is significantly larger than the previous 16 x 16 size. Additionally, we have
    not compressed the images per se as they now contain fewer pixels (16 x 16 x 32
    = 8,192), but with more depth (32 x 32 x 3,072) than before. This information
    has been rearranged to allow more effective encoding/decoding processes.
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Each encoded image has a shape of 16 x 16 x 32 due to the number of filters
    selected for the convolutional stage. As such, we cannot visualize them without
    modification. We will reshape them to be 256 x 32 in size for visualization:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Get the output of the decoder for the first five images:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Plot the original image, the mean encoder output, and the decoder:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The output is as follows:'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.33: The original image, the encoder output, and the decoder](img/C12626_05_33.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.33: The original image, the encoder output, and the decoder'
  id: totrans-406
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 11: MNIST Convolutional Autoencoder'
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will reinforce our knowledge of convolutional autoencoders
    using the MNIST dataset. Convolutional autoencoders typically achieve significantly
    improved performance when working with image-based datasets of a reasonable size.
    This is particularly useful when using autoencoders to generate artificial image
    samples:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `mnist.pkl` file, which contains the first 10,000 images and corresponding
    labels from the MNIST dataset, which are available in the accompanying source
    code.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can download the `mnist.pkl` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11).
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rescale the images to have values between 0 and 1.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to reshape the images to add a single depth channel for use with convolutional
    stages. Reshape the images to have a shape of 28 x 28 x 1.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define an input layer. We will use the same shape input as an image.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a convolutional stage, with 16 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using same padding, which means the output has
    the same length as the input image.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a max pooling layer to the encoder with a 2 x 2 kernel.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a decoding convolutional layer.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an upsampling layer.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the final convolutional stage using 1 layer as per the initial image depth.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the model by passing the first and last layers of the network to the
    `Model` class.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Display the structure of the model.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's fit the model; again, we pass the images as the training data and
    as the desired output. Train for 20 epochs as convolutional networks take a lot
    longer to compute.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and store the output of the encoding stage for the first five samples.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the encoder output for visualization, where each image is X*Y in size.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the output of the decoder for the first five images.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the decoder output to be 28 x 28 in size.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the original images back to be 28 x 28 in size.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the original image, the mean encoder output, and the decoder.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the end of this activity, you will have developed an autoencoder comprising
    convolutional layers within the neural network. Note the improvements made in
    the decoder representations. The output will be similar to the following:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12626_05_34.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: Expected original image, the encoder output, and the decoder'
  id: totrans-433
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 340.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we started with an introduction to artificial neural networks,
    how they are structured, and the processes by which they learn to complete a particular
    task. Starting with a supervised learning example, we built an artificial neural
    network classifier to identify objects within the CIFAR-10 dataset. We then progressed
    to the autoencoder architecture of neural networks and learned how we can use
    these networks to prepare a dataset for use in an unsupervised learning problem.
    Finally, we completed this investigation with autoencoders, looking at convolutional
    neural networks and the benefits these additional layers can provide. This chapter
    prepared us well for the final instalment in dimensionality reduction, as we look
    at using and visualizing the encoded data with t-distributed nearest neighbors
    (t-SNE). T-distributed nearest neighbors provides an extremely effective method
    of visualizing high-dimensional data even after applying reduction techniques
    such as PCA. T-SNE is particularly useful method for unsupervised learning.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL

<html><head></head><body>
		<div id="_idContainer015" class="Content">
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Section 1: <span lang="en-US" xml:lang="en-US">Bagging and Boosting</span></h1>
			<p><span lang="en-US" xml:lang="en-US">An XGBoost model using scikit-learn defaults opens the book after preprocessing data with pandas and building standard regression and classification models. The practical theory behind XGBoost is explored by advancing through decision trees (XGBoost base learners), random forests (bagging), and gradient boosting to compare scores and fine-tune ensemble and tree-based hyperparameters.</span></p>
			<p><span lang="en-US" xml:lang="en-US">This section comprises the following chapters:</span></p>
			<ul>
				<li><p><a href="B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a><em class="italic">, Machine Learning Landscape</em></p></li>
				<li><p><a href="B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047"><em class="italic">Chapter 2</em></a><em class="italic">, Decision Trees in Depth </em></p></li>
				<li><p><a href="B15551_03_Final_NM_ePUB.xhtml#_idTextAnchor070"><em class="italic">Chapter 3</em></a><em class="italic">, Bagging with Random Forests</em></p></li>
				<li><p><a href="B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a><em class="italic">, From Gradient Boosting to XGBoost</em></p></li>
			</ul>
		</div>
	</body></html>
<html><head></head><body>
<section id="chapter-6-modeling-with-bambi" class="level2 chapterHead" data-number="1.10">&#13;
<h1 class="chapterHead" data-number="1.10">Chapter 6<br/>&#13;
<span id="x1-1200006"/>Modeling with Bambi</h1>&#13;
<blockquote>&#13;
<p>A good tool improves the way you work. A great tool improves the way you think. – Jeff Duntemann</p>&#13;
</blockquote>&#13;
<p>In <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>, we described the basic ingredients of linear regression models and how to generalize them to better fit our needs. In this chapter, we are going to keep learning about linear models, but this time, we are going to work with Bambi [<a href="Bibliography.xhtml#XCapretto_2022">Capretto et al.</a>, <a href="Bibliography.xhtml#XCapretto_2022">2022</a>], a high-level Bayesian model-building interface written on top of PyMC. Bambi is designed to make it extremely easy to fit linear models, including hierarchical ones. We will see that Bambi’s domain is more comprehensive than just linear models.</p>&#13;
<p>We are going to learn about:</p>&#13;
<ul>&#13;
<li><p>Using Bambi to build and fit models</p></li>&#13;
<li><p>Analyzing results with Bambi</p></li>&#13;
<li><p>Polynomial regression and splines</p></li>&#13;
<li><p>Distributional models</p></li>&#13;
<li><p>Categorical predictors</p></li>&#13;
<li><p>Interactions</p></li>&#13;
<li><p>Variable selection with Kulprit</p></li>&#13;
</ul>&#13;
<p><span id="x1-120001r263"/></p>&#13;
<section id="one-syntax-to-rule-them-all" class="level3 sectionHead" data-number="1.10.1">&#13;
<h2 class="sectionHead" data-number="1.10.1">6.1 <span id="x1-1210001"/>One syntax to rule them all</h2>&#13;
<p><span id="dx1-121001"/></p>&#13;
<p>PyMC has a very simple and expressive syntax that allows us to build arbitrary models. That’s usually a blessing, but it can be a burden too. Bambi instead focuses on regression models, and this restriction leads to a more focused syntax and features, as we will see.</p>&#13;
<p>Bambi uses a Wilkinson-formula syntax similar to the one used by many R packages like nlme, lme4, and brms. Let’s assume <code>data </code>is a pandas DataFrame like the one shown in <em>Table <a href="#x1-121002r1">6.1</a></em>.</p>&#13;
<table id="TBL-11" class="tabular">&#13;
<tbody>&#13;
<tr id="TBL-11-1-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-11-1-1" class="td11" style="text-align: right; white-space: nowrap;"/>&#13;
<td id="TBL-11-1-2" class="td11" style="text-align: right; white-space: nowrap;"><em>y</em></td>&#13;
<td id="TBL-11-1-3" class="td11" style="text-align: right; white-space: nowrap;"><em>x</em></td>&#13;
<td id="TBL-11-1-4" class="td11" style="text-align: right; white-space: nowrap;"><em>z</em></td>&#13;
<td id="TBL-11-1-5" class="td11" style="text-align: center; white-space: nowrap;"><em>g</em></td>&#13;
</tr>&#13;
<tr id="TBL-11-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-11-2-1" class="td11" style="text-align: right; white-space: nowrap;">0</td>&#13;
<td id="TBL-11-2-2" class="td11" style="text-align: right; white-space: nowrap;">-0.633494</td>&#13;
<td id="TBL-11-2-3" class="td11" style="text-align: right; white-space: nowrap;">-0.196436</td>&#13;
<td id="TBL-11-2-4" class="td11" style="text-align: right; white-space: nowrap;">-0.355148</td>&#13;
<td id="TBL-11-2-5" class="td11" style="text-align: center; white-space: nowrap;">Group A</td>&#13;
</tr>&#13;
<tr id="TBL-11-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-11-3-1" class="td11" style="text-align: right; white-space: nowrap;">1</td>&#13;
<td id="TBL-11-3-2" class="td11" style="text-align: right; white-space: nowrap;">2.32684</td>&#13;
<td id="TBL-11-3-3" class="td11" style="text-align: right; white-space: nowrap;">0.0163941</td>&#13;
<td id="TBL-11-3-4" class="td11" style="text-align: right; white-space: nowrap;">-1.22847</td>&#13;
<td id="TBL-11-3-5" class="td11" style="text-align: center; white-space: nowrap;">Group B</td>&#13;
</tr>&#13;
<tr id="TBL-11-4-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-11-4-1" class="td11" style="text-align: right; white-space: nowrap;">2</td>&#13;
<td id="TBL-11-4-2" class="td11" style="text-align: right; white-space: nowrap;">0.999604</td>&#13;
<td id="TBL-11-4-3" class="td11" style="text-align: right; white-space: nowrap;">0.107602</td>&#13;
<td id="TBL-11-4-4" class="td11" style="text-align: right; white-space: nowrap;">-0.391528</td>&#13;
<td id="TBL-11-4-5" class="td11" style="text-align: center; white-space: nowrap;">Group C</td>&#13;
</tr>&#13;
<tr id="TBL-11-5-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-11-5-1" class="td11" style="text-align: right; white-space: nowrap;">3</td>&#13;
<td id="TBL-11-5-2" class="td11" style="text-align: right; white-space: nowrap;">-0.119111</td>&#13;
<td id="TBL-11-5-3" class="td11" style="text-align: right; white-space: nowrap;">0.804268</td>&#13;
<td id="TBL-11-5-4" class="td11" style="text-align: right; white-space: nowrap;">0.967253</td>&#13;
<td id="TBL-11-5-5" class="td11" style="text-align: center; white-space: nowrap;">Group A</td>&#13;
</tr>&#13;
<tr id="TBL-11-6-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-11-6-1" class="td11" style="text-align: right; white-space: nowrap;">4</td>&#13;
<td id="TBL-11-6-2" class="td11" style="text-align: right; white-space: nowrap;">2.07504</td>&#13;
<td id="TBL-11-6-3" class="td11" style="text-align: right; white-space: nowrap;">0.991417</td>&#13;
<td id="TBL-11-6-4" class="td11" style="text-align: right; white-space: nowrap;">0.590832</td>&#13;
<td id="TBL-11-6-5" class="td11" style="text-align: center; white-space: nowrap;">Group B</td>&#13;
</tr>&#13;
<tr id="TBL-11-7-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-11-7-1" class="td11" style="text-align: right; white-space: nowrap;">5</td>&#13;
<td id="TBL-11-7-2" class="td11" style="text-align: right; white-space: nowrap;">-0.412135</td>&#13;
<td id="TBL-11-7-3" class="td11" style="text-align: right; white-space: nowrap;">0.691132</td>&#13;
<td id="TBL-11-7-4" class="td11" style="text-align: right; white-space: nowrap;">-2.13044</td>&#13;
<td id="TBL-11-7-5" class="td11" style="text-align: center; white-space: nowrap;">Group C</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="IMG---Caption"><span id="x1-121002r1"/> <span id="x1-121003"/><strong>Table 6.1</strong>: A dummy pandas DataFrame</p>&#13;
<p>Using this data, we want to build a linear model that predicts <code>y </code>from <code>x</code>. Using PyMC, we would do something like the model in the following code block:</p>&#13;
<p><span id="x1-121004r1"/> <span id="x1-121005"/><strong>Code 6.1</strong></p>&#13;
<pre id="listing-69" class="source-code"><code>with pm.Model() as lm: </code>&#13;
<code>    Intercept = pm.Normal("Intercept", 0, 1) </code>&#13;
<code>    x = pm.Normal("x", 0, 1) </code>&#13;
<code>    y_sigma = pm.HalfNormal("sigma", 1) </code>&#13;
<code>    y_mean = Intercept + x * data["x"] </code>&#13;
<code>    y = pm.Normal("y", y_mean, y_sigma, observed=data["y"])</code></pre>&#13;
<p>The formula syntax used by Bambi allows us to define an equivalent model in a much more compact way:</p>&#13;
<p><span id="x1-121012r2"/> <span id="x1-121013"/><strong>Code 6.2</strong></p>&#13;
<pre id="listing-70" class="source-code"><code>a_model = bmb.Model("y ∼ x", data)</code></pre>&#13;
<p>On the left side of the tilde (<span class="cmsy-10x-x-109">∼</span>), we have the dependent variable, and on the right side, the independent variable(s). With this syntax, we are just specifying the mean (<em>μ</em> in the PyMC’s model <code>lm</code>). By default, Bambi assumes the likelihood is Gaussian; you can change this with the <code>family </code>argument. The formula syntax does not specify priors distribution, just how the dependent and independent variables are related. Bambi will automatically define (very) weakly informative priors for us. We can get more information by printing a Bambi model. If you print <code>a_model</code>, you should get something like this:</p>&#13;
<pre id="fancyvrb1" class="fancyvrb"><code>    Formula: y ~ x      Family: gaussian          Link: mu = identity  Observations: 117      Priors:  target = mu      Common-level effects          Intercept ~ Normal(mu: 0.02, sigma: 2.8414)          x ~ Normal(mu: 0.0, sigma: 3.1104)        Auxiliary parameters          sigma ~ HalfStudentT(nu: 4.0, sigma: 1.1348)</code></pre>&#13;
<p>The first line shows the formula we used to define the model, and the second line is the likelihood. The third line is the link function. Then we have the number of observations used to fit the model, and the next is telling us we are linearly modeling the parameter <code>mu </code>of the Gaussian. The latter part of the output shows the model structure: the common-level effects, in this case, the intercept (<code>Intercept</code>) and the slope (<code>x</code>), and the auxiliary parameters, i.e., all the parameters not linearly modeled, in this case, the standard deviation of the Gaussian.</p>&#13;
<p>You can override the default priors by passing a dictionary to the <code>priors</code> argument to <code>bmb.Model</code>. For instance, if we want to define a custom prior for the coefficient of the variable <code>x </code>and also for the auxiliary parameter <code>sigma</code>, we can do this:</p>&#13;
<p><span id="x1-121039r3"/> <span id="x1-121040"/><strong>Code 6.3</strong></p>&#13;
<pre id="listing-71" class="source-code"><code>priors = {"x": bmb.Prior("HalfNormal", sigma=3), </code>&#13;
<code>          "sigma": bmb.Prior("Gamma",  mu=1, sigma=2), </code>&#13;
<code>          } </code>&#13;
<code>a_model_wcp = bmb.Model("y ∼ x", data, priors=priors)</code></pre>&#13;
<p>As a result, we will get the following model specifications:</p>&#13;
<pre id="fancyvrb2" class="fancyvrb"><code>       Formula: y ~ x          Family: gaussian            Link: mu = identity    Observations: 117          Priors:      target = mu          Common-level effects              Intercept ~ Normal(mu: 0.02, sigma: 2.837)              x ~ HalfNormal(sigma: 3.0)            Auxiliary parameters              sigma ~ Gamma(mu: 1.0, sigma: 2.0)</code></pre>&#13;
<p>If you want to omit the intercept from your model, you can do it like this:</p>&#13;
<p><span id="x1-121069r4"/> <span id="x1-121070"/><span class="id">Code 6.4: </span><span class="content"/></p>&#13;
<pre id="listing-72" class="source-code"><code>no_intercept_model = bmb.Model("y ∼ 0 + x", data)</code></pre>&#13;
<p>Or even like this:</p>&#13;
<p><span id="x1-121072r5"/> <span id="x1-121073"/><span class="id">Code 6.5: </span><span class="content"/></p>&#13;
<pre id="listing-73" class="source-code"><code>no_intercept_model = bmb.Model("y ∼ -1 + x", data)</code></pre>&#13;
<p>Print the model <code>no_intercept_model</code>, and you will see that the intercept is not there anymore.</p>&#13;
<p>What if we want to include more variables? We can do it like this:</p>&#13;
<p><span id="x1-121075r6"/> <span id="x1-121076"/><strong>Code 6.6</strong></p>&#13;
<pre id="listing-74" class="source-code"><code>model_2 = bmb.Model("y ∼ x + z", data)</code></pre>&#13;
<p>We can also include group-level effects (hierarchies); for example, if we want to use the variable <code>g </code>to partially pool the estimates of <code>x</code>, we can do it like this:</p>&#13;
<p><span id="x1-121078r7"/> <span id="x1-121079"/><strong>Code 6.7</strong></p>&#13;
<pre id="listing-75" class="source-code"><code>model_h = bmb.Model("y ∼ x + z + (x | g)", data)</code></pre>&#13;
<p>We can see a visual representation of this model in <em>Figure <a href="#x1-121081r1">6.1</a></em>. Notice the variables <code>1|g_offset </code>and <code>x|g_offset</code>. By default, Bambi fits a noncentered hierarchical model; you can change this with the argument <code>noncentered</code>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file169.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-121081r1"/><strong>Figure 6.1</strong>: A visual representation of <code>model_h</code></p>&#13;
<p>The formula syntax is very simple, but it is also very powerful. We have just scratched the surface of what we can do with it. Instead of describing the syntax all at once, we are going to show it by example. If you want to go deeper, you can check Formulae documentation <a href="https://bambinos.github.io/formulae/" class="url">https://bambinos.github.io/formulae/</a>. formulae is the Python package in charge of parsing Wilkinson’s formulas for Bambi. <span id="x1-121082r266"/></p>&#13;
</section>&#13;
<section id="the-bikes-model-bambis-version" class="level3 sectionHead" data-number="1.10.2">&#13;
<h2 class="sectionHead" data-number="1.10.2">6.2 <span id="x1-1220002"/>The bikes model, Bambi’s version</h2>&#13;
<p><span id="dx1-122001"/></p>&#13;
<p>The first model we are going to use to illustrate how to use Bambi is the bikes model from <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>. We can load the data with:</p>&#13;
<p><span id="x1-122002r8"/> <span id="x1-122003"/><strong>Code 6.8</strong></p>&#13;
<pre id="listing-76" class="source-code"><code>bikes = pd.read_csv("data/bikes.csv")</code></pre>&#13;
<p>Now we can build and fit the model:</p>&#13;
<p><span id="x1-122005r9"/> <span id="x1-122006"/><strong>Code 6.9</strong></p>&#13;
<pre id="listing-77" class="source-code"><code>model_t = bmb.Model("rented ∼ temperature", bikes, family="negativebinomial") </code>&#13;
<code>idata_t = model_t.fit()</code></pre>&#13;
<p><em>Figure <a href="#x1-122009r2">6.2</a></em> shows a visual representation of the model. If you want to visually inspect the priors, you can use <code>model.plot_priors()</code>:</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file170.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-122009r2"/><strong>Figure 6.2</strong>: A visual representation of the bikes model, computed with the command <code>model.graph()</code></p>&#13;
<p>Let’s now plot the posterior mean and the posterior predictive distribution (predictions). Omitting some details needed to make the plots look nice, the code to do this is:</p>&#13;
<p><span id="x1-122010r10"/> <span id="x1-122011"/><strong>Code 6.10</strong></p>&#13;
<pre id="listing-78" class="source-code"><code>_, axes = plt.subplots(1, 2, sharey=True, figsize=(12, 4)) </code>&#13;
<code>bmb.interpret.plot_predictions(model_t, idata_t, </code>&#13;
<code>                               "temperature", ax=axes[0]) </code>&#13;
<code>bmb.interpret.plot_predictions(model_t, idata_t, </code>&#13;
<code>                               "temperature", pps=True, ax=axes[1])</code></pre>&#13;
<p><code>plot_predictions </code>is a function from Bambi’s submodule <code>interpret</code>. This function helps to analyze regression models by plotting conditional adjusted predictions, visualizing how a parameter of the (conditional) response distribution varies as a function of (some) interpolated explanatory variables. We <span id="dx1-122017"/>can see the result of this code in <em>Figure <a href="#x1-122018r3">6.3</a></em>. The left panel shows the posterior mean and the 94% HDI, while the right panel shows the posterior predictive distribution (the predicted distribution of the rented bikes). Notice that the uncertainty for the predictions is much larger than the uncertainty for the mean (<code>pps=False</code>). This is because the posterior predictive distribution accounts for the uncertainty in the model parameters and the uncertainty in the data, whereas the posterior distribution of the mean only accounts for the uncertainty in the intercept and slope parameters.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file171.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-122018r3"/><strong>Figure 6.3</strong>: Posterior mean and posterior predictive distribution for the bikes model</p>&#13;
<p>The utility of <code>plot_cap </code>becomes more evident when we have more than one explanatory variable. For example, let’s fit a model that uses both temperature and humidity to predict the number of rented bikes:</p>&#13;
<p><span id="x1-122019r11"/> <span id="x1-122020"/><strong>Code 6.11</strong></p>&#13;
<pre id="listing-79" class="source-code"><code>model_th = bmb.Model("rented ∼ temperature + humidity", bikes, </code>&#13;
<code>                     family="negativebinomial") </code>&#13;
<code> </code>&#13;
<code>idata_th = model_th.fit() </code>&#13;
<code> </code>&#13;
<code>bmb.interpret.plot_predictions(model_th, idata_th, ["temperature", "humidity"], </code>&#13;
<code>                               subplot_kwargs={"group":None, "panel":"humidity"})</code></pre>&#13;
<p>In <em>Figure <a href="#x1-122029r4">6.4</a></em>, we can see five panels, each one showing the change of the number of rented bikes with the <span id="dx1-122028"/>temperature at different values of <code>humidity</code>. As you can see, the number of rented bikes increases with temperature, but the slope is larger when humidity is low.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file172.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-122029r4"/><strong>Figure 6.4</strong>: Posterior mean for the bikes model with temperature and humidity</p>&#13;
<p><span id="x1-122030r282"/></p>&#13;
</section>&#13;
<section id="polynomial-regression" class="level3 sectionHead" data-number="1.10.3">&#13;
<h2 class="sectionHead" data-number="1.10.3">6.3 <span id="x1-1230003"/>Polynomial regression</h2>&#13;
<p>One way to fit curves <span id="dx1-123001"/>using a linear regression model is by building a polynomial, like this:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file173.jpg" class="math-display" alt="μ = 𝛽0 + 𝛽1x + 𝛽2x2 + 𝛽3x3 + 𝛽4x4...𝛽mxm "/>&#13;
</div>&#13;
<p>We call <em>m</em> the degree of the polynomial.</p>&#13;
<p>There are two important things to notice. First, polynomial regression is still linear regression; the linearity refers to the coefficients (the <em>β</em>s), not the variables (the <em>x</em>s). The second thing to note is that we are creating new variables out of thin air. The only observed variable is <code>x</code>, the rest are just powers of <code>x</code>. Creating new variables from observed ones is a perfectly valid ”trick” when doing regression; sometimes the transformation can be motivated or justified by theory (like taking the square root of the length of babies), but sometimes it is just a way to fit a curve. The intuition with polynomials is that for a given value of <code>x</code>, the higher the degree of the polynomial, the more flexible the curve can be. A polynomial of degree 1 is a line, a polynomial of degree 2 is a curve that can go up or down, a polynomial of degree 3 is a curve that can go up and then down (or the other way around), and so on. Notice I said ”can” because if we have a polynomial of degree 3, like <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>x</em> + <em>β</em><sub>2</sub><em>x</em><sup>2</sup> + <em>β</em><sub>3</sub><em>x</em><sup>3</sup>, but the coefficients <em>β</em><sub>2</sub> and <em>β</em><sub>3</sub> are 0 (or practically 0), then the curve will be a line.</p>&#13;
<p>There are two ways to define a polynomial regression with Bambi. We can write the <em>raw</em> polynomials:</p>&#13;
<p><span id="x1-123002r12"/> <span id="x1-123003"/><strong>Code 6.12</strong></p>&#13;
<pre id="listing-80" class="source-code"><code>"y ∼ x + I(x ** 2) + I(x ** 3) + I(x ** 4)"</code></pre>&#13;
<p>Here, we use the identity function <code>I() </code>to make it clear that we want to elevate <em>x</em> to some power. We need this because the <code>** </code>operator has a special meaning for Bambi. If we use this syntax, we are telling Bambi to model the mean of <em>y</em> as <em>α</em> + <em>β</em><sub>0</sub><em>x</em> + <em>β</em><sub>0</sub><em>x</em><sup>2</sup> + <em>β</em><sub>0</sub><em>x</em><sup>3</sup> + <em>β</em><sub>0</sub><em>x</em><sup>4</sup>.</p>&#13;
<p>Alternatively, we can write:</p>&#13;
<p><span id="x1-123005r13"/> <span id="x1-123006"/><strong>Code 6.13</strong></p>&#13;
<pre id="listing-81" class="source-code"><code>"y ∼ poly(x, 4)"</code></pre>&#13;
<p>This will also generate a polynomial of degree 4, but the polynomial terms will be orthogonal to each other, meaning the correlation between the terms is reduced. Without going into the mathematical details, this has at least two important consequences with respect to the <em>standard</em> polynomial. First, the estimation can be numerically more stable, and second, the interpretation of the coefficients is different. In <code>standard </code>polynomial regression, the coefficients can be <span id="dx1-123008"/>difficult to interpret, as changing the value of one coefficient affects the entire polynomial. In contrast, orthogonal polynomials allow you to interpret the effect of each term more clearly, as they are independent of each other. While the interpretation of the coefficients is different, other results remain the same. For instance, you should get the same predictions with both approaches.</p>&#13;
<p>Let’s build an orthogonal polynomial of degree 4 to model the bike data. For this example, we are going to use the <code>hour </code>variable:</p>&#13;
<p><span id="x1-123009r14"/> <span id="x1-123010"/><strong>Code 6.14</strong></p>&#13;
<pre id="listing-82" class="source-code"><code>model_poly4 = bmb.Model("rented ∼ poly(temperature, degree=4)", bikes, </code>&#13;
<code>                       family="negativebinomial") </code>&#13;
<code>idata_poly4 = model_poly4.fit()</code></pre>&#13;
<p><em>Figure <a href="#x1-123014r5">6.5</a></em> shows the posterior mean and the posterior predictive distribution. On the first row, you will see a polynomial of degree 1, which is equivalent to a linear model. On the second row, you will see a polynomial of degree 4.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file174.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-123014r5"/><strong>Figure 6.5</strong>: Posterior mean and posterior predictive distribution for the bikes model with temperature and humidity</p>&#13;
<p>One problem with polynomials is that they act <em>globally</em>. When we apply a polynomial of degree <em>m</em>, we are saying that the relationship between the independent and dependent variables is of degree <em>m</em> for the entire dataset. This can be problematic when different regions of our data need different levels of flexibility. This could lead, for example, to curves that are too flexible. As the degree increases, the fit becomes more sensitive to the removal of points, or equivalently to the addition of future data. In other words, as the degree increases, the model <span id="dx1-123015"/>becomes more prone to overfitting. Bayesian polynomial regression usually suffers less of this ”excess” of flexibility because we usually don’t use flat priors, and we do not compute a single set of coefficients, but the entire posterior distribution. Still, we can do better. <span id="x1-123016r291"/></p>&#13;
</section>&#13;
<section id="splines" class="level3 sectionHead" data-number="1.10.4">&#13;
<h2 class="sectionHead" data-number="1.10.4">6.4 <span id="x1-1240004"/>Splines</h2>&#13;
<p>A general way to <span id="dx1-124001"/>write very flexible models is to apply functions <em>B</em><sub><em>m</em></sub> to <em>X</em><sub><em>m</em></sub> and then multiply them by coefficients <em>β</em><sub><em>m</em></sub>:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file175.jpg" class="math-display" alt="μ = 𝛽0 + 𝛽1B1 (X1) + 𝛽2B2(X2 )+ ⋅⋅⋅+ 𝛽mBm (Xm ) "/>&#13;
</div>&#13;
<p>We are free to pick <em>B</em><sub><em>m</em></sub> as we wish; for instance, we can pick polynomials. But we can also pick other functions. A popular choice is to use B-splines; we are not going to discuss their definition, but we can think of them as a way to create smooth curves in such a way that we get flexibility, as with polynomials, but less prone to overfitting. We achieve this by using piecewise polynomials, that is, polynomials that are restricted to affect only a portion of the data. <em>Figure <a href="#x1-124002r6">6.6</a></em> shows three examples of piecewise polynomials of increasing degrees. The dotted vertical lines show the ”knots,” which are the points used to restrict the regions, the dashed gray line represents the function we want to approximate, and the black lines are the piecewise polynomials.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file176.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-124002r6"/><strong>Figure 6.6</strong>: Piecewise polynomials of increasing degrees</p>&#13;
<p><em>Figure <a href="#x1-124004r7">6.7</a></em> shows examples of <span id="dx1-124003"/>splines of degree 1 and 3; the dots at the bottom represent the knots, and the dashed lines are the B-splines. At the top, we have all the B-splines with equal weight; we use grayscale to highlight that we have many B-splines. On the bottom panel, each B-spline is weighted differently (we multiply them by <em>β</em><sub><em>m</em></sub> coefficients); if we sum the weighted B-splines, we get the black line as a result. This black line is what we usually call ”the spline.” We can use Bayesian statistics to find the proper weights for the B-splines.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file177.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-124004r7"/><strong>Figure 6.7</strong>: B-splines of degree 1 (piecewise linear) or 3 (cubic spline) and the resulting splines.</p>&#13;
<p>We can use B-splines with Bambi by using the <code>bs </code>function. For example, let’s fit a spline of degree 3 to the bikes data:</p>&#13;
<p><span id="x1-124005r15"/> <span id="x1-124006"/><strong>Code 6.15</strong></p>&#13;
<pre id="listing-83" class="source-code"><code>num_knots = 6 </code>&#13;
<code>knots = np.linspace(0, 23, num_knots+2)[1:-1] </code>&#13;
<code>model_spline = bmb.Model("rented ∼ bs(hour, degree=3, knots=knots)", bikes, </code>&#13;
<code>                         family="negativebinomial") </code>&#13;
<code>idata_spline = model_spline.fit()</code></pre>&#13;
<p><em>Figure <a href="#x1-124013r8">6.8</a></em> shows that the <span id="dx1-124012"/>number of rental bikes is at the lowest number late at night. There is then an increase, probably as people wake up and go to work or school, or do other activities. We have a first peak at around hour 8, then a slight decline, followed by the second peak at around hour 18, probably because people commute back home, after which there is a steady decline. Notice that the curve is not very smooth; this is not because of the spline but because of the data. We have measurements at discrete times (every hour).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file178.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-124013r8"/><strong>Figure 6.8</strong>: Posterior mean for the spline model</p>&#13;
<p>When working with splines, one important decision we must make is determining the number and placement of knots. This can be a somewhat daunting task since the optimal number of knots and their spacing are not immediately apparent. A useful suggestion for determining the knot locations is to consider placing them based on quantiles rather than uniformly – something like <code>knots = np.quantile(bikes.hour, np.linspace(0, 1, num_knots))</code>. By doing so, we would position more knots in areas where we have a greater amount of data, while placing <span id="dx1-124014"/>fewer knots in areas with less data. This results in a more adaptable approximation that effectively captures the variability in regions with a higher density of data points. Additionally, we may want to fit splines with varying numbers of knots and positions and then evaluate the results, using tools such as LOO, as we saw in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>. <span id="x1-124015r298"/></p>&#13;
</section>&#13;
<section id="distributional-models" class="level3 sectionHead" data-number="1.10.5">&#13;
<h2 class="sectionHead" data-number="1.10.5">6.5 <span id="x1-1250005"/>Distributional models</h2>&#13;
<p><span id="dx1-125001"/> <span id="dx1-125002"/></p>&#13;
<p>We saw earlier that we can use linear models for parameters other than the mean (or location parameter). For example, we can use a linear model for the mean and a linear model for the standard deviation of a Gaussian distribution. These models are usually called distributional models. The syntax for distributional models is very similar; we just need to add a line for the auxiliary parameters we want to model. For instance, <em>σ</em> for a Gaussian, or <em>α</em> for a NegativeBinomial.</p>&#13;
<p>Let’s now reproduce an example from <em>Chapter <a href="CH04.xhtml#x1-760004">4</a></em>, the babies example:</p>&#13;
<p><span id="x1-125003r16"/> <span id="x1-125004"/><strong>Code 6.16</strong></p>&#13;
<pre id="listing-84" class="source-code"><code>formula = bmb.Formula( </code>&#13;
<code>    "length ∼ np.sqrt(month)", </code>&#13;
<code>    "sigma ∼ month" </code>&#13;
<code>) </code>&#13;
<code>model_dis = bmb.Model(formula, babies) </code>&#13;
<code>idata_dis = model_dis.fit()</code></pre>&#13;
<p><em>Figure <a href="#x1-125011r9">6.9</a></em> shows the posterior distribution values of sigma for <code>model_dis</code> (varying sigma) and for a model with constant sigma. We can see that when sigma is allowed to vary, we obtain values below and above the estimate for a constant sigma, meaning that we are both under- and over-estimating this parameter when we don’t allow it to change.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file179.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-125011r9"/><strong>Figure 6.9</strong>: Constant and varying sigma for the babies data</p>&#13;
<p><em>Figure <a href="#x1-125012r10">6.10</a></em> shows the posterior fit for <code>model_dis</code>. Notice that the model can capture the increase in variability as the babies grow. This figure is very similar to <em>Figure <a href="CH04.xhtml#x1-88022r13">4.13</a></em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file180.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-125012r10"/><strong>Figure 6.10</strong>: Posterior fit for <code>model_dis</code></p>&#13;
<p>When working with PyMC, we saw that sampling from the posterior predictive distribution, at not observed values, requires us to define the ”Xs” as <code>Mutable data </code>and then update the variable before computing the posterior predictive distribution. With Bambi, this is not necessary. We can use the <code>predict</code> method to predict new values by passing the new values to the <code>data</code> argument. For example, let’s predict the length of a baby at 0.5 months (15 days):</p>&#13;
<p><span id="x1-125013r17"/> <span id="x1-125014"/><strong>Code 6.17</strong></p>&#13;
<pre id="listing-85" class="source-code"><code>model_dis.predict(idata_dis, kind="pps", data=pd.DataFrame({"month":[0.5]}))</code></pre>&#13;
<p><span id="x1-125016r301"/></p>&#13;
</section>&#13;
<section id="categorical-predictors" class="level3 sectionHead" data-number="1.10.6">&#13;
<h2 class="sectionHead" data-number="1.10.6">6.6 <span id="x1-1260006"/>Categorical predictors</h2>&#13;
<p>A categorical variable represents distinct groups or categories that can take on a limited set of values from those categories. These values are typically labels or names that don’t possess numerical <span id="dx1-126001"/>significance on their own. Some examples are:</p>&#13;
<ul>&#13;
<li><p>Political affiliation: conservative, liberal, or progressive.</p></li>&#13;
<li><p>Sex: female or male.</p></li>&#13;
<li><p>Customer satisfaction level: very unsatisfied, unsatisfied, neutral, satisfied, or very satisfied.</p></li>&#13;
</ul>&#13;
<p>Linear regression models can easily accommodate categorical variables; we just need to encode the categories as numbers. There are a few options to do so. Bambi can easily handle the details for us. The devil is in the interpretation of the results, as we will explore in the next two sections. <span id="x1-126002r243"/></p>&#13;
<section id="categorical-penguins" class="level4 subsectionHead" data-number="1.10.6.1">&#13;
<h3 class="subsectionHead" data-number="1.10.6.1">6.6.1 <span id="x1-1270001"/>Categorical penguins</h3>&#13;
<p>For the current example, we are going to use the palmerpenguins dataset, <a href="Bibliography.xhtml#Xpenguins">Horst et al.</a> [<a href="Bibliography.xhtml#Xpenguins">2020</a>], which contains 344 <span id="dx1-127001"/>observations of 8 variables. For the moment, we are interested in modeling the mass of the penguins as a function of the length of their bills. It is expected that the mass of the penguins increases as the bill length increases. The novelty of this example is that we are going to consider the categorical variable, <code>species</code>. In this dataset, we have 3 categories or levels for the species variable, namely, Adelie, Chinstrap, and Gentoo. <em>Figure <a href="#x1-127002r11">6.11</a></em> shows a scatter plot for the variables we want to model.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file181.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-127002r11"/><strong>Figure 6.11</strong>: Bill length vs mass for 3 species of penguins</p>&#13;
<p>Let’s load the data <span id="dx1-127003"/>and fit the model:</p>&#13;
<p><span id="x1-127004r18"/> <span id="x1-127005"/><strong>Code 6.18</strong></p>&#13;
<pre id="listing-86" class="source-code"><code>penguins = pd.read_csv("data/penguins.csv").dropna() </code>&#13;
<code> </code>&#13;
<code>model_p = bmb.Model("body_mass ∼ bill_length + species", data=penguins) </code>&#13;
<code>idata_p = model_p.fit()</code></pre>&#13;
<p>Notice that there is no special syntax to define Bambi’s model for categorical variables. Bambi can detect and handle them automatically.</p>&#13;
<p><em>Figure <a href="#x1-127011r12">6.12</a></em> show a forest plot for <code>model_p</code>. Notice something unexpected? There are no posterior values for Adelie. This is no mistake. By default, Bambi encodes categorical variables with N levels (3 species) as N-1 dummy variables (2 species). Thus the coefficients species-Chinstrap and species-Gentoo are modeled as deflections from the baseline model:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file182.jpg" class="math-display" alt="mass = 𝛽0 + 𝛽1bill length "/>&#13;
</div>&#13;
<p>To make this more clear, let’s check a couple of plots. We can read <em>Figure <a href="#x1-127011r12">6.12</a></em> as saying that the body mass of Chinstrap is, on average, -0.89 relative to Adelie’s body mass. The same goes for Gentoo, but <span id="dx1-127010"/>this time, we have to add 0.66 to the mean of the baseline model.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file183.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-127011r12"/><strong>Figure 6.12</strong>: Forest plot from <code>model_p</code></p>&#13;
<p>You can check that these two statements are true by looking at <em>Figure <a href="#x1-127012r13">6.13</a></em>. See how the three lines are essentially parallel to each other with Adelie in the middle, Chinstrap below (-0.89), and Gentoo above (0.58).</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file184.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-127012r13"/><strong>Figure 6.13</strong>: Mean in-sample predictions from <code>model_p</code></p>&#13;
<p><span id="x1-127013r307"/></p>&#13;
</section>&#13;
<section id="relation-to-hierarchical-models" class="level4 subsectionHead" data-number="1.10.6.2">&#13;
<h3 class="subsectionHead" data-number="1.10.6.2">6.6.2 <span id="x1-1280002"/>Relation to hierarchical models</h3>&#13;
<p>In <em>Chapter <a href="CH03.xhtml#x1-670003">3</a></em>, we <span id="dx1-128001"/>discussed and contrasted pooled and hierarchical (or partially pooled) models. There we showed that it is often the case that we take advantage of the structure or hierarchies in data. Following the logic of that chapter, you could argue that Adelie, Gentoo, and Chinstrap, while being different species, are all penguins. So modeling their body masses hierarchically may be a good idea. And you would be right to think so. So what is the difference between such a model and the one we used in this section?</p>&#13;
<p>The distinguishing factor lies in the subtleties of the slope and intercept components. In the case of the latter, the slope remains the same across all three penguin species, while the intercepts can vary: <code>Intercept + 0 </code>for Adelie, <code>Intercept + species[Chinstrap] </code>for Chinstrap, and <code>Intercept + species[Gentoo] </code>for Gentoo. Thus, this model highlights the distinct intercepts while keeping the slope uniform.</p>&#13;
<p>If instead we had built the hierarchical model <code>body_mass ~(bill_length|species)</code>, we would have been asking for a partially pooled slope and intercept. And if instead we had modeled <code>body_mass ~(0 + bill_length | species)</code>, we would have been asking for a partially pooled slope and a common intercept.</p>&#13;
<p>Besides these particular models, when thinking about using a predictor as a grouping variable or as a categorical predictor, it is usually useful to ask if the variable includes all possible categories (like all days of the week, all species, and so on) or only a subgroup (some schools, or a few musical genres). If we have all possible categories, then we may prefer to model it as a categorical predictor, otherwise, as a grouping variable.</p>&#13;
<p>As we already discussed, we often create more than one model before deciding which one we like the most. The <em>best</em> model is the one that aligns with the goals of your analysis, provides meaningful insights, and accurately represents the underlying patterns in your data. It’s often a good idea to explore multiple models, compare their performance using appropriate criteria (such as those discussed in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>), and consider the practical implications of each model for your research or decision-making process. <span id="x1-128002r306"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="interactions" class="level3 sectionHead" data-number="1.10.7">&#13;
<h2 class="sectionHead" data-number="1.10.7">6.7 <span id="x1-1290007"/>Interactions</h2>&#13;
<p><span id="dx1-129001"/></p>&#13;
<p>An interaction effect, or statistical interaction, happens when the effect of an independent variable on the response changes depending on the value of another independent variable. An interaction can occur between two or more variables. Some examples are:</p>&#13;
<ul>&#13;
<li><p><strong>Education level and income impact</strong>: Higher education may have a stronger positive effect on income for one gender compared to the other, resulting in an interaction between education and gender.</p></li>&#13;
<li><p><strong>Medication efficacy and age</strong>: A drug that works better for older individuals than younger ones.</p></li>&#13;
<li><p><strong>Exercise and diet effects on weight loss</strong>: It could be that the diet’s effect on weight loss is small for people who do little or no exercise and large for people who do moderate exercise.</p></li>&#13;
<li><p><strong>Temperature and humidity for crop growth</strong>: Some crops could thrive in hot and humid conditions, while others might perform better in cooler and less humid environments.</p></li>&#13;
</ul>&#13;
<p>We have an interaction when the combined effect of two or more variables acting together is not equal to the sum of their individual effects. So we cannot model an interaction if we have a model like the following:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file185.jpg" class="math-display" alt="μ = 𝛼 + 𝛽0X0 + 𝛽1X1 "/>&#13;
</div>&#13;
<p>The most common way to model an interaction effect is by multiplying two (or more) variables. Take, for example, a model like the following:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file186.jpg" class="math-display" alt=" main terms ◜----◞◟----◝ μ = 𝛼 + 𝛽0X0 + 𝛽1X1 + 𝛽◟2X0◝X◜1◞ interaction term "/>&#13;
</div>&#13;
<p>It is common when modeling interaction effects to also include the main effect/terms.</p>&#13;
<div id="tcolobox-13" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>New predictors</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>Multiplying two variables can be seen as a trick, similar to the one we use for polynomial regression (or any transformation of a given variable). Instead of multiplying a predictor with itself, we multiply two different predictors and obtain a new one.</p>&#13;
</div>&#13;
</div>&#13;
<p>Defining an interaction between two variables is easy for a PyMC model; we just need to multiply the two predictors together and also add a coefficient. For a Bambi model, it is even easier; we use the <code>: </code>operator. To make the difference crystal clear, let’s look at an example of a model with and without interactions:</p>&#13;
<p><span id="x1-129002r19"/> <span id="x1-129003"/><strong>Code 6.19</strong></p>&#13;
<pre id="listing-87" class="source-code"><code># No interaction </code>&#13;
<code>model_noint = bmb.Model("body_mass ∼ bill_depth + bill_length", </code>&#13;
<code>                        data=penguins) </code>&#13;
<code> </code>&#13;
<code>#Interaction </code>&#13;
<code>model_int = bmb.Model("body_mass ∼ bill_depth + bill_length + </code>&#13;
<code>                      bill_depth:bill_length", </code>&#13;
<code>                      data=penguins) </code>&#13;
<code> </code>&#13;
<code>idata_noint = model_noint.fit() </code>&#13;
<code>idata_int = model_int.fit()</code></pre>&#13;
<p>We now use Bambi’s <code>plot_prediction </code>to compare how different values of <code>bill_length </code>affect <code>body_mass </code>as a function of <code>bill_depth </code>generate. <em>Figure <a href="#x1-129015r14">6.14</a></em> shows the result. We have the mean regression fit for <code>bill_depth </code>evaluated at 5 fixed values of <code>bill_length</code>. On the left, we have the result for <code>model_noint </code>(no interactions), and on the right, for <code>model_int </code>(with interactions). We can see that when we don’t have interactions, the fitted lines for <code>bill_depth </code>are parallel at different levels of <code>bill_length</code>. Instead, when we have interactions, the lines are no longer parallel, precisely because the effect of changing <code>bill_depth </code>on how much <code>body_mass </code>changes is no longer constant but modulated by the values of <code>bill_length</code>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file187.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-129015r14"/><strong>Figure 6.14</strong>: Mean in-sample predictions from <code>model_noint </code>(left) and <code>model_int </code>(right)</p>&#13;
<p>If you generate a figure like <em>Figure <a href="#x1-129015r14">6.14</a></em>, but instead of fixing <code>bill_length</code>, you decide to fix <code>bill_depth</code>, you will observe a similar behavior.</p>&#13;
<p>In the GitHub repository for this book ( <a href="https://github.com/aloctavodia/BAP3" class="url">https://github.com/aloctavodia/BAP3</a>), you are going to find the file <code>interactions.ipynb</code>. This script generates a figure in 3D, which I hope will help you build intuition about what we are doing when adding interactions. If you run it, you will see that when there are no interactions, we are fitting a 2D plane, a flat surface like a sheet of paper. But when adding interactions, you are fitting a curved surface. Compare the result of <code>interactions.ipynb </code>with <em>Figure <a href="#x1-129015r14">6.14</a></em>.</p>&#13;
<p>We have just seen visually that interpreting linear models with interactions is not as easy as interpreting linear models without them. Let’s see this mathematically.</p>&#13;
<p>Let’s assume we have a model with 2 variables, <em>X</em><sub>0</sub> and <em>X</em><sub>1</sub>, and an interaction between them:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file188.jpg" class="math-display" alt="μ = 𝛼 + 𝛽0X0 + 𝛽1X1 + 𝛽2X0X1 "/>&#13;
</div>&#13;
<p>We can rewrite this model as:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file189.jpg" class="math-display" alt="μ = 𝛼+ (◟𝛽0-+◝𝛽◜2X1-)◞X0 + 𝛽1X1 slope of X0 "/>&#13;
</div>&#13;
<p>Or even like this:</p>&#13;
<div class="math-display">&#13;
<img src="../media/file190.jpg" class="math-display" alt="μ = 𝛼+ 𝛽0X0 + (◟𝛽1 +◝𝛽◜2X0-)◞X1 slope of X1 "/>&#13;
</div>&#13;
<p>From this expression, we can see that:</p>&#13;
<ul>&#13;
<li><p>The interaction term can be understood as a linear model inside a linear model.</p></li>&#13;
<li><p>The interaction is symmetric; we can think of it as the slope of <em>X</em><sub>0</sub> as a function of <em>X</em><sub>1</sub> and at the same time as the slope of <em>X</em><sub>1</sub> as a function of <em>X</em><sub>0</sub>. This can also be seen from the interactive figure.</p></li>&#13;
<li><p>We know from before that the <em>β</em><sub>0</sub> coefficient can be interpreted as the amount of change of <em>μ</em> per unit change of <em>X</em><sub>0</sub> (that is why we call it the slope). If we add an interaction term, then this is only true at <em>X</em><sub>1</sub> = 0. Try using the interactive figure to see this by yourself. Mathematically, this is true because when <em>X</em><sub>1</sub> = 0, then <em>β</em><sub>2</sub><em>X</em><sub>1</sub> = 0, and thus the slope of <em>X</em><sub>0</sub> reduces to <em>β</em><sub>0</sub><em>X</em><sub>0</sub>. By symmetry, the same reasoning can be applied to <em>β</em><sub>1</sub>.</p></li>&#13;
</ul>&#13;
<p><span id="x1-129016r311"/></p>&#13;
</section>&#13;
<section id="interpreting-models-with-bambi" class="level3 sectionHead" data-number="1.10.8">&#13;
<h2 class="sectionHead" data-number="1.10.8">6.8 <span id="x1-1300008"/>Interpreting models with Bambi</h2>&#13;
<p>We have been using <code>bmb.interpret_plot_predictions </code>a lot in this chapter. But that’s not the only tool that <span id="dx1-130001"/>Bambi offers us to help us understand models. One of them is <code>bmb.interpret_plot_comparisons</code>. This tool helps us answer the question, ”What is the expected predictive difference when we compare two values of a given variable while keeping all the rest at constant values?”.</p>&#13;
<p>Let’s use <code>model_int </code>from the previous section, so we don’t need to fit a new model. We use the following code block to generate <em>Figure <a href="#x1-130007r15">6.15</a></em>:</p>&#13;
<p><span id="x1-130002r20"/> <span id="x1-130003"/><strong>Code 6.20</strong></p>&#13;
<pre id="listing-88" class="source-code"><code>bmb.interpret.plot_comparisons(model_int, idata_int, </code>&#13;
<code>                               contrast={"bill_depth":[1.4, 1.8]}, </code>&#13;
<code>                               conditional={"bill_length":[3.5, 4.5, 5.5]})</code></pre>&#13;
<p><em>Figure <a href="#x1-130007r15">6.15</a></em> shows that when comparing a hypothetical penguin with <code>bill_depth </code>of 1.8 against one with <code>bill_depth </code>of 1.4, the expected difference is:</p>&#13;
<ul>&#13;
<li><p>Approx 0.8 kg for a bill length of 3.5 cm</p></li>&#13;
<li><p>-0.6 kg for a bill length of 4.5 cm</p></li>&#13;
<li><p>Approx -2 kg for a bill length of 5.5 cm</p></li>&#13;
</ul>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file191.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-130007r15"/><strong>Figure 6.15</strong>: Contrast of <code>bill_depth </code>from 1.8 to 1.4 cm for 3 fixed values of <code>bill_length</code></p>&#13;
<p>If you want the information in tabular form, use the function <code>bmb.interpret.comparisons </code>and you will get a DataFrame instead of a plot.</p>&#13;
<p>Another useful function is <code>bmb.interpret_plot_slopes</code>, which can be used to compute the ”instant rate of change” or <span id="dx1-130008"/>slope at a given value. We use the following code block to generate <em>Figure <a href="#x1-130014r16">6.16</a></em>:</p>&#13;
<p><span id="x1-130009r21"/> <span id="x1-130010"/><strong>Code 6.21</strong></p>&#13;
<pre id="listing-89" class="source-code"><code>bmb.interpret.plot_slopes(model_int, idata_int, </code>&#13;
<code>                          wrt={"bill_depth":1.8}, </code>&#13;
<code>                          conditional={"bill_length":[3.5, 4.5, 5.5]},</code></pre>&#13;
<p><em>Figure <a href="#x1-130014r16">6.16</a></em> shows that the slopes at a <code>bill_depth </code>of 1.8 are:</p>&#13;
<ul>&#13;
<li><p><span class="cmsy-10x-x-109">≈ </span>2 kg/cm for a bill length of 3.5 cm</p></li>&#13;
<li><p>-1.4 kg/cm for a bill length of 4.5 cm</p></li>&#13;
<li><p><span class="cmsy-10x-x-109">≈ </span>-5 kg/cm for a bill length of 5.5 cm</p></li>&#13;
</ul>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file192.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-130014r16"/><strong>Figure 6.16</strong>: Slopes of <code>bill_depth </code>at 1.8 cm for 3 fixed values of <code>bill_length</code></p>&#13;
<p>If you want the information in tabular form, use the function <code>bmb.interpret.slopes</code> and you will get a DataFrame instead of a plot.</p>&#13;
<p>In this section, we have just scratched the surface of what we can do with the tools in the <code>bmb.interpret </code>module. This <span id="dx1-130015"/>module is a very useful feature of Bambi, especially for models with interactions and/or models with link functions other than the identity function. I highly recommend you read the Bambi documentation for more examples and details not covered here. <span id="x1-130016r314"/></p>&#13;
</section>&#13;
<section id="variable-selection" class="level3 sectionHead" data-number="1.10.9">&#13;
<h2 class="sectionHead" data-number="1.10.9">6.9 <span id="x1-1310009"/>Variable selection</h2>&#13;
<p>Variable selection refers to the process of identifying the most relevant variables in a model from a larger set of potential predictors. We perform variable selection under the assumption that only a subset of <span id="dx1-131001"/>variables have a considerable impact on the outcome of interest, while others contribute little or no additional value.</p>&#13;
<p>Arguably the ”most Bayesian thing to do” when building a model is to include all the variables that we may think of in a single model and then use the posterior from that model to make predictions or gain an understanding of the relationships of the variables. This is the ”most Bayesian” approach because we are using as much data as possible and incorporating in the posterior the uncertainty about the importance of the variables. However, being <em>more Bayesian than Bayes</em> is not always the best idea. We already saw in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em> that Bayes factors can be problematic, even when they are a direct consequence of Bayes’ theorem.</p>&#13;
<p>Performing variable selection is a good idea when:</p>&#13;
<ul>&#13;
<li><p>We need to reduce the measurement cost. For instance, in medicine, we may have the money and resources to run a pilot study and measure 30 variables for 200 patients. But we cannot do the same for thousands. Or we may be able to place a lot of sensors in an open field to better model crop gains, but we cannot extend that to the size of a country. Reducing costs is not always about money or time; when working with humans or other animals, reducing pain and discomfort is important too. For example, we may <span id="dx1-131002"/>want to predict the risk of a patient having a heart attack. We can do this by measuring a lot of variables, but we can also do it by measuring just a few variables that are less invasive.</p></li>&#13;
<li><p>We want to reduce the computational cost. This is not a problem for small and simple models, but when we have a lot of variables, a lot of data, or both, the computational cost can be prohibitive.</p></li>&#13;
<li><p>We seek a better understanding of significant correlation structures. That is, we are interested in understanding which variables provide better predictions. It is important to state that we are not talking about causality. While statistical models, in particular, GLMS, can be used to infer causality, doing so requires extra steps and assumptions. In this book, we do not discuss how to perform causal inference. For a very gentle introduction to causal inference, please see this video: <a href="https://www.youtube.com/watch?v=gV6wzTk3o1U" class="url">https://www.youtube.com/watch?v=gV6wzTk3o1U</a>. If you are more serious, you can check out the online book Causal Inference: The Mixtape by Scott Cunningham [<a href="Bibliography.xhtml#Xcunningham2021causal">Cunningham</a>, <a href="Bibliography.xhtml#Xcunningham2021causal">2021</a>] <a href="https://mixtape.scunning.com/" class="url">https://mixtape.scunning.com/</a>.</p></li>&#13;
<li><p>When we desire a model that is more resilient to changes in the data-generating distribution, we can see variable selection as a method to make the model more robust against unrepresentative data.</p></li>&#13;
</ul>&#13;
<p><span id="x1-131003r310"/></p>&#13;
<section id="projection-predictive-inference" class="level4 subsectionHead" data-number="1.10.9.1">&#13;
<h3 class="subsectionHead" data-number="1.10.9.1">6.9.1 <span id="x1-1320001"/>Projection predictive inference</h3>&#13;
<p><span id="dx1-132001"/> <span id="dx1-132002"/></p>&#13;
<p>There are many methods to perform variable selection. In this section, we will focus on one of them called projection predictive inference [<a href="Bibliography.xhtml#XPiironen2020">Piironen et al.</a>, <a href="Bibliography.xhtml#XPiironen2020">2020</a>, <a href="Bibliography.xhtml#Xmclatchie2023">McLatchie et al.</a>, <a href="Bibliography.xhtml#Xmclatchie2023">2023</a>]. The main reason we are focusing on this single method is that it has shown very good performance across a broad range of fields.</p>&#13;
<p>The main steps of projective prediction inference are:</p>&#13;
<ol>&#13;
<li><div id="x1-132004x1">&#13;
<p>Generate a reference model, i.e., a model with all the variables you think can be relevant and/or you were able to measure.</p>&#13;
</div></li>&#13;
<li><div id="x1-132006x2">&#13;
<p>Generate a set of submodels, i.e., models that only include some subset of the variables in the reference model.</p>&#13;
</div></li>&#13;
<li><div id="x1-132008x3">&#13;
<p>Project the reference model’s posterior distribution into the submodels.</p>&#13;
</div></li>&#13;
<li><div id="x1-132010x4">&#13;
<p>Pick the smallest model that makes predictions close enough to the reference model.</p>&#13;
</div></li>&#13;
</ol>&#13;
<p>When doing projection predictive inference, we only need to perform Bayesian inference once, just for the reference model. For the submodels, the posteriors are projected. Without going into the technical details, the projection consists of finding the parameters for the submodels in such a way that the predictions of the submodels are as close as possible to the predictions of the reference model. The projection can be done in a computationally efficient way so the cost of estimating a posterior is orders of magnitude cheaper than with MCMC methods. This is relevant because the total number of possible submodels explodes as we increase the number of variables in the reference model. Consider that we need to evaluate all possible combinations, without repeating variables. For instance, say we have four variables (<em>A</em>, <em>B</em>, <em>C</em>, and <em>D</em>) and we need to evaluate 7 models, namely, <em>A</em>, <em>B</em>, <em>C</em>, <em>AB</em>, <em>BC</em>, <em>AC</em>, and the reference model <em>ABC</em>. Seven does not sound like a lot, but by the time we reach 8 variables, we will need to evaluate 92 different models. See, we double the number of variables, and the number of models increases more than 10 times!</p>&#13;
<p>Of course, there are ways to reduce the total number of submodels to explore. For instance, we could use some cheap method to filter out the most promising variables and only do projection predictive inference on those. Another alternative is known as forward search; that is, we first fit as many models as the variables we have. We then select one model/variable, the one generating the closest predictions to the reference model. We then generate all the submodels with 2 variables that included the variable selected in the previous step and so on. If we do this forward procedure for a reference model with 8 variables instead of 92 different models, we will need to evaluate just 36.</p>&#13;
<p>Another aspect that is relevant to consider when doing projection predictive inference is that we only provided priors for the reference model. The submodels don’t have explicit priors; they just inherit, somehow, the priors of the reference model through the projection procedure.</p>&#13;
<p>One reason projective prediction works in practice is thanks to the use of a reference model. By fitting the submodels to the in-sample predictions made by the reference model, instead of the observed data, we are filtering out the noise in the data. This helps separate the more relevant variables from the less relevant ones. Another factor is the use of cross-validation in selecting the submodels, as discussed in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>. <span id="x1-132011r320"/></p>&#13;
</section>&#13;
<section id="projection-predictive-with-kulprit" class="level4 subsectionHead" data-number="1.10.9.2">&#13;
<h3 class="subsectionHead" data-number="1.10.9.2">6.9.2 <span id="x1-1330002"/>Projection predictive with Kulprit</h3>&#13;
<p><span id="dx1-133001"/> <span id="dx1-133002"/></p>&#13;
<p>Kulprit is a Python package for projection predictive inference. It works with Bambi, as we can pass a reference model built with it and Kulprit will do all the hard work for us. To illustrate how to use Kulprit, we are going to use the body fat dataset [<a href="Bibliography.xhtml#Xpenrose1985">Penrose et al.</a>, <a href="Bibliography.xhtml#Xpenrose1985">1985</a>]. This dataset has measurements from 251 individuals, including their age, weight, height, the circumference of the abdomen, etc. Our purpose is to predict the percentage of body fat (as estimated by the <code>siri </code>variable). Since obtaining accurate measurements of body fat is expensive and potentially annoying for patients, we want to reduce the measurements while keeping a good predictive accuracy for <code>siri</code>. The original dataset included 13 variables; to keep this example really simple, I have preselected 6.</p>&#13;
<p>The first thing we need to do is to define and fit a Bambi model, as usual. We have to be sure that we include the argument <code>idata_kwargs=’log_likelihood’:True</code>. Internally, Kulprit computes the ELPD, and as we discussed in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>, we need the log likelihood in the InferenceData object to be able to estimate the ELPD:</p>&#13;
<p><span id="x1-133003r22"/> <span id="x1-133004"/><strong>Code 6.22</strong></p>&#13;
<pre id="listing-90" class="source-code"><code>model = bmb.Model("siri ∼ age + weight + height + abdomen + thigh + wrist", </code>&#13;
<code>                  data=body) </code>&#13;
<code>idata = model.fit(idata_kwargs={'log_likelihood': True})</code></pre>&#13;
<p>After this, we are ready to use Kulprit. First, we need to call the <code>ProjectionPredictive </code>class and pass the Bambi model and the idata resulting from the fit of that model. Then we ask Kulprit to perform a search; by default, it will do a forward search:</p>&#13;
<p><span id="x1-133008r23"/> <span id="x1-133009"/><span class="id">Code 6.23: </span><span class="content"/></p>&#13;
<pre id="listing-91" class="source-code"><code>ppi = kpt.ProjectionPredictive(model, idata) </code>&#13;
<code>ppi.search()</code></pre>&#13;
<p>After the search has finished, we can ask Kulprit to compare the submodels in terms of the ELPD. The submodels will show ordered from lowest ELPD to highest, as in <em>Figure <a href="#x1-133012r17">6.17</a></em>. On the x-axis, we have the submodel size, i.e., number of variables; we start at zero because we include the intercept-only model. The dashed gray line corresponds to the ELPD for the reference model.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file193.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-133012r17"/><strong>Figure 6.17</strong>: Comparison of the submodels obtained with Kulprit. Generated with <code>ppi.plot_compare</code></p>&#13;
<p>We can see then that a submodel of size 3 is practically equivalent to the reference model. But what variables are exactly included in this and the other submodels? If we print the <code>ppi </code>object, after performing a search, we will get an ordered list of the formulas for the submodels matching the order in the plot obtained with the command <code>ppi.plot_compare</code>:</p>&#13;
<p><span id="x1-133013r24"/> <span id="x1-133014"/><strong>Code 6.24</strong></p>&#13;
<pre id="listing-92" class="source-code"><code>print(ppi)</code></pre>&#13;
<pre class="console"><code>  0 siri ~ 1&#13;
  1 siri ~ abdomen&#13;
  2 siri ~ abdomen + wrist&#13;
  3 siri ~ abdomen + wrist + height&#13;
  4 siri ~ abdomen + wrist + height + age&#13;
  5 siri ~ abdomen + wrist + height + age + weight&#13;
  6 siri ~ abdomen + wrist + height + age + weight + thigh</code></pre>&#13;
<p>Then we can see that the model of size 3 is the one including the variables <code>abdomen</code>, <code>wrist</code>, and <code>height</code>. This result tells us that if we want to choose a model with fewer variables than the reference model but with similar predictive accuracy, then this is a good choice. Depending on the context, other submodels may also be a good idea. For instance, we may argue that the difference between the submodel of sizes 2 and 3 is rather small. Thus, we may be willing to sacrifice some accuracy in favor of an even smaller model. For this example, measuring the height of patients may not be that problematic, but for other scenarios, adding a third variable could be expensive, annoying, dangerous, etc.</p>&#13;
<p>Another way to interpret <em>Figure <a href="#x1-133012r17">6.17</a></em> is by noticing how close the ELPDs are for models with size 3 or larger. It may be the case that if we repeat the analysis with a slightly different dataset, or even the same dataset but with more posterior samples, we could get a slightly different order. Thus, if we have many models of size 3 with potentially the same practical predictive accuracy, we could justify the selection of the third variable by external factors such as how easy or cheap it is to measure, or which one will be less painful for patients, etc. In summary, as with other statistical tools, results should not be taken blindly but in context; you should have the final word and the tools should help you inform your decisions.</p>&#13;
<p>OK, let’s say that we are indeed interested in the submodel of size 3 computed by Kulprit; we can get it with:</p>&#13;
<p><span id="x1-133016r25"/> <span id="x1-133017"/><strong>Code 6.25</strong></p>&#13;
<pre id="listing-93" class="source-code"><code>submodel = ppi.project(3)</code></pre>&#13;
<p>From the <code>submodel </code>object, we can then retrieve some useful information like Bambi’s model <code>submodel.model </code>or the InferenceData object <code>submodel.idata</code>.</p>&#13;
<p>One word of caution about interpreting these two objects—<code>submodel.model </code>is a Bambi model generated from a formula. Thus, its priors will be those automatically computed by Bambi. But, the posterior that Kulprit computes, which is stored in <code>submodel.idata.posterior</code>, does not come directly from this model. Instead, it is computed using projection predictive inference (not MCMC) with priors that are implicitly inherited during the projection step (not explicit priors). <em>Figure <a href="#x1-133019r18">6.18</a></em> shows such a projected posterior.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file194.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-133019r18"/><strong>Figure 6.18</strong>: Projected posterior for submodel of size 3</p>&#13;
<p>Can we trust projected posteriors? Under very general conditions this should be a valid posterior so we can trust it. It should be enough to give you an approximate idea of the values of the parameters and, of course, it is enough for variable selection. The lack of explicit priors could make the interpretation of the model more difficult, but if you only care about predictions, that should not be an issue. Of course, you can always use Bambi (or PyMC) to explicitly compute the full posterior as usual and specify the priors yourself if needed. <em>Figure <a href="#x1-133020r19">6.19</a></em> shows a forest plot for the posterior of the submodel as computed with Bambi (True) and approximated with Kulprit (Projected). Notice that there are two possible sources of differences here: the intrinsic differences between MCMC and projection predictive methods and the different priors for both models.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file195.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-133020r19"/><strong>Figure 6.19</strong>: Comparison of the posterior of the submodel (<code>siri ~abdomen + wrist + height</code>) as computed by Kulprit and the reference model as computed by Bambi; variables not shared by both models have been omitted</p>&#13;
<p>Kulprit is a very new library that will keep evolving, and users can expect numerous enhancements and refinements shortly. If Kulprit interests you, you can help with its development by reporting issues, suggesting ideas, improving the documentation, or working on its codebase at <a href="https://github.com/bambinos/kulprit" class="url">https://github.com/bambinos/kulprit</a>. <span id="x1-133021r319"/></p>&#13;
</section>&#13;
</section>&#13;
<section id="summary-5" class="level3 sectionHead" data-number="1.10.10">&#13;
<h2 class="sectionHead" data-number="1.10.10">6.10 <span id="x1-13400010"/>Summary</h2>&#13;
<p>In this chapter, we have seen how to use Bambi to fit Bayesian models as an alternative to the pure PyMC model. We start with the simplest case, a model with a single predictor, and then move to more complex models, including polynomials, splines, distributional models, models with categorical predictors, and interactions.</p>&#13;
<p>The main advantage of Bambi is that it is very easy to use; it is very similar to R’s <code>formula </code>syntax. And internally, Bambi defines weakly informative priors and handles details that can be cumbersome for complex models. The main disadvantage is that it is not as flexible as PyMC. The range of models that Bambi can handle is a small subset of those from PyMC. Still, this subset contains many of the most commonly used statistical models in both industry and academia. The strength of Bambi is not just easy model building, but easier model interpretation. Across the chapter, we have seen how to use Bambi’s <code>interpret </code>module to gain a better understanding of the models we fit. Finally, we have seen how to use Kulprit to perform projection predictive inference and perform variable selection. Projection predictive inference offers a promising approach to variable selection, and Kulprit is a promising Pythonic way of doing it. <span id="x1-134001r330"/></p>&#13;
</section>&#13;
<section id="exercises-5" class="level3 sectionHead" data-number="1.10.11">&#13;
<h2 class="sectionHead" data-number="1.10.11">6.11 <span id="x1-13500011"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-135002x1">&#13;
<p>Read the Bambi documentation ( <a href="https://bambinos.github.io/bambi/" class="url">https://bambinos.github.io/bambi/</a>) and learn how to specify custom priors.</p>&#13;
</div></li>&#13;
<li><div id="x1-135004x2">&#13;
<p>Apply what you learned in the previous point and specify a HalfNormal prior for the slope of <code>model_t</code>.</p>&#13;
</div></li>&#13;
<li><div id="x1-135006x3">&#13;
<p>Define a model like <code>model_poly4</code>, but using <code>raw </code>polynomials, compare the coefficients and the mean fit of both models.</p>&#13;
</div></li>&#13;
<li><div id="x1-135008x4">&#13;
<p>Explain in your own words what a distributional model is.</p>&#13;
</div></li>&#13;
<li><div id="x1-135010x5">&#13;
<p>Expand <code>model_spline </code>to a distributional model. Use another spline to model the <em>α</em> parameter of the NegativeBinomial family.</p>&#13;
</div></li>&#13;
<li><div id="x1-135012x6">&#13;
<p>Create a model named <code>model_p2 </code>for the <code>body_mass </code>with the predictors <code>bill_length</code>, <code>bill_depth</code>, <code>flipper_length</code>, and <code>species</code>.</p>&#13;
</div></li>&#13;
<li><div id="x1-135014x7">&#13;
<p>Use LOO to compare the model in the previous point and <code>model_p.</code></p>&#13;
</div></li>&#13;
<li><div id="x1-135016x8">&#13;
<p>Use the functions in the <code>interpret </code>module to interpret <code>model_p2</code>. Use both plots and tables.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-6" class="level3 likesectionHead" data-number="1.10.12">&#13;
<h2 class="likesectionHead" data-number="1.10.12"><span id="x1-13600011"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/></p>&#13;
<p><span id="x1-136001r265"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
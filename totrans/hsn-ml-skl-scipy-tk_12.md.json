["```py\nfrom sklearn.datasets import fetch_openml\ndata = fetch_openml(data_id=1220)\n\ndf = pd.DataFrame(\n    data['data'],\n    columns=data['feature_names']\n).astype(float)\n\ndf['target'] = pd.Series(data['target']).astype(int) \n```", "```py\ndf.sample(n=5, random_state=42)\n```", "```py\nfor feature in data['feature_names']:\n    print(\n       'Cardinality of {}: {:,}'.format(\n            feature, df[feature].value_counts().shape[0]\n        )\n    )\n```", "```py\nCardinality of impression: 99\nCardinality of ad_id: 19,228\nCardinality of advertiser_id: 6,064\nCardinality of depth: 3\nCardinality of position: 3\nCardinality of keyword_id: 19,803\nCardinality of title_id: 25,321\nCardinality of description_id: 22,381\nCardinality of user_id: 30,114\n```", "```py\nfrom sklearn.model_selection import train_test_split\nx, y = df[data['feature_names']], df['target']\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.25, random_state=42\n)\n```", "```py\n          pip install -U imbalanced-learn\n\n```", "```py\nfrom imblearn.metrics import geometric_mean_score\ngeometric_mean_score(y_true, y_pred)\n```", "```py\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomTreesEmbedding\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import precision_score, recall_score\nfrom imblearn.metrics import geometric_mean_score\n\ndef predict_and_evalutate(x_train, y_train, x_test, y_test, sample_weight=None, title='Unweighted'):\n\n    clf = Pipeline(\n        [\n            ('Embedder', RandomTreesEmbedding(n_estimators=10, max_leaf_nodes=20, random_state=42)), \n            ('Scaler', MaxAbsScaler()),\n            ('Classifier', LogisticRegression(solver='saga', max_iter=1000, random_state=42))\n        ]\n    )\n    clf.fit(x_train, y_train, Classifier__sample_weight=sample_weight)\n    y_test_pred = clf.predict(x_test)\n\n    print(\n        'Precision: {:.02%}, Recall: {:.02%}; G-mean: {:.02%} @ {}'.format(\n            precision_score(y_test, y_test_pred),\n            recall_score(y_test, y_test_pred),\n            geometric_mean_score(y_test, y_test_pred),\n            title\n        )\n    )\n\n    return clf\n```", "```py\nclf = predict_and_evalutate(x_train, y_train, x_test, y_test) \n```", "```py\n(1 - y_train.mean()) / y_train.mean() \n```", "```py\nsample_weight = (1 * (y_train == 0)) + (5 * (y_train == 1))\nclf = predict_and_evalutate(\n    x_train, y_train, x_test, y_test, \n    sample_weight=sample_weight\n)\n```", "```py\ndef calculate_feature_coeff(clf):\n    return pd.DataFrame(\n        {\n            'Features': [\n                f'EmbFeature{e}' \n                for e in range(len(clf[-1].coef_[0]))\n            ] + ['Intercept'],\n            'Coeff': list(\n                clf[-1].coef_[0]\n            ) + [clf[-1].intercept_[0]]\n        }\n\n    ).set_index('Features').tail(10)\n```", "```py\ndf_coef_list = []\nweight_options = [1, 2, 5]\n\nfor w in weight_options:\n\n    print(f'\\nMinority Class (Positive Class) Weight = Weight x {w}')\n    sample_weight = (1 * (y_train == 0)) + (w * (y_train == 1))\n    clf = predict_and_evalutate(\n        x_train, y_train, x_test, y_test, \n        sample_weight=sample_weight\n    )\n    df_coef = calculate_feature_coeff(clf)\n    df_coef = df_coef.rename(columns={'Coeff': f'Coeff [w={w}]'})\n    df_coef_list.append(df_coef)\n```", "```py\npd.concat(df_coef_list, axis=1).round(2).style.bar(\n    subset=[f'Coeff [w={w}]' for w in weight_options], \n    color='#999',\n    align='zero'\n)\n```", "```py\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc_curve(y, y_proba, ax, label):\n    fpr, tpr, thr = roc_curve(y, y_proba)\n    auc_value = auc(fpr, tpr)\n    pd.DataFrame(\n        {\n            'FPR': fpr,\n            'TPR': tpr\n        }\n    ).set_index('FPR')['TPR'].plot(\n        label=label + f'; AUC = {auc_value:.3f}',\n        kind='line',\n        xlim=(0,1),\n        ylim=(0,1),\n        color='k',\n        ax=ax\n    )\n    return (fpr, tpr, auc_value)\n```", "```py\nfrom sklearn.metrics import roc_curve, auc\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)\n\nax.plot(\n    [0, 1], [0, 1], \n    linestyle='--', \n    lw=2, color='k',\n    label='Chance', alpha=.8\n)\n\nfor w in weight_options:\n\n    sample_weight = (1 * (y_train == 0)) + (w * (y_train == 1))\n\n    clf = Pipeline(\n        [\n            ('Embedder', RandomTreesEmbedding(n_estimators=20, max_leaf_nodes=20, random_state=42)), \n            ('Scaler', MaxAbsScaler()),\n            ('Classifier', LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42))\n        ]\n    )\n    clf.fit(x_train, y_train, Classifier__sample_weight=sample_weight)\n    y_test_pred_proba = clf.predict_proba(x_test)[:,1]\n\n    plot_roc_curve(\n        y_test, y_test_pred_proba, \n        label=f'\\nMinority Class Weight = Weight x {w}',\n        ax=ax\n    ) \n\nax.set_title('Receiver Operating Characteristic (ROC)')\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\n\nax.legend(ncol=1, fontsize='large', shadow=True)\n\nfig.show() \n```", "```py\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nx_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train)\n```", "```py\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(sampling_strategy=0.5)\nx_train_resampled, y_train_resampled = rus.fit_resample(x_train, y_train)\n```", "```py\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\nenn = EditedNearestNeighbours(n_neighbors=5)\nx_train_resampled, y_train_resampled = enn.fit_resample(x_train, y_train)\n```", "```py\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomTreesEmbedding\nfrom sklearn.pipeline import Pipeline\n\ndef sample_and_predict(x_train, y_train, x_test, y_test, sampler=None):\n\n    if sampler:\n        x_train, y_train = sampler.fit_resample(x_train, y_train)\n\n    clf = Pipeline(\n        [\n            ('Embedder', RandomTreesEmbedding(n_estimators=10, max_leaf_nodes=20, random_state=42)), \n            ('Scaler', MaxAbsScaler()),\n            ('Classifier', LogisticRegression(solver='saga', max_iter=1000, random_state=42))\n        ]\n    )\n    clf.fit(x_train, y_train)\n    y_test_pred_proba = clf.predict_proba(x_test)[:,1]\n\n    return y_test, y_test_pred_proba\n```", "```py\nfrom sklearn.metrics import roc_curve, auc\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)\n\n# Original Data\n\ny_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=None)\nplot_roc_curve(\n    y_test, y_test_pred_proba, \n    label='Original Data',\n    ax=ax\n) \n\n# RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=42)\ny_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=rus)\n\nplot_roc_curve(\n    y_test, y_test_pred_proba, \n    label='RandomUnderSampler',\n    ax=ax\n) \n\n# EditedNearestNeighbours\n\nnc = EditedNearestNeighbours(n_neighbors=5)\ny_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=nc)\n\nplot_roc_curve(\n    y_test, y_test_pred_proba, \n    label='EditedNearestNeighbours',\n    ax=ax\n) \n\nax.legend(ncol=1, fontsize='large', shadow=True)\n\nfig.show()\n```", "```py\nfrom sklearn.metrics import roc_curve, auc\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)\n\n# RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\ny_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=ros)\nplot_roc_curve(\n    y_test, y_test_pred_proba, \n    label='RandomOverSampler',\n    ax=ax\n)\n\n# SMOTE \n\nsmote = SMOTE(random_state=42)\ny_test, y_test_pred_proba = sample_and_predict(x_train, y_train, x_test, y_test, sampler=smote)\nplot_roc_curve(\n    y_test, y_test_pred_proba, \n    label='SMOTE',\n    ax=ax\n) \n\nax.legend(ncol=1, fontsize='large', shadow=True)\n\nfig.show()\n```", "```py\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8), sharey=False)\n\n# BalancedBaggingClassifier\n\nclf = BalancedBaggingClassifier(n_estimators=500, n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\ny_test_pred_proba = clf.predict_proba(x_test)[:,1]\n\nplot_roc_curve(\n    y_test, y_test_pred_proba, \n    label='Balanced Bagging Classifier',\n    ax=ax\n) \n\n# BalancedRandomForestClassifier\n\nclf = BalancedRandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\ny_test_pred_proba = clf.predict_proba(x_test)[:,1]\n\nplot_roc_curve(\n    y_test, y_test_pred_proba, \n    label='Balanced Random Forest Classifier',\n    ax=ax\n) \n\nfig.show()\n```", "```py\ndf_engineers = pd.DataFrame(\n    {\n        'IQ': [110, 120, 124, 123, 112, 114],\n        'Gender': ['M', 'F', 'M', 'F', 'M', 'F'],\n        'Is Hired? (True Label)': [0, 1, 1, 1, 1, 0],\n        'Is Hired? (Predicted Label)': [1, 0, 1, 1, 1, 0],\n    }\n)\n```", "```py\ndef equal_opportunity_score(df, true_label, predicted_label, feature_name, feature_value):\n    opportunity_to_value = df[\n        (df[true_label] == 1) & (df[feature_name] == feature_value)\n    ][predicted_label].mean() / df[\n        (df[true_label] == 1) & (df[feature_name] != feature_value)\n    ][predicted_label].mean()\n    opportunity_to_other_values = 1 / opportunity_to_value\n    better_opportunity_to_value = opportunity_to_value > opportunity_to_other_values\n    return {\n        'Score': min(opportunity_to_value, opportunity_to_other_values),\n        f'Better Opportunity to {feature_value}': better_opportunity_to_value\n    }\n```", "```py\nequal_opportunity_score(\n    df=df_engineers, \n    true_label='Is Hired? (True Label)', \n    predicted_label='Is Hired? (Predicted Label)', \n    feature_name='Gender',\n    feature_value='F'\n)\n```"]
<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Collaborative Filters</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we mathematically defined the collaborative filtering problem and gained an understanding of various data mining techniques that we assumed would be useful in solving this problem. </p>
<p><span>The time has finally come for us to put our skills to the test. In the first section, we will construct a well-defined framework that will allow us to build and test our collaborative filtering models effortlessly. This framework will consist of the data, the evaluation metric, and a corresponding function to compute that metric for a given model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will be required to have Python installed on a system. Finally, to use the Git repository of this book, the user needs to install Git.</p>
<p>The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Hands-On-Recommendation-Systems-with-Python">https://github.com/PacktPublishing/Hands-On-Recommendation-Systems-with-Python</a>.</p>
<p>Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2mFmgRo">http://bit.ly/2mFmgRo</a><a href="http://bit.ly/2mFmgRo">.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The framework</h1>
                </header>
            
            <article>
                
<p>Just like the knowledge-based and content-based recommenders, we will build our collaborative filtering models in the context of movies. Since collaborative filtering demands data on user behavior, we will be using a different dataset known as MovieLens.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MovieLens dataset</h1>
                </header>
            
            <article>
                
<p><span>The MovieLens dataset is made publicly available by GroupLens Research, a computer science lab at the University of Minnesota. It is one of the most popular benchmark datasets used to test the potency of various collaborative filtering models and is usually available in most recommender libraries and packages:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-338 image-border" src="assets/dc0f89d7-6e9c-4790-a6c0-06725f2dbe8a.png" style="width:74.17em;height:67.92em;"/></div>
<p>MovieLens gives us user ratings on a variety of movies and is available in various sizes. The full version consists of more than 26,000,000 ratings applied to 45,000 movies by 270,000 users. However, for the sake of fast computation, we will be using the much smaller 100,000 dataset, which contains 100,000 ratings applied by 1,000 users to 1,700 movies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading the dataset</h1>
                </header>
            
            <article>
                
<p>Without any further ado, let's go ahead and download the 100,000 dataset. The dataset available on the official GroupLens site does not provide us with user demographic information anymore. Therefore, we will use a legacy dataset made available on Kaggle by Prajit Datta.</p>
<div class="packt_infobox">Download the MovieLens 100,000 dataset at <a href="https://www.kaggle.com/prajitdatta/movielens-100k-dataset/data">https://www.kaggle.com/prajitdatta/movielens-100k-dataset/data</a>.</div>
<p>Unzip the folder and rename it <kbd>movielens</kbd><em>. </em>Next, move this folder into the <kbd>data</kbd><em> </em>folder within <kbd>RecoSys</kbd><em>. </em>The MovieLens dataset should contain around 23 files. However, the only files we are interested in are <kbd>u.data</kbd>, <kbd>u.user</kbd>, and <kbd>u.item</kbd><em>. </em>Let's explore these files in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the data</h1>
                </header>
            
            <article>
                
<p>As mentioned in the previous section, we are only interested in three files in the <kbd>movielens</kbd><em> </em>folder: <kbd>u.data</kbd>, <kbd>u.user</kbd>, and <kbd>u.item</kbd>.<em> </em>Although these files are not in CSV format, the code required to load them into a Pandas DataFrame is almost identical.</p>
<p>Let's start with <kbd>u.user</kbd><em>:</em></p>
<pre>#Load the u.user file into a dataframe<br/>u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']<br/><br/>users = pd.read_csv('../data/movielens/u.user', sep='|', names=u_cols,<br/> encoding='latin-1')<br/><br/>users.head()</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/87603c24-aeb9-49c5-8d2e-9a08936e0a34.png" style="width:20.92em;height:12.25em;"/></div>
<p>We see that the <kbd>u.user</kbd><em> </em>file contains demographic information about our users, such as their <span class="packt_screen">age</span>, <span class="packt_screen">sex</span>, <span class="packt_screen">occupation</span>, and <span class="packt_screen">zip_code</span>.</p>
<p>Next, let's take a look at the <kbd>u.item</kbd><em> </em>file, which gives us information about the movies that have been rated by our users:</p>
<pre>#Load the u.items file into a dataframe<br/>i_cols = ['movie_id', 'title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',<br/> 'Animation', 'Children\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',<br/> 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']<br/><br/>movies = pd.read_csv('../data/movielens/u.item', sep='|', names=i_cols, encoding='latin-1')<br/><br/>movies.head()</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1d17c470-2f66-4030-b088-a6e80885e42a.png"/></div>
<p>We see that this file gives us information regarding the movie's title, <span class="packt_screen">release date</span>, <span class="packt_screen">IMDb URL</span>, and its genre(s). Since we are focused on building only collaborative filters in this chapter, we do not require any of this information, apart from the movie title and its corresponding ID:</p>
<pre>#Remove all information except Movie ID and title<br/>movies = movies[['movie_id', 'title']]</pre>
<p>Lastly, let's import the <kbd>u.data</kbd><em> </em>file into our notebook. This is arguably the most important file as it contains all the ratings that every user has given to a movie. It is from this file that we will construct our ratings matrix:</p>
<pre>#Load the u.data file into a dataframe<br/>r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']<br/><br/>ratings = pd.read_csv('../data/movielens/u.data', sep='\t', names=r_cols,<br/> encoding='latin-1')<br/><br/>ratings.head()</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4d99de2f-2ad1-4930-8837-e0adc93bd4e6.png"/></div>
<p class="mce-root">We see that every row in our new <kbd>ratings</kbd><em> </em>DataFrame denotes a rating given by a user to a particular movie at a particular time. However, for the purposes of the exercises in this chapter, we are not really worried about the time at which the ratings were given. Therefore, we will just go ahead and drop it:</p>
<pre>#Drop the timestamp column<br/>ratings = ratings.drop('timestamp', axis=1)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and test data</h1>
                </header>
            
            <article>
                
<p>The <kbd>ratings</kbd><em> </em>DataFrame contains user ratings for movies that range from 1 to 5. Therefore, we can model this problem as an instance of supervised learning where we need to predict the rating, given a user and a movie. Although the ratings can take on only five discrete values, we will model this as a regression problem. </p>
<p>Consider a case where the true rating given by a user to a movie is 5. A classification model will not distinguish between the predicted ratings of 1 and 4. It will treat both as misclassified. However, a regression model will penalize the former more than the latter, which is the behavior we want.</p>
<p>As we saw in <a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank">Chapter 5</a><span>, </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Getting Started with Data Mining Techniques</span></span></em>, one of the first steps towards building a supervised learning model is to construct the test and training sets. The model will learn using the training dataset and its potency will be judged using the testing dataset.</p>
<p>Let's now split our ratings dataset in such a way that 75% of a user's ratings is in the training dataset and 25% is in the testing dataset. We will do this using a slightly hacky way: we will assume that the <kbd>user_id</kbd><em> </em>field is the target variable (or <kbd>y</kbd>) and that our <kbd>ratings</kbd><em> </em>DataFrame consists of the predictor variables (or <kbd>X</kbd>)<em>. </em>We will then pass these two variables into scikit-learn's <kbd>train_test_split</kbd><em> </em>function and <kbd>stratify</kbd><em> </em>it along <em>y. </em>This ensures that the proportion of each class is the same in both the training and testing datasets:</p>
<pre>#Import the train_test_split function<br/>from sklearn.model_selection import train_test_split<br/><br/>#Assign X as the original ratings dataframe and y as the user_id column of ratings.<br/>X = ratings.copy()<br/>y = ratings['user_id']<br/><br/>#Split into training and test datasets, stratified along user_id<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y, random_state=42)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>We know from <a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank">Chapter 5</a><span>, </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Getting Started with Data Mining Techniques</span></span></em> that the RMSE, or root mean squared error, is the most commonly used performance metric for regressors. We will be using the RMSE to assess our modeling performance too. <kbd>scikit-learn</kbd> already gives us an implementation of the mean squared error. So, all that we have to do is define a function that returns the square root of the value returned by <kbd>mean_squared_error</kbd><em>:</em></p>
<pre>#Import the mean_squared_error function<br/>from sklearn.metrics import mean_squared_error<br/><br/>#Function that computes the root mean squared error (or RMSE)<br/>def rmse(y_true, y_pred):<br/>    return np.sqrt(mean_squared_error(y_true, y_pred))</pre>
<p>Next, let's define our baseline collaborative filter model. All our <strong>collaborative filter</strong> (or <strong>CF</strong>) models will take in a <kbd>user_id</kbd><em> </em>and <kbd>movie_id</kbd><em> </em>as input and output a floating point number between 1 and 5. We define our baseline model in such a way that it returns <kbd>3</kbd> regardless of <kbd>user_id</kbd><em> </em>or <kbd>movie_id</kbd><em>:</em></p>
<pre>#Define the baseline model to always return 3.<br/>def baseline(user_id, movie_id):<br/>    return 3.0</pre>
<p>To test the potency of our model, we compute the RMSE obtained by that particular model for all user-movie pairs in the test dataset:</p>
<pre>#Function to compute the RMSE score obtained on the testing set by a model<br/>def score(cf_model):<br/>    <br/>    #Construct a list of user-movie tuples from the testing dataset<br/>    id_pairs = zip(X_test['user_id'], X_test['movie_id'])<br/>    <br/>    #Predict the rating for every user-movie tuple<br/>    y_pred = np.array([cf_model(user, movie) for (user, movie) in id_pairs])<br/>    <br/>    #Extract the actual ratings given by the users in the test data<br/>    y_true = np.array(X_test['rating'])<br/>    <br/>    #Return the final RMSE score<br/>    return rmse(y_true, y_pred)</pre>
<p>We're all set. Let's now compute the RMSE obtained by our baseline model:</p>
<pre>score(baseline)<br/><br/><strong>OUTPUT:<br/>1.2470926188539486</strong></pre>
<p class="mce-root"/>
<p>We obtain a score of <kbd>1.247</kbd>. For the models that we build in the subsequent sections, we will try to obtain an RMSE that is less than that obtained for the baseline. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User-based collaborative filtering</h1>
                </header>
            
            <article>
                
<p>In <a href="c4bff0e9-57b3-44ec-90cb-9e5950696b27.xhtml" target="_blank">Chapter 1</a><span>, </span><em>Getting Started with Recommender Systems</em>, we learned what user-based collaborative filters do: they find users similar to a particular user and then recommend products that those users have liked to the first user.</p>
<p>In this section, we will implement this idea in code. We will build filters of increasing complexity and gauge their performance using the framework we constructed in the previous section.</p>
<p>To aid us in this process, let's first build a ratings matrix (described in <a href="c4bff0e9-57b3-44ec-90cb-9e5950696b27.xhtml" target="_blank">Chapters 1</a>, <em>Getting Started with Recommender Systems</em> and <a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank"/><a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank"/><a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank">Chapter 5</a>, <em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Getting Started with Data Mining Techniques</span></span></em>) where each row represents a user and each column represents a movie. Therefore, the value in the i<sup>th</sup> row and j<sup>th</sup> column will denote the rating given by user <kbd>i</kbd> to movie <kbd>j</kbd>. As usual, pandas gives us a very useful function, called <kbd>pivot_table</kbd>,<em> </em>to construct this matrix from our <kbd>ratings</kbd><em> </em>DataFrame:</p>
<pre>#Build the ratings matrix using pivot_table function<br/>r_matrix = X_train.pivot_table(values='rating', index='user_id', columns='movie_id')<br/><br/>r_matrix.head()</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/42d726f8-5bda-4f13-944d-7516f2744963.png"/></div>
<p>We now have a new <kbd>r_matrix</kbd> DataFrame, where each row is a user and each column is a movie. Also, notice that most values in the DataFrame are unspecified. This gives us a picture of how sparse our matrix is.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean</h1>
                </header>
            
            <article>
                
<p>Let's first build one of the simplest collaborative filters possible. This simply takes in <kbd>user_id</kbd><em> </em>and <kbd>movie_id</kbd><em> </em>and outputs the mean rating for the movie by all the users who have rated it. No distinction is made between the users. In other words, the rating of each user is assigned equal weight.</p>
<p>It is possible that some movies are available only in the test set and not the training set (and consequentially, not in our ratings matrix). In such cases, we will just default to a rating of <kbd>3.0</kbd>, like the baseline model:</p>
<pre>#User Based Collaborative Filter using Mean Ratings<br/>def cf_user_mean(user_id, movie_id):<br/>    <br/>    #Check if movie_id exists in r_matrix<br/>    if movie_id in r_matrix:<br/>        #Compute the mean of all the ratings given to the movie<br/>        mean_rating = r_matrix[movie_id].mean()<br/>    <br/>    else:<br/>        #Default to a rating of 3.0 in the absence of any information<br/>        mean_rating = 3.0<br/>    <br/>    return mean_rating<br/><br/>#Compute RMSE for the Mean model<br/>score(cf_user_mean)<br/><br/><strong>OUTPUT:<br/>1.0234701463131335</strong></pre>
<p>We see that the score obtained for this model is lower and therefore better than the baseline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weighted mean</h1>
                </header>
            
            <article>
                
<p>In the previous model, we assigned equal weights to all the users. However, it makes intuitive sense to give more preference to those users whose ratings are similar to the user in question than the other users whose ratings are not. </p>
<p>Therefore, let's alter our previous model by introducing a weight coefficient. This coefficient will be one of the similarity metrics that we computed in the previous chapter. Mathematically, it is represented as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/be3fa7ee-447f-4d84-b674-97d3f20b2b9a.png" style="width:22.00em;height:4.58em;"/></div>
<p class="mce-root"/>
<p>In this formula, <em>r</em><sub><em>u,m</em> </sub>represents the rating given by user <em>u</em> to movie <em>m.</em></p>
<p>For the sake of this exercise, we will use the cosine score as our similarity function (or sim). Recall how we constructed a movie cosine similarity matrix while building our content-based engine. We will be building a very similar cosine similarity matrix for our users in this section.</p>
<p>However, scikit-learn's<em> </em><kbd>cosine_similarity</kbd><em> </em>function does not work with <kbd>NaN</kbd> values. Therefore, we will convert all missing values to zero in order to compute our cosine similarity matrix:</p>
<pre>#Create a dummy ratings matrix with all null values imputed to 0<br/>r_matrix_dummy = r_matrix.copy().fillna(0)<br/><br/># Import cosine_score <br/>from sklearn.metrics.pairwise import cosine_similarity<br/><br/>#Compute the cosine similarity matrix using the dummy ratings matrix<br/>cosine_sim = cosine_similarity(r_matrix_dummy, r_matrix_dummy)<br/><br/>#Convert into pandas dataframe <br/>cosine_sim = pd.DataFrame(cosine_sim, index=r_matrix.index, columns=r_matrix.index)<br/><br/>cosine_sim.head(10)</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5f2374b3-5ba3-43f4-88e2-9234f4c15f23.png"/></div>
<p>With the user cosine similarity matrix in hand, we are now in a position to efficiently calculate the weighted mean scores for this model. However, implementing this model in code is a little more nuanced than its simpler mean counterpart. This is because we need to only consider those cosine similarity scores that have a corresponding, non-null rating. In other words, we need to avoid all users that have not rated movie <em>m:</em></p>
<pre>#User Based Collaborative Filter using Weighted Mean Ratings<br/>def cf_user_wmean(user_id, movie_id):<br/>    <br/>    #Check if movie_id exists in r_matrix<br/>    if movie_id in r_matrix:<br/>        <br/>        #Get the similarity scores for the user in question with every other user<br/>        sim_scores = cosine_sim[user_id]<br/><br/>        #Get the user ratings for the movie in question<br/>        m_ratings = r_matrix[movie_id]<br/><br/>        #Extract the indices containing NaN in the m_ratings series<br/>        idx = m_ratings[m_ratings.isnull()].index<br/><br/>        #Drop the NaN values from the m_ratings Series<br/>        m_ratings = m_ratings.dropna()<br/><br/>        #Drop the corresponding cosine scores from the sim_scores series<br/>        sim_scores = sim_scores.drop(idx)<br/>        <br/>        #Compute the final weighted mean<br/>        wmean_rating = np.dot(sim_scores, m_ratings)/ sim_scores.sum()<br/>    <br/>    else:<br/> #Default to a rating of 3.0 in the absence of any information<br/> wmean_rating = 3.0<br/> <br/> return wmean_rating<br/><br/><br/>score(cf_user_wmean)<br/><br/><strong>OUTPUT:<br/>1.0174483808407588</strong></pre>
<p><span>Since we are dealing with positive ratings, the cosine similarity score will always be positive. Therefore, we do not need to explicitly add in a modulus function while computing the normalizing factor (the denominator of the equation that ensures the final rating is scaled back to between 1 and 5).</span></p>
<p class="mce-root"/>
<p><span>However, if you're working with a similarity metric that can be negative in this scenario (for instance, the Pearson correlation score), it is important that we factor in the modulus.</span></p>
<p>Running this code takes significantly more time than the previous model. However, we achieve a (very small) improvement in our RMSE score. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User demographics</h1>
                </header>
            
            <article>
                
<p>Finally, let's take a look at filters that leverage user demographic information. The basic intuition behind these filter is that users of the same demographic tend to have similar tastes. Therefore, their effectiveness depends on the assumption that women, or teenagers, or people from the same area will share the same taste in movies.</p>
<p>Unlike the previous models, these filters do not take into account the ratings given by all users to a particular movie. Instead, they only look at those users that fit a certain demographic.</p>
<p>Let's now build a gender demographic filter. All this filter does is identify the gender of a user, compute the (weighted) mean rating of a movie by that particular gender, and return that as the predicted value.</p>
<p>Our <kbd>ratings</kbd><em> </em>DataFrame does not contain the users' demographics. We will import that information from the <kbd>users</kbd><em> </em>DataFrame by merging them into one (using pandas, as usual). Readers familiar with SQL can see that this is extremely similar to the JOIN functionality:</p>
<pre>#Merge the original users dataframe with the training set <br/>merged_df = pd.merge(X_train, users)<br/><br/>merged_df.head()</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6bf67257-c48f-44a6-b898-82685bd2ec96.png" style="width:28.00em;height:12.42em;"/></div>
<p>Next, we need to compute the <kbd>mean</kbd><em> </em>rating of each movie by gender. Pandas makes this possible with the <kbd>groupby</kbd><em> </em>method:</p>
<pre>#Compute the mean rating of every movie by gender<br/>gender_mean = merged_df[['movie_id', 'sex', 'rating']].groupby(['movie_id', 'sex'])         ['rating'].mean()</pre>
<p>We are now in a position to define a function that identifies the gender of the user, extracts the average rating given to the movie in question by that particular gender, and return that value as output:</p>
<pre>#Set the index of the users dataframe to the user_id<br/>users = users.set_index('user_id')<br/><br/>#Gender Based Collaborative Filter using Mean Ratings<br/>def cf_gender(user_id, movie_id):<br/>    <br/>    #Check if movie_id exists in r_matrix (or training set)<br/>    if movie_id in r_matrix:<br/>        #Identify the gender of the user<br/>        gender = users.loc[user_id]['sex']<br/>        <br/>        #Check if the gender has rated the movie<br/>        if gender in gender_mean[movie_id]:<br/>            <br/>            #Compute the mean rating given by that gender to the movie<br/>            gender_rating = gender_mean[movie_id][gender]<br/>        <br/>        else:<br/>            gender_rating = 3.0<br/>    <br/>    else:<br/>        #Default to a rating of 3.0 in the absence of any information<br/>        gender_rating = 3.0<br/>    <br/>    return gender_rating<br/><br/>score(cf_gender)<br/><br/><strong>OUTPUT:</strong><br/><strong>1.0330308800874282</strong></pre>
<p>We see that this model actually performs worse than the standard mean ratings collaborative filter. This indicates that a user's gender isn't the strongest indicator of their taste in movies.</p>
<p>Let's try building one more demographic filter, but this time using both gender and occupation:</p>
<pre>#Compute the mean rating by gender and occupation<br/>gen_occ_mean = merged_df[['sex', 'rating', 'movie_id', 'occupation']].pivot_table(<br/>    values='rating', index='movie_id', columns=['occupation', 'sex'], aggfunc='mean')<br/><br/>gen_occ_mean.head()</pre>
<p>We see that the <kbd>pivot_table</kbd> method gives us the required DataFrame. However, this could have been done using <kbd>groupby</kbd><em> </em>too. <kbd>pivot_table</kbd><em> </em>is simply a more compact, easier-to-use interface for the <kbd>groupby</kbd><em> </em>method:</p>
<pre>#Gender and Occupation Based Collaborative Filter using Mean Ratings<br/>def cf_gen_occ(user_id, movie_id):<br/>    <br/>    #Check if movie_id exists in gen_occ_mean<br/>    if movie_id in gen_occ_mean.index:<br/>        <br/>        #Identify the user<br/>        user = users.loc[user_id]<br/>        <br/>        #Identify the gender and occupation<br/>        gender = user['sex']<br/>        occ = user['occupation']<br/>        <br/>        #Check if the occupation has rated the movie<br/>        if occ in gen_occ_mean.loc[movie_id]:<br/>            <br/>            #Check if the gender has rated the movie<br/>            if gender in gen_occ_mean.loc[movie_id][occ]:<br/>                <br/>                #Extract the required rating<br/>                rating = gen_occ_mean.loc[movie_id][occ][gender]<br/>                <br/>                #Default to 3.0 if the rating is null<br/>                if np.isnan(rating):<br/>                    rating = 3.0<br/>                <br/>                return rating<br/>            <br/>    #Return the default rating <br/>    return 3.0<br/><br/>score(cf_gen_occ)<br/><br/><strong>OUTPUT:<br/>1.1391976012043645</strong></pre>
<p>We see that this model performs the worst out of all the filters we've built so far, beating only the baseline. This strongly suggests that tinkering with user demographic data may not be the best way to go forward with the data that we are currently using. However, you are encouraged to try different permutations and combinations of user demographics to see what performs best. You are also encouraged to try other techniques of improving the model, such as using a weighted mean for the <kbd>aggfunc</kbd> of the<em> </em><kbd>pivot_table</kbd><em> </em>and experimenting with different (perhaps more informed) default ratings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Item-based collaborative filtering</h1>
                </header>
            
            <article>
                
<p>Item-based collaborative filtering is essentially user-based collaborative filtering where the users now play the role that items played, and vice versa.</p>
<p>In item-based collaborative filtering, we compute the pairwise similarity of every item in the inventory. Then, given <kbd>user_id</kbd><em> </em>and <kbd>movie_id</kbd><em>, </em>we compute the weighted mean of the ratings given by the user to all the items they have rated. The basic idea behind this model is that a particular user is likely to rate two items that are similar to each other similarly.</p>
<p>Building an item-based collaborative filter is left as an exercise to the reader. The steps involved are exactly the same except now, as mentioned earlier, the movies and users have swapped places.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-based approaches</h1>
                </header>
            
            <article>
                
<p class="mce-root">The collaborative filters we have built thus far are known as memory-based<em> </em>filters. This is because they only make use of similarity metrics to come up with their results. They learn any parameters from the data or assign classes/clusters to the data. In other words, they do not make use of machine learning algorithms.</p>
<p>In this section, we will take a look at some filters that do. We spent an entire chapter looking at various supervised and unsupervised learning techniques. The time has finally come to see them in action and test their potency.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>In our weighted mean-based filter, we took every user into consideration when trying to predict the final rating. In contrast, our demographic-based filters only took users that fit a certain demographic into consideration. We saw that the demographic filters performed poorly compared to the weighted mean filter.</p>
<p>But does this necessarily imply that we need to take all users into consideration to achieve better results?</p>
<p>One of the major drawbacks of the demographic filters was that they were based on the assumption that people from a certain demographic think and rate alike. However, we can safely say that this is an overreached assumption. Not all men like action movies. Nor do all children like animated movies. Similarly, it is extremely far-fetched to assume that people from a particular area or occupation will have the same taste.</p>
<p>We need to come up with a way of grouping users with a much more powerful metric than demographics. From <a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml">Chapter 5</a><span>, </span><em>Getting Started with Data Mining Techniques</em>, we already know of one extremely powerful tool: <kbd>clustering</kbd><em>.</em></p>
<p>It is possible to use a clustering algorithm, such as k-means, to group users into a cluster and then take only the users from the same cluster into consideration when predicting ratings.</p>
<p>In this section, we will use k-means' sister algorithm, kNN<em>, </em>to build our clustering-based collaborative filter. In a nutshell, given an user, <em>u</em>, and a movie, <em>m</em>, these are the steps involved:</p>
<ol>
<li>Find the k-nearest neighbors of <em>u </em>who have rated movie <em>m</em></li>
<li>Output the average rating of the <em>k</em> users for the movie <em>m</em></li>
</ol>
<p>That's it. This extremely simply algorithm happens to be one of the most popularly used. </p>
<p>Just like kNN, we will not be implementing the kNN-based collaborative filter from scratch. Instead, we will use an extremely popular and robust library called <kbd>surprise</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6f23c30a-a1c2-4fa8-9209-dfb7090f95de.png"/></div>
<p>Surprise is a scikit (or scientific kit) for building recommender systems in Python. You can think of it as scikit-learn's recommender systems counterpart. According to its documentation, <kbd>surprise</kbd><em> </em>stands for Simple Python Recommendation System Engine. Within a very short span of time, <kbd>surprise</kbd><em> </em>has gone on to become one of the most popularly used recommender libraries. This is because it is extremely robust and easy to use. It gives us ready-to-use implementations of most of the popular collaborative filtering algorithms and also allows us to integrate an algorithm of our own into the framework. </p>
<p>To download <kbd>surprise</kbd>, like any other Python library, open up your Terminal and type the following command:</p>
<pre><strong>sudo pip3 install scikit-surprise</strong></pre>
<p>Let's now build and evaluate our kNN-based collaborative filter. Although <em>surprise </em>has the MovieLens datasets available within the library, we will still use the external data we have in order to get a feel for using the library with alien datasets:</p>
<pre>#Import the required classes and methods from the surprise library<br/>from surprise import Reader, Dataset, KNNBasic, evaluate<br/><br/>#Define a Reader object<br/>#The Reader object helps in parsing the file or dataframe containing ratings<br/>reader = Reader()<br/><br/>#Create the dataset to be used for building the filter<br/>data = Dataset.load_from_df(ratings, reader)<br/><br/>#Define the algorithm object; in this case kNN<br/>knn = KNNBasic()<br/><br/>#Evaluate the performance in terms of RMSE<br/>evaluate(knn, data, measures=['RMSE'])</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f3296d5e-c2bf-4166-abd0-8ceef92b158f.png" style="width:26.50em;height:35.25em;"/></div>
<p>The output indicates that the filter is making use of a technique known as fivefold <kbd>cross-validation</kbd><em>. </em>In a nutshell, this means that <kbd>surprise</kbd><em> </em>divides the data into five equal parts. It then uses four parts as the training data and tests it on the fifth part. This is done five times, in such a way that every part plays the role of the test data once.</p>
<p>We see that the RMSE obtained by this model is <span class="packt_screen">0.9784</span>. This is, by far, the best result we have achieved. </p>
<p>Let's now take a tour of some other model-based approaches to collaborative filtering and implement a few of them using the <em>surprise </em>library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning and dimensionality reduction</h1>
                </header>
            
            <article>
                
<p>Consider our ratings matrix once again. It is of the <em>m</em> × <em>n</em> shape, where every row represents one of the <em>m</em> users and every column represents one of the <em>n </em>items.</p>
<p>Let's now remove one of the <em>n </em>columns (say n<sub>j</sub>). We now have an <em>m </em>× (<em>n</em>-1) <span>matrix. If we treat the <em>m</em></span> <span>× (<em>n</em>-1) matrix as the predictor variables and n</span><sub>j</sub> <span>as the target variable, we can use supervised learning algorithms to train on the values available in n</span><sub>j</sub> <span>to predict values that are not. This can be repeated n times for every column to eventually complete our matrix.</span></p>
<p>One big problem is that most supervised learning algorithms do not work with missing data. In standard problems, it is common practice to impute the missing values with the mean or median of the column it belongs to.</p>
<p>However, our matrix suffers from heavy data sparsity. More than 99% of the data in the matrix is unavailable. Therefore, it is simply not possible to impute values (such as mean or median) without introducing a large bias. </p>
<p>One solution that may come to mind is to compress the predictor matrix in such a way that all the values are available. Unfortunately, dimensionality reduction techniques, such as SVD and PCA, also do not work in an environment with missing values. </p>
<p>While working toward a solution for the Netflix Problem, Simon Funk came up with a solution that could be used to reduce the <em>m</em> × (<em>n</em>-1) matrix into a lower-dimensional <em>m</em> × <em>d</em> matrix where <em>d</em> &lt;&lt; <em>n</em>. He used standard dimensionality-reduction techniques (in his case, the SVD) but with slight tweaks. Explaining the technique is outside the scope of this book, but is presented in the Appendix for advanced readers. For the sake of this chapter, we will treat this technique as a black box that converts an <em>m</em> × <em>n</em> sparse matrix into an <em>m</em> × <em>d</em> dense matrix where <em>d</em> &lt;&lt; <em>n</em>, and call it <kbd>SVD-like</kbd><em>.</em></p>
<p>Let's now turn our attention to perhaps the most famous recommendation algorithm of all time: singular-value decomposition<em>.</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Singular-value decomposition</h1>
                </header>
            
            <article>
                
<p>In <a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank"/><a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank">C<span><span>hapter 5</span></span></a><a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank"/><span><span>, <em>Getting Started with Data Mining Techniques</em></span></span>, we mentioned that the math behind singular-value decomposition is well outside the scope of this book. However, let's try to gain an understanding of how it works from a layman's perspective.</p>
<p>Recall from <a href="cde5090f-2e41-4e6f-ab11-f5179f1ee2a6.xhtml" target="_blank">Chapter 5</a><span>, </span><em>Getting Started with Data Mining Techniques,</em> that <strong>PCA</strong> ( <strong>Principal Component Analysis</strong>) transforms an <em>m</em> <span>×</span> <em>n</em> matrix into <em>n</em>, <em>m</em>-dimensional vectors (called principal components) in such a way that each component is orthogonal to the next component. It also constructs these components in such a way that the first component holds the most variance (or information), followed by the second component, and so on.</p>
<p>Let's denote our ratings matrix as <em>A</em><em>. </em>The transpose of this matrix would be <em>A</em><em><sup>T</sup></em>, which would be of the <em>n</em> <span>×</span> <em>m</em> shape and each row would represent a movie (instead of a user).</p>
<p>We can now use PCA to construct two new matrices, <em>U </em>and <em>V</em>, from <em>A</em><em> </em>and <em>A</em><em><sup>T</sup></em>, respectively.</p>
<p>Singular-value decomposition allows us to compute <em>U </em>and <em>V </em>in one go from <em>A</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/37b83a39-36b7-47eb-b790-3981bedf634b.png" style="width:20.92em;height:12.67em;"/></div>
<p>In essence, singular-value decomposition is a matrix-factorization technique. It takes in an input, <em>A</em>, and outputs <em>U </em>and <em>V </em>such that:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2ddd6e73-d027-463c-bb87-1a9f7f0c262f.png" style="width:6.25em;height:1.33em;"/></div>
<p>Where <img class="fm-editor-equation" src="assets/d4a5f097-a8e4-4459-9b20-2b07a31b5713.png" style="width:1.08em;height:1.33em;"/> is a diagonal matrix. It is used for scaling purposes and, for the sake of this illustration, can be assumed to be merged with either <em>U </em>or <em>V. </em>Therefore, we now have:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fd8ec9cb-9e3f-47a1-9490-35d2da0d9290.png" style="width:5.58em;height:1.42em;"/></div>
<p>The <em>U </em>matrix, which is essentially composed of user principal components, is typically called the user-embedding matrix. Its counterpart, <em>V</em>, is called the movie-embedding matrix.</p>
<p>The classic version of SVD, like most other machine learning algorithms, does not work with sparse matrices. However, Simon Funk figured out a workaround for this problem, and his solution led to one of the most famous solutions in the world of recommender systems. </p>
<p>Funk's system took in the sparse ratings matrix, <em>A</em>, and constructed two dense user- and item-embedding matrices, <em>U </em>and <em>V </em>respectively. These dense matrices directly gave us the predictions for all the missing values in the original matrix, <em>A.</em></p>
<p>Let's now implement the SVD filter using the <kbd>surprise</kbd> package:</p>
<pre>#Import SVD<br/>from surprise import SVD<br/><br/>#Define the SVD algorithm object<br/>svd = SVD()<br/><br/>#Evaluate the performance in terms of RMSE<br/>evaluate(svd, data, measures=['RMSE'])</pre>
<p>Here is its output<strong>:</strong></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/833e3cf4-cefb-4416-8332-e958d03d2e84.png" style="width:23.50em;height:28.50em;"/></div>
<p>The SVD filter outperforms all other filters, with an RMSE score of <span class="packt_screen">0.9367</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This brings us to the end of our discussion on collaborative filters. In this chapter, we built various kinds of user-based collaborative filters and, by extension, learned to build item-based collaborative filters as well.</p>
<p>We then shifted our focus to model-based approaches that rely on machine learning algorithms to churn out predictions. We were introduced to the <em>surprise </em>library and used it to implement a clustering model based on kNN. We then took a look at an approach to using supervised learning algorithms to predict the missing values in the ratings matrix. Finally, we gained a layman's understanding of the singular-value decomposition algorithm and implemented it using <kbd>surprise</kbd><em>.</em></p>
<p>All the recommenders we've built so far reside only inside our Jupyter Notebooks. In the next chapter, we will learn how to deploy our models to the web, where they can be used by anyone on the internet.</p>


            </article>

            
        </section>
    </body></html>
["```py\n    import pandas as pd\n    import numpy as np\n    import warnings\n    warnings.filterwarnings('ignore')\n    ```", "```py\n    df_bikes = pd.read_csv('bike_rentals_cleaned.csv')\n    df_bikes.head()\n    ```", "```py\n    X_bikes = df_bikes.iloc[:,:-1]\n    y_bikes = df_bikes.iloc[:,-1]\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X_bikes, y_bikes, random_state=2)\n    ```", "```py\n    from sklearn.tree import DecisionTreeRegressor\n    tree_1 = DecisionTreeRegressor(max_depth=2, random_state=2)\n    tree_1.fit(X_train, y_train)\n    ```", "```py\n    y_train_pred = tree_1.predict(X_train)\n    ```", "```py\n    y2_train = y_train - y_train_pred\n    ```", "```py\n    tree_2 = DecisionTreeRegressor(max_depth=2, random_state=2)\n    tree_2.fit(X_train, y2_train)\n    ```", "```py\n    y2_train_pred = tree_2.predict(X_train)\n    y3_train = y2_train - y2_train_pred\n    tree_3 = DecisionTreeRegressor(max_depth=2, random_state=2)\n    tree_3.fit(X_train, y3_train)\n    ```", "```py\n    y1_pred = tree_1.predict(X_test)\n    y2_pred = tree_2.predict(X_test)\n    y3_pred = tree_3.predict(X_test)\n    ```", "```py\n    y_pred = y1_pred + y2_pred + y3_pred\n    ```", "```py\n    from sklearn.metrics import mean_squared_error as MSE\n    MSE(y_test, y_pred)**0.5\n    ```", "```py\n    911.0479538776444\n    ```", "```py\n    from sklearn.ensemble import GradientBoostingRegressor\n    ```", "```py\n    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=3, random_state=2, learning_rate=1.0)\n    ```", "```py\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    MSE(y_test, y_pred)**0.5\n    ```", "```py\n    911.0479538776439\n    ```", "```py\n    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=30, random_state=2, learning_rate=1.0)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    MSE(y_test, y_pred)**0.5\n    ```", "```py\n    857.1072323426944\n    ```", "```py\n    gbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    MSE(y_test, y_pred)**0.5\n    ```", "```py\n    936.3617413678853\n    ```", "```py\ngbr = GradientBoostingRegressor(max_depth=2, n_estimators=300, random_state=2)\ngbr.fit(X_train, y_train)\ny_pred = gbr.predict(X_test)\nMSE(y_test, y_pred)**0.5\n```", "```py\n653.7456840231495\n```", "```py\nfor value in learning_rate_values:\n    gbr = GradientBoostingRegressor(max_depth=2,   n_estimators=300, random_state=2, learning_rate=value)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    rmse = MSE(y_test, y_pred)**0.5\n    print('Learning Rate:', value, ', Score:', rmse)\n```", "```py\nLearning Rate: 0.001 , Score: 1633.0261400367258\nLearning Rate: 0.01 , Score: 831.5430182728547\nLearning Rate: 0.05 , Score: 685.0192988749717\nLearning Rate: 0.1 , Score: 653.7456840231495\nLearning Rate: 0.15 , Score: 687.666134269379\nLearning Rate: 0.2 , Score: 664.312804425697\nLearning Rate: 0.3 , Score: 689.4190385930236\nLearning Rate: 0.5 , Score: 693.8856905068778\nLearning Rate: 1.0 , Score: 936.3617413678853\n```", "```py\ndepths = [None, 1, 2, 3, 4]\nfor depth in depths:\n    gbr = GradientBoostingRegressor(max_depth=depth, n_estimators=300, random_state=2)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    rmse = MSE(y_test, y_pred)**0.5\n    print('Max Depth:', depth, ', Score:', rmse) \n```", "```py\nMax Depth: None , Score: 867.9366621617327\nMax Depth: 1 , Score: 707.8261886858736\nMax Depth: 2 , Score: 653.7456840231495\nMax Depth: 3 , Score: 646.4045923317708\nMax Depth: 4 , Score: 663.048387855927\n```", "```py\nsamples = [1, 0.9, 0.8, 0.7, 0.6, 0.5]\nfor sample in samples:\n    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=300, subsample=sample, random_state=2)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    rmse = MSE(y_test, y_pred)**0.5\n    print('Subsample:', sample, ', Score:', rmse)\n```", "```py\nSubsample: 1 , Score: 646.4045923317708\nSubsample: 0.9 , Score: 620.1819001443569\nSubsample: 0.8 , Score: 617.2355650565677\nSubsample: 0.7 , Score: 612.9879156983139\nSubsample: 0.6 , Score: 622.6385116402317\nSubsample: 0.5 , Score: 626.9974073227554\n```", "```py\n    params={'subsample':[0.65, 0.7, 0.75],\n            'n_estimators':[300, 500, 1000],\n             'learning_rate':[0.05, 0.075, 0.1]}\n    ```", "```py\n    from sklearn.model_selection import RandomizedSearchCV\n    gbr = GradientBoostingRegressor(max_depth=3, random_state=2)\n    ```", "```py\n    rand_reg = RandomizedSearchCV(gbr, params, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, random_state=2)\n    ```", "```py\n    rand_reg.fit(X_train, y_train)\n    best_model = rand_reg.best_estimator_\n    best_params = rand_reg.best_params_\n    print(\"Best params:\", best_params)\n    best_score = np.sqrt(-rand_reg.best_score_)\n    print(\"Training score: {:.3f}\".format(best_score))\n    y_pred = best_model.predict(X_test)\n    rmse_test = MSE(y_test, y_pred)**0.5\n    print('Test set score: {:.3f}'.format(rmse_test))\n    ```", "```py\n    Best params: {'learning_rate': 0.05, 'n_estimators': 300, 'subsample': 0.65}\n    Training score: 636.200\n    Test set score: 625.985\n    ```", "```py\n    gbr = GradientBoostingRegressor(max_depth=3, n_estimators=1600, subsample=0.75, learning_rate=0.02, random_state=2)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    MSE(y_test, y_pred)**0.5 \n    ```", "```py\n    596.9544588974487\n    ```", "```py\nfrom xgboost import XGBRegressor\nxg_reg = XGBRegressor(max_depth=3, n_estimators=1600, eta=0.02, subsample=0.75, random_state=2)\nxg_reg.fit(X_train, y_train)\ny_pred = xg_reg.predict(X_test)\nMSE(y_test, y_pred)**0.5\n```", "```py\n584.339544309016\n```", "```py\n    df = pd.read_csv('exoplanets.csv')\n    df.head() \n    ```", "```py\n    df.info()\n    ```", "```py\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 5087 entries, 0 to 5086\n    Columns: 3198 entries, LABEL to FLUX.3197\n    dtypes: float64(3197), int64(1)\n    memory usage: 124.1 MB\n    ```", "```py\n    df.isnull().sum().sum()\n    ```", "```py\n    0\n    ```", "```py\n    X = df.iloc[:,1:]\n    y = df.iloc[:,0]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n```", "```py\nimport time\n```", "```py\nstart = time.time()\ndf.info()\nend = time.time()\nelapsed = end - start\nprint('\\nRun Time: ' + str(elapsed) + ' seconds.')\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5087 entries, 0 to 5086\nColumns: 3198 entries, LABEL to FLUX.3197\ndtypes: float64(3197), int64(1)\nmemory usage: 124.1 MB\n```", "```py\nRun Time: 0.0525362491607666 seconds.\n```", "```py\n    start = time.time()\n    gbr = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=2)\n    gbr.fit(X_train, y_train)\n    y_pred = gbr.predict(X_test)\n    score = accuracy_score(y_pred, y_test)\n    print('Score: ' + str(score))\n    end = time.time()\n    elapsed = end - start\n    print('\\nRun Time: ' + str(elapsed) + ' seconds')\n    ```", "```py\n    Score: 0.9874213836477987\n    Run Time: 317.6318619251251 seconds\n    ```", "```py\n    start = time.time()\n    xg_reg = XGBClassifier(n_estimators=100, max_depth=2, random_state=2)\n    xg_reg.fit(X_train, y_train)\n    y_pred = xg_reg.predict(X_test)\n    score = accuracy_score(y_pred, y_test)\n    print('Score: ' + str(score))\n    end = time.time()\n    elapsed = end - start\n    print('Run Time: ' + str(elapsed) + ' seconds')\n    ```", "```py\n    Score: 0.9913522012578616\n    Run Time: 118.90568995475769 seconds\n    ```"]
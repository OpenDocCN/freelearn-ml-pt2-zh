<html><head></head><body>
  <div id="_idContainer234">
    <h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-84" class="chapterTitle">Forecasting with Moving Averages and Autoregressive Models</h1>
    <p class="normal">This chapter is about time-series modeling based on moving averages and autoregression. This comprises a large set of models that are very popular in different disciplines, including econometrics and statistics. We'll discuss autoregression and moving averages models, along with others that combine these two, such as ARMA, ARIMA, VAR, GARCH, and others. </p>
    <p class="normal">These models are still held in high esteem and find their applications. However, many new models have since sprung up that have been shown to be competitive or even outperform these simpler ones. Within their main application, however, in univariate forecasting, simple models often provide accurate or accurate enough predictions, so that these models constitute a mainstay in time-series modeling. </p>
    <p class="normal">We're going to cover the following topics:</p>
    <ul>
      <li class="bullet">What are classical models?<ul>
          <li class="bullet-l2">Moving average and autoregression</li>
          <li class="bullet-l2">Model selection and order</li>
          <li class="bullet-l2">Exponential smoothing</li>
          <li class="bullet-l2">ARCH and GARCH</li>
          <li class="bullet-l2">Vector autoregression</li>
        </ul>
      </li>
      <li class="bullet">Python libraries<ul>
          <li class="bullet-l2">statsmodels</li>
        </ul>
      </li>
      <li class="bullet">Python practice<ul>
          <li class="bullet-l2">Modeling in Python</li>
        </ul>
      </li>
    </ul>
    <p class="normal">We are going to start with an introduction to classical models.</p>
    <h1 id="_idParaDest-85" class="title">What are classical models?</h1>
    <p class="normal">In this chapter, we'll deal with models that could be characterized as having a longer tradition, and are rooted in statistics and mathematics. They are used heavily in econometrics and statistics. </p>
    <p class="normal">While there is considerable overlap between statistics and machine learning approaches, and each <a id="_idIndexMarker400"/>community has been absorbing the work of the other, there are still a few key differences. Whereas statistics papers are still overwhelmingly formal and deductive, machine learning researchers are more pragmatic, relying on the predictive accuracy of models. </p>
    <p class="normal">We've talked about the very early history of time-series models in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to Time-Series with Python</em>. In this chapter, we'll discuss moving averages and autoregressive approaches for forecasting. These were introduced in the early 20<sup class="Superscript--PACKT-">th</sup> century and popularized by George Box and Gwilym Jenkins in 1970 in their book "<em class="italic">Time-Series Analysis Forecasting and Control</em>." Crucially, in their book, Box and Jenkins formalized the ARIMA model and described how to apply it to time-series forecasting.</p>
    <p class="normal">Many time-series exhibit trends and seasonality, while many of the models in this chapter assume stationarity. If a time-series is stationary, its mean and standard deviation stays constant over time. This implies that the time-series has no trend and no cyclic variability.</p>
    <p class="normal">Therefore, the removal of irregular components, trends, and seasonal fluctuations is an intrinsic aspect of applying these models. The models then forecast what's left after removing seasonality and trend: business cycles. </p>
    <p class="normal">Thus, to apply classical models, a time-series usually should be decomposed into different components. Thus, classical models are usually applied as follows:</p>
    <ol>
      <li class="numbered">Test for stationarity</li>
      <li class="numbered">Differencing [if stationarity detected]</li>
      <li class="numbered">Fit method and forecast</li>
      <li class="numbered">Add back the trend and seasonality</li>
    </ol>
    <p class="normal">Most of the approaches in this chapter are relevant only to univariate time-series. Although extensions to <a id="_idIndexMarker401"/>multivariate time-series have been proposed, they are not as popular as the univariate versions. Univariate time-series consist of a single vector or, in other words, one value that changes over time. We'll see <strong class="keyword">Vector Autoregression</strong> (<strong class="keyword">VAR</strong>) at the end of the chapter though, which is an extension to multivariate <a id="_idIndexMarker402"/>time-series.</p>
    <p class="normal">Another important consideration is that most classical models are linear, which means they assume linearity in the dependencies between values at the same time and between values at different time steps. In practice, the models in this chapter work well with a range of time-series that are stationary. This means that the distribution is the same over time. Examples of this are temperature changes over time. These models are especially valuable in a context where the amount of data available is small so that the extra estimation error in non-linear models dominates any potential gains in terms of accuracy. </p>
    <p class="normal">However, the stationarity assumption implies that the application of the models in this chapter is restricted to time-series that have this property. Alternatively, we'd have to preprocess our time-series to enforce stationarity. In contrast, the development of statistical methods for nonlinear time-series analysis and forecasting has found much less prominence; however, there are some models such as the Threshold Autoregressive Model (which we won't cover here).</p>
    <p class="normal">Finally, it is left to note that, while a reasonable first approach, many time-series such as temperatures can be predicted much more accurately by high-dimensional physics-based models of the atmosphere than by statistical models. This illustrates the point of complexity: essentially, modeling is condensing a set of hypotheses and formalizing them together with parameters. </p>
    <p class="normal">Real-world time-series come from complicated processes that can be non-linear and non-stationary, and there's more than a single way of describing them, each of which has its up- and downsides. Thus, we can think of a modeling problem in terms of lots of parameters or just as a single or a couple of parameters. In a dedicated section below, we'll discuss the issue of selecting a model from a set of alternatives based on the number of parameters and the accuracy. </p>
    <p class="normal">Nowadays, nonlinear models come from a different direction of research, either neural networks or the broader <a id="_idIndexMarker403"/>field of machine learning. We'll see neural networks in <em class="chapterRef">Chapter 10</em>, <em class="italic">Deep Learning for Time-Series</em>, and we'll discuss and apply state-of-the-art machine learning in <em class="chapterRef">Chapter 7</em>, <em class="italic">Machine Learning Models for Time-Series</em>.</p>
    <h2 id="_idParaDest-86" class="title">Moving average and autoregression</h2>
    <p class="normal">Classical models can be grouped into families of models – <strong class="keyword">moving averages</strong> (<strong class="keyword">MA</strong>), <strong class="keyword">autoregressive</strong> (<strong class="keyword">AR</strong>) models, ARMA, and ARIMA. These models were formalized and popularized over <a id="_idIndexMarker404"/>time in books and papers by many <a id="_idIndexMarker405"/>mathematicians and statisticians, including Peter Whittle (1951) and George Box and Gwilym Jenkins (1970). But let's start earlier.</p>
    <p class="normal">The <strong class="keyword">moving average</strong> marked the beginning of modern time-series predictions. In a moving average, the <a id="_idIndexMarker406"/>average (usually, the arithmetic mean) of values is taken over a specific number of time points (time frame) in the past.</p>
    <p class="normal">More formally, the <strong class="keyword">simple moving average</strong>, the<a id="_idIndexMarker407"/> unweighted mean over a period of k points, is formulated as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_001.png" alt="" style="height: 2.8em;"/></figure>
    <p class="normal">where <em class="italic">x</em><sub class="" style="font-style: italic;">i</sub> represents the observed time-series.</p>
    <p class="normal">The moving average can be used to smooth out a time-series, thereby removing noise and periodic fluctuations that occur in the short term, effectively working as a low-pass filter. Therefore, as mathematician Reginald Hooker pointed out in a publication in 1902, the moving average can serve to isolate trend and oscillatory components. He conceptualized trend as the direction in which a series is moving when oscillations are disregarded.</p>
    <p class="normal">The moving average can smooth the trend and cycle over the history of a time-series; however, as a model, the <a id="_idIndexMarker408"/>moving average can be used to forecast into the future as well. The time-series is a linear regression of the current value of the series against <a id="_idIndexMarker409"/>observed values (error terms). The moving average model of order <em class="italic">q</em>, <em class="italic">MA(q)</em>, can be denoted as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_002.png" alt="" style="height: 2em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_05_003.png" alt="" style="height: 1em;"/> is the average (expectation) of <em class="italic">x</em><sub class="" style="font-style: italic;">t</sub> (usually assumed to be 0), <img src="../Images/B17577_05_004.png" alt="" style="height: 1em;"/> are parameters, and <img src="../Images/B17577_05_005.png" alt="" style="height: 1em;"/> is random noise.</p>
    <p class="normal">Educated in Cambridge, Hooker worked in the Statistical Branch of the Ministry of Agriculture, Fisheries, and Food of the United Kingdom. He was an out-of-hours statistician, writing about meteorology and socio-economic topics such as wages, marriage rates and trade, and crop forecasting.</p>
    <p class="normal">The invention of <strong class="keyword">AR</strong> techniques dates back to a paper by British statistician <a id="_idIndexMarker410"/>Udny Yule, a personal friend of Hooker's, in 1927 ("<em class="italic">On a Method of Investigating Periodicities in Disturbed Time-Series with special reference to Wolfer's Sunspot Numbers</em>"). An <strong class="keyword">autoregressive model</strong> regresses the <a id="_idIndexMarker411"/>variable on its own lagged values. In other words, the current value <a id="_idIndexMarker412"/>of the value is driven by immediately preceding values using a linear combination.</p>
    <p class="normal">Sunspot variations are highly cyclic as can be seen in this plot of sunspot observations over time (loaded through statsmodels data utilities):</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_01.png" alt="../../../../../Downloads/Machine-Learning%20for%20Time-Series%20with%20Python/sunspot_act"/></figure>
    <p class="packt_figref">Figure 5.1: Sunspot observations by year</p>
    <p class="normal">Yule formulized a <a id="_idIndexMarker413"/>linear model driven by noise as an application to sunspot numbers, the count of dark spots on the outer shell of the sun. The spots <a id="_idIndexMarker414"/>have their origin in giant explosions and indicate magnetic activity of the sun, and phenomena such as solar flares.</p>
    <p class="normal">Here are two images of low and high solar activity according to sunspot numbers (from NASA):</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_02.png" alt="hat Will Solar Cycle 25 Look Like? | NASA"/></figure>
    <p class="packt_figref">Figure 5.2: Solar activity</p>
    <p class="normal">Today, we know that the solar cycle is a nearly periodic 11-year change in the solar magnetic activity between high magnetic activity (solar maximum) and low magnetic activity (solar minimum). At the high point, explosions (solar flares) can unleash charged particles into space, potentially endangering life on Earth.</p>
    <p class="normal">Yule studied engineering at <strong class="keyword">University College London</strong> (<strong class="keyword">UCL</strong>), went to work with Heinrich Hertz in Bonn, returned to UCL to work with Karl Pearson, and was later promoted to assistant <a id="_idIndexMarker415"/>professor at UCL. After first taking the post of statistics at UCL, he moved to Cambridge. He is remembered for his <a id="_idIndexMarker416"/>book on statistics "<em class="italic">An Introduction to the Theory of Statistics</em>," which was first published in 1911 and went through many editions, as well as for his description of the process known today as preferential attachment, where the distribution of new quantities to nodes in a network is in accordance with how much the nodes already have; this is sometimes noted as "the richer get richer." </p>
    <p class="normal">Andrey Kolmogorov defined the term <strong class="keyword">stationary process</strong> in 1931, although Louis Bachelier had introduced <a id="_idIndexMarker417"/>a similar definition earlier (1900) using different terminology. <strong class="keyword">Stationarity</strong> is defined by three characteristics: </p>
    <ol>
      <li class="numbered" value="1">Finite variation </li>
      <li class="numbered">Constant mean</li>
      <li class="numbered">Constant variation</li>
    </ol>
    <p class="normal">Constant variation <a id="_idIndexMarker418"/>means that the variation of the time-series in a window between two points is constant over time: <img src="../Images/B17577_05_006.png" alt="" style="height: 1.2em;"/>, although it can change with the size of the window.</p>
    <p class="normal">This is weak stationarity. In the literature, unless otherwise specified, usually stationarity means weak stationarity. Strict stationarity means that a time-series has a probability density function that is unchanged over time. In other words, under strict stationarity, the joint distribution over <img src="../Images/B17577_05_007.png" alt="" style="height: 1em;"/> is the same as over <img src="../Images/B17577_05_008.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">In 1938, Norwegian <a id="_idIndexMarker419"/>mathematician Herman Ole Andreas Wold described the decomposition of stationary time-series. He observed that stationary time-series can be expressed as the sum of a deterministic component (autoregressive) and a stochastic component (noise). This<a id="_idIndexMarker420"/> decomposition is termed after him today, as <strong class="keyword">Wold's decomposition</strong>. </p>
    <p class="normal">This leads to the formulation of the autoregressive model of order <img src="../Images/B17577_05_009.png" alt="" style="height: 1em;"/>, <em class="italic">AR(p)</em>, as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_010.png" alt="" style="height: 1.9em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_05_011.png" alt="" style="height: 1em;"/> is a model parameter, <em class="italic">c</em> is a constant, and <img src="../Images/B17577_05_012.png" alt="" style="height: 1em;"/> represents noise. In this equation, <em class="italic">p</em> is a measure of the autocorrelation between successive values of the time-series.</p>
    <p class="normal">This work was later, in 1951, generalized to multivariate time-series in a Ph.D. thesis ("<em class="italic">Hypothesis Testing in Time-Series</em>") by New Zealander Peter Whittle, with Wold as his advisor. Peter Whittle is also credited with the integration of the AR and MA models into one, as the <strong class="keyword">autoregressive moving average</strong> (<strong class="keyword">ARMA</strong>). This was another milestone in the history of time-series <a id="_idIndexMarker421"/>modeling, bringing together the work of Yule and Hooker. </p>
    <p class="normal">The ARMA model consists of two types of lagged values, one for the autoregressive component and the other for the moving average component. Therefore, we write <em class="italic">ARMA(p, q)</em>, with the <a id="_idIndexMarker422"/>first parameter p indicating the order of the autoregression, and the second, <em class="italic">q</em>, the order of the moving average, as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_013.png" alt="" style="height: 2em;"/></figure>
    <p class="normal">ARMA assumes that the series is stationary. In practice, to ensure stationarity, preprocessing has to be applied.</p>
    <p class="normal">The model parameters were estimated via the least-squares method until George Box and Gwilym Jenkins popularized their method of a maximum-likelihood estimation of the parameters. </p>
    <p class="normal">George Box was one of the most influential figures of not only classical time-series prediction, but also the broader field of statistics. Drafted for World War II without having finished his studies in chemistry, he performed poison gas experiments for the army, teaching himself statistics for analysis in the process. </p>
    <p class="normal">Once the war was over, he studied mathematics and statistics at the University College London, completing his Ph.D. with Egon Pearson, the son of Karl Pearson, as his advisor. He later headed a research group at Princeton, and then founded the statistics department at the University of Wisconsin–Madison.</p>
    <p class="normal">Box and Jenkin's 1970 book "<em class="italic">Time-Series Analysis: Forecasting and Control</em>" outlined many applied examples for time-series forecasting and seasonal adjustment. The so-called Box-Jenkins <a id="_idIndexMarker423"/>method is one of the most popular forecasting methods. Their book also contained a description of the <strong class="keyword">autoregressive integrated moving average</strong> model (<strong class="keyword">ARIMA</strong>). </p>
    <p class="normal">ARIMA(p, d, q) includes a data preprocessing step, called <strong class="keyword">integration</strong>, to make the time-series stationary, which <a id="_idIndexMarker424"/>is by replacing values by subtracting the immediate past <a id="_idIndexMarker425"/>values, a transformation called <strong class="keyword">differencing</strong>.</p>
    <p class="normal">The model integration is parametrized by d, which is the number of times differences have been taken between current and previous values. As mentioned, the three parameters stand for the three parts of the model. </p>
    <p class="normal">There are some special cases; ARIMA(p,0,0) stands for AR(p), ARIMA(0,d,0) for I(d), and ARIMA(0,0,q) is MA(q). I(0) is sometimes used as a convention to refer to stationary time-series, which don't require any differencing to be stationary.</p>
    <p class="normal">While ARIMA type <a id="_idIndexMarker426"/>models effectively consider stationary processes, the <strong class="keyword">Seasonal Auto Regressive Integrative Moving Average</strong> models (<strong class="keyword">SARIMA</strong>), developed as an extension of the ARMA model, can describe processes that exhibit non-stationary behaviors both within and across seasons. </p>
    <p class="normal">Seasonal ARIMA models are usually stated as ARIMA(p,d,q)(P,D,Q)m. The parameters deserve more explanation:</p>
    <ul>
      <li class="bullet">m denotes the number of periods in a season</li>
      <li class="bullet">P, D, Q parametrize the autoregressive, integration, and moving average components of the seasonal part</li>
      <li class="bullet">p, d, q refer to the ARIMA terms, which we've discussed previously</li>
    </ul>
    <p class="normal">P is a measure of autocorrelation between successive seasonal components of the time-series.</p>
    <p class="normal">We can write <a id="_idIndexMarker427"/>out the seasonal parts to make this clearer. <strong class="keyword">Seasonal Autoregression</strong>, <strong class="keyword">SAR</strong>, can be stated as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_014.png" alt="" style="height: 2em;"/></figure>
    <p class="normal">where <em class="italic">s</em> is the <a id="_idIndexMarker428"/>length of the seasonality.</p>
    <p class="normal">Similarly, the <strong class="keyword">seasonal moving average</strong>, <strong class="keyword">SMA</strong>, can be written as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_015.png" alt="" style="height: 2em;"/></figure>
    <p class="normal">Please note that each of these components will use a distinct set of parameters.</p>
    <p class="normal">For example, the model SARIMA(0,1,1)(0,1,1)12 process will contain a non-seasonal MA(1) term (with the corresponding parameter <img src="../Images/B17577_05_016.png" alt="" style="height: 1em;"/>) and a seasonal MA(1) term (with the corresponding parameter <img src="../Images/B17577_05_017.png" alt="" style="height: 1em;"/>). </p>
    <h2 id="_idParaDest-87" class="title">Model selection and order</h2>
    <p class="normal">The parameter q in ARMA would typically be 3 or less, but this is more a reflection of computing resources rather than statistics. Today, to set the parameters p and q, we would typically look at the <a id="_idIndexMarker429"/>autocorrelation and partial autocorrelation plots, where we could see peaks in the correlation for each lag. </p>
    <p class="normal">When we have different models, say models of different p and q, each trained on the same dataset, how do <a id="_idIndexMarker430"/>we know which one should we use? This is where model selection comes in. </p>
    <p class="normal">Model selection is the methodology for deciding between competing models. One of the main ideas in model selection is Occam's razor, named after the English Franciscan friar and scholastic <a id="_idIndexMarker431"/>philosopher William of Ockham, who lived between circa 1287 and 1347. </p>
    <p class="normal">According to Occam's razor, when choosing between competing solutions, one should prefer the explanation with the fewest assumptions. Ockham argued based on this idea that the principle of divine interventions is so simple that miracles are a parsimonious explanation. This rule, also called "<em class="italic">lex parsimoniae</em>" in Latin, expresses that a model should be parsimonious, which <a id="_idIndexMarker432"/>means that it should be simple yet have high explanatory power. </p>
    <p class="normal">In science, simpler explanations are preferred out of the principle of falsifiability. The simpler a scientific explanation, the easier it can be tested, and possibly refuted – this lends the model scientific rigor.</p>
    <p class="normal">ARMA and other models <a id="_idIndexMarker433"/>are usually estimated with the <strong class="keyword">maximum-likelihood estimation</strong> (<strong class="keyword">MLE</strong>). In MLE, this means maximizing a likelihood function so <a id="_idIndexMarker434"/>that, given the parameters <a id="_idIndexMarker435"/>of the model, the observed data is most likely.</p>
    <p class="normal">One of the most <a id="_idIndexMarker436"/>commonly used model selection criteria for the maximum-likelihood method is the <strong class="keyword">Akaike information criterion</strong> (<strong class="keyword">AIC</strong>), after Hirotugu Akaike, who <a id="_idIndexMarker437"/>published it first in English in 1973. </p>
    <p class="normal">AIC takes the log-likelihood <em class="italic">l</em> from the maximum-likelihood method and the number of parameters <em class="italic">k</em> in the model.</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_018.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">This is saying that the AIC equals two times the number of parameters minus two times the log-likelihood. In model selection, we would prefer the model with the lowest AIC, which means it has few parameters, but also high log-likelihood.</p>
    <p class="normal">For an ARIMA model, we could write more specifically:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_019.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">I've omitted the parameter d since it doesn't introduce additional estimations.</p>
    <p class="normal">The <strong class="keyword">Bayesian Information Criterion</strong> (<strong class="keyword">BIC</strong>) was proposed for model selection a few years later (1978), by Gideon Schwarz, and looks very much like AIC. It additionally takes <em class="italic">N</em>, the number <a id="_idIndexMarker438"/>of samples in the dataset:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_020.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">According to BIC, we want a model with few parameters and high log-likelihood, but also a small number of training examples.</p>
    <h2 id="_idParaDest-88" class="title">Exponential smoothing</h2>
    <p class="normal">Exponential smoothing, dating back to the work of Siméon Poisson, is a technique for smoothing time-series data <a id="_idIndexMarker439"/>using an exponential window function, which <a id="_idIndexMarker440"/>can be used to forecast time-series with seasonality and trend. </p>
    <p class="normal">The simplest method, <strong class="keyword">simple exponential smoothing</strong>, <strong class="keyword">SES</strong>, <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> of a time-series <em class="italic">x</em><sub class="" style="font-style: italic;">t</sub> can be denoted as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_021.png" alt="" style="height: 1.8em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B17577_05_022.png" alt="" style="height: 1.6em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_05_023.png" alt="" style="height: 0.7em;"/> is the exponential smoothing factor (a value between 0 and 1).</p>
    <p class="normal">Essentially, this is the weighted moving average with weights <em class="italic">α</em> and <img src="../Images/B17577_05_024.png" alt="" style="height: 0.8em;"/>. You can think of the second term, <img src="../Images/B17577_05_025.png" alt="" style="height: 1em;"/> as recursive, where, when expanding, <img src="../Images/B17577_05_026.png" alt="" style="height: 1em;"/> gets multiplied by itself over and over – this is the exponential term. </p>
    <p class="normal">Parameter <img src="../Images/B17577_05_027.png" alt="" style="height: 0.7em;"/> controls how much the smoothed value is determined by current versus previous values. The effect of this formula, as with moving averages, is that the result becomes smoother.</p>
    <p class="normal">Interestingly, John Muth showed in 1960 that SES provided the optimal forecasts for a time-series where, at each time step, the values take a random step away from its previous value, and steps <a id="_idIndexMarker441"/>are independently and identically distributed in size plus noise. This kind of time-series is called a random walk, and sometimes, the price of a fluctuating stock is assumed to be following such a behavior.</p>
    <p class="normal">The <strong class="keyword">Theta method</strong>, another exponential smoothing method, is of particular interest to practitioners since it performed well in the M3 competition in 2000. The M3 competition, named <a id="_idIndexMarker442"/>after Spyros Makridakis, its organizer, who is a professor at the University of Nicosia and Director of the Institute for the Future, was a competition for forecasting across 3003 time-series from micro-economics, industry, finance, demographic, and other domains. One of its main conclusions was that very simple methods can perform well with univariate time-series forecasts. The M3 competition proved to be a watershed moment for forecasting, providing benchmarks and the <strong class="keyword">state of the art</strong> (<strong class="keyword">SOTA</strong>), even though the SOTA has significantly changed since then, as we'll see in <em class="chapterRef">Chapter 7</em>, <em class="italic">Machine Learning Models for Time-Series</em>.</p>
    <p class="normal">The Theta method was <a id="_idIndexMarker443"/>proposed by Vassilis Assimakopoulos and Konstantinos Nikolopoulos in 2000 and re-stated in 2001 by Rob Hyndman and Baki Billah. The Theta model can be understood as <strong class="keyword">simple exponential smoothing</strong> (<strong class="keyword">SES</strong>) with drift. </p>
    <p class="normal">This method is based on the decomposition of the de-seasonalized data into two lines. The first so-called "theta" line estimates the long-term component, the trend, and then takes the weighted average of this trend and the SES.</p>
    <p class="normal">Let's state this more formally! The trend component is forecast like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_028.png" alt="" style="height: 1.8em;"/></figure>
    <p class="normal">In this equation, <em class="italic">c</em> is the intercept, <img src="../Images/B17577_05_029.png" alt="" style="height: 1em;"/> is a coefficient multiplied by the time step, and <img src="../Images/B17577_05_030.png" alt="" style="height: 1em;"/> is the residual. <img src="../Images/B17577_05_031.png" alt="" style="height: 1em;"/> can be fit through ordinary least-squares.</p>
    <p class="normal">The formula for Theta is then taking this trend and adding it to the SES as a weighted sum:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_032.png" alt="" style="height: 2em;"/></figure>
    <p class="normal">Here, <img src="../Images/B17577_05_033.png" alt="" style="height: 1em;"/> is the forecast for <em class="italic">X</em> at time step <em class="italic">t</em>.</p>
    <p class="normal">One of the most <a id="_idIndexMarker444"/>popular exponential smoothing methods is the <strong class="keyword">Holtz-Winters method</strong>. Charles Holtz, professor <a id="_idIndexMarker445"/>at the University of Chicago, first published a method for double exponential smoothing (1957) that allowed forecasting based on trend <a id="_idIndexMarker446"/>and level. His student Peter Winters extended the method to capture seasonality in 1960 ("<em class="italic">Forecasting sales by exponentially weighted moving averages</em>"). Even later, Holtz-Winters smoothing was extended to account for multiple seasonalities (n-order smoothing).</p>
    <p class="normal">To apply the Holtz-Winters method, we first remove trend and seasonality. Then we forecast the time-series and add back the seasonality and trend.</p>
    <p class="normal">We can distinguish additive and multiplicative variations of the method. Both trend and seasonality can be either additive or multiplicative.</p>
    <p class="normal">An additive seasonality is seasonality added independently of the values of the series. A multiplicative seasonal component is added proportionally, when the seasonal effect decreases or increases with the values (or the trend) in the time-series. A visual inspection can help in deciding between the two variations.</p>
    <p class="normal">The Holtz-Winters method is also called triple exponential smoothing because it applies exponential smoothing <a id="_idIndexMarker447"/>three times as we'll see. The Holtz-Winters method <a id="_idIndexMarker448"/>captures three components: </p>
    <ul>
      <li class="bullet">An estimate of a level for each time point, <em class="italic">L</em><sub class="" style="font-style: italic;">t</sub> – this could be an average</li>
      <li class="bullet">A trend component <em class="italic">T</em></li>
      <li class="bullet">Seasonality <em class="italic">S</em><sub class="" style="font-style: italic;">t</sub> with <em class="italic">m</em> seasons (the number of seasons in a year)</li>
    </ul>
    <p class="normal">With additive trend and <a id="_idIndexMarker449"/>seasonality, in mathematical terms, the Holtz-Winters forecast for a value <img src="../Images/B17577_05_034.png" alt="" style="height: 0.8em;"/> is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_035.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">With <strong class="keyword">multiplicative seasonality</strong>, we<a id="_idIndexMarker450"/> multiply by the seasonality:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_036.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">The level is updated as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_037.png" alt="" style="height: 2.5em;"/></figure>
    <p class="normal">We are updating the current level based on a weighted average of two terms, with <img src="../Images/B17577_05_038.png" alt="" style="height: 0.7em;"/> being the weight between them. These two terms are the previous level and the de-seasonalized value of the series.</p>
    <p class="normal">In this equation, we are de-seasonalizing the series by dividing by the seasonality:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_039.png" alt="" style="height: 2.5em;"/></figure>
    <p class="normal">And the previous trend component gets added up to the previous level like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_040.png" alt="" style="height: 1.8em;"/></figure>
    <p class="normal">The trend <a id="_idIndexMarker451"/>update is as follows (for additive trend):</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_041.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">Finally, the (multiplicative) seasonality update is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_042.png" alt="" style="height: 2.5em;"/></figure>
    <p class="normal">We can switch these equations to additive variations as required. A more detailed treatment is beyond the scope of this book – we'll leave it here.</p>
    <h2 id="_idParaDest-89" class="title">ARCH and GARCH</h2>
    <p class="normal">Robert F. Engle, professor of economics <a id="_idIndexMarker452"/>at MIT, proposed a model for time-series forecasting (1982) that <a id="_idIndexMarker453"/>he named <strong class="keyword">ARCH</strong> (<strong class="keyword">Auto-Regressive Conditionally Heteroscedastic</strong>). </p>
    <p class="normal">For financial institutions, value at risk, the level of financial risk over a specific time period, is an important concept for risk management. Therefore, it is crucial to account for the covariance structure of asset returns. This is what ARCH does and explains its importance.</p>
    <p class="normal">In fact, in recognition of his contributions to the field of time-series econometrics, Engle was awarded the 2003 Nobel Prize in Economics (Nobel Memorial Prize in Economic Sciences), together <a id="_idIndexMarker454"/>with Clive Granger, who we encountered earlier. The citation specifically mentioned his groundbreaking work on ARCH.</p>
    <p class="normal">While, in ARMA-type models, returns are modeled as independent and identically distributed over time, ARCH allows for time-varying (heteroscedastic) error terms by parametrizing higher-order dependence between returns observed at varying frequencies.</p>
    <p class="normal">In ARCH, the residuals are expressed as consisting of a stochastic, <em class="italic">z</em><sub class="" style="font-style: italic;">t</sub>, and a standard deviation, <img src="../Images/B17577_05_043.png" alt="" style="height: 1em;"/>, both of which are time-dependent: <img src="../Images/B17577_05_044.png" alt="" style="height: 1em;"/>.</p>
    <p class="normal">The standard deviation of the residual at time <em class="italic">t</em> is modeled depending on the residuals of the series at previous points:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_045.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">where q is the number of preceding time points the variance depends on.</p>
    <p class="normal">The model ARCH(q) can be determined using least-squares.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The least-squares algorithm is to solve the linear equations y=X.β for β. It consists of finding <a id="_idIndexMarker455"/>the parameters that minimize the square of the error, <img src="../Images/B17577_05_046.png" alt="" style="height: 1.5em;"/>. </p>
    </div>
    <p class="normal"><strong class="keyword">GARCH</strong> (<strong class="keyword">generalized ARCH</strong>) was born when Tim Bollerslev (1986) and Stephen Taylor (1986) both <a id="_idIndexMarker456"/>independently extended Engle's model to make it more <a id="_idIndexMarker457"/>general. The main difference between GARCH and ARCH is that the residuals come from an ARCH model rather than from an autoregressive model, AR.</p>
    <p class="normal">Generally, before applying a GARCH or ARCH model, a statistical test for homoscedasticity is applied, in other words, whether the variance is constant over time. Commonly used is the ARCH-LM test, which works with the null hypothesis that the time-series has no ARCH effects.</p>
    <h2 id="_idParaDest-90" class="title">Vector autoregression</h2>
    <p class="normal">All the previous forecasting methods presented in this chapter are for univariate time-series, that is, time-series that consist of a single time-dependent variable, a single vector. In practice, we usually know more than our single sequence of measurements. </p>
    <p class="normal">For example, if our time-series is about the number of ice cream sales, we might know the temperatures or the sales of bathing suits. We could expect that the sales of ice cream are highly <a id="_idIndexMarker458"/>correlated to temperature, in fact, we might expect that ice cream is increasingly consumed when temperatures are high. Equally, we could speculate that the sales of bathing suits either coincide, pre-date, or post-date the sales of ice cream. </p>
    <p class="normal">Vector autoregression models can track the relationships between several variables as they change over time. They <a id="_idIndexMarker459"/>can capture the linear dependence of a time-series on a vector of values that precedes the current timestamp, generalizing the AR model to multivariate time-series. </p>
    <p class="normal">VAR models are <a id="_idIndexMarker460"/>characterized by their order, which refers to the number of preceding time points <a id="_idIndexMarker461"/>that go into the model. The simplest case, VAR(1), where the model takes just one lag of the series, can be formulated as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_047.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">where <em class="italic">c</em> is a constant, the intercept of the line, <img src="../Images/B17577_05_048.png" alt="" style="height: 1em;"/> are the coefficients of the model, and <img src="../Images/B17577_05_049.png" alt="" style="height: 1em;"/> is the error term at point t. <em class="italic">x</em><sub class="" style="font-style: italic;">t</sub> and <em class="italic">c</em> are vectors of length <em class="italic">k</em>, while <img src="../Images/B17577_05_050.png" alt="" style="height: 1em;"/> is a <img src="../Images/B17577_05_051.png" alt="" style="height: 0.7em;"/> matrix.</p>
    <p class="normal">A p-order model, VAR(p), is denoted as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_052.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">VAR assumes that the error terms have a mean of 0 and that there is no serial correlation of error terms.</p>
    <p class="normal">Just like vector autoregression is a multivariate generalization of autoregression, <strong class="keyword">vector ARIMA</strong> (<strong class="keyword">VARIMA</strong>) is an extension of the univariate ARIMA model to multivariate time-series. Although it was formalized already as early as 1957, available software implementations only appeared much later.</p>
    <p class="normal">In the next section, we'll look at a few libraries in Python that we can use for forecasting with classical models. </p>
    <h1 id="_idParaDest-91" class="title">Python libraries</h1>
    <p class="normal">There are a few popular libraries for classical time-series modeling in Python, but the most popular by <a id="_idIndexMarker462"/>far is statsmodels. The following chart compares the popularity of libraries in terms of the number of stars on GitHub:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_03.png" alt="forecasting_libraries.png"/></figure>
    <p class="packt_figref">Figure 5.3: Popularity of Python libraries for classical time-series forecasting</p>
    <p class="normal">Statsmodels is clearly the most popular among these libraries. I've only chosen to include libraries <a id="_idIndexMarker463"/>that are actively maintained and that implement the algorithms directly rather than importing them from other libraries. The SkTime or Darts libraries, for example, offer traditional forecasting models, but they are not implemented there, but in statsmodels.</p>
    <p class="normal">pmdarima (originally pyramid-arima) contains<a id="_idIndexMarker464"/> a parameter search to help fit the best ARIMA model to univariate time-series. Anticipy<a id="_idIndexMarker465"/> contains a number of models, such as exponential <a id="_idIndexMarker466"/>decay and step models. Arch implements tools for financial econometrics and functionality for <strong class="keyword">Autoregressive Conditional Heteroscedasticity</strong> (<strong class="keyword">ARCH</strong>).</p>
    <p class="normal">While not as active as Scikit-Learn and only maintained by a couple of people, statsmodels is the go-to library for traditional statistics and econometrics approaches to time-series, with a much stronger emphasis on parameter estimation and statistical testing than machine learning.</p>
    <h2 id="_idParaDest-92" class="title">Statsmodels</h2>
    <p class="normal">The statsmodels library <a id="_idIndexMarker467"/>can help to estimate statistical models <a id="_idIndexMarker468"/>and perform statistical tests. It's built on SciPy and NumPy and has lots of statistical functions and models. </p>
    <p class="normal">The following table illustrates some of the modeling classes relevant to this chapter:</p>
    <table id="table001-3" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-"> Class </p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Description</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">ar_model.AutoReg</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Univariate Autoregression Model</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">arima.model.ARIMA</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Autoregressive Integrated Moving Average (ARIMA) model</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">ExponentialSmoothing</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Holt Winter's Exponential Smoothing</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">SimpleExpSmoothing</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Simple Exponential Smoothing</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 5.4: A few models implemented in statsmodels</p>
    <p class="normal">The ARIMA class also has functionality for SARIMA through a <em class="italic">seasonal_order</em> parameter, ARIMA, with seasonal components. By definition, ARIMA also supports MA, AR, and differencing (integration).</p>
    <p class="normal">There are some other<a id="_idIndexMarker469"/> models, such as MarkovAutoregression, but we <a id="_idIndexMarker470"/>won't go through all of these – we will work through a selection.</p>
    <p class="normal">Some other useful functions are listed here:</p>
    <table id="table002-2" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Function</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Description</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">stattools.kpss</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Kwiatkowski-Phillips-Schmidt-Shin test for stationarity</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">stattools.adfuller</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Augmented Dickey-Fuller unit root test</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">stattools.ccf</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">The cross-correlation function</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">stattools.pacf</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Partial autocorrelation estimate</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">stats.diagnostic.het_arch</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH), also referred to as the ARCH-LM test</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">stattools.q_stat</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Ljung-Box Q Statistic</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tsa.seasonal.seasonal_decompose</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Seasonal decomposition using moving averages</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tsa.tsatools.detrend</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Detrend a vector</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Figure 5.5: Useful functions in statsmodels</p>
    <p class="normal">As a convention, we import statsmodels like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm
</code></pre>
    <p class="normal">These statsmodels algorithms <a id="_idIndexMarker471"/>are also available through SkTime, which makes <a id="_idIndexMarker472"/>them available through an interface similar to the Sklearn interface. </p>
    <p class="normal">This should be enough for a brief overview. Let's get into the modeling itself!</p>
    <h1 id="_idParaDest-93" class="title">Python practice</h1>
    <p class="normal">As mentioned in <a id="_idIndexMarker473"/>the introduction to this chapter, we are going to be using the statsmodels library for modeling. </p>
    <h2 id="_idParaDest-94" class="title">Requirements</h2>
    <p class="normal">In this chapter, we'll use several libraries, which we can quickly install from the terminal (or similarly from the anaconda navigator):</p>
    <pre class="programlisting con"><code class="hljs-con">pip install statsmodels pandas_datareader
</code></pre>
    <p class="normal">We'll execute the commands from the Python (or IPython) terminal, but equally, we could execute them from a Jupyter notebook (or a different environment).</p>
    <p class="normal">Let's get down to modeling!</p>
    <h2 id="_idParaDest-95" class="title">Modeling in Python</h2>
    <p class="normal">We'll work with a stock<a id="_idIndexMarker474"/> ticker dataset from Yahoo finance that we'll download through the yfinance library. We'll first <a id="_idIndexMarker475"/>load the dataset, do some quick exploration, and then we'll build several models mentioned in this chapter.</p>
    <p class="normal">We'll load a series of Standard &amp; Poor's depositary receipts (SPDR S&amp;P 500 ETF Trust):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">import</span> yfinance <span class="hljs-keyword">as</span> yf
  
start_date = datetime(<span class="hljs-number">2005</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
end_date = datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
df = yf.download(
    <span class="hljs-string">'SPY'</span>,
    start=start_date,
    end = end_date
)
</code></pre>
    <p class="normal">We have to indicate the date range and the ticker symbol. The daily prices come for Open, Close, and others. We'll work with the Open prices. </p>
    <p class="normal">The index <a id="_idIndexMarker476"/>column is already <a id="_idIndexMarker477"/>a pandas DateTimeIndex so we don't have to convert it. Let's plot the series!</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.title(<span class="hljs-string">'Opening Prices between {} and {}'</span>.<span class="hljs-built_in">format</span>(
    start_date.date().isoformat(),
    end_date.date().isoformat()
))
df[<span class="hljs-string">'Open'</span>].plot()
plt.ylabel(<span class="hljs-string">'Price'</span>)
plt.xlabel(<span class="hljs-string">'Date'</span>);
</code></pre>
    <p class="normal">This gives us the following graph: </p>
    <figure class="mediaobject"><img src="../Images/B17577_05_04.png" alt="ticker_price.png"/></figure>
    <p class="packt_figref">Figure 5.6: Standard &amp; Poor's depositary receipt prices over time</p>
    <p class="normal">Since this is daily data, and <a id="_idIndexMarker478"/>there are either 253 or 252 working days in the year, I've decided to resample<a id="_idIndexMarker479"/> the data to weekly data and make each year consistent.</p>
    <pre class="programlisting code"><code class="hljs-code">df1 = df.reset_index().resample(<span class="hljs-string">'W'</span>, on=<span class="hljs-string">"Date"</span>)[<span class="hljs-string">'Open'</span>].mean()
df1 = df1[df1.index.week &lt; <span class="hljs-number">53</span>]
</code></pre>
    <p class="normal">Some years have 53 weeks. We can't handle that, so we'll get rid of the 53rd week. We now have weekly data over 52 weeks across 16 years.</p>
    <p class="normal">One final fix: statsmodels can use the frequency information associated with the DateTimeIndex; however, this is often not set and <code class="Code-In-Text--PACKT-">df1.index.freq </code>is <code class="Code-In-Text--PACKT-">None</code>. So, we'll set it ourselves:</p>
    <pre class="programlisting code"><code class="hljs-code">df1 = df1.asfreq(<span class="hljs-string">'W'</span>).fillna(method=<span class="hljs-string">'ffill'</span>)
</code></pre>
    <p class="normal">If we check now, <code class="Code-In-Text--PACKT-">df1.index.freq</code> is <code class="Code-In-Text--PACKT-">&lt;Week: weekday=6&gt;.</code></p>
    <p class="normal">Setting the frequency can lead to missing values. Therefore, we are carrying over from the last valid value for missing values with the <code class="Code-In-Text--PACKT-">fillna()</code> operation. If we don't do this, some of the models won't converge and give us NaN (not a number) values back instead of forecasts.</p>
    <p class="normal">Now we need to get some idea of reasonable ranges for<a id="_idIndexMarker480"/> the order of the model. We'll look at the <a id="_idIndexMarker481"/>autocorrelation and partial autocorrelation functions for this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> statsmodels.api <span class="hljs-keyword">as</span> sm
fig, axs = plt.subplots(<span class="hljs-number">2</span>)
fig.tight_layout()
sm.graphics.tsa.plot_pacf(df1, lags=<span class="hljs-number">20</span>, ax=axs[<span class="hljs-number">0</span>])
sm.graphics.tsa.plot_acf(df1, lags=<span class="hljs-number">20</span>, ax=axs[<span class="hljs-number">1</span>])
</code></pre>
    <p class="normal">This gives us the following graph: </p>
    <figure class="mediaobject"><img src="../Images/B17577_05_05.png" alt="pcf_acf.png"/></figure>
    <p class="packt_figref">Figure 5.7: Partial autocorrelation and autocorrelation</p>
    <p class="normal">These graphs show the correlation of the time-series with itself at lags of up to 20 time steps. R or <img src="../Images/B17577_05_053.png" alt="" style="height: 0.36em;"/> values close to 0 mean that consecutive observations at the lags are not correlated with one another. Inversely, correlations close to 1 or -1 indicate that there exists a strong positive or negative correlation between these observations at the lags.</p>
    <p class="normal">Both the autocorrelation and the partial autocorrelation return confidence intervals. The correlation is significant if it goes beyond the confidence interval (represented as shaded regions).</p>
    <p class="normal">We can see that the partial autocorrelation with lag 1 is very high and much lower for higher lags. The <a id="_idIndexMarker482"/>autocorrelation is significant and high for all lags, but the significance drops <a id="_idIndexMarker483"/>as the lag increases.</p>
    <p class="normal">Let's move on to the autoregressive model. From here on, we'll use the statsmodels modeling functionality. The interface is very convenient, as you'll see.</p>
    <p class="normal">We can't use an autoregressive model straight off because it needs the time-series to be stationary, which means the mean and variance is constant over time – no seasonality, no trend. </p>
    <p class="normal">We can use statsmodels utilities to look at seasonality and trend from the time-series:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> statsmodels.tsa.seasonal <span class="hljs-keyword">import</span> seasonal_decompose
result = seasonal_decompose(df, model=<span class="hljs-string">'additive'</span>, period=<span class="hljs-number">52</span>)
result.plot()
</code></pre>
    <p class="normal">We set the period to 1 because each data point (row) corresponds to a year. </p>
    <p class="normal">Let's see what the components look like:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_06.png" alt="seasonal_decompose.png"/></figure>
    <p class="packt_figref">Figure 5.8: Seasonal decomposition of the time-series</p>
    <p class="normal">The first subplot is the original time-series. There are both seasonality and trend in this dataset, which <a id="_idIndexMarker484"/>you can see separated out in the subplots.</p>
    <p class="normal">As discussed, we need a <a id="_idIndexMarker485"/>stationary series for modeling. To establish stationarity, we need to remove the seasonal and trend components. We could also take off the seasonal or trend components that we estimated just before. Alternatively, we can use wrapper functionality in statsmodels or set the d parameter in ARIMA.</p>
    <p class="normal">We can use the Augmented Dickey-Fuller and KPSS tests to check for stationarity:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> arch.unitroot <span class="hljs-keyword">import</span> KPSS, ADF
ADF(df1)
</code></pre>
    <p class="normal">We could have used <code class="Code-In-Text--PACKT-">statsmodels.tsa.stattools.adfuller</code> or <code class="Code-In-Text--PACKT-">statsmodels.tsa.stattools.kpss</code>, but we prefer the convenience of the ARCH library versions. We are leaving it to the user to check the output of the KPSS test. We get the following output:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_07.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_Y7Tw2W/Screenshot 2021-08-22 at 15.36.52.png"/></figure>
    <p class="packt_figref">Figure 5.9: Output from the KPSS test for stationarity</p>
    <p class="normal">Given the p-value of 0.997, we can reject our null hypothesis of the unit root, and we conclude that our process is weakly stationary.</p>
    <p class="normal">So how do we find <a id="_idIndexMarker486"/>good values for the differencing? We can use the pmdarima library for <a id="_idIndexMarker487"/>this, where there is a function for precisely this purpose:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pmdarima.arima.utils <span class="hljs-keyword">import</span> ndiffs
<span class="hljs-comment"># ADF Test:</span>
ndiffs(df1, test=<span class="hljs-string">'adf'</span>)
</code></pre>
    <p class="normal">We get a value of 1. We would get the same values for the KPSS and the PP tests. This means that we can work off the first difference.</p>
    <p class="normal">Let's start with an autoregressive model. </p>
    <p class="normal">As a reminder, ARIMA is parametrized with parameters p, d, q, where:</p>
    <ul>
      <li class="bullet">p is for the autoregressive model: AR(p)</li>
      <li class="bullet">d is for the integration</li>
      <li class="bullet">q is for the moving average: MA(q)</li>
    </ul>
    <p class="normal">Therefore, ARIMA(p, d, 0) is AR(p) with a differencing of order d.</p>
    <p class="normal">It is reassuring to know that statsmodels checks and warns if the stationarity assumption is not warranted. Let's try to run to fit the following AR model:</p>
    <pre class="programlisting code"><code class="hljs-code">mod = sm.tsa.arima.ARIMA(endog=df, order=(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>))
res = mod.fit()
print(res.summary())
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.
  warn('Non-stationary starting autoregressive parameters'
</code></pre>
    <p class="normal">Since we already know <a id="_idIndexMarker488"/>we need a differencing of one degree, we can set d to 1. Let's<a id="_idIndexMarker489"/> try again. This time, we'll use the <code class="Code-In-Text--PACKT-">STLForecast</code> wrapper that removes seasonality and adds it back in. This is necessary since ARIMA can't handle seasonality out of the box:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> statsmodels.tsa.forecasting.stl <span class="hljs-keyword">import</span> STLForecast
mod = STLForecast(
  df1, sm.tsa.arima.ARIMA,
  model_kwargs=<span class="hljs-built_in">dict</span>(order=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>), trend=<span class="hljs-string">"t"</span>)
)
res = mod.fit().model_result
print(res.summary())
</code></pre>
    <p class="normal">We get this summary:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_08.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_EjesO5/Screenshot 2021-08-22 at 17.07.45.png"/></figure>
    <p class="packt_figref">Figure 5.10: Summary of our ARIMA model</p>
    <p class="normal">This result summary gives <a id="_idIndexMarker490"/>all the key statistics. We see that the model was ARIMA(1, 1, 0). The<a id="_idIndexMarker491"/> log-likelihood was -1965. We also see the BIC and AIC values that we can use for model selection if we want.</p>
    <p class="normal">Please note that we need to set <code class="Code-In-Text--PACKT-">trend="t"</code> here so that the model includes a constant. If not, we would get a spurious regression.</p>
    <p class="normal">How can we use this model? Let's do some forecasting!</p>
    <pre class="programlisting code"><code class="hljs-code">STEPS = <span class="hljs-number">20</span>
forecasts_df = res.get_forecast(steps=STEPS).summary_frame() 
</code></pre>
    <p class="normal">This gives us a forecast 20 steps into the future.</p>
    <p class="normal">Let's visualize this!</p>
    <pre class="programlisting code"><code class="hljs-code">ax = df1.plot(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
plt.ylabel(<span class="hljs-string">'SPY'</span>)
forecasts_df[<span class="hljs-string">'mean'</span>].plot(style=<span class="hljs-string">'k--'</span>)
ax.fill_between(
    forecasts_df.index,
    forecasts_df[<span class="hljs-string">'mean_ci_lower'</span>],
    forecasts_df[<span class="hljs-string">'mean_ci_upper'</span>],
    color=<span class="hljs-string">'k'</span>,
    alpha=<span class="hljs-number">0.1</span>
)
</code></pre>
    <p class="normal"> Here's what we get:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_09.png" alt="spy_forecast.png"/></figure>
    <p class="packt_figref">Figure 5.11: Forecast of prices for the SPY ticker symbol</p>
    <p class="normal">The solid line is the data that we know. The dotted line represents our forecast 20 years into the future. The<a id="_idIndexMarker492"/> gray <a id="_idIndexMarker493"/>area around our forecast is the 95% confidence interval.</p>
    <p class="normal">This doesn't look too bad. It's left as an exercise to the reader to try with different parameters. Interesting ones to change are the trend parameter and the order of the model.</p>
    <p class="normal">For the moving average, let's create different models to see the difference in their forecasts!</p>
    <p class="normal">First, we'll produce the forecasts:</p>
    <pre class="programlisting code"><code class="hljs-code">forecasts = []
qs = []
<span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">30</span>, <span class="hljs-number">10</span>):
    mod = STLForecast(
            df1, sm.tsa.arima.ARIMA, 
            model_kwargs=<span class="hljs-built_in">dict</span>(order=(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, q), trend=<span class="hljs-string">"t"</span>)
        )
    res = mod.fit()
    print(<span class="hljs-string">f"aic (</span><span class="hljs-subst">{q}</span><span class="hljs-string">): </span><span class="hljs-subst">{res.aic}</span><span class="hljs-string">"</span>)
    forecasts.append(
            res.get_forecast(steps=STEPS).summary_frame()[<span class="hljs-string">'mean'</span>]
        )
    qs.append(q)
forecasts_df = pd.concat(forecasts, axis=<span class="hljs-number">1</span>)
forecasts_df.columns = qs
</code></pre>
    <p class="normal">In the loop, we are <a id="_idIndexMarker494"/>iterating over different q parameters, choosing 0, 10, and 20. We <a id="_idIndexMarker495"/>estimate moving average models with these values of q and forecast 20 years ahead. We also print the AIC values corresponding to each q. This is the output we get:</p>
    <pre class="programlisting con"><code class="hljs-con">aic (0): 3989.0104184919096
aic (10): 3934.375909262983
aic (20): 3935.3355340835
</code></pre>
    <p class="normal">Now, let's plot the three forecasts similar to how we did before:</p>
    <pre class="programlisting code"><code class="hljs-code">ax = df1.plot()
plt.ylabel(<span class="hljs-string">'SPY'</span>)
forecasts_df.plot(ax=ax)
</code></pre>
    <p class="normal">Here's the new plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_10.png" alt="forecasts_qs.png"/></figure>
    <p class="packt_figref">Figure 5.12: Forecasts using different q parameters</p>
    <p class="normal">So, which one of these models is statistically better?</p>
    <p class="normal">Let's get back to the AIC. The lower the AIC value, the better the model given its log-likelihood and the number of parameters.</p>
    <p class="normal">In this case, the order <a id="_idIndexMarker496"/>of 10 gives us the lowest AIC, and according to this criterion, we should therefore choose q=10. Of course, we only tried three different values. I'll leave it as an exercise to come up with a more reasonable parameter value for q.</p>
    <p class="normal">Please note that the pmdarima library has functionality for finding the optimal parameter values, and the SkTime library provides an implementation for automatic discovery of the optimal order of an ARIMA model: AutoARIMA.</p>
    <p class="normal">Let's move on and make a forecast using an exponential smoothing model.</p>
    <p class="normal">In the loop, we are iterating over different q parameters, choosing 0, 10, and 20. We estimate moving average models with these values of q and forecast 20 years ahead. We also <a id="_idIndexMarker497"/>print the AIC values corresponding to each q. This is what we get:</p>
    <pre class="programlisting code"><code class="hljs-code">mod = sm.tsa.ExponentialSmoothing(
        endog=df1, trend=<span class="hljs-string">'add'</span>
    )
res = mod.fit()
</code></pre>
    <p class="normal">This fits the <a id="_idIndexMarker498"/>model to our data.</p>
    <p class="normal">Let's get the forecasts for the next 20 years:</p>
    <pre class="programlisting code"><code class="hljs-code">forecasts = pd.Series(res.forecast(steps=STEPS))
</code></pre>
    <p class="normal">Now, let's plot the forecasts:</p>
    <pre class="programlisting code"><code class="hljs-code">ax = df.plot(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
plt.ylabel(<span class="hljs-string">'SPY'</span>)
forecasts.plot(style=<span class="hljs-string">'k--'</span>)
</code></pre>
    <p class="normal">Here's the plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_11.png" alt="exponential_forecast.png"/></figure>
    <p class="packt_figref">Figure 5.13: Exponential smoothing forecast</p>
    <p class="normal">Until now, we've just looked at the graphs of 20 step-ahead forecasts. We still haven't been very sophisticated <a id="_idIndexMarker499"/>with an analysis of our model performance. Let's have a look at the errors! </p>
    <p class="normal">For this, we first have to split our dataset into training and testing. We can do an n-step ahead forecast and check the error. We'll just take the time-series running up to a certain time point as the training data, and the time points after that as test data, where we can compare the predictions to actual data points:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> statsmodels.tsa.forecasting.theta <span class="hljs-keyword">import</span> ThetaModel
train_length = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(df1) * <span class="hljs-number">0.8</span>)
tm = ThetaModel(df1[:train_length], method=<span class="hljs-string">"auto"</span>,deseasonalize=<span class="hljs-literal">True</span>)
res = tm.fit()
forecasts = res.forecast(steps=<span class="hljs-built_in">len</span>(df1)-train_length)
ax = df1.plot(figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">6</span>))
plt.ylabel(<span class="hljs-string">'SPY'</span>)
forecasts.plot(style=<span class="hljs-string">'k--'</span>)
</code></pre>
    <p class="normal">Here's the plot:</p>
    <figure class="mediaobject"><img src="../Images/B17577_05_12.png" alt="theta_forecast.png"/></figure>
    <p class="packt_figref">Figure 5.14: ThetaModel forecast</p>
    <p class="normal">The dotted line is the prediction. It doesn't seem to line up well with the actual behavior of the time-series. Let's quantify <a id="_idIndexMarker500"/>this using one of the error metrics we've discussed in the previous chapter, <em class="italic">Introduction to Machine Learning for Time-Series</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
metrics.mean_squared_error(forecasts, df1[train_length:], squared=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">We get a value of <code class="Code-In-Text--PACKT-">37.06611385754943</code>. This is the root mean squared error (we are setting the <code class="Code-In-Text--PACKT-">squared</code> parameter to <code class="Code-In-Text--PACKT-">False</code>).</p>
    <p class="normal">In forecasting competitions, such as on the Kaggle website, the lowest error wins. In real life, parsimony (simplicity) is important as well; however, we usually still aim for the lowest error (by whichever chosen metric) that we can get.</p>
    <p class="normal">There are a lot more models to explore and to play around with, but it's time to conclude this chapter.</p>
    <h1 id="_idParaDest-96" class="title">Summary</h1>
    <p class="normal">In this chapter, we've talked about time-series forecasting based on moving averages and autoregression. This topic comprises a large set of models that are very popular in different disciplines, such as econometrics and statistics. These models constitute a mainstay in time-series modeling and provide state-of-the-art forecasts. </p>
    <p class="normal">We've discussed autoregression and moving averages models, and others that combine these two, including ARMA, ARIMA, VAR, GARCH, and others. In the practice session, we've applied a few models to a dataset of stock ticker prices.</p>
  </div>
</body></html>
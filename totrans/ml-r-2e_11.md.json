["```py\n> modelLookup(\"C5.0\")\n model parameter                 label forReg forClass probModel\n1  C5.0    trials # Boosting Iterations  FALSE     TRUE      TRUE\n2  C5.0     model            Model Type  FALSE     TRUE      TRUE\n3  C5.0    winnow                Winnow  FALSE     TRUE      TRUE\n\n```", "```py\n> library(caret)\n> set.seed(300)\n> m <- train(default ~ ., data = credit, method = \"C5.0\")\n\n```", "```py\n> p <- predict(m, credit)\n\n```", "```py\n> table(p, credit$default)\n\np      no yes\n no  700   2\n yes   0 298\n\n```", "```py\n> head(predict(m, credit))\n[1] no  yes no  no  yes no\nLevels: no yes\n\n```", "```py\n> head(predict(m, credit, type = \"prob\"))\n no        yes\n1 0.9606970 0.03930299\n2 0.1388444 0.86115561\n3 1.0000000 0.00000000\n4 0.7720279 0.22797208\n5 0.2948062 0.70519385\n6 0.8583715 0.14162851\n\n```", "```py\n> ctrl <- trainControl(method = \"cv\", number = 10,\n selectionFunction = \"oneSE\")\n\n```", "```py\n> grid <- expand.grid(.model = \"tree\",\n .trials = c(1, 5, 10, 15, 20, 25, 30, 35),\n .winnow = \"FALSE\")\n\n```", "```py\n> grid\n .model .trials .winnow\n1   tree       1   FALSE\n2   tree       5   FALSE\n3   tree      10   FALSE\n4   tree      15   FALSE\n5   tree      20   FALSE\n6   tree      25   FALSE\n7   tree      30   FALSE\n8   tree      35   FALSE\n\n```", "```py\n> set.seed(300)\n> m <- train(default ~ ., data = credit, method = \"C5.0\",\n metric = \"Kappa\",\n trControl = ctrl,\n tuneGrid = grid)\n\n```", "```py\n> m\n\n```", "```py\n> library(ipred)\n> set.seed(300)\n> mybag <- bagging(default ~ ., data = credit, nbagg = 25)\n\n```", "```py\n> credit_pred <- predict(mybag, credit)\n> table(credit_pred, credit$default)\n\ncredit_pred  no yes\n no  699   2\n yes   1 298\n\n```", "```py\n> library(caret)\n> set.seed(300)\n> ctrl <- trainControl(method = \"cv\", number = 10)\n> train(default ~ ., data = credit, method = \"treebag\",\n trControl = ctrl)\n\nBagged CART \n\n1000 samples\n 16 predictor\n 2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \n\nSummary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n\nResampling results\n\n Accuracy  Kappa      Accuracy SD  Kappa SD \n 0.735     0.3297726  0.03439961   0.08590462\n\n```", "```py\n> str(svmBag)\nList of 3\n $ fit      :function (x, y, ...) \n $ pred     :function (object, x) \n $ aggregate:function (x, type = \"class\")\n\n```", "```py\n> svmBag$fit\nfunction (x, y, ...) \n{\n library(kernlab)\n out <- ksvm(as.matrix(x), y, prob.model = is.factor(y), ...)\n out\n}\n<environment: namespace:caret>\n\n```", "```py\n> bagctrl <- bagControl(fit = svmBag$fit, \n predict = svmBag$pred,\n aggregate = svmBag$aggregate)\n\n```", "```py\n> set.seed(300)\n> svmbag <- train(default ~ ., data = credit, \"bag\",\n trControl = ctrl, bagControl = bagctrl)\n> svmbag\n\nBagged Model\n1000 samples\n 16 predictors\n 2 classes: 'no', 'yes' \n\nNo pre-processing\nResampling: Cross-Validation (10 fold) \n\nSummary of sample sizes: 900, 900, 900, 900, 900, 900, ... \n\nResampling results\n\n Accuracy  Kappa      Accuracy SD  Kappa SD \n 0.728     0.2929505  0.04442222   0.1318101\n\nTuning parameter 'vars' was held constant at a value of 35\n\n```", "```py\n> set.seed(300)\n> m_adaboost <- boosting(default ~ ., data = credit)\n\n```", "```py\n> p_adaboost <- predict(m_adaboost, credit)\n\n```", "```py\n> head(p_adaboost$class)\n[1] \"no\"  \"yes\" \"no\"  \"no\"  \"yes\" \"no\"\n\n```", "```py\n> p_adaboost$confusion\n Observed Class\nPredicted Class  no yes\n no  700   0\n yes   0 300\n\n```", "```py\n> set.seed(300)\n> adaboost_cv <- boosting.cv(default ~ ., data = credit)\n\n```", "```py\n> adaboost_cv$confusion\n Observed Class\nPredicted Class  no yes\n no  594 151\n yes 106 149\n\n```", "```py\n> library(vcd)\n> Kappa(adaboost_cv$confusion)\n value       ASE\nUnweighted 0.3606965 0.0323002\nWeighted   0.3606965 0.0323002\n\n```", "```py\n> library(randomForest)\n> set.seed(300)\n> rf <- randomForest(default ~ ., data = credit)\n\n```", "```py\n> rf\n\nCall:\n randomForest(formula = default ~ ., data = credit) \n Type of random forest: classification\n Number of trees: 500\nNo. of variables tried at each split: 4\n\n OOB estimate of error rate: 23.8%\nConfusion matrix:\n no yes class.error\nno  640  60  0.08571429\nyes 178 122  0.59333333\n\n```", "```py\n> library(caret)\n> ctrl <- trainControl(method = \"repeatedcv\",\n number = 10, repeats = 10)\n\n```", "```py\n> grid_rf <- expand.grid(.mtry = c(2, 4, 8, 16))\n\n```", "```py\n> set.seed(300)\n> m_rf <- train(default ~ ., data = credit, method = \"rf\",\n metric = \"Kappa\", trControl = ctrl,\n tuneGrid = grid_rf)\n\n```", "```py\n> grid_c50 <- expand.grid(.model = \"tree\",\n .trials = c(10, 20, 30, 40),\n .winnow = \"FALSE\")\n> set.seed(300)\n> m_c50 <- train(default ~ ., data = credit, method = \"C5.0\",\n metric = \"Kappa\", trControl = ctrl,\n tuneGrid = grid_c50)\n\n```", "```py\n> m_rf\n\nResampling results across tuning parameters:\n\n mtry  Accuracy  Kappa      Accuracy SD  Kappa SD \n 2    0.7247    0.1284142  0.01690466   0.06364740\n 4    0.7499    0.2933332  0.02989865   0.08768815\n 8    0.7539    0.3379986  0.03107160   0.08353988\n 16    0.7556    0.3613151  0.03379439   0.08891300\n\n```", "```py\n> m_c50\n\nResampling results across tuning parameters:\n\n trials  Accuracy  Kappa      Accuracy SD  Kappa SD \n 10      0.7325    0.3215655  0.04021093   0.09519817\n 20      0.7343    0.3268052  0.04033333   0.09711408\n 30      0.7381    0.3343137  0.03672709   0.08942323\n 40      0.7388    0.3335082  0.03934514   0.09746073\n\n```"]
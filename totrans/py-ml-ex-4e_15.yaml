- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Making Decisions in Complex Environments with Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we focused on multimodal models for image and text
    co-learning. The last chapter of this book will be about reinforcement learning,
    which is the third type of machine learning task mentioned at the beginning of
    the book. You will see how learning from experience and learning by interacting
    with the environment differs from previously covered supervised and unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the working environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing reinforcement learning with examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the FrozenLake environment with dynamic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing Monte Carlo learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the Taxi problem with the Q-learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the working environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s get started with setting up the working environment needed for this chapter,
    including Gymnasium (which builds upon OpenAI Gym), the toolkit that gives you
    a variety of environments to develop your learning algorithms on.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing OpenAI Gym and Gymnasium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**OpenAI Gym** was a toolkit for developing and comparing reinforcement learning
    algorithms. It provided a collection of environments, or “tasks,” in which reinforcement
    learning agents can interact and learn. These environments range from simple grid-world
    games to complex simulations of real-world scenarios, allowing researchers and
    developers to experiment with a wide variety of reinforcement learning algorithms.
    It was developed by OpenAI, focused on building safe and beneficial **Artificial
    General Intelligence** (**AGI**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key features of OpenAI Gym included:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment interface**: Gym provided a consistent interface for interacting
    with environments, allowing agents to observe states, take actions, and receive
    rewards (we will learn about these terms).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extensive collection of environments**: Gym offered a diverse set of environments,
    including classic control tasks, Atari games, robotics simulations, and more.
    This allowed researchers and developers to evaluate algorithms across various
    domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy-to-use API**: Gym’s API was straightforward and easy to use, making
    it accessible to both beginners and experienced researchers. Developers could
    quickly prototype and test reinforcement learning algorithms using Gym’s environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Benchmarking**: Gym facilitated benchmarking by providing standardized environments
    and evaluation metrics. This enabled researchers to compare the performance of
    different algorithms on common tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Community contributions**: Gym was an open-source project, and the community
    actively contributed new environments, algorithms, and extensions to the toolkit.
    This collaborative effort helped to continuously expand and improve Gym’s capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, OpenAI Gym served as a valuable resource for the reinforcement learning
    community, providing a standardized platform for research, experimentation, and
    benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: Gym was a pioneering library and set the standard for simplicity for many years.
    However, it is no longer actively maintained by the OpenAI team. Recognizing this,
    some developers took the initiative to create **Gymnasium** ([https://gymnasium.farama.org/index.html](https://gymnasium.farama.org/index.html)),
    with approval from OpenAI. Gymnasium emerged as a successor to Gym, and the original
    developers from OpenAI occasionally contribute to its development, ensuring its
    reliability and continuity. In this chapter, we will be using Gymnasium, which
    is a **maintained fork** of Gym.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Gymnasium
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to install the Gymnasium library is via `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It is recommended to install the `toy-text` extension using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `toy-text` extension provides additional toy text-based environments, such
    as the FrozenLake environment (discussed later), for reinforcement learning experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the installation, you can check the available Gymnasium environments
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can see lists of the environments at [https://gymnasium.farama.org/environments/toy_text/](https://gymnasium.farama.org/environments/toy_text/)
    and [https://gymnasium.farama.org/environments/atari/](https://gymnasium.farama.org/environments/atari/),
    including walking, moon landing, car racing, and Atari games. Feel free to play
    around with Gymnasium by going through its introduction at [https://gymnasium.farama.org/content/basic_usage/](https://gymnasium.farama.org/content/basic_usage/).
  prefs: []
  type: TYPE_NORMAL
- en: To benchmark different reinforcement learning algorithms, we need to apply them
    in a standardized environment. Gymnasium is the perfect place for this, with a
    number of versatile environments. This is similar to using datasets such as MNIST,
    ImageNet, and Thomson Reuters News as benchmarks in supervised and unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Gymnasium has an easy-to-use interface for the reinforcement learning environments
    that we can write **agents** to interact with. So what’s reinforcement learning?
    What’s an agent? Let’s see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing reinforcement learning with examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will first introduce the elements of reinforcement learning
    along with an interesting example, then will move on to how we measure feedback
    from the environment, and follow with the fundamental approaches to solve reinforcement
    learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have played Super Mario (or Sonic) when you were young. During the video
    game, you control Mario to collect coins and avoid obstacles at the same time.
    The game ends if Mario hits an obstacle or falls in a gap, and you try to get
    as many coins as possible before the game ends.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is very similar to the Super Mario game. Reinforcement
    learning is about learning what to do. It involves observing situations in the
    environment and determining the right actions in order to maximize a numerical
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the list of elements in a reinforcement learning task (we also link
    each element to Super Mario and other examples so it’s easier to understand):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment**: The environment is a task or simulation. In the Super Mario
    game, the game itself is the environment. In self-driving, the road and traffic
    are the environment. In the context of Go playing chess, the board is the environment.
    The inputs to the environment are the actions sent from the **agent** and the
    outputs are **states** and **rewards** sent to the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent**: The agent is the component that takes **actions** according to the
    reinforcement learning model. It interacts with the environment and observes the
    states to feed into the model. The goal of the agent is to solve the environment—that
    is, finding the best set of actions to maximize the rewards. The agent in the
    Super Mario game is Mario, and the autonomous vehicle is the agent for self-driving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**: This is the possible movement of the agent. It is usually random
    in a reinforcement learning task at the beginning when the model starts to learn
    about the environment. Possible actions for Mario include moving left and right,
    jumping, and crouching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**States**: The states are the observations from the environment. They describe
    the situation in a numerical way at every time step. For a chess game, the state
    is the positions of all the pieces on the board. For Super Mario, the state includes
    the coordinates of Mario and other elements in the time frame. For a robot learning
    to walk, the position of its two legs is the state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rewards**: Every time the agent takes an action, it receives numerical feedback
    from the environment. The feedback is called the **reward**. It can be positive,
    negative, or zero. The reward in the Super Mario game can be, for example, +1
    if Mario collects a coin, +2 if he avoids an obstacle, -10 if he hits an obstacle,
    or 0 for other cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram summarizes the process of reinforcement learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: Reinforcement learning process'
  prefs: []
  type: TYPE_NORMAL
- en: The reinforcement learning process is an iterative loop. At the beginning, the
    agent observes the initial state, *s*[0], from the environment. Then the agent
    takes an action, *a*[0], according to the model. After the agent moves, the environment
    is now in a new state, *s*[1], and it gives a feedback reward, *R*[1]. The agent
    then takes an action, *a*[1], as computed by the model with inputs *s*[1] and
    *R*[1]. This process continues until termination, completion, or for forever.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the reinforcement learning model is to maximize the total reward.
    So how can we calculate the total reward? Is it simply by summing up rewards at
    all the time steps? Let’s see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cumulative rewards
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At time step *t*, the **cumulative rewards** (also called **returns**) *G*[1]
    can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *T* is the termination time step or infinity. *G*[t] means the total future
    reward after taking an action *a*[t] at time *t*. At each time step *t*, the reinforcement
    learning model attempts to learn the best possible action in order to maximize
    *G*[t].
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in many real-world cases, things don’t work this way where we simply
    sum up all future rewards. Take a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: Stock A rises 6 dollars at the end of day 1 and falls 5 dollars at the end of
    day 2\. Stock B falls 5 dollars on day 1 and rises 6 dollars on day 2\. After
    two days, both stocks rise 1 dollar. So, if we knew that, which one would we buy
    at the beginning of day 1? Obviously, stock A, because we won’t lose money and
    can even profit 6 dollars if we sell it at the beginning of day 2.
  prefs: []
  type: TYPE_NORMAL
- en: Both stocks have the same total reward but we favor stock A as we care more
    about immediate return than distant return. Similarly in reinforcement learning,
    we discount rewards in the distant future and the discount factor is associated
    with the time horizon. Longer time horizons should have less impact on the cumulative
    rewards. This is because longer time horizons include more irrelevant information
    and consequently are of higher variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a discount factor ![](img/B21047_15_002.png) with a value between
    0 and 1\. We rewrite the cumulative rewards incorporating the discount factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_003.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the larger the ![](img/B21047_15_004.png), the smaller the discount
    and vice versa. If ![](img/B21047_15_005.png), there is literally no discount
    and the model evaluates an action based on the sum total of all future rewards.
    If ![](img/B21047_15_006.png), the model only focuses on the immediate reward
    *R*[t][+1].
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to calculate the cumulative reward, the next thing to talk
    about is how to maximize it.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches to reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two main approaches to solving reinforcement learning problems, which
    are about finding the optimal actions to maximize the cumulative rewards. One
    is a policy-based approach and the other is value-based.
  prefs: []
  type: TYPE_NORMAL
- en: Policy-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **policy** is a function ![](img/B21047_15_007.png) that maps each input
    state to an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be either deterministic or stochastic:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic**: There is one-to-one mapping from the input state to the
    output action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic**: This gives a probability distribution over all possible actions
    ![](img/B21047_15_009.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the **policy-based** approach, the model learns the optimal policy that maps
    each input state to the best action. The agent directly learns the best course
    of action (policy) for any situation (state) it encounters.
  prefs: []
  type: TYPE_NORMAL
- en: In a policy-based algorithm, the model starts with a random policy. It then
    computes the value function of that policy. This step is called the **policy evaluation
    step**. After this, it finds a new and better policy based on the value function.
    This is the **policy improvement step**. These two steps repeat until the optimal
    policy is found.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are training a race car driver. In a policy-based approach, you
    directly teach the driver the best manoeuvres (policy) to take on different parts
    of the track (states) to achieve the fastest lap time (reward). You don’t tell
    them the estimated result (reward) of each turn, but rather guide them towards
    the optimal racing line through feedback and practice.
  prefs: []
  type: TYPE_NORMAL
- en: Value-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **value** *V* of a state is defined as the expected future cumulative reward
    to collect from the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_010.png)'
  prefs: []
  type: TYPE_IMG
- en: In the **value-based** approach, the model learns the optimal value function
    that maximizes the value of the input state. In other words, the agent takes an
    action to reach the state that achieves the largest value.
  prefs: []
  type: TYPE_NORMAL
- en: In a value-based algorithm, the model starts with a random value function. It
    then finds a new and improved value function in an iterative manner, until it
    reaches the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine you are a treasure hunter. In a value-based approach, you learn
    the estimated treasure (value) of different locations (states) in a maze. This
    helps you choose paths that lead to areas with higher potential treasure (rewards)
    without needing a pre-defined course of action (policy).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve learned there are two main approaches to solving reinforcement learning
    problems. In the next section, let’s see how to solve a concrete reinforcement
    learning example (FrozenLake) using a concrete algorithm, the dynamic programming
    method, in a policy-based and value-based way respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the FrozenLake environment with dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will focus on the policy-based and value-based dynamic programming algorithms
    in this section. But let’s start by simulating the FrozenLake environment. It
    simulates a simple grid-world scenario where an agent navigates through a grid
    of icy terrain, represented as a frozen lake, to reach a goal tile.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating the FrozenLake environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FrozenLake is a typical OpenAI Gym (now Gymnasium) environment with **discrete**
    states. It is about moving the agent from the starting tile to the destination
    tile in a grid, and at the same time avoiding traps. The grid is either 4 * 4
    (FrozenLake-v1), or 8 * 8 (FrozenLake8x8-v1). There are four types of tiles in
    the grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The starting tile**: This is state 0, and it comes with 0 reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The goal tile**: It is state 15 in the 4 * 4 grid. It gives +1 reward and
    terminates an episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The frozen tile**: In the 4 * 4 grid, states 1, 2, 3, 4, 6, 8, 9, 10, 13,
    and 14 are walkable tiles. It gives 0 reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The hole tile**: In the 4 * 4 grid, states 5, 7, 11, and 12 are hole tiles.
    It gives 0 reward and terminates an episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, an **episode** means a simulation of a reinforcement learning environment.
    It contains a list of states from the initial state to the terminal state, a list
    of actions and rewards. In the 4 * 4 FrozenLake environment, there are 16 possible
    states as the agent can move to any of the 16 tiles. And there are four possible
    actions: moving left (0), down (1), right (2), and up (3).'
  prefs: []
  type: TYPE_NORMAL
- en: The tricky part of this environment is that, as the ice surface is slippery,
    the agent won’t always move in the direction it intends and can move in any other
    walkable direction or stay unmoved with certain probabilities. For example, it
    may move to the right even though it intends to move up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s simulate the 4 * 4 FrozenLake environment by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To simulate any OpenAI Gym environment, we need to first look up its name in
    the documentation at [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).
    We get `FrozenLake-v1` in our case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We import the `gym` library and create a `FrozenLake` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The environment is initialized with the `FrozenLake-v1` identifier. Additionally,
    the `render_mode` parameter is set to `rgb_array`, indicating that the environment
    should render its state as an RGB array, suitable for visualization purposes.
  prefs: []
  type: TYPE_NORMAL
- en: We also obtain the dimensions of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every time we run a new episode, we need to reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It means that the agent starts with state 0\. Again, there are 16 possible states,
    0, 1, …, 15.
  prefs: []
  type: TYPE_NORMAL
- en: 'We render the environment to display it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you encounter any error, you may install the `pyglet` library, which embeds
    a `Matplotlib` figure within a window using canvas rendering, using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see a 4 * 4 matrix representing the FrozenLake grid and the tile (state
    0) where the agent is located:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: Initial state of FrozenLake'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now start moving the agent around. Let’s take a right action since it
    is walkable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We take a “right” (2) action, but the agent moves down to state 4, at a probability
    of 33.33%, and gets 0 reward since the episode is not done yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the rendered result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B21047_15_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Result of the agent moving right'
  prefs: []
  type: TYPE_NORMAL
- en: You may get a completely different result as the agent can move right to state
    1 at a probability of 33.33%, or stay at state 0 at a probability of 33.33% due
    to the **slippery** nature of the frozen lake.
  prefs: []
  type: TYPE_NORMAL
- en: In Gymnasium, “**terminated**” and “**truncated**” refer to different ways in
    which an episode can end in a reinforcement learning environment. When an episode
    is terminated, it means that the episode has ended naturally according to the
    rules of the environment. When an episode is truncated, it means that the episode
    is artificially terminated before it can end naturally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define a function that simulates a FrozenLake episode under a given
    policy and returns the total reward (as an easy start, let’s just assume discount
    factor ![](img/B21047_15_011.png)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `policy` is a PyTorch tensor, and `.item()` extracts the value of an element
    on the tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s play around with the environment using a random policy. We will implement
    a random policy (where random actions are taken) and calculate the average total
    reward over 1,000 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On average, there is a 1.6% chance that the agent can reach the goal if we take
    random actions. This tells us it is not as easy to solve the FrozenLake environment
    as you might think.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a bonus step, you can look into the transition matrix. The **transition
    matrix T**(*s*, *a*, *s’*) contains probabilities of taking action *a* from state
    *s* then reaching *s’*. Take state 6 as an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The keys of the returning dictionary 0, 1, 2, 3 represent four possible actions.
    The value of a key is a list of tuples associated with the action. The tuple is
    in the format of (transition probability, new state, reward, is terminal state
    or not). For example, if the agent intends to take action 1 (down) from state
    6, it will move to state 5 (H) with 33.33% probability and receive 0 reward and
    the episode will end consequently; it will move to state 10 with 33.33% probability
    and receive 0 reward; it will move to state 7 (H) with 33.33% probability and
    receive 0 reward and terminate the episode.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve experimented with the random policy in this section, and we only succeeded
    1.6% of the time. But this gets you ready for the next section where we will find
    the optimal policy using the value-based dynamic programming algorithm, called
    the **value iteration algorithm**.
  prefs: []
  type: TYPE_NORMAL
- en: Solving FrozenLake with the value iteration algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Value iteration is an iterative algorithm. It starts with random policy values
    *V*, and then iteratively updates the values based on the **Bellman optimality
    equation** ([https://en.wikipedia.org/wiki/Bellman_equation](https://en.wikipedia.org/wiki/Bellman_equation))
    until the values converge.
  prefs: []
  type: TYPE_NORMAL
- en: It is usually difficult for the values to completely converge. Hence, there
    are two criteria of convergence. One is passing a fixed number of iterations,
    such as 1,000 or 10,000\. Another one is specifying a threshold (such as 0.0001,
    or 0.00001) and we terminate the process if the changes of all values are less
    than the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, in each iteration, instead of taking the expectation (average)
    of values across all actions, it picks the action that maximizes the policy values.
    The iteration process can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_012.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the representation of the Bellman equation for the state-value function
    V(s). Here, ![](img/B21047_15_013.png) is the optimal value function; ![](img/B21047_15_014.png)
    denotes the transition probability of moving to state ![](img/B21047_15_015.png)
    from state *s* by taking action *a*; and ![](img/B21047_15_016.png) is the reward
    provided in state ![](img/B21047_15_017.png) by taking action *a*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we obtain the optimal values, we can easily compute the optimal policy
    accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s solve the FrozenLake environment using the value iteration algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we set `0.99` as the discount factor, and `0.0001` as the convergence
    threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We develop the value iteration algorithm, which computes the optimal values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `value_iteration` function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Starts with policy values as all 0s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Updating the values based on the Bellman optimality equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21047_15_019.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing the maximal change of the values across all states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuing to update the values if the maximal change is greater than the convergence
    threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, terminating the iteration process and returning the last values as
    the optimal values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We apply the algorithm to solve the FrozenLake environment along with the specified
    parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a look at the resulting optimal values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we have the optimal values, we can extract the optimal policy from the
    values. We develop the following function to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we obtain the optimal policy based on the optimal values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a look at the resulting optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This means the optimal action in state 0 is 0 (left), 3 (up) in state 1, etc.
    This doesn’t seem very intuitive if you look at the grid. But remember that the
    grid is slippery and the agent can move in another direction than the desired
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you doubt that it is the optimal policy, you can run 1,000 episodes with
    the policy and gauge how good it is by checking the average reward, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we define the `run_episode` function to simulate one episode. Then we
    print out the average reward over 1,000 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Value iteration is guaranteed to converge to the optimal value function for
    a finite environment with a finite state and action space. It provides a computationally
    efficient method for solving for the optimal policy in RL problems, especially
    when the dynamics of the environment are known. Under the optimal policy computed
    by the value iteration algorithm, the agent in FrozenLake reaches the goal tile
    74% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: The discount factor is an important parameter in RL, especially for value-based
    models. A high factor (closer to 1) makes the agent prioritize long-term rewards,
    leading to more exploration, while a low factor (closer to 0) makes it focus on
    immediate rewards. Typical tuning strategies for the discount factor include grid
    search and random search. Both could be computationally expensive for large ranges.
    Adaptive tuning is another approach, where we dynamically adjust the factor during
    training. You can start with a medium value (such as 0.9). If the agent seems
    too focused on immediate rewards, converges fast, and ignores exploration, try
    increasing the discount factor. If the agent keeps exploring and never settles
    on a good policy, try decreasing the discount factor.
  prefs: []
  type: TYPE_NORMAL
- en: Can we do something similar with the policy-based approach? Let’s see in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Solving FrozenLake with the policy iteration algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **policy iteration** algorithm has two components, policy evaluation and
    policy improvement. Similar to value iteration, it starts with an arbitrary policy
    and follows with a bunch of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the policy evaluation step in each iteration, we first compute the values
    of the latest policy, based on the **Bellman expectation equation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the policy improvement step, we derive an improved policy based on the latest
    policy values, again based on the Bellman optimality equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_021.png)'
  prefs: []
  type: TYPE_IMG
- en: These two steps repeat until the policy converges. At convergence, the latest
    policy and its value are the optimal policy and the optimal value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s develop the policy iteration algorithm and use it to solve the FrozenLake
    environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the `policy_evaluation` function that computes the values of
    a given policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the policy values with all 0s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating the values based on the Bellman expectation equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the maximal change of the values across all states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the maximal change is greater than the threshold, it keeps updating the values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, it terminates the evaluation process and returns the latest values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we develop the second component, the policy improvement, in the following
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It derives a new and better policy from the input policy values based on the
    Bellman optimality equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With both components ready, we now develop the whole policy iteration algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a random policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing policy evaluation to update the policy values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing policy improvement to generate a new policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the new policy is different from the old one, it updates the policy and runs
    another iteration of policy evaluation and improvement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, it terminates the iteration process and returns the latest policy
    and its values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we use policy iteration to solve the FrozenLake environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we display the optimal policy and its values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We got the same results as the value iteration algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy-based methods rely on estimating the gradient of the expected reward
    with respect to the policy parameters. In practice, we use techniques like REINFORCE,
    which uses simple Monte Carlo estimates, and **Proximal Policy Optimization**
    (**PPO**) employing stable gradient estimation. You can read more here: [https://professional.mit.edu/course-catalog/reinforcement-learning](https://professional.mit.edu/course-catalog/reinforcement-learning)
    (*Chapter 8*, *Policy Gradient Methods*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have just solved the FrozenLake environment with the policy iteration algorithm.
    You may wonder how to choose between the value iteration and policy iteration
    algorithms. Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 15.4: Choosing between the policy iteration and value iteration algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: We solved a reinforcement learning problem using dynamic programming methods.
    They require a fully known transition matrix and reward matrix of an environment.
    And they have limited scalability for environments with many states. In the next
    section, we will continue our learning journey with the Monte Carlo method, which
    has no requirement of prior knowledge of the environment and is much more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: Performing Monte Carlo learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Monte Carlo** (**MC**)-based reinforcement learning is a **model-free** approach,
    which means it doesn’t need a known transition matrix and reward matrix. In this
    section, you will learn about MC policy evaluation on Gymnasium’s Blackjack environment,
    and solve the environment with MC control algorithms. Blackjack is a typical environment
    with an unknown transition matrix. Let’s first simulate the Blackjack environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Simulating the Blackjack environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Blackjack is a popular card game. The game has the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: The player competes against a dealer and wins if the total value of their cards
    is higher than the dealer’s and doesn’t exceed 21.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cards from 2 to 10 have values from 2 to 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cards J, K, and Q have a value of 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value of an ace can be either 1 or 11 (called a “usable” ace).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the beginning, both parties are given two random cards, but only one of the
    dealer’s cards is revealed to the player. The player can request additional cards
    (called **hit**) or stop having any more cards (called **stick**). Before the
    player calls stick, the player will lose if the sum of their cards exceeds 21
    (called **bust**). After the player sticks, the dealer keeps drawing cards until
    the sum of cards reaches 17\. If the sum of the dealer’s cards exceeds 21, the
    player will win. If neither of the two parties busts, the one with higher points
    will win or it may be a draw.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Blackjack environment ([https://gymnasium.farama.org/environments/toy_text/blackjack/](https://gymnasium.farama.org/environments/toy_text/blackjack/))
    in Gymnasium is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An episode of the environment starts with two cards for each party, and only
    one from the dealer’s cards is observed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An episode ends if there is a win or draw.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final reward of an episode is +1 if the player wins, -1 if the player loses,
    or 0 if there is a draw.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each round, the player can take any of the two actions, hit (1) or stick
    (0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s simulate the Blackjack environment and explore its states and actions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a `Blackjack` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It returns the initial state (a 3-dimensional vector):'
  prefs: []
  type: TYPE_NORMAL
- en: Player’s current points (`11` in this example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The points of the dealer’s revealed card (`10` in this example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having a usable ace or not (`False` in this example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The usable ace variable is `True` only if the player has an ace that can be
    counted as 11 without causing a bust. If the player doesn’t have an ace, or has
    an ace but it busts, this state variable will become `False`.
  prefs: []
  type: TYPE_NORMAL
- en: For another state example `(18, 6, True)`, it means that the player has an ace
    counted as 11 and a 7, and that the dealer’s revealed card is value 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now take some actions to see how the environment works. First, we take
    a hit action since we only have 11 points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It returns a state `(12, 10, False)`, a 0 reward, and the episode not being
    done (meaning `False`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take another hit since we only have 12 points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have 13 points and think it is good enough. Then we stop drawing cards by
    taking the stick action (0):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The dealer gets some cards and beats the player. So the player loses and gets
    a -1 reward. The episode ends.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to play around with the Blackjack environment. Once you feel comfortable
    with the environment, you can move on to the next section, MC policy evaluation
    on a simple policy.
  prefs: []
  type: TYPE_NORMAL
- en: Performing Monte Carlo policy evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we applied dynamic programming to perform policy evaluation,
    which is the value function of a policy. However, it won’t work in most real-life
    situations where the transition matrix is not known beforehand. In this case,
    we can evaluate the value function using the MC method.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the value function, the MC method uses empirical mean return instead
    of expected return (as in dynamic programming). There are two approaches to compute
    the empirical mean return. One is first-visit, which averages returns **only**
    for the **first occurrence** of a state *s* among all episodes. Another one is
    every-visit, which averages returns for **every occurrence** of a state *s* among
    all episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, the first-visit approach has a lot less computation and is therefore
    more commonly used. I will only cover the first-visit approach in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we experiment with a simple policy where we keep adding new
    cards until the total value reaches 18 (or 19, or 20 if you like). We perform
    first-visit MC evaluation on the simple policy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to define a function that simulates a Blackjack episode under
    the simple policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In each round of an episode, the agent takes a hit if the current score is less
    than `hold_score` or a stick otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the MC settings, we need to keep track of states and rewards over all steps.
    And in first-visit value evaluation, we average returns only for the first occurrence
    of a state among all episodes. We define a function that evaluates the simple
    Blackjack policy with first-visit MC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function performs the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Running `n_episode` episodes under the simple Blackjack policy with the `run_episode`
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each episode, computing the `G` returns for the first visit of each state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each state, obtaining the value by averaging its first returns from all
    episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returning the resulting values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that here we ignore states where the player busts, since we know their
    values are `-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the `hold_score` as `18` and the discount rate as `1` as a Blackjack
    episode is short enough, and will simulate 500,000 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we plug in all variables to perform MC first-visit evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then print the resulting values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We have just computed the values for all possible 280 states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We have just experienced computing the values of 280 states under a simple policy
    in the Blackjack environment using the MC method. The transition matrix of the
    Blackjack environment is not known beforehand. Moreover, obtaining the transition
    matrix (size 280 * 280) will be extremely costly if we go with the dynamic programming
    approach. In the MC-based solution, we just need to simulate a bunch of episodes
    and compute the empirical average values. In a similar manner, we will search
    for the optimal policy in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Performing on-policy Monte Carlo control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MC control** is used to find the optimal policy for environments with unknown
    transition matrices. There are two types of MC control, on-policy and off-policy.
    In the **on-policy approach**, we execute the policy and evaluate and improve
    it iteratively; whereas in the off-policy approach, we train the optimal policy
    using data generated by another policy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we focus on the on-policy approach. The way it works is very
    similar to the policy iteration method. It iterates between the following two
    phases, evaluation and improvement, until convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: In the evaluation phase, instead of evaluating the state value, we evaluate
    the **action-value**, which is commonly called the **Q-value**. Q-value *Q*(*s*,
    *a*) is the value of a state-action pair (*s*, *a*) when taking the action *a*
    in state *s* under a given policy. The evaluation can be conducted in a first-visit
    or an every-visit manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the improvement phase, we update the policy by assigning the optimal action
    in each state:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21047_15_022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s now search for the optimal Blackjack policy with on-policy MC control
    by following the steps below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by developing a function that executes an episode by taking the best
    actions under the given Q-values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This serves as the improvement phase. Specifically, it does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing an episode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a random action as an exploring start
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the first action, taking actions based on the given Q-value table, that
    is ![](img/B21047_15_023.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing the states, actions, and rewards for all steps in the episode, which
    will be used for evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we develop the on-policy MC control algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function does the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initializing the Q-values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running `n_episode` episodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each episode, performing policy improvement and obtaining the training data;
    performing first-visit policy evaluation on the resulting states, actions, and
    rewards; and updating the Q-values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, finalizing the optimal Q-values and the optimal policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that the MC control function is ready, we compute the optimal policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a look at the optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You may wonder if this optimal policy is really optimal and better than the
    previous simple policy (hold at 18 points). Let’s simulate 100,000 Blackjack episodes
    under the optimal policy and the simple policy respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the function that simulates an episode under the simple policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we work on the simulation function under the optimal policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then run 100,000 episodes for both policies and keep track of their winning
    times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We print out the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Playing under the optimal policy has a 43% chance of winning, while playing
    under the simple policy has only a 40% chance.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we solved the Blackjack environment with a model-free algorithm,
    MC learning. In MC learning, the Q-values are updated until the end of an episode.
    This could be problematic for long processes. In the next section, we will talk
    about Q-learning, which updates the Q-values for every step in an episode. You
    will see how it increases learning efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the Blackjack problem with the Q-learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is also a model-free learning algorithm. It updates the Q-function
    for every step in an episode. We will demonstrate how Q-learning is used to solve
    the Blackjack environment.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Q-learning algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Q-learning** is an **off-policy** learning algorithm that optimizes the Q-values
    based on data generated by a behavior policy. The behavior policy is a greedy
    policy where it takes actions that achieve the highest returns for given states.
    The behavior policy generates learning data and the target policy (the policy
    we attempt to optimize) updates the Q-values based on the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_024.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B21047_15_025.png) is the resulting state after taking action
    *a* from state *s* and *r* is the associated reward. ![](img/B21047_15_026.png)
    means that the behavior policy generates the highest Q-value given state ![](img/B21047_15_027.png).
    Hyperparameters ![](img/B21047_15_028.png) and ![](img/B21047_15_029.png) are
    the learning rate and discount factor respectively. The Q-learning equation updates
    the Q-value (estimated future reward) of a state-action pair based on the current
    Q-value, the immediate reward, and the potential future rewards the agent can
    expect by taking the best possible action in the next state.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from the experience generated by another policy enables Q-learning
    to optimize its Q-values in every single step in an episode. We gain the information
    from a greedy policy and use this information to update the target values right
    away.
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing to note is that the target policy is epsilon-greedy, meaning
    it takes a random action with a probability of ![](img/B21047_15_030.png) (value
    from 0 to 1) and takes a greedy action with a probability of ![](img/B21047_15_031.png)
    . The epsilon-greedy policy combines **exploitation** and **exploration**: it
    exploits the best action while exploring different actions.'
  prefs: []
  type: TYPE_NORMAL
- en: Developing the Q-learning algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it is time to develop the Q-learning algorithm to solve the Blackjack environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with defining the epsilon-greedy policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Given ![](img/B21047_15_032.png) possible actions, each action is taken with
    a probability ![](img/B21047_15_033.png), and the action with the highest state-action
    value is chosen with an additional probability ![](img/B21047_15_034.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with a large exploration factor ![](img/B21047_15_035.png) and
    reduce it over time until it reaches `0.1`. We define the starting and final ![](img/B21047_15_036.png)
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we implement the Q-learning algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We initialize the action-value function Q and calculate the epsilon decay rate
    for the epsilon-greedy exploration strategy. For each episode, we let the agent
    take actions following the epsilon-greedy policy, and update the Q function for
    each step based on the off-policy learning equation. The exploration factor also
    decreases over time. We run `n_episode` episodes and finally extract the optimal
    policy from the learned action-value function Q by selecting the action with the
    maximum value for each state.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then initiate a variable to store the performance of each of 10,000 episodes,
    measured by the reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we perform Q-learning to obtain the optimal policy for the Blackjack
    problem with the following hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, discount rate ![](img/B21047_15_037.png) and learning rate ![](img/B21047_15_038.png)
    for more exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'After 10,000 episodes of learning, we plot the rolling average rewards over
    the episodes as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the end result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21047_15_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: Average reward over episodes'
  prefs: []
  type: TYPE_NORMAL
- en: The average reward steadily rises throughout the training process, eventually
    converging. This indicates that the model becomes proficient in solving the problem
    after training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we simulate 100,000 episodes for the optimal policy we obtained using
    Q-learning and calculate the winning chance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Playing under the optimal policy has a 42% chance of winning.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we solved the Blackjack problem with off-policy Q-learning.
    The algorithm optimizes the Q-values in every single step by learning from the
    experience generated by a greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter commenced with configuring the working environment, followed by
    an examination of the core concepts of reinforcement learning, accompanied by
    practical examples. Subsequently, we delved into the FrozenLake environment, employing
    dynamic programming techniques such as value iteration and policy iteration to
    tackle it effectively. Monte Carlo learning was introduced for value estimation
    and control in the Blackjack environment. Finally, we implemented the Q-learning
    algorithm to address the same problem, providing a comprehensive overview of reinforcement
    learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you try to solve the 8 * 8 FrozenLake environment with the value iteration
    or policy iteration algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you implement the every-visit MC policy evaluation algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code187846872178698968.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/New_Packt_Logo1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[packt.com](https://www.packt.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  prefs: []
  type: TYPE_NORMAL
- en: Why subscribe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve your learning with Skill Plans built especially for you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get a free eBook or video every month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully searchable for easy access to vital information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy and paste, print, and bookmark content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  prefs: []
  type: TYPE_NORMAL
- en: Other Books You May Enjoy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/978-1-80107-430-8.png)](https://www.packtpub.com/en-us/product/mastering-pytorch-9781801074308)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mastering PyTorch – Second Edition**'
  prefs: []
  type: TYPE_NORMAL
- en: Ashish Ranjan Jha
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-80107-430-8'
  prefs: []
  type: TYPE_NORMAL
- en: Implement text, vision, and music generation models using PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a deep Q-network (DQN) model in PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy PyTorch models on mobile devices (Android and iOS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Become well versed in rapid prototyping using PyTorch with fastai
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform neural architecture search effectively using AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easily interpret machine learning models using Captum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design ResNets, LSTMs, and graph neural networks (GNNs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create language and vision transformer models using Hugging Face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/978-1-83546-231-7.png)](https://www.packtpub.com/en-us/product/building-llm-powered-applications-9781835462317)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Building LLM Powered Applications**'
  prefs: []
  type: TYPE_NORMAL
- en: Valentina Alto
  prefs: []
  type: TYPE_NORMAL
- en: 'ISBN: 978-1-83546-231-7'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the core components of LLM architecture, including encoder-decoder blocks
    and embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use AI orchestrators like LangChain, with Streamlit for the frontend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get familiar with LLM components such as memory, prompts, and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to use non-parametric knowledge and vector databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the implications of LFMs for AI research and industry applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customize your LLMs with fine tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn about the ethical implications of LLM-powered applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packt is searching for authors like you
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  prefs: []
  type: TYPE_NORMAL
- en: Share your thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you’ve finished *Python Machine Learning By Example - Fourth Edition*, we’d
    love to hear your thoughts! If you purchased the book from Amazon, please [click
    here to go straight to the Amazon review page](https://packt.link/r/1835085628)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL

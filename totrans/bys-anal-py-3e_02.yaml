- en: Chapter 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical Models
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical models are one honking great idea – let’s do more of those! - The
    zen of Bayesian modeling
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In *Chapter [2](CH02.xhtml#x1-440002)*, we saw a tips example where we had multiple
    groups in our data, one for each of Thursday, Friday, Saturday, and Sunday. We
    decided to model each group separately. That’s sometimes fine, but we should be
    aware of our assumptions. By modeling each group independently, we are assuming
    the groups are unrelated. In other words, we are assuming that knowing the tip
    for one day does not give us any information about the tip for another day. That
    could be too strong an assumption. Would it be possible to build a model that
    allows us to share information between groups? That’s not only possible, but is
    also the main topic of this chapter. Lucky you!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shrinkage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.1 Sharing information, sharing priors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical models are also known as multilevel models, mixed-effects models,
    random-effects models, or nested models. They are particularly useful when dealing
    with data that can be described as grouped or having different levels, such as
    data nested within geographic regions (for example, cities belonging to a province
    and provinces belonging to a country), or with a hierarchical structure (such
    as students nested within schools, or patients nested within hospitals) or repeated
    measurements on the same individuals.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file87.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.1**: The differences between a pooled model, an unpooled model,
    and a hierarchical model'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical models are a natural way to share information between groups. In
    a hierarchical model, the parameters of the prior distributions are themselves
    given a prior distribution. These higher-level priors are often called hyperpriors;
    ”hyper” means ”over” in Greek. Having hyperpriors allows the model to share information
    between groups, while still allowing differences between groups. In other words,
    we can think of the parameters of the prior distributions as belonging to a common
    population of parameters. *Figure [3.1](#x1-68003r1)* shows a diagram with the
    high-level differences between a pooled model (a single group), an unpooled model
    (all separated groups), and a hierarchical model, also known as a partially pooled
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a hierarchical model may seem confusingly simple, almost trivial,
    but it has subtle implications. That is why in the rest of this chapter we will
    use different examples to understand their implications. I am sure that these
    examples will not only help you better understand this concept but will also convince
    you that it is a very useful tool to apply to your own problems.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Hierarchical shifts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Proteins are molecules formed by 20 units called amino acids. Each amino acid
    can appear in a protein 0 or more times. Just as a melody is defined by a sequence
    of musical notes, a protein is defined by a sequence of amino acids. Some musical
    note variations can result in small variations of the melody and other variations
    in completely different melodies. Something similar happens with proteins. One
    way to study proteins is by using nuclear magnetic resonance (the same technique
    used for medical imaging). This technique allows us to measure various quantities,
    one of which is called a chemical shift. You may remember that we saw an example
    using chemical shifts in *Chapter [2](CH02.xhtml#x1-440002)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to compare a theoretical method of computing chemical shift
    against the experimental observations to evaluate the ability of the theoretical
    method to reproduce the experimental values. Luckily for us, someone has already
    run the experiments and carried out the theoretical calculations, and we just
    need to compare them. The following dataset contains chemical shift values for
    a set of proteins. If you inspect the `cs_data` DataFrame, you will see that it
    has four columns:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is a code that identifies the protein (you can get a lot of information
    about that protein by entering that code at [https://www.rcsb.org/](https://www.rcsb.org/))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second column has the name of the amino acid (you might notice that there
    are only 19 unique names; one of the amino acids is missing from this dataset)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third contains theoretical values of chemical shift (calculated using quantum
    methods)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fourth has experimental values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have the data, how should we proceed? One option is to take the
    empirical differences and fit a Gaussian or maybe Student’s t model. Because amino
    acids are a family of chemical compounds, it would make sense to assume they are
    all the same and estimate a single Gaussian for all the differences. But you may
    argue that there are 20 different kinds of amino acids, each one with different
    chemical properties, and hence a better choice is to fit 20 separated Gaussians.
    What should we do?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to think about which option is the best. If we combine all
    the data, our estimates are going to be more accurate, but we will not be able
    to get information from individual groups (amino acids). On the contrary, if we
    treat them as separate groups, we will get a much more detailed analysis but with
    less accuracy. What should we do?
  prefs: []
  type: TYPE_NORMAL
- en: When in doubt, everything! (Not sure this is good general advice for your life,
    but I like the song [https://www.youtube.com/watch?v=1di09XZUlIw](https://www.youtube.com/watch?v=1di09XZUlIw)).
    We can build a hierarchical model; that way, we allow estimates at the group level
    but with the restriction that they all belong to a larger group or population.
    To better understand this, let’s build a hierarchical model for the chemical shift
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the difference between a non-hierarchical (unpooled) model and a hierarchical
    one, we are going to build two models. The first one is essentially the same as
    the `comparing_groups` model from *Chapter [2](CH02.xhtml#x1-440002)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 3.1**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will build the hierarchical version of the model. We are adding two
    hyperpriors, one for the mean of *μ* and one for the standard deviation of *μ*.
    We are leaving *σ* without hyperpriors; in other words, we are assuming that the
    variance between observed and theoretical values should be the same for all groups.
    This is a modeling choice, and you may face a problem where this seems unacceptable
    and consider it necessary to add a hyperprior for *σ*; feel free to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 3.2**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure [3.2](#x1-69030r2)* shows the graphical representation of the `cs_h`
    and `cs_nh` models. We can see that we have one more level for `cs_h` representing
    the hyperpriors for *μ*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file88.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.2**: Graph representation of the non-hierarchical (left) and hierarchical
    (right) models for the chemical shift data. Each subfigure was generated with
    the `pm.model_to_graphviz(.)` function'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to compare the results using ArviZ’s `plot_forest` function. We
    can pass more than one model to this function. This is useful when we want to
    compare the values of parameters from different models such as with the present
    example. In *Figure [3.3](#x1-69031r3)*, we have a plot for the 40 estimated means,
    one per amino acid (20) for each of the two models. We also have their 94% HDI
    and the inter-quantile range (the central 50% of the distribution). The vertical
    dashed line is the global mean according to the hierarchical model. This value
    is close to zero, as expected for theoretical values faithfully reproducing experimental
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file89.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.3**: Chemical shift differences for the hierarchical and non-hierarchical
    models'
  prefs: []
  type: TYPE_NORMAL
- en: The most relevant part of this plot is that the estimates from the hierarchical
    model are pulled toward the partially pooled mean, or equivalently they are shrunken
    in comparison to the unpooled estimates. You will also notice that the effect
    is more notorious for those groups farther away from the mean (such as `PRO`)
    and that the uncertainty is on par with or smaller than that from the non-hierarchical
    model. The estimates are partially pooled because we have one estimate for each
    group, but estimates for individual groups restrict each other through the hyperprior.
    Therefore, we get an intermediate situation between having a single group with
    all chemical shifts together and having 20 separate groups, one per amino acid.
    And that, ladies, gentlemen, and non-binary-gender-fluid people, is the beauty
    of hierarchical models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Water quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose we want to analyze the quality of water in a city, so we take samples
    by dividing the city into neighborhoods. We may think we have two options for
    analyzing this data:'
  prefs: []
  type: TYPE_NORMAL
- en: Study each neighborhood as a separate entity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pool all the data together and estimate the water quality of the city as a single
    big group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have probably already noticed the pattern here. We can justify the first
    option by saying we obtain a more detailed view of the problem, which otherwise
    could become invisible or less evident if we average the data. The second option
    can be justified by saying that if we pool the data, we obtain a bigger sample
    size and hence a more accurate estimation. But we already know we have a third
    option: we can do a hierarchical model!'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we are going to use synthetic data. I love using synthetic
    data; it is a great way to understand things. If you don’t understand something,
    simulate it! There are many uses for synthetic data. Here, we are going to imagine
    we have collected water samples from three different regions of the same city
    and measured the lead content of water; samples with concentrations of lead above
    recommendations from the World Health Organization are marked with zero and samples
    with values below the recommendations are marked with one. This is a very simple
    scenario. In a more realistic example, we would have a continuous measurement
    of lead concentration and probably many more groups. Nevertheless, for our current
    purposes, this example is good enough to uncover the details of hierarchical models.
    We can generate the synthetic data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 3.3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are simulating an experiment where we have measured three groups, each one
    consisting of a certain number of samples; we store the total number of samples
    per group in the `N_samples` list. Using the `G_samples` list, we keep a record
    of the number of good-quality samples per group. The rest of the code is there
    just to generate a list of the data, filled with zeros and ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model for this problem is similar to the one we used for the coin problem,
    except for two important features:'
  prefs: []
  type: TYPE_NORMAL
- en: We have defined two hyperpriors that will influence the Beta prior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instead of setting hyperpriors on the parameters *α* and *β*, we are defining
    the Beta distribution in terms of *μ*, the mean, and *ν*, the concentration (or
    precision) of the Beta distribution. The precision is analog to the inverse of
    the standard deviation; the larger the value of *ν*, the more concentrated the
    Beta distribution will be. In statistical notation, our model is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![ μ ∼ Beta(𝛼μ,𝛽μ) ν ∼ ℋ𝒩 (σν) θi ∼ Beta(μ,ν) y ∼ Bernoulli(θ) i i ](img/file90.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice that we are using the subscript *i* to indicate that the model has groups
    with different values for some of the parameters. Using Kruschke diagrams (see
    *Figure [3.4](#x1-70010r4)*), we can recognize that the new model has one additional
    level compared to the one from *Figure [1.14](CH01.xhtml#x1-39001r14)*. Notice
    also that for this model, we are parametrizing the Beta prior distribution in
    terms of *μ* and *ν* instead of *α* and *β*. This is a common practice in Bayesian
    statistics, and it is done because *μ* and *ν* are more intuitive parameters than
    *α* and *β*.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file91.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.4**: Hierarchical model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write the model in PyMC:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 3.4**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 3.4 Shrinkage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To show you one of the main consequences of hierarchical models, I will require
    your assistance, so please join me in a brief experiment. I will need you to print
    and save the summary computed with `az.summary(idata_h)`. Then, I want you to
    rerun the model two more times after making small changes to the synthetic data.
    Remember to save the summary after each run. In total, we will have three runs:'
  prefs: []
  type: TYPE_NORMAL
- en: One run setting all the elements of `G_samples` to 18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One run setting all the elements of `G_samples` to 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One last run setting one element to 18 and the other two to 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before continuing, please take a moment to think about the outcome of this experiment.
    Focus on the estimated mean value of *θ* in each experiment. Based on the first
    two runs of the model, could you predict the outcome for the third case?
  prefs: []
  type: TYPE_NORMAL
- en: 'If we put the result in a table, we get something more or less like this; remember
    that small variations could occur due to the stochastic nature of the sampling
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '| G_samples | Mean |'
  prefs: []
  type: TYPE_TB
- en: '| 18, 18, 18 | 0.6, 0.6, 0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 3, 3, 3 | 0.11, 0.11, 0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 18, 3, 3 | 0.55, 0.13, 0.13 |'
  prefs: []
  type: TYPE_TB
- en: '**Table 3.1**: Sample data and corresponding means'
  prefs: []
  type: TYPE_NORMAL
- en: In the first row, we can see that for a dataset of 18 good samples out of 30,
    we get a mean value for *θ* of 0.6; remember that now the mean of *θ* is a vector
    of 3 elements, 1 per group. Then, on the second row, we have only 3 good samples
    out of 30 and the mean of *θ* is 0.11\. These results should not be surprising;
    our estimates are practically the same as the empirical means. The interesting
    part comes in the third row. Instead of getting a mix of the mean estimates of
    *θ* from the other two rows, such as 0.6, 0.11, and 0.11, we get different values,
    namely 0.55, 0.13, and 0.13.
  prefs: []
  type: TYPE_NORMAL
- en: What on Earth happened? Did we make a mistake somewhere? Nothing like that.
    What we are seeing is that the estimates have shrunk toward the common mean. This
    is totally OK; indeed this is just a consequence of our model. By using hyperpriors,
    we are estimating the parameters of the Beta prior distribution from the data.
    Each group is informing the rest, and each group is informed by the estimation
    of the others.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure [3.5](#x1-71004r5)* shows the posterior estimates of *μ* and *ν* plugged
    into a Beta distribution. In other words, this is the posterior distribution of
    the inferred Beta prior distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file92.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.5**: Posterior distribution of the inferred Beta prior distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Why is shrinkage desirable? Because it contributes to more stable inferences.
    This is, in many ways, similar to what we saw with the Student’s t-distribution
    and the outliers; using a heavy-tailed distribution results in a more robust model
    to data points away from the mean. Introducing hyperpriors results in a more conservative
    model, one that is less responsive to extreme values in individual groups. Imagine
    that the sample sizes are different from each neighborhood, some small, some large;
    the smaller the sample size, the easier it is to get bogus results. At an extreme,
    if you take only one sample in a given neighborhood, you may just hit the only
    really old lead pipe in the whole neighborhood or, on the contrary, the only one
    made out of PVC. In one case, you will overestimate the bad quality and in the
    other underestimate it. Under a hierarchical model, the misestimation of one group
    will be ameliorated by the information provided by the other groups. A larger
    sample size will also do the trick but, more often than not, that is not an option.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of shrinkage depends on the data; a group with more data will pull
    the estimate of the other groups harder than a group with fewer data points. If
    several groups are similar and one group is different, the similar groups are
    going to inform the others of their similarity and reinforce a common estimation,
    while pulling the estimation for the less similar group toward them; this is exactly
    what we saw in the previous example. The hyperpriors also have a role in modulating
    the amount of shrinkage. We can effectively use an informative prior distribution
    to shrink our estimate to some reasonable value if we have trustworthy information
    about the group-level distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Shrinkage
  prefs: []
  type: TYPE_NORMAL
- en: In a hierarchical model, groups sharing a common hyperprior are effectively
    sharing information through the hyperprior. This results in shrinkage, that is,
    individual estimates are shrunk toward the common mean. By partially pooling the
    data, we are modeling the groups as some middle ground between the groups being
    independent of each other and being a single big group.
  prefs: []
  type: TYPE_NORMAL
- en: Nothing prevents us from building a hierarchical model with just two groups,
    but we would prefer to have several groups. Intuitively, the reason is that getting
    shrinkage is like assuming each group is a data point, and we are estimating the
    standard deviation at the group level. Generally, we do not trust an estimation
    with too few data points unless we have a strong prior informing our estimation.
    Something similar is true for a hierarchical model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Hierarchies all the way up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Various data structures lend themselves to hierarchical descriptions that can
    encompass multiple levels. For example, consider professional football (soccer)
    players. As in many other sports, players have different positions. We may be
    interested in estimating some skill metrics for each player, for the positions,
    and for the overall group of professional football players. This kind of hierarchical
    structure can be found in many other domains as well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Medical research: Suppose we are interested in estimating the effectiveness
    of different drugs for treating a particular disease. We can categorize patients
    based on their demographic information, disease severity, and other relevant factors
    and build a hierarchical model to estimate the probability of cure or treatment
    success for each subgroup. We can then use the parameters of the subgroup distribution
    to estimate the overall probability of cure or treatment success for the entire
    patient population.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Environmental science: Suppose we are interested in estimating the impact of
    a certain pollutant on a particular ecosystem. We can categorize different habitats
    within the ecosystem (e.g., rivers, lakes, forests, wetlands) and build a hierarchical
    model to estimate the distribution of pollutant levels within each habitat. We
    can then use the parameters of the habitat distribution to estimate the overall
    distribution of pollutant levels within the ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Market research: Suppose we are interested in understanding the purchasing
    behavior of consumers for a particular product across different regions. We can
    categorize consumers based on their demographic information (e.g., age, gender,
    income, education) and build a hierarchical model to estimate the distribution
    of purchasing behavior for each subgroup. We can then use the parameters of the
    subgroup distribution to estimate the distribution of purchasing behavior for
    the overall group of consumers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going back to our football players, we have collected data from the *Premier
    League*, *Ligue 1*, *Bundesliga*, *Serie A*, and *La Liga* over the course of
    four years (2017 to 2020). Let’s suppose we are interested in the goals-per-shot
    metric. This is what statisticians usually call a *success rate*, and we can estimate
    it with a Binomial model where the parameter *n* is the number of shots and the
    observations *y* is the number of goals. This leaves us with an unknown value
    for *p*. In previous examples, we have been calling this parameter *θ* and we
    have used a Beta distribution to model it. We will do the same now, but hierarchically.
    See *Figure [3.6](#x1-72002r6)* for a graphical representation of the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file93.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.6**: Hierarchical model for the football players example. Notice
    that we have one more level than in previous hierarchical models'
  prefs: []
  type: TYPE_NORMAL
- en: In our model, *θ* represents the success rate for each player, and thus it is
    a vector of size `n_players`. We use a Beta distribution to model *θ*. The hyperparameters
    of the Beta distribution will be the vectors *μ*[*p*] and *ν*[*p*], which are
    vectors of size 4, representing the four positions in our dataset (defender `DF`,
    midfielder `MF`, forward `FW`, and goalkeeper `GK`). We will need to properly
    index the vectors *μ*[*p*] and *ν*[*p*] to match the total number of players.
    Finally, we will have two global parameters, *μ* and *ν*, representing the professional
    football players.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyMC model is defined in the following block of code. `pm.Beta(’`*μ*`’,
    1.7, 5.8)` was chosen with the help of PreliZ as a prior with 95% of the mass
    between 0 and 0.5\. This is an example of a weakly informative prior, as there
    is little doubt that a success rate of 0.5 is a high value. Sports statistics
    are well-studied, and there is a lot of prior information that could be used to
    define stronger priors. For this example, we will settle on this prior. A similar
    justification can be done for the prior `pm.Gamma(’![](img/nu.PNG)’, mu=125, sigma=50)`,
    which we define as the maximum entropy Gamma prior with 90% of the mass between
    50 and 200:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 3.5**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the top panel of *Figure [3.7](#x1-72024r7)*, we have the posterior distribution
    for the global parameter *μ*. The posterior distribution is close to 0.1\. This
    means that overall for a professional football player (from a top league), the
    probability of scoring a goal is on average 10%. This is a reasonable value, as
    scoring goals is not an easy task and we are not discriminating positions, i.e,
    we are considering players whose main role is not scoring goals. In the middle
    panel, we have the estimated *μ*[*p*] value for the forward position; as expected,
    it is higher than the global parameter *μ*. In the bottom panel, we have the estimated
    *θ* value for Lionel Messi, with a value of 0.17, which is higher than the global
    parameter *μ* and the forward position *μ*[*p*] value. This is also expected,
    as Lionel Messi is the best football player in the world, and his main role is
    scoring goals.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file94.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.7**: Posterior distribution for the mean global parameter (top),
    mean forward position (middle), and the *θ* parameter for Messi (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure [3.8](#x1-72025r8)* shows a forest plot for the posterior distribution
    for the parameter *μ*[*p*]. The posterior distribution for the forward position
    is centered around 0.13, as we have already seen, and is the highest of the four.
    This makes sense as the role of the players at a forward position is scoring goals
    as well as assisting them. The lowest value of *μ*[*p*] is for the goalkeeper
    position. This is expected, as the main role is to stop the opposing team from
    scoring, and not to score goals. The interesting aspect is that the uncertainty
    is very high; this is because we have very few goalkeepers scoring goals in our
    dataset, three to be precise. The posterior distributions for the defender and
    midfielder positions are somewhat in the middle, being slightly higher for the
    midfielder. We can explain this as the main role of a midfielder is to defend
    and attack, and thus the probability of scoring a goal is higher than a defender
    but lower than a forward.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file95.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3.8**: Posterior distribution for the parameter *μ*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p, the mean position*'
  prefs: []
  type: TYPE_NORMAL
- en: You Need to Know When to Stop
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to create hierarchical models with as many levels as we want.
    But unless the problem necessitates additional structure, adding more levels than
    required does not enhance the quality of our model or inferences. Instead, we
    will get entangled in a web of hyperpriors and hyperparameters without the ability
    to assign any meaningful interpretation to them. The goal of building models is
    to make sense of data, and thus useful models are usually those that reflect and
    take advantage of the structure of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we have presented one of the most important concepts to learn
    from this book: hierarchical models. We can build hierarchical models every time
    we can identify subgroups in our data. In such cases, instead of treating the
    subgroups as separate entities or ignoring the subgroups and treating them as
    a single group, we can build a model to partially pool information among groups.
    The main effect of this partial pooling is that the estimates of each subgroup
    will be biased by the estimates of the rest of the subgroups. This effect is known
    as shrinkage and, in general, is a very useful trick that helps to improve inferences
    by making them more conservative (as each subgroup informs the others by pulling
    estimates toward it) and more informative. We get estimates at the subgroup level
    and the group level.'
  prefs: []
  type: TYPE_NORMAL
- en: Paraphrasing the Zen of Python, we can certainly say *hierarchical models are
    one* *honking great idea, let’s do more of those!* In the following chapters,
    we will keep building hierarchical models and learn how to use them to build better
    models. We will also discuss how hierarchical models are related to the pervasive
    overfitting/underfitting issue in statistics and machine learning in *Chapter
    [5](CH05.xhtml#x1-950005)*. In *Chapter [10](CH10.xhtml#x1-18900010)*, we will
    discuss some technical problems that we may find when sampling from hierarchical
    models and how to diagnose and fix those problems.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using your own words explain the following concepts in two or three sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete pooling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No pooling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Partial pooling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the exercise we did with `model_h`. This time, without a hierarchical
    structure, use a flat prior such as Beta(*α* = 1*,*β** = 1). Compare the results
    of both models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a hierarchical version of the tips example from *Chapter [2](CH02.xhtml#x1-440002)*,
    by partially pooling across the days of the week. Compare the results to those
    obtained without the hierarchical structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each subpanel in *Figure [3.7](#x1-72024r7)*, add a reference line representing
    the empirical mean value at each level, that is, the global mean, the forward
    mean, and Messi’s mean. Compare the empirical values to the posterior mean values.
    What do you observe?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amino acids are usually grouped into categories such as `polar`, `non-polar`,
    `charged`, and `special`. Build a hierarchical model similar to `cs_h` but including
    a group effect for the amino acid category. Compare the results to those obtained
    in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our community Discord space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG

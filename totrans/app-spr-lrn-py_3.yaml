- en: '*Chapter 3*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe regression models and explain the difference between regression and
    classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of gradient descent, how it is used in linear regression
    problems, and how it can be applied to other model architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use linear regression to construct a linear model for data in an *x-y* plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the performance of linear models and use the evaluation to choose the
    best model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use feature engineering to create dummy variables for constructing more complicated
    linear models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct time series regression models using autoregression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter covers regression problems and analysis, introducing us to linear
    regression as well as multiple linear regression, gradient descent, and autoregression.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first two chapters, we were introduced to the concept of supervised machine
    learning in Python and the essential techniques required for loading, cleaning,
    exploring, and visualizing raw data sources. We discussed the criticality of the
    correlations between the specified inputs and desired output for the given problem,
    as well as how the initial data preparation process can sometimes take a lot of
    the time spent on the entire project.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will delve into the model building process and will construct
    our first supervised machine learning solution using linear regression. So, let's
    get started.
  prefs: []
  type: TYPE_NORMAL
- en: Regression and Classification Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed two distinct methods, supervised learning and unsupervised learning,
    in *Chapter 1*, *Python Machine Learning Toolkit*. Supervised learning problems
    aim to map input information to a known output value or label, but there are two
    further subcategories to consider. Both supervised and unsupervised learning problems
    can be further divided into regression or classification problems. Regression
    problems, which are the subject of this chapter, aim to predict or model continuous
    values, for example, predicting the temperature tomorrow in degrees Celsius or
    determining the location of a face within an image. In contrast, classification
    problems, rather than returning a continuous value, predict membership of one
    of a specified number of classes or categories. The example supervised learning
    problem in *Chapter 1*, *Python Machine Learning Toolkit*, where we wanted to
    determine or predict whether a wig was from the 1960s or 1980s, is a good example
    of a supervised classification problem. There, we attempted to predict whether
    a wig was from one of two distinct groups or classes; class 1 being the 1960s
    and class 2 being the 1980s. Other classification problems include predicting
    whether a passenger of the Titanic survived or perished or the classic MNIST problem
    ([http://yann.lecun.com/exdb/mnist/](B13323_03_ePub_Final_NT.xhtml#_idTextAnchor116)).
    MNIST is a database of 70,000 labeled images of handwritten digits 0 through 9\.
    The task in classifying examples from MNIST is to take one of the 70,000 input
    images and predict or classify which digit 0-9 is written in the image; so, the
    model must predict the membership of the image in one of 10 different classes.
  prefs: []
  type: TYPE_NORMAL
- en: Data, Models, Training, and Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we begin our deep dive into regression problems, we will first look
    at the four major stages involved in creating any machine learning model, supervised
    regression or otherwise. These stages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model architecture specification
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The design and execution of the training process
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The evaluation of the trained model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is advised that you ensure you are completely confident in your understanding
    of this pipeline and of what is described within this section, as each of these
    stages is critical in achieving high or even reasonable system performance. We
    will consider each of these stages in the context of the wig classification problem
    from *Chapter 1*, *Python Machine Learning Toolkit*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Preparation**'
  prefs: []
  type: TYPE_NORMAL
- en: The first stage of the pipeline, data preparation, was the focus of a significant
    component of *Chapter 1*, *Python Machine Learning Toolkit*, and thus will not
    be the subject of further analysis in this section. It is important, however,
    that the criticality of the data specification, collection, and cleaning/tidying
    process is well understood. We cannot expect to produce a high-performing system
    if the input data is sub-optimal. One common phrase that you should always remember
    with regard to data quality is *junk in, junk out*. If you put junk data in, you
    are going to get a junk result out. In our wig example, we are looking for a sample
    size at least in the order of hundreds, ideally thousands, that have been correctly
    labeled as either from the 1960s or 1980s. We do not want samples that have been
    incorrectly labeled or aren't even from either era.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Architecture**'
  prefs: []
  type: TYPE_NORMAL
- en: The second stage, model architecture specification, will be described in more
    detail in this chapter. This stage defines the type of model that is to be used,
    as well as the types and values of the parameters that comprise the model itself.
    The model is essentially a mathematical equation that is used to define the relationship
    between the input data and the desired result. As with any equation, the model
    is composed of variables and constants combined by a set of processes, for example,
    addition, subtraction, or convolution. The nature and values of the model parameters
    will vary depending upon the type of model selected and the level of complexity
    at which the model is able to describe the relationship being observed. Simpler
    models will contain fewer parameters with greater constraints on their values,
    while more complex models have a greater number of possibly varying parameters.
    In this chapter, we will be employing a linear model, which is one of the simpler
    models available, compared with some others, such as convolutional neural network
    models, which may have more than one million parameters that need to be optimized
    for a good result. This simplicity should not be mistaken for a lack of power,
    or a lack of ability to describe relationships within data, but simply that fewer
    parameters are available for tuning (that is, changing the values to optimize
    performance).
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Training**'
  prefs: []
  type: TYPE_NORMAL
- en: The third stage of the system pipeline is the design and execution of the training
    process, that is, the mechanism by which the values for the parameters of the
    model are determined. In a supervised learning problem, we can think about the
    training process as being analogous to being a student within a classroom. In
    a typical classroom environment, the teacher already has the answers to a given
    problem and is attempting to show the students how to solve the problem given
    some set of inputs. In such a scenario, the student is the model, and the parameters
    are all within the student's brain and are the means by which the student correctly
    answers the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The training process is the method the teacher uses to train the student to
    correctly answer the problem; this method can be tweaked and changed in response
    to the student's ability to learn and understand the content. Once a model architecture
    has been defined (that is, the student in the class), it is the training process
    that is used to provide the guidance and constraints required to approach an optimal
    solution. Just as some students perform better in different learning environments,
    so do models. Thus, there is an additional set of parameters known as **hyperparameters**,
    which, while not being used within the model itself to make predictions given
    some set of input data, are defined, used, and tuned in an attempt to optimize
    the performance of the model against a specified cost (or error) function (for
    example, root mean squared error). We will also discuss hyperparameters in more
    detail in this chapter, but for the time being, it is simplest to think about
    hyperparameters as the environment in which the actual parameters of the model
    are determined.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: The final stage of the pipeline is the evaluation of the model, which yields
    the final performance metric. This is the mechanism through which we know whether
    the model is worth publishing, better than a previous version, or has been effectively
    translated across programming languages or development environments. We will cover
    some of these metrics in more detail in *Chapter 6*, *Model Evaluation*, and as
    such this will not be discussed in detail at this stage. Just keep in mind that
    whichever validation technique is selected, it needs to be capable of consistently
    reporting and independently measuring the performance of the model against the
    dataset. Again, using our wig dataset as the example, the evaluation stage would
    look at how many correct predictions the model achieved, given wig images and
    the known eras as the labels.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start our investigation into regression problems with the selection
    of a linear model. Linear models, while being a great first choice due to their
    intuitive nature, are also very powerful in their predictive power, assuming datasets
    contain some degree of linear or polynomial relationship between the input features
    and values. The intuitive nature of linear models often arises from the ability
    to view data as plotted on a graph and observe a trending pattern in the data
    with, say, the output (the *y* axis value for the data) trending positively or
    negatively with the input (*x* axis value). While often not presented as such,
    the fundamental components of linear regression models are also often learned
    during high school mathematics classes. You may recall that the equation of a
    straight line, or linear model, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Equation of a straight line](img/C12622_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Equation of a straight line'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here, *x* is the input value and *y* is the corresponding output or predicted
    value. The parameters of the model are the gradient or slope of the line (change
    in *y* values divided by change in *x*) defined by *m* as well as the *y*-intercept
    value *b*, which indicates where the line crosses the *y* axis. With such a model,
    we can provide values for the *m* and *b* parameters to construct a linear model.
    For example, *y = 2x + 1*, has a slope of 2, indicating the changes in *y* values
    are at a rate of twice that of *x*; the line crosses the *y* intercept at 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Parameters of a straight line](img/C12622_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Parameters of a straight line'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, we have an understanding of the parameters that are required to define a
    straight line, but this isn't really doing anything particularly interesting.
    We just dictated the parameters of the model to construct a line. What we want
    to do is take a dataset and construct a model that best describes a dataset. As
    mentioned before, this dataset needs to have something that approximates a linear
    relationship between the input features and output values. For this purpose, we
    have created a synthetic dataset of recorded air temperatures from the years 1841
    to 2010, which is available in the accompanying code bundle of this book or on
    GitHub at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](B13323_03_ePub_Final_NT.xhtml#_idTextAnchor115).
    This dataset is composed of values designed to demonstrate the subject matter
    of this chapter and should not be mistaken for data collected from a scientific
    study.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 28: Plotting Data with a Moving Average'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in *Chapter 1*, *Python Machine Learning Toolkit*, a thorough
    understanding of the dataset being used is critical if a high-performing model
    is to be built. So, with this in mind, let''s use this exercise to load, plot,
    and interrogate the data source:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `numpy`, `pandas`, and `matplotlib` packages with alternative handles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the pandas `read_csv` function to load the CSV file containing the `synth_temp.csv`
    dataset, and then display the first five lines of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3: The first five rows](img/C12622_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.3: The first five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since we are only interested in the data from 1901 to 2010, remove all rows
    prior to 1901:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4: The first five rows after removing all rows prior to 1901](img/C12622_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.4: The first five rows after removing all rows prior to 1901'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The original dataset contains multiple temperature measurements per year, with
    more measurements for the later years (12 for 2010) and less for the earlier years
    (6 for 1841); however, we are interested in a list of yearly average temperatures.
    Group the data by year and use the `agg` method of the DataFrame to create the
    yearly averages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.5: Yearly average data](img/C12622_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.5: Yearly average data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Given that the data is quite noisy, a moving average filter would provide a
    useful indicator of the overall trend. A moving average filter simply computes
    the average over the last *N* values and assigns this average to the *(N+1)**th*
    sample. Compute the values for a moving average signal for the temperature measurements
    using a window of 10 years:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6: Values for a moving average signal](img/C12622_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.6: Values for a moving average signal'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the first 9 samples are `NaN`, which is because of the size of the
    moving average filter window. The window size is 10, thus 9 (10-1) samples are
    required to generate the first average, and thus the first 9 samples are `NaN`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, plot the measurements by year along with the moving average signal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7: Mean annual air temperature](img/C12622_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Mean annual air temperature'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Figure 3.7* is the expected output of this exercise and is a plot of the mean
    land temperature measurements for each year with a 10-year moving average trending.
    By simply looking at this plot, we can immediately make a couple of interesting
    observations. The first observation that we can make is that the temperature remained
    relatively consistent from the year 1901 to about 1960, after which there is an
    increasing trend until the data ends in 2010\. Secondly, there is a reasonable
    amount of scatter or noise in the measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Plotting Data with a Moving Average'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this activity, we have acquired a dataset of weather information from Austin,
    Texas (`austin_weather.csv`), available in the accompanying source code, and will
    be looking at the changes in average daily temperature. We will plot a moving
    average filter for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we will need to import a few libraries, which can be done
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset into a pandas DataFrame from the CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We only need the `Date` and `TempAvgF` columns; remove all others from the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initially, we will only be interested in the first year's data, so we need to
    extract that information only. Create a column in the DataFrame for the year value
    and extract the year value as an integer from the strings in the `Date` column
    and assign these values to the `Year` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that temperatures are recorded daily.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Repeat this process to extract the month values and store the values as integers
    in a `Month` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the first year's worth of data to a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute a 20-day moving average filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the raw data and moving average signal, with the *x* axis being the day
    number in the year.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 325.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Least Squares Method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The field of machine learning and artificial intelligence evolved essentially
    as a specialized branch of statistics, and as such it is important to reflect
    on these origins from time to time to have a thorough understanding of how models
    are able to be used as predictive tools. It is also interesting to see the points
    where machine learning grew out of statistics and compare the more modern methods
    available today. Linear regression models are a great example of this as they
    can be used to demonstrate more classical solving techniques such as the least
    squares method, as well as more modern methods, such as gradient descent, which
    we will also cover in this chapter. Linear models also have the additional advantage
    of containing mathematical concepts commonly learned in high school, such as the
    equation of a straight line, providing a useful platform for describing the methods
    used to fit data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The traditional method of solving linear models, which is executed by toolkits
    such as scikit-learn, SciPy, Minitab, and Excel, is the **least squares method**,
    and this is the first method we will cover. Referring to our standard equation
    for a straight line (*Figure 3.1*), *m* is the slope or gradient of the line and
    *c* is the *y* axis offset. These values can be directly calculated in the least
    squares method by first determining the average *x* and *y* values, which will
    be denoted as ![](img/C12622_Formula_03_01.png) and ![](img/C12622_Formula_03_02.png)
    respectively. With the mean values calculated, we can then calculate the gradient,
    *m*, by multiplying the differences in *x* values from the mean with the differences
    in *y* values from the mean and dividing by the squared differences in *x* from
    the mean. The offset can then be calculated by solving for *b* using the newly
    calculated *m* and ![](img/C12622_Formula_03_01.png) and ![](img/C12622_Formula_03_02.png).
    This is represented mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12622_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Least squares method'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can consider this in more practical terms, recalling that the gradient is
    simply the change in the vertical (or *y*) values divided by the horizontal (or
    *x*) values. In the context of mean annual air temperature over time, we can see
    that we are taking the sum of the differences in the individual temperature values
    from the mean value multiplied by the individual differences in the time values
    from the mean. By dividing the result by the sum of the squared differences in
    time from the mean, the trending gradient is completed, providing part of the
    temperature over time model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we don't need to worry about computing these values by hand, though it
    wouldn't be that hard to do. But specialized libraries such as SciPy and scikit-learn
    can be used do to the work for us as well as worrying about some of the details
    such as computational efficiency. For the purposes of this section, we will use
    scikit-learn as our library of choice as it provides a great introduction to the
    scikit-learn interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'One implementation detail to note is that the scikit-learn linear regression
    model is actually a wrapper around the SciPy ordinary least squares function and
    provides some additional convenience methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: scikit-learn’s implementation of linear regression](img/C12622_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: scikit-learn''s implementation of linear regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The scikit-learn Model API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The scikit-learn API uses a reasonably simple code pattern irrespective of the
    type of model being constructed. Put simply, the model must first be defined with
    all appropriate hyperparameters that are relevant to the training or fitting process.
    In defining the model, a model object is returned, which is then used during the
    second stage of model construction, which is training or fitting. Calling the
    `fit` method on the model object with the appropriate training data will then
    train the model with the defined hyperparameters. We will now use this pattern
    to construct our first linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 29: Fitting a Linear Model Using the Least Squares Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will construct our first linear regression model using
    the least squares method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the scikit-learn `LinearRegression` model for this exercise, so
    import the class from the `linear_regression` module of scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Construct a linear regression model using the default values; that is, compute
    a value for the *y* intercept and do not normalize the input data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.10: Linear regression model](img/C12622_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.10: Linear regression model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we are ready to fit or train the model to the data. We will provide the
    year values as the input and the mean yearly temperature as the output. Note that
    the `fit` method of scikit-learn models expects 2D arrays to be provided as the
    `X` and `Y` value. As such, the year or index values need to be reshaped to suit
    the method. Get the values of the index as a NumPy array using the `.values` method
    and reshape the values to `((-1, 1))` which is an *N x 1* array. The value `-1`
    in a NumPy shape definition represents that its value is inferred from the current
    shape of the array and the target shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11: Output of the fit method](img/C12622_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.11: Output of the fit method'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Get the parameters for the model by printing the values for `model.coef_` (which
    is the value for *m*) and `model.intercept_` (which is the value for the *y* intercept):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Output of model co-efficient and model intercept](img/C12622_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.12: Output of model co-efficient and model intercept'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that we have our generated model, we can predict some values to construct
    our trendline. So, let''s use the first, last, and average year value as the input
    to predict the local temperature. Construct a NumPy array with these values and
    call the array `trend_x`. Once you are done, pass the values for `trend_x` to
    the `predict` method of the model to get the predicted values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.13: Array showing min, mean, and max](img/C12622_03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.13: Array showing min, mean, and max'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now plot the trendline produced by the model, with the model parameters over
    the previous plot with the raw data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.14: Linear regression – a first simple linear model](img/C12622_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Linear regression – a first simple linear model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we have the model, we need to evaluate its performance to see how well
    it fits the data and to compare against other models we may like to generate.
    We will cover this topic in much more detail in *Chapter 6*, *Model Evaluation*,
    where we'll look at validation and cross validation, but for the moment we will
    compute the **R-squared** value for the model against the dataset. R-squared,
    which is commonly reported in statistical-based modeling, is a ratio of the sum
    of squares between the predicted and actual values and the actual value from its
    own mean. A perfect fit will have an r2 of 1, and the score decreases to 0 as
    the performance degrades.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15: R-squared score](img/C12622_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: R-squared score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can compute the R2 value using the `score` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12622_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: R-squared score for the model against the dataset'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, looking at the trendline in *Figure 3.14*, we can see that the linear model
    is OK. It definitely performs better in the linear region of the moving average
    post 1960, but could use some improvement for the data earlier than 1970\. Is
    there something we can do to manage this? It seems something that two separate
    linear models could perform better than one. The data prior to 1960 could form
    one model and the post-1960 data another? We could do that and just split the
    data and create two separate models, evaluate them separately, and put them together
    in a piece-wise fashion. But we could also include similar features in our existing
    model through the use of dummy variables.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before continuing, it is important to note that when reporting the performance
    of machine learning models, the data used to train the model is *not* to be used
    to evaluate the model, as it will give an optimistic view of the model's performance.
    We will cover the concept of validation, which includes evaluating and reporting
    model performance, in *Chapter 6*, *Model Evaluation*. For the purpose of this
    chapter, however, we will use the training data to check the model's performance;
    just remember that once you have completed *Chapter 6*, *Model Evaluation*, you
    will know better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Linear Regression Using the Least Squares Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this activity, we will use the Austin, Texas weather dataset that we used
    in the previous activity. We will plot a linear regression model using the least
    squares method for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we will need to import a few libraries and load data from
    a previous activity, which can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the measurements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the rolling average values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a linear regression model using the default parameters, that is, calculate
    a *y* intercept for the model and do not normalize the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now fit the model, where the input data is the day number for the year (1 to
    365) and the output is the average temperature. To make later calculations easier,
    insert a column (`DayOfYear`) that corresponds with the day of the year for that
    measurement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model with the `DayOfYear` values as the input and `df_first_year.TempAvgF`
    as the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the parameters of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's check the trendline provided by the model. Plot this simply using the
    first, middle, and last values (days in years) in the linear equation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the values with the trendline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the performance of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's check how well the model fits the data. Calculate the r2 score to find
    out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 329.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Linear Regression with Dummy Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dummy variables are categorical variables that we can introduce into a model,
    using information provided within the existing dataset. The design and selection
    of these variables is considered a component of feature engineering, and depending
    upon the choice of variables, the results may vary. We made the observation earlier
    that the moving average begins to continually increase from approximately 1960
    and that the initial plateau ends at approximately 1945\. We will introduce two
    dummy variables, `Gt_1960` and `Gt_1945`; these variables will indicate whether
    the time period for the measurements is greater than the year 1960 and greater
    than the year 1945\. Dummy variables are typically assigned the values 0 or 1
    to indicate the lack of or presence of the assigned category for each row. In
    our example, given the magnitude of the `Year` values, we will need to increase
    the positive value of the dummy variables as, with a value of 1, they will have
    little to no effect given that the values for `Year` are in the thousands. Throughout
    the following exercise, we will demonstrate that regression models can be composed
    of both discrete and continuous values and that, depending on an appropriate choice
    of dummy variables, performance can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 30: Introducing Dummy Variables'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will introduce two dummy variables into our linear regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, assign the index values of the `df_group_year` DataFrame to
    the `Year` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a dummy variable with a column labeled `Gt_1960`, where the value is
    `0` if the year is less than 1960 and `10` if greater:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17: Added column Gt_1960](img/C12622_03_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.17: Added column Gt_1960'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create a dummy variable with a column labeled `Gt_1945`, where the value is
    `0` if the year is less than 1945 and `10` if greater:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.18: Added column Gt_1945](img/C12622_03_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.18: Added column Gt_1945'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Call the `tail()` method to look at the last two rows of the `df_group_year`
    DataFrame to confirm that the post 1960 and 1945 labels have been correctly assigned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.19: Last two values](img/C12622_03_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.19: Last two values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fit the linear model with the additional dummy variables by passing the `Year`,
    `Gt_1960`, and `Gt_1945` columns as inputs to the model, with `AverageTemperature`
    again as the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.20: Linear model fitted on data](img/C12622_03_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.20: Linear model fitted on data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Check the R-squared score for the new model against the training data to see
    whether we made an improvement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.21: R-squared score for the model](img/C12622_03_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.21: R-squared score for the model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have made an improvement! This is a reasonable step in accuracy given that
    the first model''s performance was 0.8618\. We will plot another trendline, but
    we will need more values than before to accommodate the additional complexity
    of the dummy variables. Use `linspace` to create 20 linearly spaced values between
    1902 and 2013:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get this output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.22: Array of 20 years created using linspace](img/C12622_03_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.22: Array of 20 years created using linspace'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create an array of zeros in the shape *20 x 3* and fill the first column of
    values with `x`, the second column with the dummy variable value for greater than
    1960, and the third column with the dummy variable value for greater than 1945:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.23: Finding trend_x](img/C12622_03_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.23: Finding trend_x'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now get the *y* values for the trendline by making predictions for `trend_x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.24: Finding trend_y](img/C12622_03_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.24: Finding trend_y'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the trendline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25: Predictions using dummy variables](img/C12622_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.25: Predictions using dummy variables'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Incorporating dummy variables made quite an improvement to the model, but looking
    at the trendline, it doesn't seem like a reasonable path for natural phenomena
    such as temperature to take and could be suffering from overfitting. We will cover
    overfitting in more detail in *Chapter 5*, *Ensemble Modeling*; however, let's
    use linear regression to fit a model with a smoother prediction curve, such as
    a parabola.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: Dummy Variables'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this activity, we will use the Austin, Texas weather dataset that we used
    in the previous activity. In this activity, we will use dummy variables to enhance
    our linear regression model for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we will need to import a few libraries and load data from
    a previous activity, which can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Plot the raw data (`df`) and moving average (`rolling`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at the result of the previous step, there seems to be an inflection
    point around day 250\. Create a dummy variable to introduce this feature into
    the linear model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the first and last samples to confirm that the dummy variable is correct.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a least squares linear regression model and fit the model to the `DayOfYear`
    values and the dummy variable to predict `TempAvgF`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the R2 score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `DayOfYear` values, create a set of predictions using the model to
    construct a trendline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the trendline against the data and moving average.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 334.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Parabolic Model with Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linear regression models are not simply constrained to straight-line linear
    models. We can fit some more complicated models using the exact same techniques.
    We have mentioned that there seems to be some parabolic characteristics to the
    data, so let''s try fitting a parabolic model. As a reminder, the equation for
    a parabola is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12622_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: Equation for a parabola'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The addition of this squared term transforms the model from a straight line
    to one that has a parabolic (or arc like) trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27: Parabolic curve](img/C12622_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27: Parabolic curve'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 31: Parabolic Models with Linear Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to fit a parabolic model using linear regression, we just need to
    manipulate our inputs a little. In this exercise, we''ll see how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is provide a squared term for year values. For
    convenience, create a copy of the index and store it in a `Year` column. Now square
    the `Year` column to provide parabolic features and assign the result to the `Year2`
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.28: First five rows](img/C12622_03_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.28: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fit the data to the model. This time, we will need to provide two sets of values
    as the inputs to the model, `Year` and `Year2`, which is equivalent to passing
    *x* and *x**2* to the parabolic equation. As we are providing two columns of data,
    we do not need to reshape the input data as it will be provided as an *N x 2*
    array by default. The target *y* value remains the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.29: Model fitted](img/C12622_03_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.29: Model fitted'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the parameters of the model by looking at the coefficients and the intercept;
    there will now be two coefficients to print:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30: Model coefficients and intercept](img/C12622_03_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.30: Model coefficients and intercept'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Evaluate the performance of the model using the `score` method. Has the performance
    improved?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.31: R-squared score](img/C12622_03_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.31: R-squared score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Yes, the model has improved slightly on the dummy variable method, but let''s
    look at the trendline to see whether it is a more reasonable fit. Plot the trendline
    as we did before. Again, to effectively plot the parabolic arc of the trendline,
    we will need more predicted values. Use `linspace` to create 20 linearly spaced
    values between 1902 and 2013:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.32: Finding 20 increments using linspace](img/C12622_03_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.32: Finding 20 increments using linspace'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now the model we trained takes two columns of year data as an input: the first
    column containing squared yearly values and the second just the year value itself.
    To provide the data to the model, create a NumPy array (`trend_x`) of zeros with
    20 rows and 2 columns. Square the values for `x` and assign to the first column
    of `trend_x`, and simply assign `x` to the second column of `trend_x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.33: Trends for the x variable](img/C12622_03_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.33: Trends for the x variable'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now get the *y* values for the trendline by making predictions for `trend_x`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.34: Trends for the y variable](img/C12622_03_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.34: Trends for the y variable'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the trendline as per the straight-line model. Remember that the *x* axis
    values for `trend_y` are the years; that is, the second column of `trend_x`, and
    not the years squared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.35: Linear regression with a parabolic model](img/C12622_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.35: Linear regression with a parabolic model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Referring to *Figure 3.35*, we can see the performance benefit in using the
    parabolic model, with the trendline almost following the 10-year moving average.
    This is a reasonably good fit given the amount of noise in the yearly average
    raw data. In such a case, it should not be expected that the model will fit the
    data perfectly. If our model was to perfectly fit the observed examples, there
    would be a very strong case for overfitting the data, leading to poor predictive
    power with unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Other Model Types with Linear Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have tried a standard linear model as well as a dummy variable. In this activity,
    we will experiment with a few different functions to try and get a better fit
    for the data. For each different function, try to make sure you print the function
    parameters, R2 value, and plot the trendline against the original and moving average
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Try a few different functions, experiment with the data, and see how good your
    predictions can get. In this activity, we will use the sine function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we will need to import a few libraries and load data from
    a previous activity, which can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a sine curve function as the basis of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the parameters of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the r2 value to measure the performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the trendline values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the trendline with the raw data and the moving average.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 338.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generic Model Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The least squares method of constructing a linear regression model is a useful
    and accurate method of training, assuming that the dimensionality of the dataset
    is low and that the system memory is sufficiently large to be able to manage the
    dataset and, in the case of the scikit-learn implementation, the matrix division
    operation. In recent times, large datasets have become more readily available,
    with universities, governments, and even some companies releasing large datasets
    for free online; as such, it may be relatively easy to exceed system memory when
    using the least squares method of regression modeling. In this situation, we will
    need to employ a different method of training the algorithm, such as gradient
    descent, which is not *as* susceptible to high dimensionality, allows large datasets
    to be trained, and avoids the use of memory intensive matrix operations. Before
    we look at gradient descent in a little more detail, we will revisit the process
    of training a model in a more general form, as most training methods, including
    gradient descent, adhere to this generic process (*Figure 3.36*). The training
    process involves the repeated exposure of the model and its parameters to a set
    of example training data and passing the predicted values issued by the model
    to a specified cost or error function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function is used to determine how close the model is to its target
    values and a measure of progress throughout the training process. The final piece
    of the process is the definition of the training hyperparameters, which, as discussed
    at the start of this chapter, are the means by which the process of updating the
    model is regulated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.36: Generic training process](img/C12622_03_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.36: Generic training process'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of gradient descent can be summarized as a means of updating the
    parameters of the model proportionally and in response to an error within the
    system, as defined by the cost function. There are a number of cost functions
    that can be selected, depending on the type of model being fitted or the problem
    being solved. We will select the simple but effective mean squared error cost
    function, but first we will rewrite our model equation in notation consistent
    with that generally used within machine learning literature. Using the equation
    of a straight line as our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.37: Equation of a straight line](img/C12622_03_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.37: Equation of a straight line'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.38: Shortened linear model](img/C12622_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.38: Shortened linear model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Where ![](img/C12622_Formula_03_03.png) is the prediction made by the model
    and, by convention, ![](img/C12622_Formula_03_04.png) is used to represent the
    intercept term. With the new model notation, we can define the mean squared error
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.39: Mean squared error](img/C12622_03_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.39: Mean squared error'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Where *y**t* is the corresponding ground truth value and *N* is the number of
    training samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these two functions defined, we can now look at the gradient descent algorithm
    in greater detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent starts by taking an initial, random guess at the values for
    all ![](img/C12622_Formula_03_05.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A prediction for each of the samples in the training set is made using the random
    values for ![](img/C12622_Formula_03_05.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The error for those parameters ![](img/C12622_Formula_03_06.png) is then computed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The values for ![](img/C12622_Formula_03_05.png) are then modified, making a
    small adjustment proportional to the error, in an attempt to minimize the error.
    More formally, the update process takes the current value for ![](img/C12622_Formula_03_07.png)
    and subtracts the component of ![](img/C12622_Formula_03_06.png) attributed to
    ![](img/C12622_Formula_03_07.png) times the small adjustment ![](img/C12622_Formula_03_08.png),
    otherwise known as the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Without delving too deeply into the mathematical details, the equation to update
    the parameters or weights (![](img/C12622_Formula_03_09.png)) can be written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.40: Gradient descent update step](img/C12622_03_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.40: Gradient descent update step'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s discuss this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: The`:=` operator denotes variable reassignment or update as a computer programming
    concept.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This training process will continue until convergence; that is, until the changes
    to the weights are so small that there is essentially no change to the parameters,
    or until we intervene and stop the process, as is the case in cross-validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value assigned to the learning rate is critical for the training process
    as it defines how large the changes to the weights are and subsequently how big
    the steps to take down the error curve are. If the value is too small, the training
    process may take far too long or may get stuck in areas of local minima of the
    error curve and not find an optimal global value. Alternatively, if the steps
    are too large, the training process can become unstable, as they pass over the
    local and global minima.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This process is visualized in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.41: Gradient descent process](img/C12622_03_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.41: Gradient descent process'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a general hint for setting the learning rate, start larger, say around 0.1,
    and if a solution cannot be found, that is, the error is a `NaN` (not a number),
    reduce by a factor of 10\. Keep going until the training is continuing and the
    error is continually reducing. Once you are happy with the model and are almost
    done, reduce the learning rate by a small amount and let the training run for
    longer.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this process may sound complicated, it isn''t anywhere near as scary
    as it looks. Gradient descent can be summarized by making a one-time only guess
    at the values for the weights, calculating the error in the guess, making small
    adjustments to the weights, and continually repeating the process until the error
    converges at a minimum value. To reinforce our understanding, let''s look at a
    more concrete example. We will use gradient descent to train the original linear
    regression model we constructed in *Exercise 29: Fitting a Linear Model Using
    the Least Squares Method*, replacing the least squares method with gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 32: Linear Regression with Gradient Descent'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can start the gradient descent process, we need to spend a little
    bit of time setting up the model. In our Jupyter notebook, perform these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a function to define our linear model. This is where the advantage of
    using the shortened form of the linear model (*Figure 3.38*) comes in handy. We
    can use linear algebra multiplication between the weights (theta) and the input
    values, *x*, which is the equivalent of ![](img/C12622_Formula_03_10.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In order to use this linear algebra multiplication technique, we must modify
    the input data by inserting a row of ones to represent the bias term. Create an
    array of ones with a shape of two columns (one for the gradient term of the weights
    and one for the bias term). Insert the normalized `Year` values into the first
    row of the newly created array.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To use the input data in a gradient descent process, we must also normalize
    all of the values to be between 0 and 1\. This is a critical aspect of the process,
    as if one variable has values in the order of, say 1,000, and the second in the
    order of 10, then the first variable will be 100 times more influential in the
    training process and could lead to the inability to train the model. By ensuring
    that all variables are scaled between 0 and 1, they will have equal influence
    during training. Scale the input by dividing the values for `Year` by the maximum
    value:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You''ll get this output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.42: Modified data](img/C12622_03_42.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.42: Modified data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As we have learned, we need to take an initial guess at the values for the
    weights. We need to define two weight values, one for the gradient and one for
    the *y* intercept. To ensure that the same first random number is initialized
    each time, seed the NumPy random number generator. Seeding the random number generator
    ensures that each time the script is run, the same set of random numbers are produced.
    This ensures the consistency of the same model in multiple runs and provides the
    opportunity to check the performance of the model against possible changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the weights with a normally distributed random number with a mean
    of 0 and standard deviation of 0.1\. We want the initialized weights to be random,
    but still close to zero to give them a chance to find a good solution. In order
    to execute the matrix multiplication operation in `h_x`, reshape the random numbers
    to one row and two columns (one for the gradient and one for the *y* intercept):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.43: Theta value](img/C12622_03_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.43: Theta value'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Define the ground truth values as the average yearly temperatures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the cost function (mean squared error) as a Python function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the learning rate as discussed earlier. This is a very important parameter
    and it must be set appropriately. As mentioned earlier, set it too small and the
    model may take a very long time to find a minimum; set it too large and it may
    not reach it at all. Define the learning rate as `1e-6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that implements a step of gradient descent (*Figure 3.40*).
    The function will take the predicted and true values as well as the values for
    *x* and *gamma*, and return the value to be added to the weights (theta):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the maximum number of epochs (or iterations) we want the training process
    to run for. Each epoch predicts the values of *y* (the normalized annual mean
    land temperature) given *x* and updates the weights in accordance with the error
    in the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make an initial prediction and calculate the error or cost in that prediction
    using the defined `h_x` and `J_theta` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.44: Initial cost of J theta](img/C12622_03_44.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.44: Initial cost of J theta'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Complete the first update step by hand. Use the newly predicted values to call
    the `update` function, make another call to `h_x` to get the predicted values,
    and get the new error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.45: Updated cost of J theta](img/C12622_03_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.45: Updated cost of J theta'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice the small reduction in the error; as such, many epochs of training will
    be required. Put the `predict` and `update` function calls in a `for` loop for
    `max_epochs` and print the corresponding error at each tenth epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.46: Ten epochs](img/C12622_03_46.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.46: Ten epochs'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Visualize the training history by plotting `epoch_hist` versus `error_hist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.47: Training history curve: a very important tool](img/C12622_03_47.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.47: Training history curve: a very important tool'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the error reaches an asymptote at 30,000 epochs, and thus `max_epochs`
    could be reduced.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `r2_score` function from `sklearn.metrics` to compute the R-squared
    score for the model trained using gradient descent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.48: R-squared score](img/C12622_03_48.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.48: R-squared score'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To plot the trendline for the new model, again create 20 linearly spaced year
    values between 1901 and 2013:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.49: Values using linspace](img/C12622_03_49.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.49: Values using linspace'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In order to use this data with our model, we must first normalize the maximum
    value to scale between 0 and 1 and insert a row of ones. Execute this step in
    a similar way to when the data was prepared for training in *Step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.50: Trends in x](img/C12622_03_50.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.50: Trends in x'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Call the `h_x` model function with the weights saved from the training process
    to get predicted *y* values for the trendline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.51: Trends in y](img/C12622_03_51.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.51: Trends in y'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the trendline with the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.52: Mean air temperature measurements using gradient descent](img/C12622_03_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.52: Mean air temperature measurements using gradient descent'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Congratulations! You have just trained your first model with gradient descent.
    This is an important step as this simple tool can be used to construct more complicated
    models such as logistic regression and neural network models. We must first, however,
    note one important observation: the r-squared value produced by the gradient descent
    model is not as high as the least squares model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first step of gradient descent, we guess some plausible values for the
    weights, and then make small adjustments to the weights in an attempt to reduce
    the error and stop training only when the error stops reducing. Gradient descent
    finds its power in two specific applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Solving more complicated models for which a mathematically optimal solution
    has yet to be or cannot be found
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a means of training with datasets or parameters that are so large
    that physical hardware restrictions, such as available memory, prevent the use
    of other methods such as least squares
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, if the dataset is not excessively large and can be solved optimally, we
    should definitely use the more precise method. That being said, there are many
    more options available to modify the gradient descent process, including different
    types of gradient descent algorithms and more advanced uses of learning rate and
    the way the data is supplied during training. These modifications fall outside
    the scope of this book, as an entire book could be written on the gradient descent
    process and methods for improving performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 33: Optimizing Gradient Descent'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous exercise, we implemented gradient descent directly; however,
    we would not typically use this implementation. The scikit-learn method of gradient
    descent contains a number of optimizations and can be used in only a few lines
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `SGDRegressor` class and construct a model using the same parameters
    as used in the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the year values, divided by the maximum year value, as an input and fit
    with the `AverageTemperature` values as the ground truth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the values using the trained model and determine the r-squared value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the trendline as determined by the model in addition to the raw data and
    the moving average:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.53: Optimized gradient descent predicted trendline](img/C12622_03_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.53: Optimized gradient descent predicted trendline'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compare this graph to the one constructed using the manual implementation of
    gradient descent. Notice the similarities: this provides us with confidence that
    both implementations of gradient descent are correct.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: Gradient Descent'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will implement the same model as *Activity 6*, *Linear
    Regression Using the Least Squares Method*; however, we will use the gradient
    descent process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we will need to import a few libraries and load data from
    a previous activity, which can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a generic gradient descent model and normalize the day of year values
    to be between 0 and 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the details of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the *x* `(trend_x`) trendline values by dividing by the maximum. Predict
    `y_trend_values` using the gradient descent model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the data and the moving average with the trendline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 341.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Multiple Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have already covered regular linear regression, as well as linear regression
    with polynomial terms, and considered training them with both the least squares
    method and gradient descent. This section of the chapter will consider an additional
    type of linear regression: multiple linear regression, where more than one type
    of variable (or feature) is used to construct the model. To examine multiple linear
    regression, we will use a modified version of the Boston Housing Dataset, available
    from [https://archive.ics.uci.edu/ml/index.php](B13323_03_ePub_Final_NT.xhtml#_idTextAnchor114).
    The modified dataset can be found in the accompanying source code or on GitHub
    at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](B13323_03_ePub_Final_NT.xhtml#_idTextAnchor115)
    and has been reformatted for simplified use. This dataset contains a list of different
    attributes for property in the Boston area, including the crime rate per capita
    by town, the percentage of the population with a lower socio-economic status,
    as well as the average number of rooms per dwelling, and the median value of owner-occupied
    homes in the area.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 34: Multiple Linear Regression'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use the Boston Housing Dataset to construct a multiple linear model
    that predicts the median value of owner-occupied homes given the percentage of
    the population with a lower socio-economic status and the average number of rooms
    per dwelling:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the housing database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `head()` function will return the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.54: First five rows](img/C12622_03_54.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.54: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot both columns: average number of rooms (`RM`) and the percentage of the
    population of a lower socio-economic status (`PTRATIO`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.55: Parameters versus the median value](img/C12622_03_55.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.55: Parameters versus the median value'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Construct a linear regression model for the percentage of lower socio-economic
    status (`LSTAT`) versus the median property value (`MEDV`), and compute the performance
    of the model in terms of the R-squared value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.56: Model score using LSTAT](img/C12622_03_56.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.56: Model score using LSTAT'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Compute the prediction performance of the linear model trained using the average
    number of rooms to predict the property value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.57: Model score using RM](img/C12622_03_57.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.57: Model score using RM'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create a multiple linear regression model using both the `LSTAT` and `RM` values
    as input to predict the median property value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.58: Model score using LSTAT and RM](img/C12622_03_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.58: Model score using LSTAT and RM'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Autoregression Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autoregression models are part of a more classical statistical modeling technique
    that is used on time series data (that is, any dataset that changes with time)
    and extends upon the linear regression techniques covered in this chapter. Autoregression
    models are commonly used in the economics and finance industry as they are particularly
    powerful in time series datasets with a sizeable number of measurements. To reflect
    this, we will change our dataset to the S&P daily closing prices from 1986 to
    2018, which is available in the accompanying source code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.59: S&P 500 Daily Closing Price](img/C12622_03_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.59: S&P 500 Daily Closing Price'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The main principle behind autoregression models is that, given enough previous
    observations, a reasonable prediction for the future can be made; that is, we
    are essentially constructing a model using the dataset as a regression against
    itself, hence **autoregression**. This relationship can be modeled mathematically
    as a linear equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12622_03_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.60: First-order autoregression model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Where ![](img/C12622_Formula_03_11.png) is the predicted value for time, *t*,
    ![](img/C12622_Formula_03_12.png) is the first weight of the model, ![](img/C12622_Formula_03_13.png)
    is the second weight with ![](img/C12622_Formula_03_14.png) as the previous value
    in the dataset, and ![](img/C12622_Formula_03_15.png) is an error term.
  prefs: []
  type: TYPE_NORMAL
- en: The equation in *Figure 3.60* represents a model using only the previous value
    in the dataset to make a prediction. This is a first-order autoregression model
    and can be extended to include more previous samples.
  prefs: []
  type: TYPE_NORMAL
- en: The equation in *Figure 3.61* provides an example of a second-order model, including
    the previous two values.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a *k**th* order autoregression model contains values with corresponding
    parameters between ![](img/C12622_Formula_03_16.png), adding more context about
    the previous observations about the model. Again, referring to the equation in
    *Figure 3.61* and the *k**th* order autoregression, the recursive properties of
    the autoregression model can also be observed. Each prediction uses the previous
    value(s) in its summation, and thus, if we take the previously predicted values,
    they themselves use the predictions of the previous value, hence the recursion.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.61: Second and kth order autoregression model](img/C12622_03_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.61: Second and kth order autoregression model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 35: Creating an Autoregression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use the S&P 500 model to create an autoregression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the S&P 500 dataset, extract the year represented as two digits in the
    column date, and create a new column, `Year`, with the year represented in the
    four-digit format (for example, 02-Jan-86 will become 1986 and 31-Dec-04 will
    become 2004):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.62: First five rows](img/C12622_03_62.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.62: First five rows'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the raw dataset with years along the *x* axis in multiples of five:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.63: Plot of closing price through the years](img/C12622_03_63.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.63: Plot of closing price through the years'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Before we can construct an autoregression model, we must first check to see
    whether the model is able to be used as a regression against itself. To do that,
    we can once again use the pandas library to check for correlations between the
    dataset and a copy of the dataset that is shifted by a defined number of samples,
    known as `shift` method, introduce a sample lag of `3` into the first 10 values
    of the closing price and look at the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get this output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.64: Values with a lag of three](img/C12622_03_64.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.64: Values with a lag of three'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice the introduction of three NaN values into the array and that the last
    three values have dropped off the array. This is the effect of shifting, essentially
    sliding the dataset forward in time by the period defined by the lag.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Shift the dataset by a lag of 100 and plot the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.65: Plot of closing price over the year](img/C12622_03_65.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.65: Plot of closing price over the year'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that we have an understanding of the time shift, we will confirm that the
    data can be correlated against itself. To do this, use the pandas `autocorrelation_plot`
    method to check for randomness within the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.66: Relation of autocorrelation versus the lag](img/C12622_03_66.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.66: Relation of autocorrelation versus the lag'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: All of the information required to determine whether autoregression is possible
    is defined within this plot. We can see on the *x* axis, the values for **Lag**
    range from 0 to 8,000 samples, and the values for **Autocorrelation** vary from
    approximately -0.4 to 1\. There are five other additional lines of interest; however,
    at this scale on the *y* axis, it is difficult to see them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the *y* axis limits to be between -0.1 and 0.1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.67: Plot of autocorrelation versus lag](img/C12622_03_67.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.67: Plot of autocorrelation versus lag'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see in the enhanced view that there are two gray dashed lines, which
    represent the 99% confidence band that the series is non-random. The solid gray
    line represents the 95% confidence band. Once the autocorrelation plot approaches
    zero within these bands, the time series with the specified lag becomes sufficiently
    random that autoregression models would not be appropriate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To further solidify our understanding, create a plot of the closing prices versus
    the closing prices with a lag of 100 samples. According to our autocorrelation
    plot, there is a high correlation between these sets. What does that look like?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.68: Autocorrelation plot](img/C12622_03_68.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.68: Autocorrelation plot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create a plot of closing prices versus closing prices with a lag of 4,000 samples.
    Again, looking at the autocorrelation plot at a lag of 4,000, the autocorrelation
    value is approximately 0, indicating that there is no real correlation between
    the two and it is mostly random:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.69: Plot of closing prices versus closing prices with a lag of 4,000
    samples](img/C12622_03_69.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.69: Plot of closing prices versus closing prices with a lag of 4,000
    samples'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we are ready to create our model. To do this, however, we will need another
    Python package, the `statsmodel` package ([http://www.statsmodels.org](B13323_03_ePub_Final_NT.xhtml#_idTextAnchor113)),
    which is similar to scikit-learn but is dedicated to creating models and executing
    tests using the more classical statistical techniques. Install the `statsmodel`
    package. You can do this either using `conda install` or `pip`. For the Anaconda
    installation, the `conda install` method is preferred:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the autoregression class (`AR`) from `statsmodel` and construct the
    model using the closing price data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model using the `fit` method and print out the lag that was selected
    for use and the coefficients of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.70: Lag co-efficients](img/C12622_03_70.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.70: Lag coefficients'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that there are 36 coefficients for each of the weights and one constant;
    only an extract is shown for simplicity. All coefficients can be found in the
    `Ex7-AutoRegressors.ipynb` Jupyter notebook in the accompanying source code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the model to create a set of predictions starting at sample 36 (the lag)
    and finishing at 500 samples after the dataset has ended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C12622_03_71.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.71: Prediction values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the predictions'' values over the top of the original dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.72: Plot of price through the year](img/C12622_03_72.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.72: Plot of price through the year'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the predictions do an excellent job of following the dataset, and
    that after the dataset has ended, the predictions are relatively linear. Given
    that the model is constructed from the previous samples, it makes sense that it
    becomes less certain once the dataset has finished, particularly as there are
    no repetitive patterns in the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The fit seems really close – what does the difference between the predictions
    and original dataset look like? Enhance the model to observe the differences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This provides the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.73: Predictions on the original dataset values](img/C12622_03_73.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.73: Predictions on the original dataset values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From this exercise using an autoregressor, we can see that there is significant
    predictive power in using these models when there is missing data from the set
    or when we are attempting to predict between measurement intervals. The autoregressor
    model shown for the S&P 500 dataset was able to effectively provide predictions
    within the range of observed samples. However, outside of this range, when predicting
    future values for which no measurements have been taken, the predictive power
    may be somewhat limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Autoregressors'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will now use autoregressors to model the Austin weather
    dataset and predict future values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we will need to import a few libraries and load data from
    a previous activity, which can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps to be performed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Plot the complete set of average temperature values (`df.TempAvgF`) with years
    on the *x* axis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a 20-day lag and plot the lagged data on the original dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct an autocorrelation plot to see whether the average temperature can
    be used with an autoregressor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose an acceptable lag and an unacceptable lag and construct lag plots using
    these values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an autoregressor model, note the selected lag, calculate the r2 value,
    and plot the autoregressor model with the original plot. The model is to project
    past the available data by 1,000 samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a set of predictions for 1,000 days after the last sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the predictions, as well as the original dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enhance the view to look for differences by showing the 100th to 200th sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 344.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we took our first big leap into constructing machine learning
    models and making predictions with labeled datasets. We began our analysis by
    looking at a variety of different ways to construct linear models, starting with
    the precise least squares method, which is very good when modeling small amounts
    of data that can be processed using the available computer memory. The performance
    of our vanilla linear model was improved using dummy variables, which we created
    from categorical variables, adding additional features and context to the model.
    We then used linear regression analysis with a parabolic model to further improve
    performance, fitting a more natural curve to the dataset. We also implemented
    the gradient descent algorithm, which we noticed, while not as precise as the
    least squares method was for our limited dataset, was most powerful when the dataset
    cannot be processed on the resources available on the system.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we investigated the use of autoregression models, which predict future
    values based on the experience of previous data in the set. Using autoregressors,
    we were able to accurately model the closing price of the S&P 500 over the years
    1986 – 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have experience with supervised regression problems, we will turn
    our attention to classification problems in the next chapter.
  prefs: []
  type: TYPE_NORMAL

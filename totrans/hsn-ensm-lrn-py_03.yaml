- en: Getting Started with Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning involves a combination of techniques that allows multiple
    machine learning models, called base learners (or, sometimes, weak learners),
    to consolidate their predictions and output a single, optimal prediction, given
    their respective inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will give an overview of the main problems that ensembles
    try to solve, namely, bias and variance, as well as the relationship between them.
    This will help us understand the motivation behind identifying the root cause
    of an under-performing model and using an ensemble to address it. Furthermore,
    we will go over the basic categories of the methodologies available, as well as
    the difficulties we can expect to encounter when implementing ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics covered in this chapter are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Bias, variance, and the trade-off between the two
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The motivation behind using ensemble learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the root cause of an under-performing model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficulties in applying ensemble learning successfully
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02)'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2JKkWYS](http://bit.ly/2JKkWYS).
  prefs: []
  type: TYPE_NORMAL
- en: Bias, variance, and the trade-off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning models are not perfect; they are prone to a number of errors.
    The two most common sources of errors are bias and variance. Although two distinct
    problems, they are interconnected and relate to a model's available degree of
    freedom or complexity.
  prefs: []
  type: TYPE_NORMAL
- en: What is bias?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias refers to the inability of a method to correctly estimate the target. This
    does not only apply to machine learning. For example, in statistics, if we want
    to measure a population's average and do not sample carefully, the estimated average
    will be biased. In simple terms, the method's (sampling) estimation will not closely
    match the actual target (average).
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, bias refers to the difference between the expected prediction
    and its target. Biased models cannot properly fit the training data, resulting
    in poor in-sample performance and out-of-sample performance. A good example of
    a biased model arises when we try to fit a sine function with a simple linear
    regression. The model cannot fit the sine function, as it lacks the required complexity
    to do so. Thus, it will not be able to perform well in-sample or out-of-sample.
    This problem is called underfitting. A graphical example is illustrated in the
    following figure :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a3d6234-f46c-4cad-8796-7ab8628778b7.png)'
  prefs: []
  type: TYPE_IMG
- en: A biased linear regression model for sine function data
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formula for bias is the difference between the target value
    and the expected prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65ccbf7c-cef5-44d5-9493-84f1cfd3b861.png)'
  prefs: []
  type: TYPE_IMG
- en: What is variance?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Variance refers to how much individuals vary within a group. Again, variance
    is a concept from statistics. Taking a sample from a population, variance indicates
    how much each individual's value differs from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, variance refers to the model's variability or sensitivity
    to data changes. This means that high-variance models can generally fit the training
    data well and so achieve high in-sample performance, but perform poorly out-of-sample.
    This is due to the model's complexity. For example, a decision tree can have high
    variance if it creates a rule for every single instance in the training dataset.
    This is called **overfitting**. The following figure depicts a decision tree trained
    on the preceding dataset. Blue dots represent the training data and orange dots
    represent the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As is evident, the model fits the training data perfectly but does not perform
    on the test data so well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30604a53-1e14-4353-ad9b-39214f6d22b4.png)'
  prefs: []
  type: TYPE_IMG
- en: A high-variance decision tree model on the sine function
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formula for variance is depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/257b5bbd-0482-441b-a392-ba62d837f00c.png)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, this is the standard formula for population variance, assuming
    that our population is comprised of our models, as they have been produced by
    the machine learning algorithm. For example, as we saw earlier in [Chapter 1](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml), *A
    Machine Learning Refresher*, neural networks can have different training outcomes,
    depending on their initial weights. If we consider all the neural networks with
    the same architecture, but different initial weights, by training them, we will
    have a population of different models.
  prefs: []
  type: TYPE_NORMAL
- en: Trade-off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bias and variance are two of the three major components that comprise a model''s
    error. The third is called the irreducible error and can be attributed to inherent
    randomness or variability in the data. The total error of a model can be decomposed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78620b7b-b43f-4b03-a677-6c991c9c441f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we saw earlier, bias and variance stem from the same source: model complexity.
    While bias arises from too little complexity and freedom, variance thrives in
    complex models. Thus, it is not possible to reduce bias without increasing variance
    and vice versa. Nevertheless, there is an optimal point of complexity, where the
    error is minimized as bias and variance are at an optimal trade-off point. When
    the model''s complexity is at this optimal point (the red dotted line in the next
    figure), then the model performs best both in-sample and out-of-sample. As is
    evident in the next figure, the error can never be reduced to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, although some may think that it is better to reduce the bias,
    even at the cost of increased variance, it is clear that the model would not perform
    better, even if it was unbiased, due to the error that variance inevitably induces:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b59162a8-d7de-44fe-8b33-b599478bb9d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Bias-variance trade-off and its effect on the error
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the perfect model, with a minimum amount of combined
    bias and variance, or reducible error. Although the model does not fit the data
    perfectly, this is due to noise that is inherent in the dataset. If we try to
    fit the training data better, we will induce overfitting (variance). If we try
    to simplify the model further, we will induce underfitting (bias):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30ecba78-5dd9-4246-83f7-365dc7db0a82.png)'
  prefs: []
  type: TYPE_IMG
- en: Perfect model for our data, a sine function
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning involves a collection of machine learning methods aimed at
    improving the predictive performance of algorithms by combining many models. We
    will analyze the motivation behind using such methods to solve problems that arise
    from high bias and variance. Furthermore, we will present methods that allow the
    identification of bias and variance in machine learning models, as well as basic
    classes of ensemble learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning aims to solve the problems of bias and variance. By combining
    many models, we can reduce the ensemble's error, while retaining the individual
    models' complexities. As we saw earlier, there is a certain lower limit imposed
    on each model error, which is related to the model complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we mentioned that the same algorithm can produce different models,
    due to the initial conditions, hyperparameters, and other factors. By combining
    different, diverse models, we can reduce the expected error of the group, while
    each individual model remains unchanged. This is due to statistics, rather than
    pure learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to better demonstrate this, let''s consider an ensemble of 11 base
    learners for a classification, each with a probability of misclassification (error)
    equal to *err*=0.15 or 15%. Now, we want to create a simple ensemble. We always
    assume that the output of most base learners is the correct answer. Assuming that
    they are diverse (in statistics, uncorrelated), the probability that the majority
    of them is wrong is 0.26%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83545e56-347d-450e-82b7-46f736d3c1f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As is evident, the more base learners we add to the ensemble, the more accurate
    the ensemble will be, under the condition that each learner is uncorrelated to
    the others. Of course, this is increasingly difficult to achieve. Furthermore,
    the law of diminishing returns applies. Each new uncorrelated base learner contributes
    less to the overall error reduction than the previously added base learner. The
    following figure shows the ensemble error percentage for a number of uncorrelated
    base learners. As is evident, the greatest reduction is applied when we add two
    uncorrelated base learners:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4da2f13b-6bd6-4d1d-915f-6a2600b83d12.png)'
  prefs: []
  type: TYPE_IMG
- en: The relation between the number of base learners and the ensemble error
  prefs: []
  type: TYPE_NORMAL
- en: Identifying bias and variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although bias and variance have theoretical formulas, it is difficult to calculate
    their actual values. A simple way to estimate them empirically is with learning
    and validation curves.
  prefs: []
  type: TYPE_NORMAL
- en: Validation curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Validation curves refer to an algorithm's achieved performance, given different
    hyperparameters. For each hyperparameter value, we perform k-fold cross validations
    and store the in-sample performance and out-of-sample performance. We then calculate
    and plot the mean and standard deviation of in-sample and out-of-sample performance
    for each hyperparameter value. By examining the relative and absolute performance,
    we can gauge the level of bias and variance in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Borrowing the `KNeighborsClassifier` example from [Chapter 1](https://cdp.packtpub.com/hands_on_ensemble_learning_with_python/wp-admin/post.php?post=25&action=edit#post_24), *A
    Machine Learning Refresher*, we modify it in order to experiment with different
    neighbor numbers. We start by loading the required libraries and data. Notice
    that we import `validation_curve` from `sklearn.model_selection`. This is scikit-learn''s
    own implementation of validation curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define our features and targets (`x` and `y`), as well as our base
    learner. Furthermore, we define our parameter search space with `param_range =
    [2,3,4,5]` and use `validation_curve`. In order to use it, we must define our
    base learner, our features, targets, the parameter''s name that we wish to test,
    as well as the parameter''s values to test. Furthermore, we define the cross-validation''s
    K folds with `cv=10`, as well as the metric that we wish to calculate, with `scoring="accuracy"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward,we calculate the mean and standard deviation for both in-sample performance
    (`train_scores`) as well as out-of-sample performance (`test_scores`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the means and deviations. We plot the means as curves, using
    `plt.plot`. In order to plot the standard deviations, we create a transparent
    rectangle surrounding the curves, with a width equal to the standard deviation
    at each hyperparameter value point. This is achieved with the use of `plt.fill_between`,
    by passing the value points as the first parameter, the lowest rectangle''s point
    as the second parameter, and the highest point as the third. Furthermore, `alpha=0.1`
    instructs `matplotlib` to make the rectangle transparent (combining the rectangle''s
    color with the background in a 10%-90% ratio, respectively):'
  prefs: []
  type: TYPE_NORMAL
- en: Sections 3 and 4 are adapted from the scikit-learn examples found [https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The script finally outputs the following. As the curves close the distance between
    them, the variance generally reduces. The further away they both are from the
    desired accuracy (taking into account the irreducible error), the bias increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the relative standard deviations are also an indicator of variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2029909d-0236-464d-b286-61b25feaed3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Validation curves for K-Nearest-Neighbors, 2 to 5 neighbor
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table presents the bias and variance identification based on
    validation curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Great** | **Small** |'
  prefs: []
  type: TYPE_TB
- en: '| **Distance between curves** | High Variance | Low Variance |'
  prefs: []
  type: TYPE_TB
- en: '| **Distance from desired accuracy** | High Bias | Low Bias |'
  prefs: []
  type: TYPE_TB
- en: '| **Relative rectangle area ratio** | High Variance | Low Variance |'
  prefs: []
  type: TYPE_TB
- en: Bias and variance identification based on validation curves
  prefs: []
  type: TYPE_NORMAL
- en: Learning curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to identify bias and variance is to generate learning curves. Like
    validation curves, we generate a number of in-sample and out-of-sample performance
    statistics with cross-validation. Instead of experimenting with different hyperparameter
    values, we utilize different amounts of training data. Again, by examining the
    means and standard deviations of in-sample and out-of-sample performance, we can
    get an idea about the amount of bias and variance inherent in our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn implements learning curves in the `sklearn.model_selection` module
    as `learning_curve`. Once again, we will use the `KNeighborsClassifier` example
    from [Chapter 1](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml), *A Machine Learning
    Refresher*. First, we import the required libraries and load the breast cancer
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Following that, we define the amount of training instances that will be used
    at each cross-validation set with `train_sizes = [50, 100, 150, 200, 250, 300]`,
    instantiate the base learner, and call `learning_curve`. The function returns
    a tuple of the train set sizes, the in-sample performance scores, and out-of-sample
    performance scores. The function accepts the base learner, the dataset features
    and targets, and the train set sizes as parameters in a list with `train_sizes=train_sizes`
    and the number of cross-validation folds with `cv=10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we calculate the mean and standard deviation of in-sample and out-of-sample
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the means and standard deviations as curves and rectangles,
    as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The final output is depicted as follows. The model seems to reduce its variance
    for the first 200 training samples. After that, it seems that the means diverge,
    as well as the standard deviation of the cross-validation score increasing, thus
    indicating an increase in variance.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, although both curves have above 90% accuracy for training sets with
    at least 150 instances, this does not imply low bias. Datasets that are highly
    separable (good quality data with low noise) tend to produce such curves—no matter
    what combination of algorithms and hyperparameters we choose. Moreover, noisy
    datasets (for example, instances with the same features that have different targets)
    will not be able to produce high accuracy models—no matter what techniques we
    use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, bias must be measured by comparing the learning and validation curves
    to a desired accuracy (one that is considered achievable, given the dataset quality),
    rather than its absolute value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1e5331a-a5fe-4ce5-bc55-42fedbddc08e.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning curves for K-Nearest-Neighbors, 50 to 300 training instances
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ensemble methods are divided into two major classes or taxonomies: generative
    and non-generative methods. Non-generative methods are focused on combining the
    predictions of a set of pretrained models. These models are usually trained independently
    of one another, and the ensemble algorithm dictates how their predictions will
    be combined. Base classifiers are not affected by the fact that they exist in
    an ensemble.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we will cover two main non-generative methods: voting and stacking.
    Voting, as the name implies(see [Chapter 3](ad9aa66b-7b30-4779-8914-0ff58140b3e8.xhtml), *Voting*),
    refers to techniques that allow models to vote in order to produce a single answer,
    similar to how individuals vote in national elections. The most popular (most
    voted for) answer is selected as the winner. [Chapter 4](49a05219-d6cb-4893-aaac-49280842b647.xhtml), *Stacking*,
    on the other hand, refers to methods that utilize a model (the meta-learner) that
    learns how to best combine the base learner''s predictions. Although stacking
    entails the generation of a new model, it does not affect the base learners, thus
    it is a non-generative method.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative methods, on the other hand, are able to generate and affect the base
    learners that they use. They can either tune their learning algorithm or the dataset
    used to train them, in order to ensure diversity and high model performance. Furthermore,
    some algorithms can induce randomness in models, in order to further enforce diversity.
  prefs: []
  type: TYPE_NORMAL
- en: The main generative methods that we will cover in this book are bagging, boosting,
    and random forests. Boosting is a technique mainly targeting biased models. Its
    main idea is to sequentially generate models, such that each new model addresses
    biases inherent in the previous models. Thus, by iteratively correcting previous
    errors, the final ensemble has a significantly lower bias. Bagging aims to reduce
    variance. The bagging algorithm resamples instances of the training dataset, creating
    many individual and diverse datasets, originating from the same dataset. Afterward,
    a separate model is trained on each sampled dataset, forcing diversity between
    the ensemble models. Finally, Random Forests, is similar to bagging, in that it
    resamples from the training dataset. Instead of sampling instances, it samples
    features, thus creating even more diverse trees, as features strongly correlated
    to the target may be absent in many trees.
  prefs: []
  type: TYPE_NORMAL
- en: Difficulties in ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although ensemble learning can greatly increase the performance of machine learning
    models, it comes at a cost. There are difficulties and drawbacks in correctly
    implementing it. Some of these difficulties and drawbacks will now be discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Weak or noisy data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important ingredient of a successful model is the dataset. If the data
    contains noise or incomplete information, there is not a single machine learning
    technique that will generate a highly performant model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's illustrate this with a simple example. Suppose we study populations (in
    the statistical sense) of cars and we gather data about the color, shape, and
    manufacturer. It is difficult to generate a very accurate model for either variable,
    as a lot of cars are the same color and shape but are made by a different manufacturer.
    The following table depicts this sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best any model can do is achieve 33% classification accuracy, as there
    are three viable choices for any given feature combination. Adding more features
    to the dataset can greatly improve the model''s performance. Adding more models
    to an ensemble cannot improve performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Color** | **Shape** | **Manufacturer** |'
  prefs: []
  type: TYPE_TB
- en: '| Black | Sedan | BMW |'
  prefs: []
  type: TYPE_TB
- en: '| Black | Sedan | Audi |'
  prefs: []
  type: TYPE_TB
- en: '| Black | Sedan | Alfa Romeo |'
  prefs: []
  type: TYPE_TB
- en: '| Blue | Hatchback | Ford |'
  prefs: []
  type: TYPE_TB
- en: '| Blue | Hatchback | Opel |'
  prefs: []
  type: TYPE_TB
- en: '| Blue | Hatchback | Fiat |'
  prefs: []
  type: TYPE_TB
- en: Car dataset
  prefs: []
  type: TYPE_NORMAL
- en: Understanding interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By employing a large number of models, interpretability is greatly reduced.
    For example, a single decision tree can easily explain how it produced a prediction,
    by simply following the decisions made at each node. On the other hand, it is
    difficult to interpret why an ensemble of 1,000 trees predicted a single value.
    Moreover, depending on the ensemble method, there may be more to explain than
    the prediction process itself. How and why did the ensemble choose to train these
    specific models. Why did it not choose to train other models? Why did it not choose
    to train more models?
  prefs: []
  type: TYPE_NORMAL
- en: When the model's results are to be presented to an audience, especially a not-so-highly-technical
    audience, simpler but more easily explainable models may be a better solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, when the prediction must also include a probability (or confidence
    level), some ensemble methods (such as boosting) tend to deliver poor probability
    estimates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccb77e87-ebd1-47a0-b835-3e054f2bb8f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Interpretability of a single tree versus a 1000
  prefs: []
  type: TYPE_NORMAL
- en: Computational cost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another drawback of ensembles is the computational cost they impose. Training
    a single neural network is computationally expensive. Training a 1000 of them
    requires a 1000 times more computational resources. Furthermore, some methods
    are sequential by nature. This means that it is not possible to harness the power
    of distributed computing. Instead, each new model must be trained when the previous
    model is completed. This imposes time penalties on the model's development process,
    on top of the increased computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: Computational costs do not only hinder the development process; when the ensemble
    is put into production, the inference time will suffer as well. If the ensemble
    consists of 1,000 models, then all of those models must be fed with new data,
    produce predictions, and then those predictions must be combined in order to produce
    the ensemble output. In latency-sensitive settings (financial exchanges, real-time
    systems, and so on), sub-millisecond execution times are expected, thus a few
    microseconds of added latency can make a huge difference.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the models that comprise the ensemble must possess certain characteristics.
    There is no point in creating any ensemble from a number of identical models.
    Generative methods may produce their own models, but the algorithm used as well
    as its initial hyperparameters are usually selected by the analyst. Furthermore,
    the model's achievable diversity depends on a number of factors, such as the size
    and quality of the dataset, and the learning algorithm itself.
  prefs: []
  type: TYPE_NORMAL
- en: A single model that is similar in behavior to the data-generating process will
    usually outperform any ensemble, both in terms of accuracy as well as latency.
    In our bias-variance example, the simple sine function will always outperform
    any ensemble, as the data is generated from the same function with some added
    noise. An ensemble of many linear regressions may be able to approximate the sine
    function, but it will always require more time to train and execute. Furthermore,
    it will not be able to generalize (predict out-of-sample) as well as the sine
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the concepts of bias and variance, as well as
    the trade-off between them. They are essential in understanding how and why a
    model may under-perform, either in-sample or out-of-sample. We then introduced
    the concept and motivation of ensemble learning, how to identify bias and variance
    in models, as well as basic categories of ensemble learning methods. We presented
    ways to measure and plot bias and variance, using scikit-learn and matplotlib.
    Finally, we talked about the difficulties and drawbacks of implementing ensemble
    learning methods. Some key points to remember are the following.
  prefs: []
  type: TYPE_NORMAL
- en: High-bias models usually have difficulty performing well in-sample. This is
    also called **underfitting**. It is due to the model's simplicity (or lack of
    complexity). High-variance models usually have difficulty generalizing or performing
    well out-of-sample, while they perform reasonably well in-sample. This is called
    **overfitting**. It is usually due to the model's unnecessary complexity. The
    **b****ias-variance trade-off** refers to the fact that as the model's complexity
    increases, its bias decreases, while its variance increases. Ensemble learning
    aims to address high bias or variance, by combining the predictions of many diverse
    models. These models are usually called **base-learners**. For model selection,
    **v****alidation curves** indicate how a model performs in-sample and out-of-sample
    for a given set of hyperparameters. **Learning curves** are the same as validation
    curves but instead of a set of hyperparameters, they use different train set sizes. Substantial
    distance between the train and test curves indicates high variance. A big rectangle
    area around the test curve also indicates high variance. A substantial distance
    between both curves from the target accuracy indicates high bias. Generative methods
    have control over the generation and training of their base learners; non-generative
    methods do not. Ensemble learning can have a negligible or negative impact on
    performance when data is poor or models are correlated. It can impact negatively
    on the interpretability of models and the computational resources required.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will present the Voting ensemble, as well as how to
    use it for both regression and classification problems.
  prefs: []
  type: TYPE_NORMAL

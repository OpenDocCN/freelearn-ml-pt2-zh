- en: Working with Non-Parametric Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we introduced parametric models and explored how to implement
    linear and logistic regression. In this chapter, we will cover the non-parametric
    model family. We will start by covering the bias-variance trade-off, and explaining
    how parametric and non-parametric models differ at a fundamental level. Later,
    we'll get into decision trees and clustering methods. Finally, we'll address some
    of the pros and cons of the non-parametric models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The bias/variance trade-off
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to non-parametric models and decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a decision tree from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various clustering methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing **K-Nearest Neighbors** (**KNNs**) from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-parametric models – the pros and cons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need to install the following software, if you haven''t
    already done so:'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files for this chapter can be found at [https:/​/​github.​com/​PacktPublishing/
  prefs: []
  type: TYPE_NORMAL
- en: Supervised-Machine-Learning-with-Python](https:/%E2%80%8B/%E2%80%8Bgithub.%E2%80%8Bcom/%E2%80%8BPacktPublishing/%20Supervised-Machine-Learning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: The bias/variance trade-off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to continue our discussion of error due to **bias**,
    and introduce a new source of error called **variance**. We will begin by clarifying
    what we mean by error terms and then dissect various sources of modeling errors.
  prefs: []
  type: TYPE_NORMAL
- en: Error terms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the central topics of model building is reducing error. However, there
    are several types of errors, two of which we have control over to some extent.
    These are called **bias** and **variance**. There is a trade-off in the ability
    for a model to minimize either bias or variance, and this is called the **bias-variance
    trade-off** or the **bias-variance dilemma**.
  prefs: []
  type: TYPE_NORMAL
- en: Some models do well at controlling both to an extent. However, this is a dilemma
    that, for the most part, is always going to be present in your modeling considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Error due to bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'High bias can also be called underfitting or over-generalization. High bias
    generally leads to an inflexible model that misses the true relationship between
    features in the target function that we are modeling. In the following diagram,
    the true relationship between *x* and *y* is oversimplified and the true function
    of *f(x)*, which is essentially a logic function, is missed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52bed5c5-28a1-4bc7-8087-1ed23193d643.png)'
  prefs: []
  type: TYPE_IMG
- en: Parametric models tend to suffer high bias problems more than non-parametric
    models. Examples of this include linear and logistic regression, which we will
    explore in more detail in the final section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Error due to variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In contrast, for the high bias that you''re now familiar with, error due to
    variance can be thought of as the variability of a model''s prediction for a given
    sample. Imagine you repeat the modeling process many times; the variance is how
    much the predictions for a given sample will vary across different inductions
    of the model. High variance models are commonly referred to as overfitting, and
    suffer the exact inverse of high bias. That is, they do not generalize enough.
    High variance usually comes from a model''s insensitivity to the signal as a result
    of its hypersensitivity to noise. Generally, as model complexity increases, variance
    becomes our primary concern. Notice in the diagram that a polynomial term has
    led to a very overfitting model, where a simple **logit** function would have
    sufficed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caf73d07-a9f2-4b3d-9759-7957874afafd.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike high bias problems, high variance problems can be addressed with more
    training data, which can help the model learn to generalize a bit better. So,
    examples of high variance models, which we haven't yet covered, are decision trees
    and KNN. We're going to cover both of these in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Learning curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will examine a handy way to diagnose high bias or variance
    called **learning curves**. In this example Python snippet, we will leverage the
    function in the `packtml.utils` submodule called `plot_learning_curve`, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is going to take an estimator and fit it on various sizes of
    training data defined in the `train_sizes` parameter. What is displayed is the
    model performance on the train and the corresponding validation set for each incremental
    model fit. So, this example uses our linear regression class to model the Boston
    housing data, which is a regression problem and displays symptoms of high bias.
    Notice that our error is very similar for the training and validation sets. It
    got there very rapidly, but it''s still relatively high. They don''t improve as
    our training set grows at all. We get the output for the preceding code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bb09654-b826-464c-981a-9d322d1a7c55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, if we model the same data with a decision tree regressor, we
    notice the symptoms of high variance or overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a huge discrepancy between the **Training score** and **Validation
    score**, and even though it gets better with more data, it never quite reaches
    convergence. We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0e0dc71-135b-4802-8a4a-996d3e6db219.png)'
  prefs: []
  type: TYPE_IMG
- en: Strategies for handling high bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you determine that you''re suffering from a high bias problem, you can try
    making your model more complex by engineering more informative signal-rich features.
    For example, here, one thing you could try doing is creating new features that
    are polynomial combinations of your *x1* so, you can create logit function of
    *x1*, and that would model our function perfectly. You can also try tuning some
    of the hyperparameters, for instance, KNNs, even though it''s a high variance
    model, and it can become highly biased very quickly as you increase the *k* hyperparameter,
    and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d8f5203-9b9c-4b73-8e96-2f0c952b665a.png)'
  prefs: []
  type: TYPE_IMG
- en: Strategies for handling high variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you, instead, find yourself facing a high variance problem, we''ve already
    seen how more training data can help, to an extent. You can also perform some
    feature selection to pare down the model''s complexity. The most robust solution
    lies in bagging or ensembling, which combines the output to mini models, which
    all, in turn, vote on each sample''s label or output regression score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/477ce929-ccc1-4fd0-8f69-882886f9d56f.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we're going to more formally define non-parametric learning
    algorithms and introduce decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to non-parametric models and decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to formally define what non-parametric learning
    algorithms are, and introduce some of the concepts and math behind our first algorithm,
    called **decision trees**.
  prefs: []
  type: TYPE_NORMAL
- en: Non-parametric learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-parametric models do not learn parameters. They do learn characteristics
    or attributes about the data, but not parameters in the formal sense. We will
    not end up extracting a vector of coefficients. The easiest example is a decision
    tree. A decision tree is going to learn where to recursively split data so that
    its leaves are as pure as possible. So, in that sense, the decision function is
    a splitting point for each leaf that is not a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Characteristics of non-parametric learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-parametric models tend to be a bit more flexible and do not make as many
    assumptions about the underlying structure of the data. Many linear models, or
    parametric models, for instance, assume that a normal distribution for each feature
    is required to be independent of one another. This is not the case with most non-parametric
    models. As we covered in the last section, the bias-variance trade-off also knows
    that non-parametric models will require more data to train, so as not to be as
    afflicted by high variance problems.
  prefs: []
  type: TYPE_NORMAL
- en: Is a model parametric or not?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you find yourself wondering whether or not a model is parametric, it's probably
    not the most important question to answer. You should select the modeling technique
    that best suits your data. However, a good rule of thumb is how many characteristics
    or parameters a model learns. If it's related to the feature space or dimensionality,
    it's probably parametric, for instance, learning the number of coefficients theta
    in a linear regression. If, instead, it's related to the number of samples, it's
    probably non-parametric, for instance, the depth of the decision tree or the number
    of neighbors in clustering.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive example – decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A decision tree will start out with all of the data, iteratively making splits
    until each leaf has maximized its purity or some other stopping criteria is met.
    In this example, we will start out with three samples. The tree learns that splitting
    on the color feature will be our most informative step towards maximizing its
    leaf purity. So, that''s the first thing to note. The first split is the most
    informative split that will best segment the data into two pieces. As shown in
    the following diagram, the potato class is isolated on the left by splitting on
    color. We have perfectly classified the potato. However, the other two samples
    still need to be split. So, the tree learns that, if it''s orange and round, it''s
    a sweet potato. Otherwise, if it''s just orange and not round, it''s a carrot,
    and it goes left one more time. Here, we can see a perfect split of all of our
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f35a17a3-c73c-4c27-8952-478a124f2999.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision trees – an introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What we''re interested in doing with decision trees is defining a flexible
    extensible algorithm that can achieve the decision tree. This is where the **Classification
    and Regression Trees** (**CART**) algorithm comes in. CART is generalizable to
    either task and it learns, essentially, by asking questions of the data. At each
    split point, CART will scan the entire feature space, sampling values from each
    feature to identify the best feature and value for the split. It does this by
    evaluating the information gain formula, which seeks to maximize a gain in purity
    in the split, which is pretty intuitive. *Gini Impurity* is computed at the leaf
    level, and is a way of measuring how pure or impure a leaf is; its formula is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6268b54-06ba-49be-8da4-7dc894aee979.png)'
  prefs: []
  type: TYPE_IMG
- en: '*IG* at the bottom is our information gain, and it''s the gini of the root
    node, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fce97018-e381-4527-9384-4960853433e3.png)'
  prefs: []
  type: TYPE_IMG
- en: How do decision trees make decisions?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first address the objective before looking at the math. We will compute
    the information gain of a split to determine the best splitting point. If information
    gain is positive, that means we have learned something from that split, which
    might be the optimal point. If information gain is negative, it means we're actually
    going in the wrong direction. What we have done is created a non-informative split.
    Each split in the tree will select the point that maximizes information gain.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here''s the setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2152363-2f8b-4a2f-8327-3bf389e2ece8.png)'
  prefs: []
  type: TYPE_IMG
- en: A Gini impurity of 0 would be particularly pure. A higher impurity essentially
    means that a more random collection of classes has found itself in that leaf.
    So, our root is fairly impure. Now our tree will scan the entire feature space,
    sampling values from each feature. It will evaluate the information gained on
    if we were to split there. So, let's say that our tree selects *x12*. We will
    split along the same value that's sampled that variable. What we want to know
    is, if we end up getting more pure leaf nodes from this split, we will compute
    the information gain. To do that, we have to compute the Gini for each of the
    leaf nodes that we just created.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at an example of this problem using the `packtml` library. We
    have the `example_information_gain.py` file, which is in the `examples/decision_tree`
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will compute the information gain using the `InformationGain` class
    from `packtml.decision_tree.metrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output when we run `example_information_gain.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/064c2ef2-3f84-4a24-8500-8b100b2fd139.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we're going to go a bit deeper and learn how a decision
    tree produces the candidate split for us to evaluate.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we computed the information gained for a given split.
    Recall that it's computed or calculated by computing the Gini impurity for the
    parent node in each `LeafNode`. A higher information again is better, which means
    we have successfully reduced the impurities of the child nodes with our split.
    However, we need to know how a candidate split is produced to be evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each split, beginning with the root, the algorithm will scan all the features
    in the data, selecting a random number of values for each. There are various strategies
    to select these values. For the general use case, we will describe and select
    a *k* random approach:'
  prefs: []
  type: TYPE_NORMAL
- en: For each of the sample values in each feature, we simulate a candidate split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values above the sampled value go to one direction, say left, and values above
    that go the other direction, that is, to the right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, for each candidate split, we're going to compute the information gain,
    and select the feature value combination that produces the highest information
    gain, which is the best split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the best split, we will recurse down each split as a new parent until the
    stopping criteria are met
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, regarding where and when to stop the criteria, there are various methods
    we can use for this. A common one is maximum tree depth. If we get too deep, we
    start to overfit. So, we might prune our tree when it grows five times deep, for
    instance. Another, is a minimum number of samples per leaf. If we have 1 million
    training samples, we grow our tree until there's one sample per leaf; we're also
    probably overfitting. So, the min samples leaf parameter will allow us to stop
    splitting a leaf once there are, say, 50 samples remaining after a split. This
    is a tunable hyperparameter that you can work within your cross-validation procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting a tree by hand
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now look into an exercise. Let''s imagine we have this training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbadade4-c934-4312-b1d4-61859101f0cb.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding data, where is the optimal split point? What feature or value
    combination should we use to define our rule?
  prefs: []
  type: TYPE_NORMAL
- en: If we split on x1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will compute the Gini impurity of the root node, which is the pre-split
    state. We get *0.444*, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ace640a-6b82-49f9-938a-94cd878bf06f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next stage in the algorithm is to iterate each feature. There are three
    cases, shown as follows. Using our *IG* formula, we can compute which is the best
    split point for this feature. The first happens to be the best, in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbf032eb-f9f3-498e-9bbe-a0a42ca5c3ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Splitting on the second case, where *x1* is greater than or equal to *4*, is
    not a good idea since the result is no different than the state at the root. Therefore,
    our information gain is *0*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72157416-b405-492b-a5ae-d6668cac95a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the last case, splitting when *x1* is greater than or equal to *37* does
    yield a positive IG since we have successfully split one sample of the positive
    class away from the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cadc0ea0-fc63-4d29-9e00-94f92d172e50.png)'
  prefs: []
  type: TYPE_IMG
- en: If we split on x2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, we don''t know if we''re done yet. So, we will iterate to *x2*, where
    there might be a better split point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f99b20b-6803-4822-aee8-4094b8f563c3.png)'
  prefs: []
  type: TYPE_IMG
- en: The candidate split shows us that neither potential split is the optimal split
    when compared to the current best that we've identified in *x1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the best split is *x1* greater than or equal to *21*, which will
    perfectly separate our class labels. You can see in this decision tree when we
    produce that split, sure enough, we get perfectly separated classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2aee3630-4c83-49b8-9d60-fc674c593e8b.png)'
  prefs: []
  type: TYPE_IMG
- en: However, in a larger example, we may not have perfectly separated our classes,
    if we had millions of samples, for instance. Hence, we would recurse at this point,
    finding new split points for each node until we hit our stopping criteria. At
    this point, let's use our `packtml` library to run this exact example and show
    that we do in fact identify the same optimal split point, and prove that is not
    just trickery of the hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyCharm, the `example_classification_split.py` file is open. This is located
    inside your `examples` directory and within the `decision_tree` examples directory.
    You can see we''re going to import two things from `packtml`. Both of them happen
    to be inside the `decision_tree` submodule where you got `RandomSplitter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We already looked at `InformationGain` a little bit in the last section to
    compute our information gain candidate split. Here, we will look at how we actually
    create the candidate split. We get the following data along with the corresponding
    class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`RandomSplitter` will evaluate each of the preceding values since `n_val_sample`
    is `3`. So, it''s going to compute three candidates split points for each feature,
    and we will find out which of them are the best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the preceding code, we see `best_feature` is `0` and `best_value`
    is `21`, meaning that anything greater than or equal to `21` in feature `0` will
    go left, and everything else goes right. The `InformationGain` we get is `0.444`,
    which, sure enough, when we computed it by hand, is exactly what we expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b3a8a2e-2d07-4764-9e59-4ebfdc49d163.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we'll cover how we can implement a decision tree from scratch
    inside the `packtml` library.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a decision tree from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start out by looking at the implementation of our splitting metrics.
    Then we'll cover some of our splitting logic, and finally, we'll see how we can
    wrap the tree so that we can generalize from classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Classification tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go ahead and walk through a classification tree example. We will be using
    the information gain criteria. In PyCharm there are three scripts open, two of
    which are `metrics.py` and `cart.py`, both of which are found inside of the `packtml/decision_tree`
    submodule. Then we have the `example_classification_decision_tree.py` file, which
    is in `examples/decision_tree`. Let's start with metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you open up the `cart.py` file, we have an order in which we should step
    through this so that you can understand how the decision tree class is going to
    work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting with the `metrics.py` file from the top, you can see that `_all_`
    is going to include four different metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`entropy` and `gini_impurity` are both classification metrics. We have talked
    about `gini_impurity`. You can see here that both of them are calling the `clf_metric`
    private function as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, `gini` and `entropy` acts essentially the same way, except that at the
    end, `gini` computes a norm essentially on itself where `entropy` is `log2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: One thing to note here is that entropy and Gini are going to make a huge difference
    in how your tree performs. Gini is actually canon for the CART algorithm, but
    we included entropy here so you could see that this is something you can use if
    you want to.
  prefs: []
  type: TYPE_NORMAL
- en: '`BaseCriterion` is our base class for a splitting criterion. We have two splitting
    criteria, `InformationGain` and `VarianceReduction`. Both of them are going to
    implement `compute_uncertainty`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you remember from the last section, uncertainty is essentially the level
    of impurity, or entropy, induced by the split. When we compute `InformationGain`
    using either `gini` or `entropy`, our uncertainty is going to be `metric` pre-split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we compute `uncertainty`, we would pass in a node and say compute Gini,
    for instance, on all of the samples inside of the node before we split, and then,
    when we call to actually compute `InformationGain`, we pass in `mask` for whether
    something is going `left` or `right`. We will compute the Gini on the left and
    right side, and return `InformationGain`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how we compute `InformationGain`, and this is just the wrapper class
    that we have built. `VarianceReduction` is very similar, except the `compute_uncertainty`
    function is simply going to return the variance of *y*. When we call this, we
    are subtracting the uncertainty of the pre-split node, minus the sum of the uncertainties
    for the left and right on the split. What we''re doing here is maximizing the
    reduction of the variance between each split respectively. That way, we can know
    if a split is good. It separates along a relatively intuitive line, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'These are our two splitting criteria: `InformationGain` and `VarianceReduction`.
    We''re going to use `InformationGain` for classification and `VarianceReduction`
    for regression. Since we''re talking about classification right now, let''s focus
    on `InformationGain`. Moving over to the `cart.py` file, we see that the next
    thing we want to talk about is `RandomSplitter`.'
  prefs: []
  type: TYPE_NORMAL
- en: In one of the last sections, we learned about a strategy to produce candidate
    splits. This is essentially `RandomSplitter`. There are a lot of different strategies
    you can use here. We're going to use a bit of entropy so that we can get through
    this class and this algorithm relatively quickly, without getting into the nitty-gritty.
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomSplitter` will take several arguments. We want `random_state` so that
    we can replicate this split later. The criterion is an instance of either `InformationGain`
    or `VarianceReduction` and the number of values that we want to sample from each
    feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our `find_best` function will scan the entire feature space, sample the
    number of values per split or per feature, and determine `best_value` and `best_feature`
    on which to split. This will produce our best split for the tree at the time.
    So, `best_gain` will start as `0`. If it''s negative it''s a bad one, so we don''t
    want to split at all. If it''s positive then it''s better than our current best,
    and so we''ll take that and increment it to find our best. We want to find our
    `best_feature` and our `best_value`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, for each of the columns of our dataset, we''re going to go ahead and grab
    out the feature. This is just a NumPy array, a 1D NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create a set so that we can keep track of which values we have already
    seen, if we happen to sample the same one over and over again. We will permute
    this feature so that we can shuffle it up and scan over each of the values in
    the feature. One thing you''ll note here is that we could collect just the unique
    values of the feature. But first of all, it''s kind of expensive to get the unique
    values. Secondly, that throws away all the distributional information about the
    feature. By doing this, we happen to have more of a certain value than another,
    or more values grouped more closely together. This is going to allow us to get
    a little bit more of a true sample of the feature itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If the number of `seen_values` in our set is equal to the number of values
    that we want to sample, we''re going to break out. So, if we say there are `100`
    unique values, but we''ve already seen `25`, we''re going to break out. Otherwise,
    if we have already seen this value in that set, we''re going to keep going. We
    don''t want to compute the same thing over a value that we have already computed.
    So, here we will add that value to the set, and create our mask for whether we
    split left or right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, there''s one more corner case. If we have grabbed the minimum value, then
    our mask is going to take everything in one direction, which is what this is checking.
    We don''t want that, because, otherwise, we''re not creating a true split. So,
    if that''s the case, then we `continue`, and sample again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the gain, either `InformationGain` or `VarianceReduction`,
    which computes the Gini on the left and right side and subtracts that from the
    original uncertainty. If the `gain` is good, meaning if it''s better than the
    current best we''ve seen, then we have a new `best_feature` and a new `best_value`,
    and we store that. So, we loop over this and go over the randomly sampled values
    within each feature and determine the `best_feature` to split on and the `best_value`
    in that feature to split on. If we don''t have one, it means we never found a
    viable split, which happens in rare cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will look in `LeafNode`. If you have ever built a binary tree before,
    then you would be familiar with the concept of `LeafNode`. `LeafNode` is going
    to store a left and a right pointer, both typically initialized to null to show
    that there''s nothing there. So, the leaf node, in this case, is going to be the
    guts of our decision tree. It provides the skeleton where the tree itself is just
    a wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`LeafNode` is going to store `split_col`, the feature that we are splitting
    on, `split_val`, and `split_gain`, as well as `class_statistic`. So, `class_statistic`
    for classification is going to be the node, where we vote for the most common
    value. In regression, it''s going to be the mean. If you want to get really fancy
    you might use the median or some other strategy for regression. However, we''re
    just going to use the mean because we''re keeping it simple here. So, a constructor
    is going to store these values and initialize our left and right as null again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now in the `create_split` function, we actually get to the tree structure itself.
    But this is going to essentially split the node and create a new left and right.
    Hence, it goes from the terminal node to the next split downward, which we can
    recurse over. We will take the current set for that current dataset from the `X`
    and `y` split. Given that the value in the feature that we have already initialized
    will create our mask for left and right, if we''re going all left or all right,
    that''s where it stores. Otherwise, it''s going to produce this split, segmenting
    out the rows on the left side and the rows on the right side, or else the rows
    are none. If there''s no split on the left/right, we just use none and we will
    return `X_left`, `X_right`, `y_left` and `y_right`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The terminal is just a shortcut here for left and right. If we have either,
    then it''s not terminal. But, if it has both null for left and right, then it''s
    a terminal node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `predict_record` function internally for producing predictions
    inside `LeafNode`. This is going to use that `class_statistic` function that we
    have. `class_statistic` is either the mode for classification or the mean for
    regression. For predicting whether or not a record goes left or right, we recurse
    down, and that is just what is happening here in `predict`, which we''ll get to,
    and look at how we produce predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the trees themselves are two classes. We have `CARTRegressor` and `CARTClassifier`.
    Both of these are going to wrap the `BaseCART` class, which we will walk through
    right now. `BaseCART`, as with most of our base simple estimators that we''ve
    already walked through, is going to take two arguments for certain, which are
    `X` and `y`—our training data and our training labels. It''s also going to take
    our criterion, which we will pass at the bottom. It''s either your `InformationGain`
    for classification, `VarianceReduction` for regression, `min_samples_split`, and
    all these other hyperparameters, which we''ve already kind of talked through.
    The first thing we''re going to do is, as usual, check our `X` and `y` to make
    sure that we have all continuous values, that we''re not missing any data. This
    is just assigning `self` attributes for the hyperparameters and we will create
    our `splitter` as `RandomSplitter`, which we''re going to use in this process.
    This is how we grow the tree. It all happens in `find_next_split`. So, this is
    going to take three arguments. We''ve got our `X`, our `y`, and then the count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Essentially, we will recurse over the `find_next_split` function until our
    tree is fully grown or pruned. Since we''re recursing, we always set our base
    case first. If `current_depth` is equal to `maximum_depth` that we want to grow
    a tree, or the size, the number of samples in `X`, is less than or equal to the
    `min_samples_split` in our split, both of which are our terminal criteria, and
    we will return `None`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, we will grab our splitter and find the best split between `X` and
    `y`, which gives us our `best_feature`, `best_value`, and `gain`, either `VarianceReduction`
    or `InformationGain`. Next, we have just found our first split. So, now we will
    create the node that corresponds to that split. The node is going to take all
    of those same arguments, plus the target statistics. When we produce predictions
    for the node, if it''s terminal, we return the node for that label; otherwise,
    we return the mean for our training labels. That''s how we assign that prediction
    there. So, now we have our node, and we want to create our split. So, we get `X_right`
    and `X_left`. We can recurse down both sides of the tree. We will use that node
    to create the split on `X` and `Y`. So, if `X` is `None`, `X_left` is `None`,
    which means we''re not going to go down to the left side anymore. If it is not
    `None`, then we can assign a node to the left, which is going to recurse on `find_next_split`.
    If `X_right` is `None` then it means we''re not going to grow it on the right
    anymore. If it''s not `None`, we can do the same thing. So, we''re going to assign
    our right side by recursing down `find_next_split`. We recurse over this, continually
    adding `current_depth + 1`, until one side has reached its `maximum_depth`. Otherwise,
    the size of the splits are no longer long enough for `min_sample_split` and we
    stop growing. So, we reach that point where we stop growing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, for predicting, we will traverse down the tree until we find the point
    where a record belongs. So, for each row in `X`, we will predict a row, which
    we have already looked at in the `LeafNode` class, traversing down the left or
    right until we find the node where that row belongs. Then we''ll return `class_statistics`.
    So, if the row gets to a node, it says this belongs here. If the node for that
    class, for classification, was `1`, then we return `1`. Otherwise, if the mean
    was, say, `5.6`, then we return the same. That''s how we produce these predictions,
    which we''re just going to bundle into a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'So, let''s look at how a classification decision tree can perform on some real
    data. In the following example script, we will import `CARTClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create two different bubbles inside of our 2D access on `multivariate_normal`.
    Using this `multivariate_normal` inside of `RandomState`, we will stack that all
    together and produce `train_test_split` as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We will fit `CARTClassifier` and perform two different classifiers. We will
    do the first one. Knowing what you now know about variance and bias, you know
    that classifier or non-parametric models, particularly the decision tree, are
    capable of having very high variance: they can overfit really easily. So, if we
    use a really shallow depth, then we''re more likely to not overfit. In the second
    one, we''re going to try to overfit as much as we can with a max depth of `25`.
    Since we have a pretty small dataset, we can be reasonably certain that this is
    probably going to overfit. We''ll see that when we look at the actual output of
    this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we fit the two of these, look at the accuracy, and plot them. Let''s go
    ahead and run the code and see how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b83c0c9e-95c3-4083-986c-9d3ad10eb106.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you run the preceding code, we get the test''s accuracy of 95% on our underfitted
    tree as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9a554fc-c31a-41a8-82cb-c0070e92a9a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's see how a regression tree can perform. We walked through the same
    exact implementation of our regression tree, except we're going to use the variance
    reduction. Rather than using the mode voting here for producing predictions, we're
    going to use the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the `examples` directory, we have the `example_regression_decision_tree.py`
    file. So, here we will import `CARTRegressor` and use `mean_squared_error` as
    our loss function to determine how well we did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We will just create random values here in a sine wave. That''s what we want
    to be our function as our output here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to do the same kind of thing that we did in the classification
    tree. We will fit a simple `max_depth=3` tree for a regression tree and then a
    `max_depth=10` tree for the second one. It''s not going to overfit quite as much,
    but it''ll show how we increase our predictive capacity as we grow a bit deeper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we''re just plotting the outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go ahead and run this. Rather than `example_classification_decision_tree.py`,
    we''re going to run `example_regression_decision_tree.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6040481b-c400-4601-89a7-2bbfa8b7a408.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, first, you can see that our mean squared error decreases with the `max_depth`
    growing, which is good. You can also see that our outcome starts to model this
    sine wave pretty well as we increase the depth, and we''re able to learn this
    non-linear function very well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d363e0b-682a-4d90-a01e-4099456ded17.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we're going to look at clustering methods and move on from
    decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Various clustering methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the different clustering methods. First, let's
    look at what clustering is. Then we'll explain some of the mathematical tricks
    that we can use in clustering. And finally, we're going to introduce our newest
    non-parametric algorithm KNN.
  prefs: []
  type: TYPE_NORMAL
- en: What is clustering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is about as intuitive as it gets in terms of machine learning models.
    The idea is we can segment groups of samples based on their nearness to one another.
    The hypothesis is the samples that are closer are more similar in some respects.
    So, there are two reasons we might want to cluster. The first is for discovery
    purposes, and we usually do this when we make no assumptions about the underlying
    structure of the data, or don't have labels. And so, this typically is done in
    a purely unsupervised sense. But as this is obviously a supervised learning book,
    we're going to focus on the second use case, which uses clustering as a classification
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: Distance metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, before we get into the algorithms, I want to address some mathematical esotericism.
    When you have two points, or any number of points, in a 2D space, it's fairly
    easy to conceptualize. It's basically calculating the hypotenuse along some right
    triangle, in terms of measuring the distance. However, what happens when you have
    a really high dimensional space? That's what we're going to get into, and we have
    a lot of clustering problems.
  prefs: []
  type: TYPE_NORMAL
- en: So, the most common distance metric is the Euclidean distance. This is essentially
    a generalization of the 2D approach. It's the square root of the sum of squared
    differences between two vectors, and it can be used in any dimensional space.
    There's a lot of others that we're not going to get into. Two of them are **Manhattan**
    and **Mahalanobis**. Manhattan is a lot like the Euclidean distance. However,
    rather than having the squared difference, it's going to have the absolute value
    of the difference.
  prefs: []
  type: TYPE_NORMAL
- en: KNN – introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'KNN is a really simple intuitive approach to building a clustering classifier.
    The idea is, given a set of labeled samples, when a new sample is introduced,
    we look at the k nearest points, and we make an estimate for its class membership
    based on the majority of points around it. So, in the following diagram we would
    classify this new question mark as a positive sample since the majority of its
    neighbors are positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a81eab9b-1197-4342-acde-ce1ae5e74b3e.png)'
  prefs: []
  type: TYPE_IMG
- en: KNN – considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few considerations you should take into account here. KNN is a bit
    interesting and can fluctuate between wildly high bias or high variance depending
    on its hyperparameter *K*. If *K* is too large and you're comparing a new sample
    to the entire training set, it favors the majority class. Essentially, whichever
    is more, we vote that way. This would be a highly underfitted model. If *K* is
    too small, it gives higher priority to the immediately adjacent samples, which
    means that the model is extremely overfitted. In addition to considerations around
    *K*, you may also want to consider centering and scaling your data. Otherwise,
    your distance metric will not be very sensitive to small-scale features. For instance,
    if one of your features is thousands of dollars for a house and the other feature
    is the number of bathrooms, *1.5* to *3.5* or so, you're going to implicitly be
    favoring the dollars versus the number of bathrooms. So, you might want to center
    and scale.
  prefs: []
  type: TYPE_NORMAL
- en: A classic KNN algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classic KNN algorithm will compute the distances between the training samples
    and store them in a distance-partitioned heap structure, such as **KDTree** or
    a ball tree, which are essentially sorted-heap binary trees. We then query the
    tree for test samples. Our approach in this class is going to be a little bit
    different in order to be a bit more intuitive and readable.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll cover how we can implement it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing KNNs from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will jump into the `packtml` code base, and see how we can
    implement it from scratch. We'll start by revisiting the classic algorithm we
    covered in the last section, and then we'll look at the actual Python code, which
    has some implementation changes.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the archetypal KNN algorithm. The efficient implementation is going to
    be to pre-compute the distances and store them in a special heap. Of course, with
    most things in computer science, there's the clever way and then there's the easy-to-read
    way. We're going to do things a bit differently in an effort to maximize the readability,
    but it's the same fundamental algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: KNN clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've got two files we want to look at. The first is the source code in the
    `packtml` Python package. Second, we're going to look an example of the KNN applied
    to the `iris` dataset. Let's go ahead and jump over to PyCharm, where there are
    two files open. Inside of the `clustering` submodule, we have the `knn.py` file
    open. This is where we're going to find the KNN class and all the implementation
    details. Then in the examples directory, in the `clustering` subdirectory, we
    have the `example_knn_classifier.py` file open as well. We'll walk through that
    after we've gone through the implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, regarding other libraries, we''re going to use scikit-learn''s utils to
    validate the `X`, `y`, and classification targets. However, we''re also going
    to use the `metrics.pairwise` submodule to use `euclidean_distances`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to use a different distance metric, we could also import Manhattan,
    as mentioned in the earlier section. But for this, we''re just going to use Euclidean.
    So, if you want to adjust that later, feel free. Our KNN class here is going to
    take three parameters. As usual for `BaseSimpleEstimator`, we''re going to take
    our `X` and `y`, which are our training vectors and our training label, and then
    `k`, which is our tuning parameter for the number of neighbors that we want to
    compute around each sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our constructor is pretty simple. We''re going to check our `X` and `y`
    and basically store them. Then we assign `k` to a `self` attribute. Now, in other
    implementations we might go ahead and compute our KDTree or our ball tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we''re going to do a brute force method, where we don''t compute the distances
    until we predict. This is a lazy evaluation of distances. In our predict function,
    we are going to take our `X`, which is our test array. `X`, which we assigned
    in the constructor. We will compute `euclidean_distances` between our training
    array and our test array. Here, we get an `M` by `M` matrix, where `M` is the
    number of samples in our test array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to find the nearest distance, we `argsort` distances by the column
    to show which samples are closest. Following, is the array of distances, and we
    are going to `argsort` it along the `axis` column, such that we get the samples
    that are closest, based on the distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We will slice the labels based on `top_k` along `y`. These are basically the
    class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Since it''s a classification, we''re interested in `mode`. Take the mode using
    the `mode` function along that `axis` and `ravel` it into a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, we''re just computing the distances for the predict function, argsorting
    the closest distances, then finding the corresponding labels, and taking the mode.
    Now, over in the `examples/clustering` directory, go to `example_knn_classifier.py`.
    We''re going to use the `load_iris` function from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We will only use the first two dimensions so that we can visualize it in a
    relatively intuitive fashion. Perform the training split, and then center and
    scale using `StandardScaler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the `KNNClassifier` with `k=10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will plot it by typing the following command. Make sure you''ve
    got your environment activated, as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eed9a1bd-cf77-4838-801e-e3921dbe7935.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output for `k = 10`, and we get about 73-74% test accuracy. Note that we''re
    only using two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d072a43a-558f-42b2-983c-feb58cb1ba79.png)'
  prefs: []
  type: TYPE_IMG
- en: So, now that you're a KNN expert, you can build one from scratch. In the next
    section, we will compare non-parametric models to parametric models.
  prefs: []
  type: TYPE_NORMAL
- en: Non-parametric models – pros/cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss every statistician's favorite philosophical
    debate, which is the pros and cons of non-parametric models versus parametric
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Pros of non-parametric models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-parametric models are able to learn some really complex relationships between
    your predictors and the output variable, which can make them really powerful for
    non-trivial modeling problems. Just like the regression sinusoidal wave we modeled
    in the decision trees, a lot of non-parametric models are fairly tolerant to data
    scale as well. The major exception here is the clustering techniques, but these
    techniques can pose a major advantage for models such as decision trees, which
    don't require the same level of pre-processing that parametric models might. Finally,
    if you find yourself suffering from high variance, you can always add more training
    data, with which your model is likely to get better.
  prefs: []
  type: TYPE_NORMAL
- en: Cons of non-parametric models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are the not-so-good parts of non-parametric models as well. Several of
    these we have already covered. So, as you may know, they can be slower to fit
    or predict, and less intuitive in many cases than a lot of parametric models.
    If speed is less critical than accuracy, non-parametric models may be a great
    candidate for your model. Likewise, with explainability, these models can be over-complicated
    and tough to understand. Finally, one of the advantages of non-parametric models
    is the ability to get better with more data, which can be a weakness if data is
    hard to get. They generally do require a bit more data to train effectively than
    their parametric brethren.
  prefs: []
  type: TYPE_NORMAL
- en: Which model to use?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parametric models that we've already covered have some really great and convenient
    attributes. There are several reasons you may opt for a parametric model over
    a non-parametric model. Particularly if you're in a regulated industry, we need
    to explain the models more easily. Non-parametric models, on the other hand, may
    create a better, more complex model. But if you don't have a good chunk of data,
    it may not perform very well. It is best not to get overly philosophical about
    which one you should or should not use. Just use whichever best fits your data
    and meets your business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we initially got introduced to non-parametric models and then
    we walked through the decision trees. In the next sections, we learned the splitting
    criteria and how they produce splits. We also learned about the bias-variance
    trade-off, and how non-parametric models tend to favor a higher variance set of
    error, while parametric models favor high bias. Next, we looked into clustering
    methods and even coded a KNN class from scratch. Finally, we wrapped up with the
    pros and cons of non-parametric methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will get into some more of the advanced topics in supervised
    machine learning, including recommender systems and neural networks.
  prefs: []
  type: TYPE_NORMAL

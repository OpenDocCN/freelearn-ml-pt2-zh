- en: Ensembles – When One Model Is Not Enough
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法——当一个模型不足以应对时
- en: In the previous three chapters, we saw how **neural networks** help directly
    and indirectly in solving natural language understanding and image processing
    problems. This is because neural networks are proven to work well with **homogeneous
    data**; that is, if all the input features are of the same breed—pixels, words,
    characters, and so on. On the other hand, when it comes to **heterogeneous****data**,
    it is the **ensemble****methods** that are known to shine. They are well suited
    to deal with heterogeneous data—for example, where one column contains users'
    ages, the other has their incomes, and a third has their city of residence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的三章中，我们看到**神经网络**如何直接或间接地帮助解决自然语言理解和图像处理问题。这是因为神经网络已被证明能够很好地处理**同质数据**；即，如果所有输入特征属于同一类——像素、单词、字符等。另一方面，当涉及到**异质数据**时，**集成方法**被认为能够发挥优势。它们非常适合处理异质数据——例如，一列包含用户的年龄，另一列包含他们的收入，第三列包含他们的居住城市。
- en: You can view ensemble estimators as meta-estimators; they are made up of multiple
    instances of other estimators. The way they combine their underlying estimators
    is what differentiates between the different ensemble methods—for example, the
    **bagging** versus the **boosting** methods. In this chapter, we are going to
    look at these methods in detail and understand their underlying theory. We will
    also learn how to diagnose our own models and understand why they make certain
    decisions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将集成估计器视为元估计器；它们由多个其他估计器的实例组成。它们组合底层估计器的方式决定了不同集成方法之间的差异——例如，**袋装法**与**提升法**。在本章中，我们将详细探讨这些方法，并理解它们的理论基础。我们还将学习如何诊断自己的模型，理解它们为何做出某些决策。
- en: As always, I would also like to seize the opportunity to shed light on general
    machine learning concepts while dissecting each individual algorithm. In this
    chapter, we will see how to handle the estimators' uncertainties using the classifiers'
    probabilities and the regression ranges.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我还希望借此机会在剖析每个单独的算法时，顺便阐明一些常见的机器学习概念。在本章中，我们将看到如何利用分类器的概率和回归范围来处理估计器的不确定性。
- en: 'The following topics will be discussed in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下内容：
- en: The motivation behind ensembles
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法的动机
- en: Averaging/bagging ensembles
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均法/袋装集成方法
- en: Boosting ensembles
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升集成方法
- en: Regression ranges
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归范围
- en: The ROC curve
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC 曲线
- en: Area under the curve
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线下的面积
- en: Voting and stacking ensembles
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投票法与堆叠集成方法
- en: Answering the question why ensembles?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回答为什么选择集成方法的问题？
- en: The main idea behind ensembles is to combine multiple estimators so that they
    make better predictions than a single estimator. However, you should not expect
    the mere combination of multiple estimators to just lead to better results. The
    combined predictions of multiple estimators who make the exact same mistakes will
    be as wrong as each individual estimator in the group. Therefore, it is helpful
    to think of the possible ways to mitigate the mistakes that individual estimators
    make. To do so, we have to revisit our old friend the bias and variance dichotomy.
    We will meet few machine learning teachers better than this pair.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法背后的主要思想是将多个估计器结合起来，使它们的预测效果比单一估计器更好。然而，你不应该仅仅期待多个估计器的简单结合就能带来更好的结果。多个估计器的预测组合如果犯了完全相同的错误，其结果会与每个单独的估计器一样错误。因此，考虑如何减轻单个估计器所犯的错误是非常有帮助的。为此，我们需要回顾一下我们熟悉的偏差-方差二分法。很少有机器学习的老师比这对概念更能帮助我们了。
- en: If you recall from [Chapter 2](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml),
    *Making Decisions with Trees*, when we allowed our decision trees to grow as much
    as they can, they tended to fit the training data like a glove but failed to generalize
    to newer data points. We referred to this as overfitting, and we have seen the
    same behavior with unregularized linear models and with a small number of nearest
    neighbors. Conversely, aggressively restricting the growth of trees, limiting
    the number of the features in linear models, and asking too many neighbors to
    vote caused the models to become biased and underfit the data at hand. So, we
    had to tread a thin line between trying to find the optimum balance between the
    bias-variance and the underfitting-overfitting dichotomies.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we are going to follow a different approach. We will
    deal with the bias-variance dichotomy as a continuous scale, starting from one
    side of this scale and using the concept of *ensemble* to move toward the other
    side. In the next section, we are going to start by looking at high-variance estimators
    and averaging their results to reduce their variance. Later on, we will start
    from the other side and use the concept of boosting to reduce the estimators'
    biases.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple estimators via averaging
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"To derive the most useful information from multiple sources of evidence, you
    should always try to make these sources independent of each other."'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: – Daniel Kahneman
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: If a single fully grown decision tree overfits, and if having many voters in
    the nearest neighbors algorithm has an opposite effect, then why not combine the
    two concepts? Rather than having a single tree, let's have a forest that combines
    the predictions of each tree in it. Nevertheless, we do not want all the trees
    in our forest to be identical; we would love them to be as diverse as possible.
    The **bagging** and random forest meta-estimators are the most common examples
    here. To achieve diversity, they make sure that each one of the individual estimators
    they use is trained on a random subset of the training data—hence the *random*
    prefix in random forest. Each time a random sample is drawn, it can be done with
    replacement (**bootstrapping**) or without replacement (**pasting**). The term
    bagging stands for **bootstrap aggregation** as the estimators draw their samples
    with replacement. Furthermore, for even more diversity, the ensembles can assure
    that each tree sees a random subset of the training features.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Both ensembles use decision tree estimators by default, but the **bagging**
    ensemble can be reconfigured to use any other estimator. Ideally, we would like
    to use high-variance estimators. The decisions made by the individual estimators
    are combined via voting or averaging.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Boosting multiple biased estimators
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"If I have seen further than others, it is by standing upon the shoulders of
    giants."'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: –Isaac Newton
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to fully grown trees, a shallow tree tends to be biased. Boosting
    a biased estimator is commonly performed via **AdaBoost** or **gradient boosting**.
    The AdaBoost meta-estimator starts with a weak or biased estimator, then each
    consequent estimator learns from the mistakes made by its predecessors. We saw
    in [Chapter 2](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml), *Making Decisions
    with Trees*, that we can give each individual training sample a different weight
    so that the estimators can give more emphasis to some samples versus others. In
    **AdaBoost**, erroneous predictions made by the preceding estimators are given
    more weight for their successors to pay more attention to.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The **gradient boosting** meta-estimator follows a slightly different approach.
    It starts with a biased estimator, computes its loss function, then builds each
    consequent estimator to minimize the loss function of its predecessors. As we
    saw earlier, gradient descent always comes in handy when iteratively minimizing
    loss functions, hence the *gradient* prefix in the name of the gradient boosting
    algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Due to the iterative nature of the two ensembles, they both have a learning
    rate to control their learning speed and to make sure they don't miss the local
    minima when converging. Like the **bagging** algorithm, **AdaBoost** is not limited
    to decision trees as its base estimator.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good idea about the different ensemble methods, we can use
    real-life data to demonstrate how they work in practice. Each of the ensemble
    methods described here can be used for classification and regression. The classifier
    and regressor hyperparameters are almost identical for each ensemble. Therefore,
    I will pick a regression problem to demonstrate each algorithm and briefly show
    the classification capabilities of the random forest and gradient boosting algorithms
    since they are the most commonly used ensembles.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to download a dataset prepared by the**University
    of California, Irvine** (**UCI**). It contains 201 samples for different cars,
    along with their prices. We will be using this dataset in a later section to predict
    the car prices via regression.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the UCI Automobile dataset
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Automobile dataset was created by Jeffrey C. Schlimmer and published in
    UCI''s machine learning repository. It contains information about 201 automobiles,
    along with their prices. The names of the features are missing. Nevertheless,
    I could get them from the dataset''s description ([http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names](http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names)).
    So, we can start by seeing the URL and the feature names, as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, we use the following code to download our data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is mentioned in the dataset's description that missing values are replaced
    with a question mark. To make things more Pythonic, we set `na_values` to `'?'`
    to replace these question marks with NumPy's **Not a Number**(**NaN**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can perform our **Exploratory Data Analysis** (**EDA**), check the
    percentages of the missing values, and see how to deal with them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with missing values
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can check which columns have the most missing values:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This gives us the following list:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since the price is our target value, we can just ignore the four records where
    the prices are unknown:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As for the remaining features, I''d say let''s drop the`normalized-losses`
    column since 41 of its values are missing. Later on, we will use the data imputation
    techniques to deal with the other columns with fewer missing values. You can drop
    the `normalized-losses` column using the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: At this point, we have a data frame with all the required features and their
    names. Next, we want to split the data into training and test sets, and then prepare
    our features. The different feature types require different preparations. You
    may need to separately scale the numerical features and encode the categorical
    ones. So, it is good practice to be able to differentiate between the numerical
    and the categorical features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating between numerical features and categorical ones
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we are going to create a dictionary to separately list the numerical
    and categorical features. We will also make a combined list of the two, and provide
    the name of the target column, as in the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By doing so, you can deal with the columns differently. Furthermore, just for
    my own sanity and to notprint too many zeros in the future, I rescaled the prices
    to be in thousands, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can also display certain features separately. Here, we print a random sample,
    where just the categorical features are shown:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here are the resulting rows. I set `random_state` to `42` to make sure we all
    get the same random rows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/806e2ee8-cfdf-4819-b53d-a58a4a38dd61.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: All other transformations, such as scaling, imputing, and encoding, should be
    done after splitting the data into training and test sets. That way, we can ensure
    that no information is leaked from the test set into the training samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into training and test sets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we keep 25% of the data for testing and use the rest for training:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we can use the information from the previous section to create our `x`
    and `y` values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As usual, with regression tasks, it is handy to understand the distribution
    of the target values:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A histogram is usually a good choice for understanding distributions, as seen
    in the following graph:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e1e4eaa-47ca-4d98-a27d-299eb324a237.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: We may come back to this distribution later to put our regressor's mean error
    in perspective. Additionally, you can use this range for sanity checks. For example,
    if you know that all the prices you have seen fall in the range of 5,000 to 45,000,
    you may decide when to put your model in production to fire an alert any time
    it returns prices far from this range.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会稍后回到这个分布，来将回归模型的平均误差放到合理的范围内。此外，你还可以使用这个范围进行合理性检查。例如，如果你知道所有看到的价格都在5,000到45,000之间，你可能会决定在将模型投入生产时，如果模型返回的价格远离这个范围，就触发警报。
- en: Imputing the missing values and encoding the categorical features
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充缺失值并编码类别特征
- en: 'Before bringing our ensembles to action, we need to make sure we do not have
    null values in our data. We will replace the missing values with the most frequent
    value in each column using the `SimpleImputer` function from[Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml),
    *Preparing Your Data*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用我们的集成方法之前，我们需要确保数据中没有空值。我们将使用来自[第4章](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)《准备数据》的`SimpleImputer`函数，用每列中最常见的值来替换缺失值：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You may have already seen me complain many times about the scikit-learn transformers,
    which do not respect the column names and insist on converting the input data
    frames into NumPy arrays. To stop myself from complaining again, let me solve
    my itch by using the following `ColumnNamesKeeper` class. Whenever I wrap it around
    a transformer, it will make sure all the data frames are kept unharmed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经看到我多次抱怨scikit-learn的转换器，它们不尊重列名，并且坚持将输入数据框转换为NumPy数组。为了不再抱怨，我通过使用以下`ColumnNamesKeeper`类来解决我的痛点。每当我将它包装在转换器周围时，它会确保所有的数据框都保持不变：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, it mainly saves the column name when the `fit` method is called.
    Then, we can use the saved names to recreate the data frames after the transformation
    steps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它主要在调用`fit`方法时保存列名。然后，我们可以使用保存的列名在变换步骤后重新创建数据框。
- en: The code for `ColumnNamesKeeper` can be simplified further by inheriting from
    `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin`. You can check
    the source code of any of the library's built-in transformers if you are willing
    to write more scikit-learn-friendly transformers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnNamesKeeper`的代码可以通过继承`sklearn.base.BaseEstimator`和`sklearn.base.TransformerMixin`来进一步简化。如果你愿意编写更符合scikit-learn风格的转换器，可以查看该库内置转换器的源代码。'
- en: 'Now, I can call `SimpleImputer` again while preserving `x_train` and `x_test`
    as data frames:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我可以再次调用`SimpleImputer`，同时保持`x_train`和`x_test`作为数据框：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We learned in [Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml), *Preparing
    Your Data*, that `OrdinalEncoder`**is recommended for tree-based algorithms, in
    addition to any other non-linear algorithms. The `category_encoders` library doesn't
    mess with the column names, and so we can use `OrdinalEncoder` without the need
    for`ColumnNamesKeeper` this time. In the following code snippet, we also specify
    which columns to encode (the categorical columns) and which to keep unchanged
    (the remaining ones):**
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)《准备数据》中学到，`OrdinalEncoder`**推荐用于基于树的算法，此外还适用于任何其他非线性算法。`category_encoders`库不会改变列名，因此我们这次可以直接使用`OrdinalEncoder`，无需使用`ColumnNamesKeeper`。在以下代码片段中，我们还指定了要编码的列（类别列）和保持不变的列（其余列）：**
- en: '**[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE15]'
- en: In addition to `OrdinalEncoder`, you can also test the encoders mentioned in
    the target encoding in [Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)*,
    Preparing Your Data*. They, too, are meant to be used with the algorithms explained
    in this chapter. In the next section, we are going to use the random forest algorithm
    with the data we have just prepared.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`OrdinalEncoder`，你还可以测试[第4章](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)《准备数据》中提到的目标编码器*。它们同样适用于本章中解释的算法。在接下来的部分，我们将使用随机森林算法来处理我们刚准备好的数据。
- en: Using random forest for regression
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行回归
- en: 'The random forest algorithm is going to be the first ensemble to deal with
    here. It''s an easy-to-grasp algorithm with straightforward hyperparameters. Nevertheless,
    as we usually do, we will start by training the algorithm using its default values,
    as follows, then explain its hyperparameters after that:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法将是我们首先要处理的集成方法。它是一个容易理解的算法，具有直接的超参数设置。尽管如此，我们通常的做法是先使用默认值训练算法，如下所示，然后再解释其超参数：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Since each tree is independent of the others, I set `n_jobs` to `-1` to use
    my multiple processors to train the trees in parallel. Once they are trained and
    the predictions are obtained, we can print the following accuracy metrics:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will print the following scores:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The average car price is 13,400\. So, a **Mean Absolute Error** (**MAE***)*
    of `1.35` seems reasonable. As for the **Mean Squared Error** (**MSE**), it makes
    sense to use its square root to keep it in the same units as the MAE. In brief,
    given the high R² score and the low errors, the algorithm seems to perform well
    with its default values. Furthermore, you can plot the errors to get a better
    understanding of the model''s performance:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'I''ve excluded some of the formatting lines to keep the code concise. In the
    end, we get the following graphs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d5c005a-a4d0-47be-b420-cdd3984f4ac7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: By plotting the predictions versus the actuals, we can make sure that the models
    don't systematically overestimate or underestimate. This is shown via the 45^o
    slope of the scattered points on the left. A lower slope for the scattered points
    would have systematically reflected an underestimation. Having the scattered points
    aligned on a straight line assures us that there aren't non-linearities that the
    model couldn't capture. The histogram to the right shows that most of the errors
    are below 2,000\. It is good to understand what mean and maximum errors you can
    expect to get in the future.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Checking the effect of the number of trees
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, each tree is trained on a random sample from the training data.
    This is achieved by setting the `bootstrap` hyperparameter to `True`. In bootstrap
    sampling, a sample may be used during training more than once, while another sample
    may not be used at all.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: When `max_samples` is kept as `None`, each tree is trained on a random sample
    of a size that is equal to the entire training data size. You can set `max_samples`
    to a fraction that is less than 1, then each tree is trained on a smaller random
    sub-sample. Similarly, we can set `max_features` to a fraction that is less than
    1 to make sure each tree uses a random subset of the available features. These
    parameters help each tree to have its own personality and to ensure the diversity
    of the forest. To put it more formally, these parameters increase the variance
    of each individual tree. So, it is advised to have as many trees as possible to
    reduce the variance we have just introduced.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we compare three forests, with a different number of trees in each:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we can plot the MAE for each forest to see the merits of having more
    trees:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/476a677f-6df2-4dea-8fa5-d6a7545117d0.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Clearly, we have just encountered a new set of hyperparameters to tune `bootstrap`,
    `max_features`, and `max_samples`. So, it makes sense to apply cross-validation
    for hyperparameter tuning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the effect of each training feature
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once a random forest is trained, we can list the training features, along with
    their importance. As usual, we put the outcome in a data frame by using the column
    names and the `feature_importances_` attribute, as shown:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the resulting data frame:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c526a3c-1ceb-4c52-87a5-4a83419a4181.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Unlike with linear models, all the values here are positive. This is because
    these values only show the importance of each feature, regardless of whether it
    is positively or negatively correlated with the target. This is common for decision
    trees, as well as for tree-based ensembles. Thus, we can use **Partial Dependence
    Plots**(**PDPs**)to show the relationship between the target and the different
    features. Here, we only plot it for the top six features according to their importance:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting graphs are easier to read, especially when the relationship between
    the target and the features is non-linear:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3eadf45-3663-4c33-8c33-b1c380f5ddf6.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: We can now tell that cars with bigger engines, more horsepower, and less mileage
    per gallon tend to be more expensive.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: PDPs are not just useful for ensemble methods, but also for any other complex
    non-linear model. Despite the fact the neural networks have coefficients for each
    layer, the PDP is essential in understanding the network as a whole. Furthermore,
    you can also understand the interaction between the different feature pairs by
    passing the list of features as a list tuples, with a pair of features in each
    tuple.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for classification
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate the random forest classifier, we are going to use a synthetic
    dataset. We first create the dataset using the built-in `make_hastie_10_2` class:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This previous code snippet creates a random dataset. I set `random_state` to
    a fixed number to make sure we both get the same random data. Now, we can split
    the resulting data into training and test sets:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then, to evaluate the classifier, we are going to introduce a new concept called
    the **Receiver Operating Characteristic** (**ROC**) curve in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"Probability is expectation founded upon partial knowledge. A perfect acquaintance
    with all the circumstances affecting the occurrence of an event would change expectation
    into certainty, and leave neither room nor demand for a theory of probabilities."'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: – George Boole (Boolean data types are named after him)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: In a classification problem, the classifier assigns probabilities to each sample
    to reflect how likely it is that each sample belongs to a certain class. We get
    these probabilities via the classifier's `predict_proba()` method. The `predict()`
    method is typically a wrapper on top of the `predict_proba()` method. In a binary-classification
    problem, it assigns each sample to a specific class if the probability of it belonging
    to the class is above 50%. In practice, we may not always want to stick to this
    50% threshold, especially as different thresholds usually change the **T****rue
    Positive Rates** (**TPRs**) and **False Positive Rates** (**FPRs**) for each class.
    So, you can choose a different threshold to optimize for a desired TPR.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，分类器会为每个样本分配一个概率值，以反映该样本属于某一类别的可能性。我们通过分类器的`predict_proba()`方法获得这些概率值。`predict()`方法通常是`predict_proba()`方法的封装。在二分类问题中，如果样本属于某个类别的概率超过50%，则将其分配到该类别。实际上，我们可能并不总是希望遵循这个50%的阈值，尤其是因为不同的阈值通常会改变**真正阳性率**(**TPRs**)和**假阳性率**(**FPRs**)在每个类别中的表现。因此，你可以选择不同的阈值来优化所需的TPR。
- en: 'The best way to decide which threshold suits your needs is to use a ROC curve.
    This helps us see the TPR and FPR for each threshold. To create this curve, we
    will train our random forest classifier on the synthetic dataset we have just
    created, but get the classifier''s probabilities this time:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的方法来决定哪个阈值适合你的需求是使用ROC曲线。这有助于我们看到每个阈值下的TPR和FPR。为了创建这个曲线，我们将使用我们刚创建的合成数据集来训练我们的随机森林分类器，但这次我们会获取分类器的概率值：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we can calculate the TPR and FPR for each threshold, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按以下方式计算每个阈值的TPR和FPR：
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s stop for a moment to explain what TPR and FPR mean:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来稍作解释，看看TPR和FPR是什么意思：
- en: The**TPR**, also known as **recall** or **sensitivity**, is calculated as the
    number of **True Positive** (**TP**) cases divided by all the positive cases;
    that is, ![](img/d14315f8-a807-4550-9467-be22f09a947b.png), where *FN* is the
    positive cases falsely classified as negative (false negatives).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TPR**，也叫做**召回率**或**敏感度**，计算方法是**真正阳性**(**TP**)案例的数量除以所有正类案例的数量；即，！[](img/d14315f8-a807-4550-9467-be22f09a947b.png)，其中*FN*是被错误分类为负类的正类案例（假阴性）。'
- en: The **True Negative Rates** (**TNR**), also known as **specificity**, is calculated
    as the number of **True Negative** (**TN**) cases divided by all the negative
    cases; that is, ![](img/edcaf90b-1764-47ed-84bc-e42e16bfc779.png), where *FP*
    is the negative cases falsely classified as positive (false positives).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正阴性率**(**TNR**)，也叫做**特异度**，计算方法是**真正阴性**(**TN**)案例的数量除以所有负类案例的数量；即，！[](img/edcaf90b-1764-47ed-84bc-e42e16bfc779.png)，其中*FP*是被错误分类为正类的负类案例（假阳性）。'
- en: The **FPR** is defined as 1 minus TNR; that is, ![](img/a73fab77-1996-4d38-88fc-669a544b5249.png).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FPR**定义为1减去TNR，也就是！[](img/a73fab77-1996-4d38-88fc-669a544b5249.png)。'
- en: The **False Negative Rate** (**FNR**) is defined as 1 minus TPR; that is, ![](img/1d8f94e7-78b4-4197-8feb-678071889616.png).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性率**(**FNR**)定义为1减去TPR；即，！[](img/1d8f94e7-78b4-4197-8feb-678071889616.png)。'
- en: 'Now, we can put the calculated TPR and FPR for our dataset into the following
    table:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们计算出的TPR和FPR放入以下表格中：
- en: '![](img/0abc76af-774e-4e2d-99b5-294fb69ab66c.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0abc76af-774e-4e2d-99b5-294fb69ab66c.png)'
- en: 'Even better than a table, we can plot them into a graph using the following
    code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 比表格更好的是，我们可以使用以下代码将其绘制成图表：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'I''ve omitted the graph''s styling code for the sake of brevity. I also added
    a 45^o line and the **Area Under the Curve** (**AUC**), which I will explain in
    a bit:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我省略了图表的样式代码。我还添加了一个45°的线条和**曲线下面积**(**AUC**)，稍后我会解释这个概念：
- en: '![](img/c989db9d-5d8a-4d47-a9a1-8d3b031676ed.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c989db9d-5d8a-4d47-a9a1-8d3b031676ed.png)'
- en: 'A classifier that randomly assigns each sample to a certain class will have
    a ROC curve that looks like the dashed 45^o line. Any improvement over this will
    make the curve more convex upward. Obviously, random forest''s ROC curve is better
    than chance. An optimum classifier will touch the upper-left corner of the graph.
    Therefore, the AUC can be used to reflect how good the classifier is. An area
    above `0.5` is better than chance, and an area of `1.0` is the best possible value.
    We typically expect values between `0.5` and `1.0`. Here, we got an AUC of `0.94`.
    The AUC can be calculated using the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can also use the ROC and AUC to compare two classifiers. Here, I trained
    the random forest classifier with the `bootstrap` hyperparameter set to `True`
    and compared it to the same classifier when `bootstrap` was set to `False`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0457044f-36f5-4ba0-a1ab-4ae6c68f4a9b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'No wonder the `bootstrap` hyperparameter is set to `True` by default—it gives
    better results. Now, you have seen how to use random forest algorithms to solve
    classification and regression problems. In the next section, we are going to explain
    a similar ensemble: the bagging ensemble.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Using bagging regressors
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will go back to the Automobile dataset as we are going to use the **bagging
    regressor** this time. The bagging meta-estimator is very similar to random forest.
    It is built of multiple estimators, each one trained on a random subset of the
    data using a bootstrap sampling method. The key difference here is that although
    decision trees are used as the base estimators by default, any other estimator
    can be used as well. Out of curiosity, let's use the **K-Nearest Neighbors** (**KNN**)
    regressor as our base estimator this time. However, we need to prepare the data
    to suit the new regressor's needs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a mixture of numerical and categorical features
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is recommended to put all features on the same scale when using distance-based
    algorithms such as KNN*.* Otherwise, the effect of the features with higher magnitudes
    on the distance metric will overshadow the other features. As we have a mixture
    of numerical and categorical features here, we need to create two parallel pipelines
    to prepare each feature set separately.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a top-level view of our pipeline:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/306f62b0-7a03-40b8-bbd2-30a85bba31f4.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: 'Here, we start by building the four transformers in our pipelines: `Imputer`,
    `Scaler`**,** and `OneHotEncoder`. We also wrap them in `ColumnNamesKeeper`, which
    we created earlier in this chapter:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we put them into two parallel pipelines:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we concatenate the outputs of the pipelines for both the training
    and the test sets:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: At this point, we are ready to build our bagged KNNs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Combining KNN estimators using a bagging meta-estimator
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`BaggingRegressor` has a `base_estimator` hyperparameter, where you can set
    the estimators you want to use. Here, `KNeighborsRegressor` is used with a single
    neighbor. Since we are aggregating multiple estimators to reduce their variance,
    it makes sense to have a high variance estimator in the first place, hence the
    small number of neighbors here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This new setup gives us an MAE of `1.8`. We can stop here, or we may decide
    to improve the ensemble's performance by tuning its big array of hyperparameters.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we can try different estimators other than KNN, each with its
    own hyperparameters. Then, the bagging ensemble also has its own hyperparameters.
    We can change the number of estimators via `n_estimators`. Then, we can decide
    whether to use the entire training set or a random subset of it for each estimator
    via `max_samples`. Similarly, we can also pick a random subset of the columns
    to use for each estimator to use via `max_features`. The choice of whether to
    use bootstrapping for the rows and the columns can be made via the `bootstrap`
    and `bootstrap_features`hyperparameters, respectively.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Finally, since each estimator is trained separately, we can use a machine with
    a high number of CPUs and parallelize the training process by setting `n_jobs`
    to `-1`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have experienced two versions of the averaging ensembles, it is
    time to check their boosting counterparts. We will start with the gradient boosting
    ensemble, then move to the AdaBoost ensemble.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Using gradient boosting to predict automobile prices
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If I were ever stranded on a desert island and had to pick one algorithm to
    take with me, I'd definitely chose the gradient boosting ensemble! It has proven
    to work very well on many classification and regression problems. We are going
    to use it with the same automobile data from the previous sections. The classifier
    and the regressor versions of this ensemble share the exact same hyperparameters,
    except for the loss functions they use. This means that everything we are going
    to learn here will be useful to us whenever we decide to use gradient boosting
    ensembles for classification.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the averaging ensembles we have seen so far, the boosting ensembles
    build their estimators iteratively. The knowledge learned from the initial ensemble
    is used to build its successors. This is the main downside of boosting ensembles,
    where parallelism is unfeasible. Putting parallelism aside, this iterative nature
    of the ensemble calls for a learning rate to be set. This helps the gradient descent
    algorithm reach the loss function''s minima easily. Here, we use 500 trees, each
    with a maximum of 3 nodes, and a learning rate of `0.01`. Furthermore, the **Least
    Squares** (**LS**) loss is used here; think MSE. More on the available loss functions
    in a moment:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This new algorithm gives us the following performance on the test set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, this setting gave a lower MSE compared to random forest, while
    random forest had a better MAE. Another loss function that the gradient boosting**regressor
    can use is **Least Absolute Deviation** (**LAD**); think MAE, this time. LAD may
    help when dealing with outliers, and it can sometimes reduce the model's MAE performance
    on the test set. Nevertheless, it did not improve the MAE for the dataset at hand.
    We also have a percentile (quantile) loss, but before going deeper into the supported
    loss functions, we need to learn how to diagnose the learning process.**
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '**The main hyperparameters to set here are the number of trees, the depth of
    the trees, the learning rate, and the loss function. As a rule of thumb, you should
    aim for a higher number of trees and a low learning rate. As we will see in a
    bit, these two hyperparameters are inversely proportional to each other. Controlling
    the depth of your trees is purely dependent on your data. In general, we need
    to have shallow trees and let boosting empower them. Nevertheless, the depth of
    the tree controls the number of feature interactions we want to capture. In a
    stub (a tree with a single split), only one feature can be learned at a time.
    A deeper tree resembles a nested `if` condition where a few more features are
    at play each time. I usually start with`max_depth` set to around `3` and `5` and
    tune it along the way.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the learning deviance
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With each additional estimator, we expect the algorithm to learn more and the
    loss to decrease. Yet, at some point, the additional estimators will keep overfitting
    on the training data while not offering much improvement for the test data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: To have a clear picture, we need to plot the calculated loss with each additional
    estimator for both the training and test sets. As for the training loss, it is
    saved by the gradient boosting meta-estimator into its `loss_` attribute. For
    the test loss, we can use the meta-estimator's `staged_predict()` methods. This
    method can be used for a given dataset to make predictions for each intermediate
    iteration.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have multiple loss functions to choose from, gradient boosting also
    provides a`loss_()` method, which calculates the loss for us based on the loss
    function used. Here, we create a new function to calculate the training and test
    errors for each iteration and put them into a data frame:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Since we are going to use an LS loss here, you can simply replace the `estimator.loss_()`
    method with `mean_squared_error()` and get the exact same results. But let's keep
    the `estimator.loss_()` function for a more versatile and reusable code.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we train our gradient boosting regressor, as usual:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we use the trained model, along with the test set, to plot the training
    and test learning deviance:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Running the code gives us the following graph:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3779f19-026c-41c1-b736-d59e91380aa9.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: The beauty of this graph is that it tells us that the improvements on the test
    set stopped after `120` estimators or so, despite the continuous improvement in
    the training set; that is, it started to overfit. Furthermore, we can use this
    graph to understand the effect of a chosen learning rate, as we did in *[Chapter
    7](7559b34f-080c-485d-b4bc-5f22580fc1d1.xhtml)*, *Neural Networks - Here comes
    the Deep Learning*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the learning rate settings
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rather than training one model, we will train three gradient boosting regressors
    this time, each with a different learning rate. Then, we will plot the deviance
    graph for each one side by side, as shown:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e63c903-27fd-44ab-a4d1-315f8b478d9a.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: As with other gradient descent-based models, a high learning rate causes the
    estimator to overshoot and miss the local minima. We can see this in the first
    graph where no improvements are seen despite the consecutive iterations. The learning
    rates in the second and third graphs seem reasonable. In comparison, the learning
    rate in the third graph seems to be too slow for the model to converge in 500
    iterations. You may then decide to increase the number of estimators for the third
    model to allow it to converge.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: We have learned from the bagging ensembles that using a random training sample
    with each estimator may help with overfitting. In the next section, we are going
    to see whether the same approach can also help the boosting ensembles.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Using different sample sizes
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have been using the entire training set for each iteration. This time, we
    are going to train three gradient boosting regressors, each with a different subsample
    size, and plot their deviance graphs as before. We will use a fixed learning rate
    of `0.01` and the LAD**as our loss function, as shown:**
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/0b4eacd5-f8e0-41a7-9a38-895ae96e13a6.png)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: In the first graph, the entire training sample is used for each iteration. So,
    the training loss did not fluctuate as much as in the other two graphs. Nevertheless,
    the sampling used in the second model allowed it to reach a better test score,
    despite its noisy loss graph. This was similarly the case for the third model,
    but with a slightly larger final error.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Stopping earlier and adapting the learning rate
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `n_iter_no_change` hyperparameter is used to stop the training process after
    a certain number of iterations if the validation score is not improving enough.
    The subset set aside for validation, `validation_fraction`, is used to calculate
    the validation score. The `tol`**hyperparameter is used to decide how much improvement
    we must consider as being enough.**
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '**The `fit` method in the gradient boosting algorithm accepts a callback function
    that is called after each iteration. It can also be used to set a custom condition
    to stop the training process based on it. Furthermore, It can be used for monitoring
    or for any other customizations you need. This callback function is called with
    three parameters: the order of the current iteration (`n`), an instance of gradient
    boosting (`estimator`), and its settings (`params`). To demonstrate how this callback
    function works, let''s build a function to change the learning rate to `0.01`
    for one iteration at every `10` iterations, and keep it at `0.1` for the remaining
    iterations, as shown:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, we use our `lr_changer` function, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, if we print the deviance as we usually do, we will see how after every
    10^(th) iteration, the calculated loss jumps due to the learning rate changes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b94a9c30-3e42-4298-8ec8-63bdeb95e55e.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: What I've just done is pretty useless, but it demonstrates the possibilities
    you have at hand. For example, you can borrow ideas such as the adaptive learning
    rate and the momentum from the solvers used in the neural networks and incorporate
    them here using this callback function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Regression ranges
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"I try to be a realist and not a pessimist or an optimist."'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: –Yuval Noah Harari
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: One last gem that gradient boosting regression offers to us is regression ranges.
    These are very useful in quantifying the uncertainty of your predictions.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'We try our best to make our predictions exactly the same as the actual data.
    Nevertheless, our data can still be noisy, or the features used may not capture
    the whole truth. Take the following example:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '| **x[1]** | **x[2]** | **y** |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 10 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 50 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 20 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 22 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: Consider a new sample with *x[1]* = 0 and *x[2]* = 0\. We already have three
    training examples with the exact same features, so what would the predicted *y*
    value be for this new sample? If a squared loss function is used during the training,
    then the predicted target will be close to `17.3`, which is the mean of the three
    corresponding targets*(`10`, `20`, and `22`). Now, if MAE is used, then the predicted
    target will be closer to `22`, which is the median (50^(th) percentile) of the
    three corresponding targets. Rather than the 50^(th) percentile, we can use any
    other percentiles when a **quantile** loss function is used. So, to achieve regression
    ranges, we can use two regressors with two different quantiles as the upper and
    lower bounds of our range.*
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '*Although the regression ranges work regardless of the dimensionality of the
    data at hand, the format of the page has forced us to come up with a two-dimensional
    example for more clarity. The following code creates 400samples to play with:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Here is a scatter plot of the generated *y* versus *x* values:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3c2d69c-05d6-413b-af26-4bef741ba82b.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can train two regressors with the 10^(th) and 90^(th) percentiles as
    our range boundaries and plot those regression boundaries, along with our scattered
    data points:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can see that the majority of the points fall within the range. Ideally,
    we would expect 80% of the points to fall in the **90**-**100** range:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f7f891c-eae2-48f9-a5cd-33ab625fe6ae.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'We can now use the same strategy to predict automobile prices:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, we can check what percentage of our test set falls within the regression
    range:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Calculating the mean of `df_pred_range['Actuals in Range?']` gives us `0.49`,
    which is very close to the `0.5` value we expected. Obviously, we can use wider
    or narrower ranges, depending on our use case. If your model is going to be used
    to help car owners sell their cars, you may need to give them reasonable ranges,
    since telling someone that they can sell their car for any price between $5 and
    $30,000 is pretty accurate yet useless advice. Sometimes, a less accurate yet
    useful model is better than an accurate and useless one.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Another boosting algorithm that is not used as much nowadays is the AdaBoost
    algorithm. We will briefly explore it in the next section for the sake of completeness.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Using AdaBoost ensembles
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an AdaBoost ensemble, the mistakes made in each iteration are used to alter
    the weights of the training samples for the following iterations. As in the boosting
    meta-estimator, this method can also use any other estimators instead of the decision
    trees used by default. Here, we have used it with its default estimators on the
    Automobile dataset:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The AdaBoost meta-estimator also has a `staged_predict` method, which allows
    us to plot the improvement in the training or test loss after each iteration.
    Here is the code for plotting the test error:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here is a plot for the calculated loss after each iteration:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b048dd9-613c-4fe3-94e2-649a975e5d87.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: As in the other ensembles, the more estimators we add, the more accurate it
    becomes. Once we start to overfit, we should be able to stop. That's why having
    a validation sample is essential in knowing when to stop. I used the test set
    for demonstration here, but in practice, the test sample should be kept aside
    and a validation set should be used instead.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Exploring more ensembles
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main ensemble techniques are the ones we have seen so far. The following
    ones are also good to know about and can be useful for some peculiar cases.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Voting ensembles
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, we have a number of good estimators, each with its own mistakes.
    Our objective is not to mitigate their bias or variance, but to combine their
    predictions in the hope that they don't all make the same mistakes. In these cases,
    `VotingClassifier` and `VotingRegressor` could be used. You can give a higher
    preference to some estimators versus the others by adjusting the `weights` hyperparameter.
    `VotingClassifier` has different voting strategies, depending on whether the predicted
    class labels are to be used or whether the predicted probabilities should be used
    instead.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Stacking ensembles
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rather than voting, you can combine the predictions of multiple estimators by
    adding an extra one that uses their predictions as input. This strategy is known
    as **stacking**. The inputs of the final estimator can be limited to the predictions
    of the previous estimators, or it can be a combination of their predictions and
    the original training data. To avoid overfitting, the final estimators are usually
    trained using cross-validation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Random tree embedding
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen how the trees are capable of capturing the non-linearities in the
    data. So, if we still want to use a simpler algorithm, we can just use the trees
    to transform the data and leave the prediction for the simple algorithm to do.
    When building a tree, each data point falls into one of its leaves. Therefore,
    the IDs of the leaves can be used to represent the different data points. If we
    build multiple trees, then each data point can be represented by the ID of the
    leaf it fell on in each tree. These leaves, IDs can be used as our new features
    and can be fed into a simpler estimator. This kind of embedding is useful for
    feature compression and allows linear models to capture the non-linearities in
    the data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we use an unsupervised `RandomTreesEmbedding` method to transform our
    automobile features, and then use the transformed features in a `Ridge` regression:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'From the preceding block of code, we can observe the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: This approach is not limited to `RandomTreesEmbedding`.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-boosted trees can also be used to transform the data for a downstream
    estimator to use.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both `GradientBoostingRegressor` and `GradientBoostingClassifier` have an `apply`
    function, which can be used for feature transformation.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how algorithms benefit from being assembled in the form
    of ensembles. We learned how these ensembles can mitigate the bias versus variance
    trade-off.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with heterogeneous data, the gradient boosting and random forest
    algorithms are my first two choices for classification and regression. They do
    not require any sophisticated data preparation, thanks to their dependence on
    trees. They are able to deal with non-linear data and capture feature interactions.
    Above all, the tuning of their hyperparameters is straightforward.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The more estimators in each method, the better, and you should not worry so
    much about them overfitting. As for gradient boosting, you can pick a lower learning
    rate if you can afford to have more trees. In addition to these hyperparameters,
    the depth of the trees in each of the two algorithms should be tuned via trail
    and error and cross-validation. Since the two algorithms come from different sides
    of the bias-variance spectrum, you may initially aim for forests with big trees
    that you can prune later on. Conversely, you can start with shallow trees and
    rely on your gradient-boosting meta-estimator to boost them.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: So far in this book, we have predicted a single target at a time. Here, we predicted
    the prices of automobiles and that's it. In the next chapter, we will see how
    to predict multiple targets at a time. Furthermore, when our aim is to use the
    probabilities given by a classifier, having a calibrated classifier is paramount.
    We can have a better estimation of our risks if we have probabilities that we
    trust. Thus, calibrating a classifier is going to be another topic covered in
    the next chapter.*********
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL

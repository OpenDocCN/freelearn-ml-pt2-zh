- en: Ensembles – When One Model Is Not Enough
  prefs: []
  type: TYPE_NORMAL
- en: In the previous three chapters, we saw how **neural networks** help directly
    and indirectly in solving natural language understanding and image processing
    problems. This is because neural networks are proven to work well with **homogeneous
    data**; that is, if all the input features are of the same breed—pixels, words,
    characters, and so on. On the other hand, when it comes to **heterogeneous****data**,
    it is the **ensemble****methods** that are known to shine. They are well suited
    to deal with heterogeneous data—for example, where one column contains users'
    ages, the other has their incomes, and a third has their city of residence.
  prefs: []
  type: TYPE_NORMAL
- en: You can view ensemble estimators as meta-estimators; they are made up of multiple
    instances of other estimators. The way they combine their underlying estimators
    is what differentiates between the different ensemble methods—for example, the
    **bagging** versus the **boosting** methods. In this chapter, we are going to
    look at these methods in detail and understand their underlying theory. We will
    also learn how to diagnose our own models and understand why they make certain
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: As always, I would also like to seize the opportunity to shed light on general
    machine learning concepts while dissecting each individual algorithm. In this
    chapter, we will see how to handle the estimators' uncertainties using the classifiers'
    probabilities and the regression ranges.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The motivation behind ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Averaging/bagging ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression ranges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ROC curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area under the curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voting and stacking ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answering the question why ensembles?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main idea behind ensembles is to combine multiple estimators so that they
    make better predictions than a single estimator. However, you should not expect
    the mere combination of multiple estimators to just lead to better results. The
    combined predictions of multiple estimators who make the exact same mistakes will
    be as wrong as each individual estimator in the group. Therefore, it is helpful
    to think of the possible ways to mitigate the mistakes that individual estimators
    make. To do so, we have to revisit our old friend the bias and variance dichotomy.
    We will meet few machine learning teachers better than this pair.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall from [Chapter 2](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml),
    *Making Decisions with Trees*, when we allowed our decision trees to grow as much
    as they can, they tended to fit the training data like a glove but failed to generalize
    to newer data points. We referred to this as overfitting, and we have seen the
    same behavior with unregularized linear models and with a small number of nearest
    neighbors. Conversely, aggressively restricting the growth of trees, limiting
    the number of the features in linear models, and asking too many neighbors to
    vote caused the models to become biased and underfit the data at hand. So, we
    had to tread a thin line between trying to find the optimum balance between the
    bias-variance and the underfitting-overfitting dichotomies.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we are going to follow a different approach. We will
    deal with the bias-variance dichotomy as a continuous scale, starting from one
    side of this scale and using the concept of *ensemble* to move toward the other
    side. In the next section, we are going to start by looking at high-variance estimators
    and averaging their results to reduce their variance. Later on, we will start
    from the other side and use the concept of boosting to reduce the estimators'
    biases.
  prefs: []
  type: TYPE_NORMAL
- en: Combining multiple estimators via averaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"To derive the most useful information from multiple sources of evidence, you
    should always try to make these sources independent of each other."'
  prefs: []
  type: TYPE_NORMAL
- en: – Daniel Kahneman
  prefs: []
  type: TYPE_NORMAL
- en: If a single fully grown decision tree overfits, and if having many voters in
    the nearest neighbors algorithm has an opposite effect, then why not combine the
    two concepts? Rather than having a single tree, let's have a forest that combines
    the predictions of each tree in it. Nevertheless, we do not want all the trees
    in our forest to be identical; we would love them to be as diverse as possible.
    The **bagging** and random forest meta-estimators are the most common examples
    here. To achieve diversity, they make sure that each one of the individual estimators
    they use is trained on a random subset of the training data—hence the *random*
    prefix in random forest. Each time a random sample is drawn, it can be done with
    replacement (**bootstrapping**) or without replacement (**pasting**). The term
    bagging stands for **bootstrap aggregation** as the estimators draw their samples
    with replacement. Furthermore, for even more diversity, the ensembles can assure
    that each tree sees a random subset of the training features.
  prefs: []
  type: TYPE_NORMAL
- en: Both ensembles use decision tree estimators by default, but the **bagging**
    ensemble can be reconfigured to use any other estimator. Ideally, we would like
    to use high-variance estimators. The decisions made by the individual estimators
    are combined via voting or averaging.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting multiple biased estimators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"If I have seen further than others, it is by standing upon the shoulders of
    giants."'
  prefs: []
  type: TYPE_NORMAL
- en: –Isaac Newton
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to fully grown trees, a shallow tree tends to be biased. Boosting
    a biased estimator is commonly performed via **AdaBoost** or **gradient boosting**.
    The AdaBoost meta-estimator starts with a weak or biased estimator, then each
    consequent estimator learns from the mistakes made by its predecessors. We saw
    in [Chapter 2](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml), *Making Decisions
    with Trees*, that we can give each individual training sample a different weight
    so that the estimators can give more emphasis to some samples versus others. In
    **AdaBoost**, erroneous predictions made by the preceding estimators are given
    more weight for their successors to pay more attention to.
  prefs: []
  type: TYPE_NORMAL
- en: The **gradient boosting** meta-estimator follows a slightly different approach.
    It starts with a biased estimator, computes its loss function, then builds each
    consequent estimator to minimize the loss function of its predecessors. As we
    saw earlier, gradient descent always comes in handy when iteratively minimizing
    loss functions, hence the *gradient* prefix in the name of the gradient boosting
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the iterative nature of the two ensembles, they both have a learning
    rate to control their learning speed and to make sure they don't miss the local
    minima when converging. Like the **bagging** algorithm, **AdaBoost** is not limited
    to decision trees as its base estimator.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good idea about the different ensemble methods, we can use
    real-life data to demonstrate how they work in practice. Each of the ensemble
    methods described here can be used for classification and regression. The classifier
    and regressor hyperparameters are almost identical for each ensemble. Therefore,
    I will pick a regression problem to demonstrate each algorithm and briefly show
    the classification capabilities of the random forest and gradient boosting algorithms
    since they are the most commonly used ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to download a dataset prepared by the**University
    of California, Irvine** (**UCI**). It contains 201 samples for different cars,
    along with their prices. We will be using this dataset in a later section to predict
    the car prices via regression.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the UCI Automobile dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Automobile dataset was created by Jeffrey C. Schlimmer and published in
    UCI''s machine learning repository. It contains information about 201 automobiles,
    along with their prices. The names of the features are missing. Nevertheless,
    I could get them from the dataset''s description ([http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names](http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names)).
    So, we can start by seeing the URL and the feature names, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, we use the following code to download our data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It is mentioned in the dataset's description that missing values are replaced
    with a question mark. To make things more Pythonic, we set `na_values` to `'?'`
    to replace these question marks with NumPy's **Not a Number**(**NaN**).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can perform our **Exploratory Data Analysis** (**EDA**), check the
    percentages of the missing values, and see how to deal with them.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can check which columns have the most missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the price is our target value, we can just ignore the four records where
    the prices are unknown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the remaining features, I''d say let''s drop the`normalized-losses`
    column since 41 of its values are missing. Later on, we will use the data imputation
    techniques to deal with the other columns with fewer missing values. You can drop
    the `normalized-losses` column using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we have a data frame with all the required features and their
    names. Next, we want to split the data into training and test sets, and then prepare
    our features. The different feature types require different preparations. You
    may need to separately scale the numerical features and encode the categorical
    ones. So, it is good practice to be able to differentiate between the numerical
    and the categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Differentiating between numerical features and categorical ones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we are going to create a dictionary to separately list the numerical
    and categorical features. We will also make a combined list of the two, and provide
    the name of the target column, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'By doing so, you can deal with the columns differently. Furthermore, just for
    my own sanity and to notprint too many zeros in the future, I rescaled the prices
    to be in thousands, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also display certain features separately. Here, we print a random sample,
    where just the categorical features are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the resulting rows. I set `random_state` to `42` to make sure we all
    get the same random rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/806e2ee8-cfdf-4819-b53d-a58a4a38dd61.png)'
  prefs: []
  type: TYPE_IMG
- en: All other transformations, such as scaling, imputing, and encoding, should be
    done after splitting the data into training and test sets. That way, we can ensure
    that no information is leaked from the test set into the training samples.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into training and test sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we keep 25% of the data for testing and use the rest for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use the information from the previous section to create our `x`
    and `y` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, with regression tasks, it is handy to understand the distribution
    of the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'A histogram is usually a good choice for understanding distributions, as seen
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e1e4eaa-47ca-4d98-a27d-299eb324a237.png)'
  prefs: []
  type: TYPE_IMG
- en: We may come back to this distribution later to put our regressor's mean error
    in perspective. Additionally, you can use this range for sanity checks. For example,
    if you know that all the prices you have seen fall in the range of 5,000 to 45,000,
    you may decide when to put your model in production to fire an alert any time
    it returns prices far from this range.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing the missing values and encoding the categorical features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before bringing our ensembles to action, we need to make sure we do not have
    null values in our data. We will replace the missing values with the most frequent
    value in each column using the `SimpleImputer` function from[Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml),
    *Preparing Your Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have already seen me complain many times about the scikit-learn transformers,
    which do not respect the column names and insist on converting the input data
    frames into NumPy arrays. To stop myself from complaining again, let me solve
    my itch by using the following `ColumnNamesKeeper` class. Whenever I wrap it around
    a transformer, it will make sure all the data frames are kept unharmed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it mainly saves the column name when the `fit` method is called.
    Then, we can use the saved names to recreate the data frames after the transformation
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: The code for `ColumnNamesKeeper` can be simplified further by inheriting from
    `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin`. You can check
    the source code of any of the library's built-in transformers if you are willing
    to write more scikit-learn-friendly transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I can call `SimpleImputer` again while preserving `x_train` and `x_test`
    as data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We learned in [Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml), *Preparing
    Your Data*, that `OrdinalEncoder`**is recommended for tree-based algorithms, in
    addition to any other non-linear algorithms. The `category_encoders` library doesn't
    mess with the column names, and so we can use `OrdinalEncoder` without the need
    for`ColumnNamesKeeper` this time. In the following code snippet, we also specify
    which columns to encode (the categorical columns) and which to keep unchanged
    (the remaining ones):**
  prefs: []
  type: TYPE_NORMAL
- en: '**[PRE15]'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to `OrdinalEncoder`, you can also test the encoders mentioned in
    the target encoding in [Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)*,
    Preparing Your Data*. They, too, are meant to be used with the algorithms explained
    in this chapter. In the next section, we are going to use the random forest algorithm
    with the data we have just prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The random forest algorithm is going to be the first ensemble to deal with
    here. It''s an easy-to-grasp algorithm with straightforward hyperparameters. Nevertheless,
    as we usually do, we will start by training the algorithm using its default values,
    as follows, then explain its hyperparameters after that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Since each tree is independent of the others, I set `n_jobs` to `-1` to use
    my multiple processors to train the trees in parallel. Once they are trained and
    the predictions are obtained, we can print the following accuracy metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The average car price is 13,400\. So, a **Mean Absolute Error** (**MAE***)*
    of `1.35` seems reasonable. As for the **Mean Squared Error** (**MSE**), it makes
    sense to use its square root to keep it in the same units as the MAE. In brief,
    given the high R² score and the low errors, the algorithm seems to perform well
    with its default values. Furthermore, you can plot the errors to get a better
    understanding of the model''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'I''ve excluded some of the formatting lines to keep the code concise. In the
    end, we get the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d5c005a-a4d0-47be-b420-cdd3984f4ac7.png)'
  prefs: []
  type: TYPE_IMG
- en: By plotting the predictions versus the actuals, we can make sure that the models
    don't systematically overestimate or underestimate. This is shown via the 45^o
    slope of the scattered points on the left. A lower slope for the scattered points
    would have systematically reflected an underestimation. Having the scattered points
    aligned on a straight line assures us that there aren't non-linearities that the
    model couldn't capture. The histogram to the right shows that most of the errors
    are below 2,000\. It is good to understand what mean and maximum errors you can
    expect to get in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the effect of the number of trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, each tree is trained on a random sample from the training data.
    This is achieved by setting the `bootstrap` hyperparameter to `True`. In bootstrap
    sampling, a sample may be used during training more than once, while another sample
    may not be used at all.
  prefs: []
  type: TYPE_NORMAL
- en: When `max_samples` is kept as `None`, each tree is trained on a random sample
    of a size that is equal to the entire training data size. You can set `max_samples`
    to a fraction that is less than 1, then each tree is trained on a smaller random
    sub-sample. Similarly, we can set `max_features` to a fraction that is less than
    1 to make sure each tree uses a random subset of the available features. These
    parameters help each tree to have its own personality and to ensure the diversity
    of the forest. To put it more formally, these parameters increase the variance
    of each individual tree. So, it is advised to have as many trees as possible to
    reduce the variance we have just introduced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we compare three forests, with a different number of trees in each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can plot the MAE for each forest to see the merits of having more
    trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/476a677f-6df2-4dea-8fa5-d6a7545117d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, we have just encountered a new set of hyperparameters to tune `bootstrap`,
    `max_features`, and `max_samples`. So, it makes sense to apply cross-validation
    for hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the effect of each training feature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once a random forest is trained, we can list the training features, along with
    their importance. As usual, we put the outcome in a data frame by using the column
    names and the `feature_importances_` attribute, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the resulting data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c526a3c-1ceb-4c52-87a5-4a83419a4181.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Unlike with linear models, all the values here are positive. This is because
    these values only show the importance of each feature, regardless of whether it
    is positively or negatively correlated with the target. This is common for decision
    trees, as well as for tree-based ensembles. Thus, we can use **Partial Dependence
    Plots**(**PDPs**)to show the relationship between the target and the different
    features. Here, we only plot it for the top six features according to their importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting graphs are easier to read, especially when the relationship between
    the target and the features is non-linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3eadf45-3663-4c33-8c33-b1c380f5ddf6.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now tell that cars with bigger engines, more horsepower, and less mileage
    per gallon tend to be more expensive.
  prefs: []
  type: TYPE_NORMAL
- en: PDPs are not just useful for ensemble methods, but also for any other complex
    non-linear model. Despite the fact the neural networks have coefficients for each
    layer, the PDP is essential in understanding the network as a whole. Furthermore,
    you can also understand the interaction between the different feature pairs by
    passing the list of features as a list tuples, with a pair of features in each
    tuple.
  prefs: []
  type: TYPE_NORMAL
- en: Using random forest for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate the random forest classifier, we are going to use a synthetic
    dataset. We first create the dataset using the built-in `make_hastie_10_2` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This previous code snippet creates a random dataset. I set `random_state` to
    a fixed number to make sure we both get the same random data. Now, we can split
    the resulting data into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Then, to evaluate the classifier, we are going to introduce a new concept called
    the **Receiver Operating Characteristic** (**ROC**) curve in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"Probability is expectation founded upon partial knowledge. A perfect acquaintance
    with all the circumstances affecting the occurrence of an event would change expectation
    into certainty, and leave neither room nor demand for a theory of probabilities."'
  prefs: []
  type: TYPE_NORMAL
- en: – George Boole (Boolean data types are named after him)
  prefs: []
  type: TYPE_NORMAL
- en: In a classification problem, the classifier assigns probabilities to each sample
    to reflect how likely it is that each sample belongs to a certain class. We get
    these probabilities via the classifier's `predict_proba()` method. The `predict()`
    method is typically a wrapper on top of the `predict_proba()` method. In a binary-classification
    problem, it assigns each sample to a specific class if the probability of it belonging
    to the class is above 50%. In practice, we may not always want to stick to this
    50% threshold, especially as different thresholds usually change the **T****rue
    Positive Rates** (**TPRs**) and **False Positive Rates** (**FPRs**) for each class.
    So, you can choose a different threshold to optimize for a desired TPR.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to decide which threshold suits your needs is to use a ROC curve.
    This helps us see the TPR and FPR for each threshold. To create this curve, we
    will train our random forest classifier on the synthetic dataset we have just
    created, but get the classifier''s probabilities this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can calculate the TPR and FPR for each threshold, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s stop for a moment to explain what TPR and FPR mean:'
  prefs: []
  type: TYPE_NORMAL
- en: The**TPR**, also known as **recall** or **sensitivity**, is calculated as the
    number of **True Positive** (**TP**) cases divided by all the positive cases;
    that is, ![](img/d14315f8-a807-4550-9467-be22f09a947b.png), where *FN* is the
    positive cases falsely classified as negative (false negatives).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **True Negative Rates** (**TNR**), also known as **specificity**, is calculated
    as the number of **True Negative** (**TN**) cases divided by all the negative
    cases; that is, ![](img/edcaf90b-1764-47ed-84bc-e42e16bfc779.png), where *FP*
    is the negative cases falsely classified as positive (false positives).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **FPR** is defined as 1 minus TNR; that is, ![](img/a73fab77-1996-4d38-88fc-669a544b5249.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **False Negative Rate** (**FNR**) is defined as 1 minus TPR; that is, ![](img/1d8f94e7-78b4-4197-8feb-678071889616.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can put the calculated TPR and FPR for our dataset into the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0abc76af-774e-4e2d-99b5-294fb69ab66c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even better than a table, we can plot them into a graph using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'I''ve omitted the graph''s styling code for the sake of brevity. I also added
    a 45^o line and the **Area Under the Curve** (**AUC**), which I will explain in
    a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c989db9d-5d8a-4d47-a9a1-8d3b031676ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A classifier that randomly assigns each sample to a certain class will have
    a ROC curve that looks like the dashed 45^o line. Any improvement over this will
    make the curve more convex upward. Obviously, random forest''s ROC curve is better
    than chance. An optimum classifier will touch the upper-left corner of the graph.
    Therefore, the AUC can be used to reflect how good the classifier is. An area
    above `0.5` is better than chance, and an area of `1.0` is the best possible value.
    We typically expect values between `0.5` and `1.0`. Here, we got an AUC of `0.94`.
    The AUC can be calculated using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the ROC and AUC to compare two classifiers. Here, I trained
    the random forest classifier with the `bootstrap` hyperparameter set to `True`
    and compared it to the same classifier when `bootstrap` was set to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0457044f-36f5-4ba0-a1ab-4ae6c68f4a9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'No wonder the `bootstrap` hyperparameter is set to `True` by default—it gives
    better results. Now, you have seen how to use random forest algorithms to solve
    classification and regression problems. In the next section, we are going to explain
    a similar ensemble: the bagging ensemble.'
  prefs: []
  type: TYPE_NORMAL
- en: Using bagging regressors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will go back to the Automobile dataset as we are going to use the **bagging
    regressor** this time. The bagging meta-estimator is very similar to random forest.
    It is built of multiple estimators, each one trained on a random subset of the
    data using a bootstrap sampling method. The key difference here is that although
    decision trees are used as the base estimators by default, any other estimator
    can be used as well. Out of curiosity, let's use the **K-Nearest Neighbors** (**KNN**)
    regressor as our base estimator this time. However, we need to prepare the data
    to suit the new regressor's needs.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a mixture of numerical and categorical features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is recommended to put all features on the same scale when using distance-based
    algorithms such as KNN*.* Otherwise, the effect of the features with higher magnitudes
    on the distance metric will overshadow the other features. As we have a mixture
    of numerical and categorical features here, we need to create two parallel pipelines
    to prepare each feature set separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a top-level view of our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/306f62b0-7a03-40b8-bbd2-30a85bba31f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we start by building the four transformers in our pipelines: `Imputer`,
    `Scaler`**,** and `OneHotEncoder`. We also wrap them in `ColumnNamesKeeper`, which
    we created earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we put them into two parallel pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we concatenate the outputs of the pipelines for both the training
    and the test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: At this point, we are ready to build our bagged KNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Combining KNN estimators using a bagging meta-estimator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`BaggingRegressor` has a `base_estimator` hyperparameter, where you can set
    the estimators you want to use. Here, `KNeighborsRegressor` is used with a single
    neighbor. Since we are aggregating multiple estimators to reduce their variance,
    it makes sense to have a high variance estimator in the first place, hence the
    small number of neighbors here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This new setup gives us an MAE of `1.8`. We can stop here, or we may decide
    to improve the ensemble's performance by tuning its big array of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we can try different estimators other than KNN, each with its
    own hyperparameters. Then, the bagging ensemble also has its own hyperparameters.
    We can change the number of estimators via `n_estimators`. Then, we can decide
    whether to use the entire training set or a random subset of it for each estimator
    via `max_samples`. Similarly, we can also pick a random subset of the columns
    to use for each estimator to use via `max_features`. The choice of whether to
    use bootstrapping for the rows and the columns can be made via the `bootstrap`
    and `bootstrap_features`hyperparameters, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, since each estimator is trained separately, we can use a machine with
    a high number of CPUs and parallelize the training process by setting `n_jobs`
    to `-1`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have experienced two versions of the averaging ensembles, it is
    time to check their boosting counterparts. We will start with the gradient boosting
    ensemble, then move to the AdaBoost ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Using gradient boosting to predict automobile prices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If I were ever stranded on a desert island and had to pick one algorithm to
    take with me, I'd definitely chose the gradient boosting ensemble! It has proven
    to work very well on many classification and regression problems. We are going
    to use it with the same automobile data from the previous sections. The classifier
    and the regressor versions of this ensemble share the exact same hyperparameters,
    except for the loss functions they use. This means that everything we are going
    to learn here will be useful to us whenever we decide to use gradient boosting
    ensembles for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the averaging ensembles we have seen so far, the boosting ensembles
    build their estimators iteratively. The knowledge learned from the initial ensemble
    is used to build its successors. This is the main downside of boosting ensembles,
    where parallelism is unfeasible. Putting parallelism aside, this iterative nature
    of the ensemble calls for a learning rate to be set. This helps the gradient descent
    algorithm reach the loss function''s minima easily. Here, we use 500 trees, each
    with a maximum of 3 nodes, and a learning rate of `0.01`. Furthermore, the **Least
    Squares** (**LS**) loss is used here; think MSE. More on the available loss functions
    in a moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This new algorithm gives us the following performance on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this setting gave a lower MSE compared to random forest, while
    random forest had a better MAE. Another loss function that the gradient boosting**regressor
    can use is **Least Absolute Deviation** (**LAD**); think MAE, this time. LAD may
    help when dealing with outliers, and it can sometimes reduce the model's MAE performance
    on the test set. Nevertheless, it did not improve the MAE for the dataset at hand.
    We also have a percentile (quantile) loss, but before going deeper into the supported
    loss functions, we need to learn how to diagnose the learning process.**
  prefs: []
  type: TYPE_NORMAL
- en: '**The main hyperparameters to set here are the number of trees, the depth of
    the trees, the learning rate, and the loss function. As a rule of thumb, you should
    aim for a higher number of trees and a low learning rate. As we will see in a
    bit, these two hyperparameters are inversely proportional to each other. Controlling
    the depth of your trees is purely dependent on your data. In general, we need
    to have shallow trees and let boosting empower them. Nevertheless, the depth of
    the tree controls the number of feature interactions we want to capture. In a
    stub (a tree with a single split), only one feature can be learned at a time.
    A deeper tree resembles a nested `if` condition where a few more features are
    at play each time. I usually start with`max_depth` set to around `3` and `5` and
    tune it along the way.'
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the learning deviance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With each additional estimator, we expect the algorithm to learn more and the
    loss to decrease. Yet, at some point, the additional estimators will keep overfitting
    on the training data while not offering much improvement for the test data.
  prefs: []
  type: TYPE_NORMAL
- en: To have a clear picture, we need to plot the calculated loss with each additional
    estimator for both the training and test sets. As for the training loss, it is
    saved by the gradient boosting meta-estimator into its `loss_` attribute. For
    the test loss, we can use the meta-estimator's `staged_predict()` methods. This
    method can be used for a given dataset to make predictions for each intermediate
    iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have multiple loss functions to choose from, gradient boosting also
    provides a`loss_()` method, which calculates the loss for us based on the loss
    function used. Here, we create a new function to calculate the training and test
    errors for each iteration and put them into a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Since we are going to use an LS loss here, you can simply replace the `estimator.loss_()`
    method with `mean_squared_error()` and get the exact same results. But let's keep
    the `estimator.loss_()` function for a more versatile and reusable code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we train our gradient boosting regressor, as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use the trained model, along with the test set, to plot the training
    and test learning deviance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code gives us the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3779f19-026c-41c1-b736-d59e91380aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: The beauty of this graph is that it tells us that the improvements on the test
    set stopped after `120` estimators or so, despite the continuous improvement in
    the training set; that is, it started to overfit. Furthermore, we can use this
    graph to understand the effect of a chosen learning rate, as we did in *[Chapter
    7](7559b34f-080c-485d-b4bc-5f22580fc1d1.xhtml)*, *Neural Networks - Here comes
    the Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the learning rate settings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rather than training one model, we will train three gradient boosting regressors
    this time, each with a different learning rate. Then, we will plot the deviance
    graph for each one side by side, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e63c903-27fd-44ab-a4d1-315f8b478d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: As with other gradient descent-based models, a high learning rate causes the
    estimator to overshoot and miss the local minima. We can see this in the first
    graph where no improvements are seen despite the consecutive iterations. The learning
    rates in the second and third graphs seem reasonable. In comparison, the learning
    rate in the third graph seems to be too slow for the model to converge in 500
    iterations. You may then decide to increase the number of estimators for the third
    model to allow it to converge.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned from the bagging ensembles that using a random training sample
    with each estimator may help with overfitting. In the next section, we are going
    to see whether the same approach can also help the boosting ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: Using different sample sizes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have been using the entire training set for each iteration. This time, we
    are going to train three gradient boosting regressors, each with a different subsample
    size, and plot their deviance graphs as before. We will use a fixed learning rate
    of `0.01` and the LAD**as our loss function, as shown:**
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/0b4eacd5-f8e0-41a7-9a38-895ae96e13a6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: In the first graph, the entire training sample is used for each iteration. So,
    the training loss did not fluctuate as much as in the other two graphs. Nevertheless,
    the sampling used in the second model allowed it to reach a better test score,
    despite its noisy loss graph. This was similarly the case for the third model,
    but with a slightly larger final error.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping earlier and adapting the learning rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `n_iter_no_change` hyperparameter is used to stop the training process after
    a certain number of iterations if the validation score is not improving enough.
    The subset set aside for validation, `validation_fraction`, is used to calculate
    the validation score. The `tol`**hyperparameter is used to decide how much improvement
    we must consider as being enough.**
  prefs: []
  type: TYPE_NORMAL
- en: '**The `fit` method in the gradient boosting algorithm accepts a callback function
    that is called after each iteration. It can also be used to set a custom condition
    to stop the training process based on it. Furthermore, It can be used for monitoring
    or for any other customizations you need. This callback function is called with
    three parameters: the order of the current iteration (`n`), an instance of gradient
    boosting (`estimator`), and its settings (`params`). To demonstrate how this callback
    function works, let''s build a function to change the learning rate to `0.01`
    for one iteration at every `10` iterations, and keep it at `0.1` for the remaining
    iterations, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use our `lr_changer` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we print the deviance as we usually do, we will see how after every
    10^(th) iteration, the calculated loss jumps due to the learning rate changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b94a9c30-3e42-4298-8ec8-63bdeb95e55e.png)'
  prefs: []
  type: TYPE_IMG
- en: What I've just done is pretty useless, but it demonstrates the possibilities
    you have at hand. For example, you can borrow ideas such as the adaptive learning
    rate and the momentum from the solvers used in the neural networks and incorporate
    them here using this callback function.
  prefs: []
  type: TYPE_NORMAL
- en: Regression ranges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '"I try to be a realist and not a pessimist or an optimist."'
  prefs: []
  type: TYPE_NORMAL
- en: –Yuval Noah Harari
  prefs: []
  type: TYPE_NORMAL
- en: One last gem that gradient boosting regression offers to us is regression ranges.
    These are very useful in quantifying the uncertainty of your predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We try our best to make our predictions exactly the same as the actual data.
    Nevertheless, our data can still be noisy, or the features used may not capture
    the whole truth. Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **x[1]** | **x[2]** | **y** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 22 |'
  prefs: []
  type: TYPE_TB
- en: Consider a new sample with *x[1]* = 0 and *x[2]* = 0\. We already have three
    training examples with the exact same features, so what would the predicted *y*
    value be for this new sample? If a squared loss function is used during the training,
    then the predicted target will be close to `17.3`, which is the mean of the three
    corresponding targets*(`10`, `20`, and `22`). Now, if MAE is used, then the predicted
    target will be closer to `22`, which is the median (50^(th) percentile) of the
    three corresponding targets. Rather than the 50^(th) percentile, we can use any
    other percentiles when a **quantile** loss function is used. So, to achieve regression
    ranges, we can use two regressors with two different quantiles as the upper and
    lower bounds of our range.*
  prefs: []
  type: TYPE_NORMAL
- en: '*Although the regression ranges work regardless of the dimensionality of the
    data at hand, the format of the page has forced us to come up with a two-dimensional
    example for more clarity. The following code creates 400samples to play with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a scatter plot of the generated *y* versus *x* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3c2d69c-05d6-413b-af26-4bef741ba82b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can train two regressors with the 10^(th) and 90^(th) percentiles as
    our range boundaries and plot those regression boundaries, along with our scattered
    data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the majority of the points fall within the range. Ideally,
    we would expect 80% of the points to fall in the **90**-**100** range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f7f891c-eae2-48f9-a5cd-33ab625fe6ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now use the same strategy to predict automobile prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can check what percentage of our test set falls within the regression
    range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Calculating the mean of `df_pred_range['Actuals in Range?']` gives us `0.49`,
    which is very close to the `0.5` value we expected. Obviously, we can use wider
    or narrower ranges, depending on our use case. If your model is going to be used
    to help car owners sell their cars, you may need to give them reasonable ranges,
    since telling someone that they can sell their car for any price between $5 and
    $30,000 is pretty accurate yet useless advice. Sometimes, a less accurate yet
    useful model is better than an accurate and useless one.
  prefs: []
  type: TYPE_NORMAL
- en: Another boosting algorithm that is not used as much nowadays is the AdaBoost
    algorithm. We will briefly explore it in the next section for the sake of completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Using AdaBoost ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an AdaBoost ensemble, the mistakes made in each iteration are used to alter
    the weights of the training samples for the following iterations. As in the boosting
    meta-estimator, this method can also use any other estimators instead of the decision
    trees used by default. Here, we have used it with its default estimators on the
    Automobile dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The AdaBoost meta-estimator also has a `staged_predict` method, which allows
    us to plot the improvement in the training or test loss after each iteration.
    Here is the code for plotting the test error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a plot for the calculated loss after each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b048dd9-613c-4fe3-94e2-649a975e5d87.png)'
  prefs: []
  type: TYPE_IMG
- en: As in the other ensembles, the more estimators we add, the more accurate it
    becomes. Once we start to overfit, we should be able to stop. That's why having
    a validation sample is essential in knowing when to stop. I used the test set
    for demonstration here, but in practice, the test sample should be kept aside
    and a validation set should be used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring more ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main ensemble techniques are the ones we have seen so far. The following
    ones are also good to know about and can be useful for some peculiar cases.
  prefs: []
  type: TYPE_NORMAL
- en: Voting ensembles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, we have a number of good estimators, each with its own mistakes.
    Our objective is not to mitigate their bias or variance, but to combine their
    predictions in the hope that they don't all make the same mistakes. In these cases,
    `VotingClassifier` and `VotingRegressor` could be used. You can give a higher
    preference to some estimators versus the others by adjusting the `weights` hyperparameter.
    `VotingClassifier` has different voting strategies, depending on whether the predicted
    class labels are to be used or whether the predicted probabilities should be used
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking ensembles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rather than voting, you can combine the predictions of multiple estimators by
    adding an extra one that uses their predictions as input. This strategy is known
    as **stacking**. The inputs of the final estimator can be limited to the predictions
    of the previous estimators, or it can be a combination of their predictions and
    the original training data. To avoid overfitting, the final estimators are usually
    trained using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Random tree embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen how the trees are capable of capturing the non-linearities in the
    data. So, if we still want to use a simpler algorithm, we can just use the trees
    to transform the data and leave the prediction for the simple algorithm to do.
    When building a tree, each data point falls into one of its leaves. Therefore,
    the IDs of the leaves can be used to represent the different data points. If we
    build multiple trees, then each data point can be represented by the ID of the
    leaf it fell on in each tree. These leaves, IDs can be used as our new features
    and can be fed into a simpler estimator. This kind of embedding is useful for
    feature compression and allows linear models to capture the non-linearities in
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we use an unsupervised `RandomTreesEmbedding` method to transform our
    automobile features, and then use the transformed features in a `Ridge` regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding block of code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is not limited to `RandomTreesEmbedding`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-boosted trees can also be used to transform the data for a downstream
    estimator to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both `GradientBoostingRegressor` and `GradientBoostingClassifier` have an `apply`
    function, which can be used for feature transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how algorithms benefit from being assembled in the form
    of ensembles. We learned how these ensembles can mitigate the bias versus variance
    trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with heterogeneous data, the gradient boosting and random forest
    algorithms are my first two choices for classification and regression. They do
    not require any sophisticated data preparation, thanks to their dependence on
    trees. They are able to deal with non-linear data and capture feature interactions.
    Above all, the tuning of their hyperparameters is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: The more estimators in each method, the better, and you should not worry so
    much about them overfitting. As for gradient boosting, you can pick a lower learning
    rate if you can afford to have more trees. In addition to these hyperparameters,
    the depth of the trees in each of the two algorithms should be tuned via trail
    and error and cross-validation. Since the two algorithms come from different sides
    of the bias-variance spectrum, you may initially aim for forests with big trees
    that you can prune later on. Conversely, you can start with shallow trees and
    rely on your gradient-boosting meta-estimator to boost them.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this book, we have predicted a single target at a time. Here, we predicted
    the prices of automobiles and that's it. In the next chapter, we will see how
    to predict multiple targets at a time. Furthermore, when our aim is to use the
    probabilities given by a classifier, having a calibrated classifier is paramount.
    We can have a better estimation of our risks if we have probabilities that we
    trust. Thus, calibrating a classifier is going to be another topic covered in
    the next chapter.*********
  prefs: []
  type: TYPE_NORMAL

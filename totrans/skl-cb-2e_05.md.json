["```py\nimport pandas as pd\n```", "```py\ndata_web_address = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n```", "```py\ncolumn_names = ['pregnancy_x', \n 'plasma_con', \n 'blood_pressure', \n 'skin_mm', \n 'insulin', \n 'bmi', \n 'pedigree_func', \n 'age', \n 'target']\n```", "```py\nfeature_names = column_names[:-1]\n```", "```py\nall_data = pd.read_csv(data_web_address , names=column_names)\n```", "```py\nall_data.head()\n```", "```py\n#Is an insulin level of 0 possible? Is a skin_mm of 0 possible?\n```", "```py\nall_data.describe()\n```", "```py\n#The features plasma_con, blood_pressure, skin_mm, insulin, bmi have 0s as values. These values could be physically impossible.\n```", "```py\n#If within a notebook, include this line to visualize within the notebook.\n%matplotlib inline\n\n#The default is bins=10 which is hard to read in the visualization.\nall_data.pregnancy_x.hist(bins=50)\n```", "```py\nall_data.hist(figsize=(15,9),bins=50)\n```", "```py\nall_data.target.value_counts()\n\n0    500\n1    268\nName: target, dtype: int64\n```", "```py\nimport numpy as np\nimport pandas as pd\n\nX = all_data[feature_names]\ny = all_data['target']\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\n```", "```py\ny_pred = lr.predict(X_test)\n```", "```py\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)\n\n0.74675324675324672\n```", "```py\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred,labels = [1,0])\n\narray([[27, 27],\n [12, 88]])\n```", "```py\ny_pred_proba = lr.predict_proba(X_test)\n```", "```py\ny_pred_proba\n\narray([[ 0.87110309,  0.12889691],\n [ 0.83996356,  0.16003644],\n [ 0.81821721,  0.18178279],\n [ 0.73973464,  0.26026536],\n [ 0.80392034,  0.19607966], ...\n```", "```py\npd.Series(y_pred_proba[:,1]).hist()\n```", "```py\nall_data['target'].hist()\n```", "```py\nfrom sklearn.preprocessing import binarize\n```", "```py\narray([[ 0.87110309,  0.12889691],\n [ 0.83996356,  0.16003644]\n```", "```py\ny_pred_default = binarize(y_pred_proba, threshold=0.5)\ny_pred_default\n\narray([[ 1.,  0.],\n [ 1.,  0.],\n [ 1.,  0.],\n [ 1.,  0.],\n [ 1.,  0.],\n [ 1.,  0.]\n```", "```py\ny_pred_default[:,1]\n\narray([ 0.,  0.,  0.,  0.,  0.,  0 ...\n```", "```py\nconfusion_matrix(y_test, y_pred_default[:,1],labels = [1,0])\n\narray([[27, 27],\n [12, 88]])\n```", "```py\ny_pred_low = binarize(y_pred_proba, threshold=0.2)\nconfusion_matrix(y_test, y_pred_low[:,1],labels=[1,0]) #positive class is 1 again\n\narray([[50,  4],\n [48, 52]])\n```", "```py\nfrom __future__ import division #In Python 2.x\nimport matplotlib.pyplot as plt\n\ndef npv_func(th):\n y_pred_low = binarize(y_pred_proba, threshold=th)\n\n second_column = confusion_matrix(y_test, y_pred_low[:,1],labels=[1,0])[:,1]\n npv = second_column[1]/second_column.sum()\n return npv\n\nnpv_func(0.2)\n\n0.9285714285714286\n```", "```py\nths = np.arange(0,1,0.05)\n\nnpvs = []\nfor th in np.arange(0,1.00,0.05):\n npvs.append(npv_func(th)) \n\nplt.plot(ths,npvs)\n```", "```py\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, ths = roc_curve(y_test, y_pred_proba[:,1])\n```", "```py\nplt.plot(ths,tpr)\n```", "```py\ny_pred_th= binarize(y_pred_proba, threshold=0.1)\nconfusion_matrix(y_test, y_pred_th[:,1],labels=[1,0])\n\narray([[54,  0],\n [81, 19]])\n```", "```py\ny_pred_th = binarize(y_pred_proba, threshold=0.146)\nconfusion_matrix(y_test, y_pred_th[:,1],labels=[1,0])\n\narray([[54,  0],\n [67, 33]])\n```", "```py\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, ths = roc_curve(y_test, y_pred_proba[:,1])\nplt.plot(fpr,tpr)\n```", "```py\nfrom sklearn.metrics import auc\n\nauc(fpr,tpr)\n\n0.825185185185\n```", "```py\nimport numpy as np\nimport pandas as pd\ndata_web_address = data_web_address = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\ncolumn_names = ['radius',\n 'texture',\n 'perimeter',\n 'area',\n 'smoothness' \n ,'compactness',\n 'concavity',\n 'concave points', \n 'symmetry',\n 'malignant']\n\nfeature_names = column_names[:-1]\nall_data = pd.read_csv(data_web_address , names=column_names)\n```", "```py\nall_data.dtypes\n\nradius             int64\ntexture            int64\nperimeter          int64\narea               int64\nsmoothness         int64\ncompactness       object\nconcavity          int64\nconcave points     int64\nsymmetry           int64\nmalignant          int64\ndtype: object\n```", "```py\n#changing the state of having cancer to 1, not having cancer to 0\nall_data['malignant'] = all_data['malignant'].astype(np.int)\nall_data['malignant'] = np.where(all_data['malignant'] == 4, 1,0) #4, and now 1 means malignant\nall_data['malignant'].value_counts()\n\n0    458\n1    241\nName: malignant, dtype: int64\n```", "```py\nX = all_data[[col for col in feature_names if col != 'compactness']]\ny = all_data.malignant\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)\n\n#Train and test the Logistic Regression. Use the method #predict_proba().\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred_proba = lr.predict_proba(X_test)\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfpr, tpr, ths = roc_curve(y_test, y_pred_proba[:,1])\n\nauc_score = auc(fpr,tpr)\nplt.plot(fpr,tpr,label=\"AUC Score:\" + str(auc_score))\nplt.xlabel('fpr',fontsize='15')\nplt.ylabel('tpr',fontsize='15')\nplt.legend(loc='best')\n```"]
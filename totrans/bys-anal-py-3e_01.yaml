- en: ChapterÂ 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç« 
- en: Programming Probabilistically
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡ç¼–ç¨‹
- en: Our golems rarely have a physical form, but they too are often made of clay
    living in silicon as computer code. â€“ Richard McElreath
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„é­”åƒå¾ˆå°‘æœ‰å®ä½“å½¢å¼ï¼Œä½†å®ƒä»¬é€šå¸¸ä¹Ÿç”±ç¡…ä¸­ç”Ÿé•¿çš„é»åœŸåšæˆï¼Œä½œä¸ºè®¡ç®—æœºä»£ç å­˜åœ¨ã€‚â€”â€” ç†æŸ¥å¾·Â·éº¦å…‹åŸƒå°”é‡Œæ–¯
- en: Now that we have a very basic understanding of probability theory and Bayesian
    statistics, we are going to learn how to build probabilistic models using computational
    tools. Specifically, we are going to learn about probabilistic programming with
    PyMC [[Abril-Pla etÂ al.](Bibliography.xhtml#Xpymc2023),Â [2023](Bibliography.xhtml#Xpymc2023)].
    The basic idea is that we use code to specify statistical models and then PyMC
    will solve those models for us. We will not need to write Bayesâ€™ theorem in explicit
    form. This is a good strategy for two reasons. First, many models do not lead
    to an analytic closed form, and thus we can only solve those models using numerical
    techniques. Second, modern Bayesian statistics is mainly done by writing code.
    We will be able to see that probabilistic programming offers an effective way
    to build and solve complex models and allows us to focus more on model design,
    evaluation, and interpretation, and less on mathematical or computational details.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹æ¦‚ç‡ç†è®ºå’Œè´å¶æ–¯ç»Ÿè®¡æœ‰äº†éå¸¸åŸºç¡€çš„ç†è§£ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨è®¡ç®—å·¥å…·æ„å»ºæ¦‚ç‡æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ PyMC è¿›è¡Œæ¦‚ç‡ç¼–ç¨‹ [[Abril-Pla
    ç­‰äºº](Bibliography.xhtml#Xpymc2023), [2023](Bibliography.xhtml#Xpymc2023)]ã€‚åŸºæœ¬çš„æ€æƒ³æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡ä»£ç æ¥æŒ‡å®šç»Ÿè®¡æ¨¡å‹ï¼Œç„¶å
    PyMC å°†ä¸ºæˆ‘ä»¬æ±‚è§£è¿™äº›æ¨¡å‹ã€‚æˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼åœ°å†™å‡ºè´å¶æ–¯å®šç†ã€‚è¿™æ ·åšæœ‰ä¸¤ä¸ªåŸå› ã€‚é¦–å…ˆï¼Œè®¸å¤šæ¨¡å‹æ— æ³•å¾—åˆ°è§£æè§£ï¼Œå› æ­¤æˆ‘ä»¬åªèƒ½ä½¿ç”¨æ•°å€¼æ–¹æ³•æ¥æ±‚è§£è¿™äº›æ¨¡å‹ã€‚å…¶æ¬¡ï¼Œç°ä»£è´å¶æ–¯ç»Ÿè®¡ä¸»è¦é€šè¿‡ç¼–å†™ä»£ç æ¥å®Œæˆã€‚æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œæ¦‚ç‡ç¼–ç¨‹æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥æ„å»ºå’Œæ±‚è§£å¤æ‚æ¨¡å‹ï¼Œå®ƒè®©æˆ‘ä»¬èƒ½å¤Ÿæ›´å¤šåœ°å…³æ³¨æ¨¡å‹è®¾è®¡ã€è¯„ä¼°å’Œè§£é‡Šï¼Œè€Œå°‘å…³æ³¨æ•°å­¦æˆ–è®¡ç®—çš„ç»†èŠ‚ã€‚
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Probabilistic programming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¦‚ç‡ç¼–ç¨‹
- en: A PyMC primer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyMC å…¥é—¨
- en: The coin-flipping problem revisited
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡¬å¸æŠ›æ·é—®é¢˜é‡è®¿
- en: Summarizing the posterior
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€»ç»“åéªŒåˆ†å¸ƒ
- en: The Gaussian and Student t models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜æ–¯æ¨¡å‹å’Œå­¦ç”Ÿ t åˆ†å¸ƒæ¨¡å‹
- en: Comparing groups and the effect size
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒä¸åŒç»„åˆ«å’Œæ•ˆåº”å¤§å°
- en: 2.1 Probabilistic programming
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 æ¦‚ç‡ç¼–ç¨‹
- en: Bayesian statistics is conceptually very simple. We have the *knowns* and the
    *unknowns*, and we use Bayesâ€™ theorem to condition the latter on the former. If
    we are lucky, this process will reduce the uncertainty about the *unknowns*. Generally,
    we refer to the *knowns* as **data** and treat it like constants, and the *unknowns*
    as **parameters** and treat them as *random variables*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯ç»Ÿè®¡ä»æ¦‚å¿µä¸Šè®²éå¸¸ç®€å•ã€‚æˆ‘ä»¬æœ‰*å·²çŸ¥*å’Œ*æœªçŸ¥*ï¼Œç„¶åä½¿ç”¨è´å¶æ–¯å®šç†å°†åè€…æ¡ä»¶åŒ–äºå‰è€…ã€‚å¦‚æœæˆ‘ä»¬è¶³å¤Ÿå¹¸è¿ï¼Œè¿™ä¸ªè¿‡ç¨‹å°†å‡å°‘å¯¹*æœªçŸ¥*çš„ uncertaintyã€‚é€šå¸¸ï¼Œæˆ‘ä»¬æŠŠ*å·²çŸ¥*ç§°ä¸º**æ•°æ®**å¹¶å°†å…¶è§†ä¸ºå¸¸æ•°ï¼Œ*æœªçŸ¥*åˆ™ç§°ä¸º**å‚æ•°**å¹¶å°†å…¶è§†ä¸º*éšæœºå˜é‡*ã€‚
- en: Although conceptually simple, fully probabilistic models often lead to analytically
    intractable expressions. For many years, this was a real problem and one of the
    main issues that hindered the adoption of Bayesian methods beyond some niche applications.
    The arrival of the computational era and the development of numerical methods
    that, at least in principle, can be used to solve any inference problem, have
    dramatically transformed the Bayesian data analysis practice. We can think of
    these numerical methods as *universal inference engines*. The possibility of automating
    the inference process has led to the development of **probabilistic programming
    languages** (**PPLs**), which allows a clear separation between model creation
    and inference. In the PPL framework, users specify a full probabilistic model
    by writing a few lines of code, and then inference follows automatically.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä»æ¦‚å¿µä¸Šè®²å¾ˆç®€å•ï¼Œå®Œå…¨æ¦‚ç‡æ¨¡å‹å¸¸å¸¸å¯¼è‡´è§£æä¸Šä¸å¯å¤„ç†çš„è¡¨è¾¾å¼ã€‚å¤šå¹´æ¥ï¼Œè¿™ä¸€ç›´æ˜¯ä¸€ä¸ªçœŸæ­£çš„é—®é¢˜ï¼Œä¹Ÿæ˜¯è´å¶æ–¯æ–¹æ³•æœªèƒ½å¹¿æ³›åº”ç”¨äºä¸€äº›åˆ©åŸºé¢†åŸŸçš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚è®¡ç®—æ—¶ä»£çš„åˆ°æ¥å’Œæ•°å€¼æ–¹æ³•çš„å‘å±•ï¼Œè‡³å°‘åœ¨åŸç†ä¸Šå¯ä»¥ç”¨æ¥è§£å†³ä»»ä½•æ¨æ–­é—®é¢˜ï¼Œå·²ç»æå¤§åœ°æ”¹å˜äº†è´å¶æ–¯æ•°æ®åˆ†æçš„å®è·µã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ•°å€¼æ–¹æ³•è§†ä¸º*é€šç”¨æ¨æ–­å¼•æ“*ã€‚è‡ªåŠ¨åŒ–æ¨æ–­è¿‡ç¨‹çš„å¯èƒ½æ€§å‚¬ç”Ÿäº†**æ¦‚ç‡ç¼–ç¨‹è¯­è¨€**ï¼ˆ**PPLs**ï¼‰ï¼Œå®ƒä½¿å¾—æ¨¡å‹åˆ›å»ºå’Œæ¨æ–­ä¹‹é—´æœ‰äº†æ¸…æ™°çš„åˆ†ç¦»ã€‚åœ¨
    PPL æ¡†æ¶ä¸­ï¼Œç”¨æˆ·é€šè¿‡ç¼–å†™å‡ è¡Œä»£ç æ¥æŒ‡å®šå®Œæ•´çš„æ¦‚ç‡æ¨¡å‹ï¼Œç„¶åæ¨æ–­è¿‡ç¨‹ä¼šè‡ªåŠ¨è¿›è¡Œã€‚
- en: It is expected that probabilistic programming will have a major impact on data
    science and other disciplines by enabling practitioners to build complex probabilistic
    models in a less time-consuming and less error-prone way. I think one good analogy
    for the impact that programming languages can have on scientific computing is
    the introduction of the Fortran programming language more than six decades ago.
    While nowadays Fortran has lost its shine, at one time, it was considered revolutionary.
    For the first time, scientists moved away from computational details and began
    focusing on building numerical methods, models, and simulations more naturally.
    It is interesting to see that some folks are working on making Fortran cool again,
    if you are interested you can check their work at [https://fortran-lang.org/en](https://fortran-lang.org/en).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„è®¡æ¦‚ç‡ç¼–ç¨‹å°†åœ¨æ•°æ®ç§‘å­¦å’Œå…¶ä»–å­¦ç§‘ä¸­äº§ç”Ÿé‡å¤§å½±å“ï¼Œèƒ½å¤Ÿä½¿ä»ä¸šäººå‘˜ä»¥æ›´çœæ—¶ä¸”æ›´å°‘å‡ºé”™çš„æ–¹å¼æ„å»ºå¤æ‚çš„æ¦‚ç‡æ¨¡å‹ã€‚æˆ‘è®¤ä¸ºï¼Œç¼–ç¨‹è¯­è¨€å¯¹ç§‘å­¦è®¡ç®—çš„å½±å“å¯ä»¥é€šè¿‡å…­åå¤šå¹´å‰
    Fortran ç¼–ç¨‹è¯­è¨€çš„å¼•å…¥æ¥ç±»æ¯”ã€‚è™½ç„¶ç°åœ¨ Fortran å·²ç»ä¸å†æµè¡Œï¼Œä½†æ›¾å‡ ä½•æ—¶ï¼Œå®ƒè¢«è®¤ä¸ºæ˜¯é©å‘½æ€§çš„ã€‚ç§‘å­¦å®¶ä»¬ç¬¬ä¸€æ¬¡æ‘†è„±äº†è®¡ç®—ç»†èŠ‚ï¼Œå¼€å§‹æ›´è‡ªç„¶åœ°ä¸“æ³¨äºæ„å»ºæ•°å€¼æ–¹æ³•ã€æ¨¡å‹å’Œä»¿çœŸã€‚å¾ˆæœ‰è¶£çš„æ˜¯ï¼Œæœ‰äº›äººæ­£åœ¨åŠªåŠ›è®©
    Fortran å†åº¦ç„•å‘å…‰å½©ï¼Œå¦‚æœä½ æœ‰å…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹ä»–ä»¬çš„å·¥ä½œï¼š[https://fortran-lang.org/en](https://fortran-lang.org/en)ã€‚
- en: 2.1.1 Flipping coins the PyMC way
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 ä½¿ç”¨ PyMC æŠ›ç¡¬å¸
- en: 'Letâ€™s revisit the coin-flipping problem from *Chapter [1](CH01.xhtml#x1-160001)*,
    but this time using PyMC. We will use the same synthetic data we used in that
    chapter. Since we are generating the data, we know the true value of *Î¸*, called
    `theta_real`, in the following block of code. Of course, for a real dataset, we
    will not have this knowledge:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹*ç¬¬ [1](CH01.xhtml#x1-160001) ç« *ä¸­çš„æŠ›ç¡¬å¸é—®é¢˜ï¼Œè¿™æ¬¡ä½¿ç”¨ PyMCã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸é‚£ä¸€ç« ä¸­ç›¸åŒçš„åˆæˆæ•°æ®ã€‚å› ä¸ºæˆ‘ä»¬ç”Ÿæˆäº†æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬çŸ¥é“ä»¥ä¸‹ä»£ç å—ä¸­
    *Î¸* çš„çœŸå®å€¼ï¼Œç§°ä¸º `theta_real`ã€‚å½“ç„¶ï¼Œå¯¹äºå®é™…æ•°æ®é›†ï¼Œæˆ‘ä»¬æ²¡æœ‰è¿™äº›çŸ¥è¯†ï¼š
- en: '**CodeÂ 2.1**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.1**'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have the data, we need to specify the model. Remember that this
    is done by specifying the likelihood and the prior. For the likelihood, we will
    use the Binomial distribution with parameters *n* = 1, *p* = *Î¸*, and for the
    prior, a Beta distribution with the parameters *Î±* = *Î²* = 1\. A Beta distribution
    with such parameters is equivalent to a Uniform distribution on the interval [0,
    1]. Using mathematical notation we can write the model as:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦æŒ‡å®šæ¨¡å‹ã€‚è®°ä½ï¼Œè¿™æ˜¯é€šè¿‡æŒ‡å®šä¼¼ç„¶æ€§å’Œå…ˆéªŒæ¥å®Œæˆçš„ã€‚å¯¹äºä¼¼ç„¶æ€§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‚æ•° *n* = 1ï¼Œ*p* = *Î¸* çš„äºŒé¡¹åˆ†å¸ƒï¼›å¯¹äºå…ˆéªŒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‚æ•°
    *Î±* = *Î²* = 1 çš„ Beta åˆ†å¸ƒã€‚å…·æœ‰æ­¤ç±»å‚æ•°çš„ Beta åˆ†å¸ƒç­‰åŒäºåŒºé—´ [0, 1] ä¸Šçš„å‡åŒ€åˆ†å¸ƒã€‚ä½¿ç”¨æ•°å­¦ç¬¦å·æˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å†™ä¸ºï¼š
- en: '![Î¸ âˆ¼ Beta(ğ›¼ = 1,ğ›½ = 1) Y âˆ¼ Binomial(n = 1,p = Î¸) ](img/file59.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![Î¸ âˆ¼ Beta(ğ›¼ = 1,ğ›½ = 1) Y âˆ¼ Binomial(n = 1,p = Î¸)](img/file59.jpg)'
- en: 'This statistical model has an almost one-to-one translation to PyMC:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»Ÿè®¡æ¨¡å‹å‡ ä¹å¯ä»¥ç›´æ¥ç¿»è¯‘ä¸º PyMCï¼š
- en: '**CodeÂ 2.2**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.2**'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first line of the code creates a container for our model. Everything inside
    the `with` block will be automatically added to `our_first_model`. You can think
    of this as syntactic sugar to ease model specification as we do not need to manually
    assign variables to the model. The second line specifies the prior. As you can
    see, the syntax follows the mathematical notation closely. The third line specifies
    the likelihood; the syntax is almost the same as for the prior, except that we
    pass the data using the `observed` argument. The observed values can be passed
    as a Python list, a tuple, a NumPy array, or a pandas DataFrame. With that, we
    are finished with the model specification! Pretty neat, right?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç çš„ç¬¬ä¸€è¡Œåˆ›å»ºäº†æˆ‘ä»¬æ¨¡å‹çš„å®¹å™¨ã€‚`with` å—ä¸­çš„æ‰€æœ‰å†…å®¹å°†è‡ªåŠ¨æ·»åŠ åˆ° `our_first_model` ä¸­ã€‚ä½ å¯ä»¥æŠŠè¿™çœ‹ä½œæ˜¯ä¸€ç§è¯­æ³•ç³–ï¼Œå®ƒç®€åŒ–äº†æ¨¡å‹çš„æŒ‡å®šï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨å°†å˜é‡åˆ†é…ç»™æ¨¡å‹ã€‚ç¬¬äºŒè¡ŒæŒ‡å®šäº†å…ˆéªŒã€‚å¦‚ä½ æ‰€è§ï¼Œè¯­æ³•ç´§å¯†éµå¾ªäº†æ•°å­¦ç¬¦å·ã€‚ç¬¬ä¸‰è¡ŒæŒ‡å®šäº†ä¼¼ç„¶æ€§ï¼›è¯­æ³•ä¸å…ˆéªŒå‡ ä¹ç›¸åŒï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯æˆ‘ä»¬ä½¿ç”¨
    `observed` å‚æ•°ä¼ é€’æ•°æ®ã€‚è§‚æµ‹å€¼å¯ä»¥ä½œä¸º Python åˆ—è¡¨ã€å…ƒç»„ã€NumPy æ•°ç»„æˆ– pandas DataFrame ä¼ é€’ã€‚è‡³æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†æ¨¡å‹çš„æŒ‡å®šï¼æ˜¯ä¸æ˜¯å¾ˆç®€æ´ï¼Ÿ
- en: We still have one more line of code to explain. The last line is where the magic
    happens. Behind this innocent line, PyMC has hundreds of *oompa loompas* singing
    and baking a delicious Bayesian inference just for you! Well, not exactly, but
    PyMC is automating a lot of tasks. For the time being, we are going to treat that
    line as a black box that will give us the correct result. What is important to
    understand is that under the hood we will be using numerical methods to compute
    the posterior distribution. In principle, these numerical methods are capable
    of solving any model we can write. The cost we pay for this generality is that
    the solution is going to take the form of samples from the posterior. Later, we
    will be able to corroborate that these samples come from a Beta distribution,
    as we learned from the previous chapter. Because the numerical methods are stochastic,
    the samples will vary every time we run them. However, if the inference process
    works as expected, the samples will be representative of the posterior distribution
    and thus we will obtain the same conclusion from any of those samples. The details
    of what happens under the hood and how to check if the samples are indeed trustworthy
    will be explained in *Chapter [10](CH10.xhtml#x1-18900010)*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æœ‰ä¸€è¡Œä»£ç éœ€è¦è§£é‡Šã€‚æœ€åä¸€è¡Œå°±æ˜¯é­”æ³•å‘ç”Ÿçš„åœ°æ–¹ã€‚èƒŒåè¿™ä¸ªçœ‹ä¼¼æ— å®³çš„ä»£ç è¡Œï¼ŒPyMC æ­£åœ¨è‡ªåŠ¨åŒ–è®¸å¤šä»»åŠ¡ï¼Œç®€ç›´å°±åƒæ˜¯æœ‰æˆç™¾ä¸Šåƒçš„ *oompa loompas*
    åœ¨ä¸ºæ‚¨å”±æ­Œå¹¶çƒ¤åˆ¶ç¾å‘³çš„è´å¶æ–¯æ¨æ–­ï¼å—¯ï¼Œè™½ç„¶å¹¶éå¦‚æ­¤ï¼Œä½† PyMC ç¡®å®åœ¨è‡ªåŠ¨åŒ–å¾ˆå¤šä»»åŠ¡ã€‚æš‚æ—¶ï¼Œæˆ‘ä»¬å°†æŠŠè¿™ä¸€è¡Œè§†ä½œä¸€ä¸ªé»‘ç®±ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›æ­£ç¡®çš„ç»“æœã€‚é‡è¦çš„æ˜¯è¦ç†è§£ï¼Œåœ¨åº•å±‚ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ•°å€¼æ–¹æ³•æ¥è®¡ç®—åéªŒåˆ†å¸ƒã€‚åŸåˆ™ä¸Šï¼Œè¿™äº›æ•°å€¼æ–¹æ³•èƒ½å¤Ÿè§£å†³æˆ‘ä»¬å¯ä»¥ç¼–å†™çš„ä»»ä½•æ¨¡å‹ã€‚æˆ‘ä»¬ä¸ºè¿™ç§é€šç”¨æ€§æ‰€ä»˜å‡ºçš„ä»£ä»·æ˜¯ï¼Œç»“æœå°†ä»¥æ¥è‡ªåéªŒçš„æ ·æœ¬å½¢å¼å‘ˆç°ã€‚ç¨åï¼Œæˆ‘ä»¬å°†èƒ½å¤Ÿè¯å®è¿™äº›æ ·æœ¬æ¥è‡ªä¸€ä¸ª
    Beta åˆ†å¸ƒï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šä¸€ç« å­¦åˆ°çš„é‚£æ ·ã€‚ç”±äºæ•°å€¼æ–¹æ³•æ˜¯éšæœºçš„ï¼Œæ¯æ¬¡è¿è¡Œæ—¶æ ·æœ¬éƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚ç„¶è€Œï¼Œå¦‚æœæ¨æ–­è¿‡ç¨‹æŒ‰é¢„æœŸè¿›è¡Œï¼Œè¿™äº›æ ·æœ¬å°†ä»£è¡¨åéªŒåˆ†å¸ƒï¼Œå› æ­¤æˆ‘ä»¬å°†ä»è¿™äº›æ ·æœ¬ä¸­å¾—å‡ºç›¸åŒçš„ç»“è®ºã€‚æœ‰å…³åº•å±‚å‘ç”Ÿçš„è¯¦ç»†æƒ…å†µä»¥åŠå¦‚ä½•æ£€æŸ¥æ ·æœ¬æ˜¯å¦å€¼å¾—ä¿¡ä»»ï¼Œå°†åœ¨
    *ç¬¬ [10](CH10.xhtml#x1-18900010) ç« * ä¸­è§£é‡Šã€‚
- en: 'One more thing: the `idata` variable is an `InferenceData` object, which is
    a container for all the data generated by PyMC. We will learn more about this
    later in this chapter.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ä»¶äº‹ï¼š`idata` å˜é‡æ˜¯ä¸€ä¸ª `InferenceData` å¯¹è±¡ï¼Œå®ƒæ˜¯ä¸€ä¸ªå®¹å™¨ï¼ŒåŒ…å« PyMC ç”Ÿæˆçš„æ‰€æœ‰æ•°æ®ã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« ç¨åå­¦ä¹ æ›´å¤šæœ‰å…³æ­¤çš„å†…å®¹ã€‚
- en: 'OK, so on the last line, we are asking for 1,000 samples from the posterior.
    If you run the code, you will get a message like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œåœ¨æœ€åä¸€è¡Œä¸­ï¼Œæˆ‘ä»¬è¯·æ±‚ä»åéªŒä¸­è·å¾— 1,000 ä¸ªæ ·æœ¬ã€‚å¦‚æœæ‚¨è¿è¡Œä»£ç ï¼Œæ‚¨å°†çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„æ¶ˆæ¯ï¼š
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first and second lines tell us that PyMC has automatically assigned the
    NUTS sampler (one inference engine that works very well for continuous variables),
    and has used a method to initialize that sampler (these methods need some initial
    guess of where to start sampling). The third line says that PyMC will run four
    chains in parallel, thus we will get four independent samples from the posterior.
    As PyMC attempts to parallelize these chains across the available processors in
    your machine, we will get the four for the price of one. The exact number of chains
    is computed taking into account the number of processors in your machine; you
    can change it using the `chains` argument for the `sample` function. The next
    line tells us which variables are being sampled by which sampler. For this particular
    case, this line is not adding new information because NUTS is used to sample the
    only variable we have, *Î¸*. However, this is not always the case because PyMC
    can assign different samplers to different variables. PyMC has rules to ensure
    that each variable is associated with the best possible sampler. Users can manually
    assign samplers using the `step` argument of the `sample` function, but you will
    hardly need to do that.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€è¡Œå’Œç¬¬äºŒè¡Œå‘Šè¯‰æˆ‘ä»¬ï¼ŒPyMC å·²ç»è‡ªåŠ¨åˆ†é…äº† NUTS é‡‡æ ·å™¨ï¼ˆä¸€ä¸ªéå¸¸é€‚åˆè¿ç»­å˜é‡çš„æ¨æ–­å¼•æ“ï¼‰ï¼Œå¹¶ä½¿ç”¨äº†ä¸€ç§æ–¹æ³•æ¥åˆå§‹åŒ–è¯¥é‡‡æ ·å™¨ï¼ˆè¿™äº›æ–¹æ³•éœ€è¦å¯¹å¼€å§‹é‡‡æ ·çš„ä½ç½®è¿›è¡Œä¸€äº›åˆæ­¥çŒœæµ‹ï¼‰ã€‚ç¬¬ä¸‰è¡Œè¯´
    PyMC å°†å¹¶è¡Œè¿è¡Œå››ä¸ªé“¾ï¼Œå› æ­¤æˆ‘ä»¬å°†ä»åéªŒä¸­è·å¾—å››ä¸ªç‹¬ç«‹çš„æ ·æœ¬ã€‚ç”±äº PyMC å°è¯•å°†è¿™äº›é“¾å¹¶è¡ŒåŒ–åˆ°æœºå™¨ä¸Šå¯ç”¨çš„å¤„ç†å™¨ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥ä¸€ä¸ªä»·æ ¼è·å¾—å››ä¸ªæ ·æœ¬ã€‚é“¾çš„ç¡®åˆ‡æ•°é‡æ˜¯æ ¹æ®æœºå™¨ä¸­çš„å¤„ç†å™¨æ•°é‡è®¡ç®—çš„ï¼›æ‚¨å¯ä»¥ä½¿ç”¨
    `chains` å‚æ•°æ¥æ›´æ”¹ `sample` å‡½æ•°çš„é“¾æ•°ã€‚æ¥ä¸‹æ¥çš„è¿™ä¸€è¡Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªé‡‡æ ·å™¨æ­£åœ¨é‡‡æ ·å“ªäº›å˜é‡ã€‚å¯¹äºè¿™ä¸ªç‰¹å®šçš„ä¾‹å­ï¼Œè¿™ä¸€è¡Œå¹¶æ²¡æœ‰æ·»åŠ æ–°ä¿¡æ¯ï¼Œå› ä¸º
    NUTS è¢«ç”¨æ¥é‡‡æ ·æˆ‘ä»¬å”¯ä¸€çš„å˜é‡ *Î¸*ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ€»æ˜¯è¿™æ ·ï¼Œå› ä¸º PyMC å¯ä»¥ä¸ºä¸åŒçš„å˜é‡åˆ†é…ä¸åŒçš„é‡‡æ ·å™¨ã€‚PyMC æœ‰è§„åˆ™æ¥ç¡®ä¿æ¯ä¸ªå˜é‡éƒ½ä¸æœ€ä½³çš„é‡‡æ ·å™¨å…³è”ã€‚ç”¨æˆ·å¯ä»¥ä½¿ç”¨
    `sample` å‡½æ•°çš„ `step` å‚æ•°æ‰‹åŠ¨åˆ†é…é‡‡æ ·å™¨ï¼Œä½†æ‚¨å‡ ä¹ä¸éœ€è¦è¿™ä¹ˆåšã€‚
- en: Finally, the last line is a progress bar, with several related metrics indicating
    how fast the sampler is working, including the number of iterations per second.
    If you run the code, you will see the progress bar get updated really fast. Here,
    we are seeing the last stage when the sampler has finished its work. You will
    notice that we have asked for 1,000 samples, but PyMC is computing 8,000 samples.
    We have 1,000 draws per chain to tune the sampling algorithm (NUTS, in this example).
    These draws will be discarded by default; PyMC uses them to increase the efficiency
    and reliability of the sampling method, which are both important to obtain a useful
    approximation to the posterior. We also have 1,000 productive draws per chain
    for a total of 4,000\. These are the ones we are going to use as our posterior.
    We can change the number of tuning steps with the `tune` argument of the sample
    function and the number of draws with the `draw` argument.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæœ€åä¸€è¡Œæ˜¯ä¸€ä¸ªè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºäº†ä¸é‡‡æ ·å™¨å·¥ä½œé€Ÿåº¦ç›¸å…³çš„å‡ ä¸ªæŒ‡æ ‡ï¼ŒåŒ…æ‹¬æ¯ç§’çš„è¿­ä»£æ¬¡æ•°ã€‚å¦‚æœä½ è¿è¡Œä»£ç ï¼Œä½ ä¼šçœ‹åˆ°è¿›åº¦æ¡æ›´æ–°å¾—éå¸¸å¿«ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°çš„æ˜¯æœ€åé˜¶æ®µï¼Œå³é‡‡æ ·å™¨å·²ç»å®Œæˆäº†å®ƒçš„å·¥ä½œã€‚ä½ ä¼šæ³¨æ„åˆ°ï¼Œæˆ‘ä»¬è¦æ±‚äº†
    1,000 ä¸ªæ ·æœ¬ï¼Œä½† PyMC å®é™…ä¸Šè®¡ç®—äº† 8,000 ä¸ªæ ·æœ¬ã€‚æ¯æ¡é“¾æœ‰ 1,000 ä¸ªæŠ½æ ·ç”¨äºè°ƒèŠ‚é‡‡æ ·ç®—æ³•ï¼ˆåœ¨æœ¬ä¾‹ä¸­æ˜¯ NUTSï¼‰ã€‚è¿™äº›æŠ½æ ·é»˜è®¤ä¼šè¢«ä¸¢å¼ƒï¼›PyMC
    ä½¿ç”¨å®ƒä»¬æ¥æé«˜é‡‡æ ·æ–¹æ³•çš„æ•ˆç‡å’Œå¯é æ€§ï¼Œè¿™å¯¹äºè·å¾—æœ‰ç”¨çš„åéªŒè¿‘ä¼¼æ˜¯éå¸¸é‡è¦çš„ã€‚æ¯æ¡é“¾è¿˜æœ‰ 1,000 ä¸ªæœ‰æ•ˆæŠ½æ ·ï¼Œå…±è®¡ 4,000 ä¸ªã€‚è¿™äº›æ˜¯æˆ‘ä»¬å°†ç”¨ä½œåéªŒåˆ†å¸ƒçš„æ ·æœ¬ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡
    `tune` å‚æ•°è°ƒæ•´è°ƒèŠ‚æ­¥éª¤çš„æ•°é‡ï¼Œé€šè¿‡ `draw` å‚æ•°è°ƒæ•´æŠ½æ ·çš„æ•°é‡ã€‚
- en: Faster Sampling
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¿«çš„é‡‡æ ·
- en: Under the hood, PyMC uses PyTensor, a library that allows one to define, optimize,
    and efficiently evaluate mathematical expressions involving multi-dimensional
    arrays. PyTensor significantly enhances the speed and performance of PyMC. Despite
    the advantages, itâ€™s worth noting that the samplers in PyMC are implemented in
    Python, which may result in slower execution at times. To address this limitation,
    PyMC allows external samplers. I recommend using nutpie, a sampler written in
    Rust. For more information on how to install and call nutpie from PyMC, please
    check *Chapter [10](CH10.xhtml#x1-18900010)*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¹•åï¼ŒPyMC ä½¿ç”¨ PyTensorï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ä»¥å®šä¹‰ã€ä¼˜åŒ–å’Œé«˜æ•ˆè¯„ä¼°æ¶‰åŠå¤šç»´æ•°ç»„çš„æ•°å­¦è¡¨è¾¾å¼çš„åº“ã€‚PyTensor å¤§å¤§æé«˜äº† PyMC çš„é€Ÿåº¦å’Œæ€§èƒ½ã€‚å°½ç®¡æœ‰è¿™äº›ä¼˜åŠ¿ï¼Œä½†å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPyMC
    ä¸­çš„é‡‡æ ·å™¨æ˜¯ç”¨ Python å®ç°çš„ï¼Œè¿™æœ‰æ—¶å¯èƒ½å¯¼è‡´æ‰§è¡Œé€Ÿåº¦è¾ƒæ…¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸ªé™åˆ¶ï¼ŒPyMC å…è®¸ä½¿ç”¨å¤–éƒ¨é‡‡æ ·å™¨ã€‚æˆ‘æ¨èä½¿ç”¨ nutpieï¼Œä¸€ä¸ªç”¨ Rust
    ç¼–å†™çš„é‡‡æ ·å™¨ã€‚æœ‰å…³å¦‚ä½•ä» PyMC å®‰è£…å’Œè°ƒç”¨ nutpie çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥é˜… *ç¬¬ [10](CH10.xhtml#x1-18900010)* ç« ã€‚
- en: 2.2 Summarizing the posterior
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 æ€»ç»“åéªŒåˆ†å¸ƒ
- en: 'Generally, the first task we will perform after sampling from the posterior
    is to check what the results look like. The `plot_trace` function from ArviZ is
    ideally suited to this task:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œåœ¨ä»åéªŒåˆ†å¸ƒä¸­é‡‡æ ·åï¼Œæˆ‘ä»¬æ‰§è¡Œçš„ç¬¬ä¸€é¡¹ä»»åŠ¡æ˜¯æ£€æŸ¥ç»“æœçš„è¡¨ç°ã€‚ArviZ çš„ `plot_trace` å‡½æ•°éå¸¸é€‚åˆè¿™é¡¹ä»»åŠ¡ï¼š
- en: '**CodeÂ 2.3**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.3**'
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![PIC](img/file60.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file60.png)'
- en: '**FigureÂ 2.1**: A trace plot for the posterior of `our_first_model`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.1**ï¼š`our_first_model` åéªŒçš„è¿½è¸ªå›¾'
- en: '*Figure [2.1](#x1-47005r1)* shows the default result when calling `az.plot_trace`;
    we get two subplots for each unobserved variable. The only unobserved variable
    in our model is *Î¸*. Notice that *y* is an observed variable representing the
    data; we do not need to sample that because we already know those values. Thus
    we only get two subplots. On the left, we have a **Kernel Density Estimation**
    (**KDE**) plot; this is like the smooth version of the histogram. Ideally, we
    want all chains to have a very similar KDE, like in *Figure [2.1](#x1-47005r1)*.
    On the right, we get the individual values at each sampling step; we get as many
    lines as chains. Ideally, we want it to be something that looks noisy, with no
    clear pattern, and we should have a hard time identifying one chain from the others.
    In *Chapter [10](CH10.xhtml#x1-18900010)*, we give more details on how to interpret
    these plots. The gist is that if we ran many chains, we would expect them to be
    practically indistinguishable from each other. The sampler did a good job and
    we can trust the samples.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [2.1](#x1-47005r1)* æ˜¾ç¤ºäº†è°ƒç”¨ `az.plot_trace` æ—¶çš„é»˜è®¤ç»“æœï¼›æˆ‘ä»¬ä¸ºæ¯ä¸ªæœªè§‚å¯Ÿåˆ°çš„å˜é‡è·å¾—ä¸¤ä¸ªå­å›¾ã€‚æˆ‘ä»¬æ¨¡å‹ä¸­å”¯ä¸€æœªè§‚å¯Ÿåˆ°çš„å˜é‡æ˜¯
    *Î¸*ã€‚æ³¨æ„ï¼Œ*y* æ˜¯ä¸€ä¸ªè¡¨ç¤ºæ•°æ®çš„è§‚å¯Ÿå˜é‡ï¼›æˆ‘ä»¬ä¸éœ€è¦å¯¹å…¶è¿›è¡Œé‡‡æ ·ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»çŸ¥é“è¿™äº›å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªå¾—åˆ°ä¸¤ä¸ªå­å›¾ã€‚å·¦è¾¹æ˜¯ä¸€ä¸ª **æ ¸å¯†åº¦ä¼°è®¡**ï¼ˆ**KDE**ï¼‰å›¾ï¼›è¿™ç±»ä¼¼äºç›´æ–¹å›¾çš„å¹³æ»‘ç‰ˆæœ¬ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰€æœ‰é“¾æ¡çš„
    KDE éå¸¸ç›¸ä¼¼ï¼Œå°±åƒ *å›¾ [2.1](#x1-47005r1)* ä¸­æ‰€ç¤ºã€‚å³è¾¹ï¼Œæˆ‘ä»¬å¾—åˆ°æ¯ä¸ªé‡‡æ ·æ­¥éª¤çš„ä¸ªä½“å€¼ï¼›æ¯æ¡é“¾æ¡å¯¹åº”ä¸€æ¡çº¿ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒçœ‹èµ·æ¥æ‚ä¹±æ— ç« ï¼Œæ²¡æœ‰æ˜æ˜¾çš„æ¨¡å¼ï¼Œæˆ‘ä»¬åº”è¯¥å¾ˆéš¾åŒºåˆ†ä¸åŒçš„é“¾æ¡ã€‚åœ¨
    *ç¬¬ [10](CH10.xhtml#x1-18900010)* ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æ›´å¤šå…³äºå¦‚ä½•è§£è¯»è¿™äº›å›¾è¡¨çš„ç»†èŠ‚ã€‚å…³é”®æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬è¿è¡Œäº†è®¸å¤šé“¾æ¡ï¼Œæˆ‘ä»¬æœŸæœ›å®ƒä»¬å‡ ä¹æ— æ³•åŒºåˆ†ã€‚é‡‡æ ·å™¨è¡¨ç°å¾—å¾ˆå¥½ï¼Œæˆ‘ä»¬å¯ä»¥ä¿¡ä»»è¿™äº›æ ·æœ¬ã€‚'
- en: As with other ArviZ functions, `az.plot_trace` has many options. For instance,
    we can run this function with the `combined` argument set to `True` to get a single
    KDE plot for all chains and with `kind=rank_bars` to get a **rank** **plot**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»– ArviZ å‡½æ•°ä¸€æ ·ï¼Œ`az.plot_trace` ä¹Ÿæœ‰è®¸å¤šé€‰é¡¹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½® `combined=True` å‚æ•°æ¥ç”Ÿæˆæ‰€æœ‰é“¾çš„å•ä¸ª
    KDE å›¾ï¼Œå¹¶é€šè¿‡è®¾ç½® `kind=rank_bars` æ¥ç”Ÿæˆä¸€ä¸ª **æ’å** **å›¾**ã€‚
- en: '**CodeÂ 2.4**:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.4**ï¼š'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![PIC](img/file61.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file61.png)'
- en: '**FigureÂ 2.2**: A trace plot for the posterior of `our_first_model`, using
    the options `kind="rank_bars"`, `combined=True`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.2**ï¼š`our_first_model` åéªŒçš„ç—•è¿¹å›¾ï¼Œä½¿ç”¨äº†é€‰é¡¹ `kind="rank_bars"` å’Œ `combined=True`ã€‚'
- en: A rank plot is another way to check if we can trust the samples; for this plot,
    we get one histogram per chain and we want all of them to be as uniform as possible,
    like in *Figure [2.2](#x1-47009r2)*. Some small deviations for uniformity are
    expected due to random sampling, but large deviations from uniformity are a signal
    that chains are exploring different regions of the posteriors. Ideally, we want
    all chains to explore the entire posterior. In *Chapter [10](CH10.xhtml#x1-18900010)*,
    we provide further details on how to interpret rank plots and how they are constructed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ’åå›¾æ˜¯å¦ä¸€ç§æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦å¯ä»¥ä¿¡ä»»æ ·æœ¬çš„æ–¹æ³•ï¼›å¯¹äºè¿™ä¸ªå›¾ï¼Œæˆ‘ä»¬ä¸ºæ¯æ¡é“¾ç”Ÿæˆä¸€ä¸ªç›´æ–¹å›¾ï¼Œå¹¶å¸Œæœ›å®ƒä»¬å°½å¯èƒ½å‡åŒ€ï¼Œåƒåœ¨ *å›¾ [2.2](#x1-47009r2)*
    ä¸­é‚£æ ·ã€‚ç”±äºéšæœºæŠ½æ ·ï¼Œä¸€äº›å°çš„å‡åŒ€æ€§åå·®æ˜¯å¯ä»¥æ¥å—çš„ï¼Œä½†å¤§çš„å‡åŒ€æ€§åå·®åˆ™è¡¨æ˜é“¾æ­£åœ¨æ¢ç´¢åéªŒçš„ä¸åŒåŒºåŸŸã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„é“¾éƒ½èƒ½æ¢ç´¢æ•´ä¸ªåéªŒã€‚åœ¨ *ç¬¬
    [10](CH10.xhtml#x1-18900010) ç« * ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å¦‚ä½•è§£é‡Šæ’åå›¾åŠå…¶æ„å»ºæ–¹æ³•çš„æ›´å¤šç»†èŠ‚ã€‚
- en: ArviZ provides several other plots to help interpret the posterior, and we will
    see them in the following pages. We may also want to have a numerical summary
    of the posterior. We can get that using `az.summary`, which will return a pandas
    DataFrame as shown in *Table [2.1](#x1-47014r1)*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ArviZ æä¾›äº†å‡ ç§å…¶ä»–çš„å›¾è¡¨æ¥å¸®åŠ©è§£é‡ŠåéªŒï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„é¡µé¢ä¸­çœ‹åˆ°å®ƒä»¬ã€‚æˆ‘ä»¬ä¹Ÿå¯èƒ½å¸Œæœ›è·å¾—åéªŒçš„æ•°å€¼æ€»ç»“ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `az.summary`
    æ¥å®ç°ï¼Œå®ƒå°†è¿”å›ä¸€ä¸ª pandas DataFrameï¼Œå¦‚ *è¡¨ [2.1](#x1-47014r1)* æ‰€ç¤ºã€‚
- en: '**CodeÂ 2.5**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.5**'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  | mean | sd | hdi_3% | hdi_97% |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | å‡å€¼ | æ ‡å‡†å·® | hdi_3% | hdi_97% |'
- en: '| *Î¸* | 0.34 | 0.18 | 0.03 | 0.66 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| *Î¸* | 0.34 | 0.18 | 0.03 | 0.66 |'
- en: '**TableÂ 2.1**: Summary statistics'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨ 2.1**ï¼šæ€»ç»“ç»Ÿè®¡æ•°æ®'
- en: On the first column we have the name of the variable, the second column is the
    mean of the posterior, the third column is the standard deviation of the posterior,
    and the last two columns are the lower and upper boundaries of the 94% highest
    density interval. Thus, according to our model and data, we think the value of
    *Î¸* is likely to be 0.34 with a 94% probability that it is actually between 0.03
    and 0.66\. We can report a similar summary using the standard deviation. The advantage
    of the standard deviation over the HDI is that it is a more popular statistic.
    As a disadvantage, we have to be more careful interpreting it; otherwise, it can
    lead to meaningless results. For example, if we compute the mean Â± 2 standard
    deviations, we will get the intervals (-0.02, 0.7); the upper value is not that
    far from 0.66, which we got from the HDI, but the lower bound is actually outside
    the possible values of *Î¸*, which is between 0 and 1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€åˆ—ï¼Œæˆ‘ä»¬æœ‰å˜é‡çš„åç§°ï¼Œç¬¬äºŒåˆ—æ˜¯åéªŒçš„å‡å€¼ï¼Œç¬¬ä¸‰åˆ—æ˜¯åéªŒçš„æ ‡å‡†å·®ï¼Œæœ€åä¸¤åˆ—æ˜¯ 94% æœ€é«˜å¯†åº¦åŒºé—´çš„ä¸‹é™å’Œä¸Šé™ã€‚å› æ­¤ï¼Œæ ¹æ®æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®ï¼Œæˆ‘ä»¬è®¤ä¸º
    *Î¸* çš„å€¼å¾ˆå¯èƒ½æ˜¯ 0.34ï¼Œå¹¶ä¸”ä»¥ 94% çš„æ¦‚ç‡ï¼Œå®ƒçš„çœŸå®å€¼åœ¨ 0.03 å’Œ 0.66 ä¹‹é—´ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨æ ‡å‡†å·®æŠ¥å‘Šä¸€ä¸ªç±»ä¼¼çš„æ€»ç»“ã€‚æ ‡å‡†å·®ç›¸æ¯” HDI
    çš„ä¼˜ç‚¹æ˜¯å®ƒæ˜¯ä¸€ç§æ›´å¸¸è§çš„ç»Ÿè®¡é‡ã€‚ä½†ç¼ºç‚¹æ˜¯æˆ‘ä»¬éœ€è¦æ›´å°å¿ƒåœ°è§£é‡Šå®ƒï¼Œå¦åˆ™å¯èƒ½ä¼šå¾—å‡ºæ¯«æ— æ„ä¹‰çš„ç»“æœã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è®¡ç®—å‡å€¼ Â± 2 ä¸ªæ ‡å‡†å·®ï¼Œæˆ‘ä»¬å°†å¾—åˆ°åŒºé—´ (-0.02,
    0.7)ï¼›ä¸Šé™å€¼ä¸æˆ‘ä»¬ä» HDI å¾—åˆ°çš„ 0.66 ç›¸å·®ä¸è¿œï¼Œä½†ä¸‹é™å®é™…ä¸Šè¶…å‡ºäº† *Î¸* çš„å¯èƒ½å€¼ï¼ˆå®ƒåº”è¯¥åœ¨ 0 å’Œ 1 ä¹‹é—´ï¼‰ã€‚
- en: Another way to visually summarize the posterior is to use the `az.plot_posterior`
    function that comes with ArviZ (see *Figure [2.3](#x1-47019r3)*). We used this
    function in the previous chapter for a fake posterior. We are going to use it
    now for a real posterior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§ç›´è§‚æ€»ç»“åéªŒçš„æ–¹æ³•æ˜¯ä½¿ç”¨ ArviZ ä¸­çš„ `az.plot_posterior` å‡½æ•°ï¼ˆè§ *å›¾ [2.3](#x1-47019r3)*ï¼‰ã€‚æˆ‘ä»¬åœ¨å‰ä¸€ç« ä¸­å·²ç»ç”¨å®ƒå¤„ç†äº†ä¸€ä¸ªè™šå‡çš„åéªŒã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥å¤„ç†ä¸€ä¸ªçœŸå®çš„åéªŒã€‚
- en: '**CodeÂ 2.6**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.6**'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![PIC](img/file62.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file62.png)'
- en: '**FigureÂ 2.3**: The plot shows the posterior distribution of *Î¸* and the 94%
    HDI'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.3**ï¼šè¯¥å›¾æ˜¾ç¤ºäº† *Î¸* çš„åéªŒåˆ†å¸ƒåŠ 94% HDIã€‚'
- en: By default, `plot_posterior` shows a histogram for discrete variables and KDEs
    for continuous variables. We also get the mean of the distribution (we can ask
    for the median or mode using the `point_estimate` argument) and the 94% HDI as
    a black line at the bottom of the plot. Different interval values can be set for
    the HDI with the `hdi_prob` argument. This type of plot was introduced by John
    K. Kruschke in his great book Doing Bayesian Data Analysis [[Kruschke](Bibliography.xhtml#Xkruschke_2014),Â [2014](Bibliography.xhtml#Xkruschke_2014)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œ`plot_posterior` ä¸ºç¦»æ•£å˜é‡æ˜¾ç¤ºç›´æ–¹å›¾ï¼Œä¸ºè¿ç»­å˜é‡æ˜¾ç¤ºæ ¸å¯†åº¦ä¼°è®¡ï¼ˆKDEï¼‰ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å¾—åˆ°åˆ†å¸ƒçš„å‡å€¼ï¼ˆå¯ä»¥é€šè¿‡`point_estimate`å‚æ•°è¯·æ±‚ä¸­ä½æ•°æˆ–ä¼—æ•°ï¼‰å’Œ94%çš„HDIï¼Œè¯¥é»‘çº¿ä½äºå›¾è¡¨åº•éƒ¨ã€‚å¯ä»¥ä½¿ç”¨`hdi_prob`å‚æ•°ä¸ºHDIè®¾ç½®ä¸åŒçš„åŒºé—´å€¼ã€‚è¿™ç§ç±»å‹çš„å›¾è¡¨æ˜¯ç”±John
    K. Kruschkeåœ¨ä»–çš„ä¼Ÿå¤§è‘—ä½œã€ŠDoing Bayesian Data Analysisã€‹ä¸­é¦–æ¬¡æå‡ºçš„[[Kruschke](Bibliography.xhtml#Xkruschke_2014),
    [2014](Bibliography.xhtml#Xkruschke_2014)]ã€‚
- en: 2.3 Posterior-based decisions
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 åŸºäºåéªŒçš„å†³ç­–
- en: 'Sometimes, describing the posterior is not enough. We may need to make decisions
    based on our inferences and reduce a continuous estimation to a dichotomous one:
    yes-no, healthy-sick, contaminated-safe, and so on. For instance, is the coin
    fair? A fair coin is one with a *Î¸* value of exactly 0.5\. We can compare the
    value of 0.5 against the HDI interval. From *Figure [2.3](#x1-47019r3)*, we can
    see that the HDI goes from 0.03 to 0.7 and hence 0.5 is included in the HDI. We
    can interpret this as an indication that the coin may be tail-biased, but we cannot
    completely rule out the possibility that the coin is actually fair. If we want
    a sharper decision, we will need to collect more data to reduce the spread of
    the posterior, or maybe we need to find out how to define a more informative prior.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å€™ï¼Œä»…ä»…æè¿°åéªŒæ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬å¯èƒ½éœ€è¦åŸºäºæ¨æ–­åšå‡ºå†³ç­–ï¼Œå¹¶å°†è¿ç»­ä¼°è®¡è½¬åŒ–ä¸ºäºŒåˆ†æ³•ï¼šæ˜¯-å¦ï¼Œå¥åº·-ç”Ÿç—…ï¼Œæ±¡æŸ“-å®‰å…¨ï¼Œç­‰ç­‰ã€‚ä¾‹å¦‚ï¼Œç¡¬å¸æ˜¯å¦å…¬å¹³ï¼Ÿä¸€ä¸ªå…¬å¹³çš„ç¡¬å¸æ˜¯*Î¸*å€¼æ°å¥½ä¸º0.5çš„ç¡¬å¸ã€‚æˆ‘ä»¬å¯ä»¥å°†0.5çš„å€¼ä¸HDIåŒºé—´è¿›è¡Œæ¯”è¾ƒã€‚ä»*å›¾
    [2.3](#x1-47019r3)*ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°HDIåŒºé—´ä»0.03åˆ°0.7ï¼Œå› æ­¤0.5åŒ…å«åœ¨HDIå†…ã€‚æˆ‘ä»¬å¯ä»¥å°†æ­¤è§£è¯»ä¸ºç¡¬å¸å¯èƒ½æœ‰å°¾éƒ¨åå‘ï¼Œä½†æˆ‘ä»¬ä¸èƒ½å®Œå…¨æ’é™¤ç¡¬å¸å®é™…ä¸Šæ˜¯å…¬å¹³çš„å¯èƒ½æ€§ã€‚å¦‚æœæˆ‘ä»¬å¸Œæœ›åšå‡ºæ›´æ˜ç¡®çš„å†³ç­–ï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†æ›´å¤šçš„æ•°æ®æ¥å‡å°‘åéªŒçš„åˆ†å¸ƒèŒƒå›´ï¼Œæˆ–è€…å¯èƒ½éœ€è¦é‡æ–°å®šä¹‰ä¸€ä¸ªæ›´å…·ä¿¡æ¯æ€§çš„å…ˆéªŒã€‚
- en: 2.3.1 Savage-Dickey density ratio
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 Savage-Dickey å¯†åº¦æ¯”
- en: 'One way to evaluate how much support the posterior provides for a given value
    is to compare the ratio of the posterior and prior densities at that value. This
    is called the Savage-Dickey density ratio and we can compute it with ArviZ using
    the `az.plot_bf` function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°åéªŒæä¾›ç»™æŸä¸ªç‰¹å®šå€¼çš„æ”¯æŒåŠ›åº¦çš„ä¸€ç§æ–¹æ³•æ˜¯æ¯”è¾ƒè¯¥å€¼ä¸‹åéªŒå¯†åº¦å’Œå…ˆéªŒå¯†åº¦çš„æ¯”å€¼ã€‚è¿™ç§°ä¸ºSavage-Dickeyå¯†åº¦æ¯”ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ArviZé€šè¿‡`az.plot_bf`å‡½æ•°æ¥è®¡ç®—ï¼š
- en: '**CodeÂ 2.7**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.7**'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![PIC](img/file63.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file63.png)'
- en: '**FigureÂ 2.4**: The plot shows the prior and posterior for `our_first_model`;
    the black dots represent their values evaluated at the reference value 0.5'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.4**ï¼šè¯¥å›¾æ˜¾ç¤ºäº†`our_first_model`çš„å…ˆéªŒå’ŒåéªŒï¼›é»‘è‰²ç‚¹è¡¨ç¤ºåœ¨å‚è€ƒå€¼0.5å¤„è¯„ä¼°çš„å€¼'
- en: 'From *Figure [2.4](#x1-49008r4)*, we can see that the value of `BF_01` is 1.3,
    which means that the value of *Î¸* = 0*.*5 is 1.3 times more likely under the posterior
    distribution than under the prior distribution. To compute this value we just
    divided the height of the posterior at *Î¸* = 0*.*5 by the height of the prior
    at *Î¸* = 0*.*5\. The value of `BF_10` is just the inverse ![-1- 1.3](img/file64.jpg)
    â‰ˆ 0*.*8\. We can think of this as the value of **Î¸*â‰ *0*.*5 being 0.76 times more
    likely under the posterior than under the prior. How do we interpret these numbers?
    With a pinch of salt...the following table shows one possible interpretation originally
    proposed by [Kass and Raftery](Bibliography.xhtml#Xkass_1995)Â [[1995](Bibliography.xhtml#Xkass_1995)]:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä»*å›¾ [2.4](#x1-49008r4)*ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ`BF_01`çš„å€¼ä¸º1.3ï¼Œè¿™æ„å‘³ç€åœ¨åéªŒåˆ†å¸ƒä¸‹ï¼Œ*Î¸* = 0.5çš„å€¼æ¯”åœ¨å…ˆéªŒåˆ†å¸ƒä¸‹æ›´æœ‰å¯èƒ½ï¼Œæ¦‚ç‡æ˜¯1.3å€ã€‚ä¸ºäº†è®¡ç®—è¿™ä¸ªå€¼ï¼Œæˆ‘ä»¬å°†åéªŒåœ¨*Î¸*
    = 0.5å¤„çš„é«˜åº¦é™¤ä»¥å‰éªŒåœ¨*Î¸* = 0.5å¤„çš„é«˜åº¦ã€‚`BF_10`çš„å€¼åˆ™æ˜¯å…¶å€’æ•°ï¼[-1- 1.3](img/file64.jpg) â‰ˆ 0.8ã€‚æˆ‘ä»¬å¯ä»¥ç†è§£ä¸ºåœ¨åéªŒåˆ†å¸ƒä¸‹ï¼Œ**Î¸â‰ 0.5**çš„å¯èƒ½æ€§æ˜¯å…ˆéªŒåˆ†å¸ƒä¸‹çš„0.76å€ã€‚æˆ‘ä»¬å¦‚ä½•è§£è¯»è¿™äº›æ•°å­—ï¼Ÿè¦å¸¦æœ‰ä¸€ä¸æ€€ç–‘â€¦â€¦ä»¥ä¸‹è¡¨æ ¼å±•ç¤ºäº†[Kass
    å’Œ Raftery](Bibliography.xhtml#Xkass_1995)åœ¨[1995](Bibliography.xhtml#Xkass_1995)å¹´æå‡ºçš„å…¶ä¸­ä¸€ç§å¯èƒ½çš„è§£é‡Šï¼š
- en: '| BF_01 | Interpretation |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| BF_01 | è§£é‡Š |'
- en: '| 1 to 3.2 | Not worth more than a bare mention |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1 åˆ° 3.2 | ä¸å€¼ä¸€æ |'
- en: '| 3.2 to 10 | Substantial |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 3.2 åˆ° 10 | å®è´¨æ€§ |'
- en: '| 10 to 100 | Strong |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 10 åˆ° 100 | å¼º |'
- en: '| *>* 100 | Decisive |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| *>* 100 | å†³å®šæ€§ |'
- en: '**TableÂ 2.2**: Interpretation of Bayes Factors (BF_01)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨ 2.2**ï¼šè´å¶æ–¯å› å­ï¼ˆBF_01ï¼‰çš„è§£é‡Š'
- en: The Savage-Dickey density ratio is a particular way to compute what is called
    the Bayes Factor. We will learn more about Bayes Factors, and their caveats, in
    *Chapter [5](CH05.xhtml#x1-950005)*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Savage-Dickey å¯†åº¦æ¯”ç‡æ˜¯ä¸€ç§ç‰¹å®šçš„è®¡ç®—æ–¹æ³•ï¼Œç”¨æ¥æ±‚è§£æ‰€è°“çš„è´å¶æ–¯å› å­ã€‚æˆ‘ä»¬å°†åœ¨ *ç¬¬ [5](CH05.xhtml#x1-950005)
    ç« * ä¸­å­¦ä¹ æ›´å¤šå…³äºè´å¶æ–¯å› å­åŠå…¶æ³¨æ„äº‹é¡¹ã€‚
- en: 2.3.2 Region Of Practical Equivalence
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 å®é™…ç­‰ä»·åŒºåŸŸ
- en: 'Strictly speaking, the chance of observing exactly 0.5 (that is, with infinite
    trailing zeros) is zero. Also, in practice, we generally do not care about exact
    results but results within a certain margin. Accordingly, in practice, we can
    relax the definition of fairness and we can say that a fair coin is one with a
    value of *around* 0.5\. For example, we could say that any value in the interval
    [0.45, 0.55] will be, for our purposes, practically equivalent to 0.5\. We call
    this interval a Region Of Practical Equivalence (ROPE). Once the ROPE is defined,
    we compare it with the HDI. We can get at least three scenarios:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¥æ ¼æ¥è¯´ï¼Œè§‚å¯Ÿåˆ°ç²¾ç¡®çš„ 0.5ï¼ˆå³å¸¦æœ‰æ— é™å¤šä¸ªé›¶ï¼‰çš„æ¦‚ç‡ä¸ºé›¶ã€‚æ­¤å¤–ï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸å…³å¿ƒç²¾ç¡®ç»“æœï¼Œè€Œæ˜¯å…³å¿ƒæŸä¸ªèŒƒå›´å†…çš„ç»“æœã€‚å› æ­¤ï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ”¾å®½å¯¹å…¬å¹³æ€§çš„å®šä¹‰ï¼Œè®¤ä¸ºä¸€ä¸ªå…¬å¹³çš„ç¡¬å¸çš„å€¼åº”è¯¥æ˜¯
    *æ¥è¿‘* 0.5ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼ŒåŒºé—´ [0.45, 0.55] å†…çš„ä»»ä½•å€¼å¯¹äºæˆ‘ä»¬çš„ç›®çš„æ¥è¯´ï¼Œå®é™…ä¸Šéƒ½ç­‰åŒäº 0.5ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªåŒºé—´ä¸ºå®é™…ç­‰ä»·åŒºåŸŸï¼ˆROPEï¼‰ã€‚ä¸€æ—¦å®šä¹‰äº†
    ROPEï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å…¶ä¸ HDI è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¯ä»¥å¾—åˆ°è‡³å°‘ä¸‰ç§æƒ…å†µï¼š
- en: The ROPE does not overlap the HDI; we can say the coin is not fair
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROPE ä¸ä¸ HDI é‡å ï¼›æˆ‘ä»¬å¯ä»¥è¯´ç¡¬å¸æ˜¯ä¸å…¬å¹³çš„
- en: The ROPE contains the entire HDI; we can say the coin is fair
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROPE åŒ…å«äº†æ•´ä¸ª HDIï¼›æˆ‘ä»¬å¯ä»¥è¯´ç¡¬å¸æ˜¯å…¬å¹³çš„
- en: The ROPE partially overlaps HDI; we cannot say the coin is fair or unfair
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROPE éƒ¨åˆ†ä¸ HDI é‡å ï¼›æˆ‘ä»¬ä¸èƒ½è¯´ç¡¬å¸æ˜¯å…¬å¹³çš„è¿˜æ˜¯ä¸å…¬å¹³çš„
- en: If we choose the ROPE to match the support of a parameter, like [0, 1] for the
    coin-flipping example, we will always say we have a fair coin. Notice that we
    do not need to collect data to perform any type of inference.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬é€‰æ‹©å°† ROPE ä¸å‚æ•°çš„æ”¯æŒåŒºåŸŸåŒ¹é…ï¼Œä¾‹å¦‚åœ¨ç¡¬å¸æŠ•æ·ç¤ºä¾‹ä¸­ä½¿ç”¨ [0, 1]ï¼Œæˆ‘ä»¬å°†æ€»æ˜¯è®¤ä¸ºç¡¬å¸æ˜¯å…¬å¹³çš„ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ”¶é›†æ•°æ®æ¥è¿›è¡Œä»»ä½•æ¨æ–­ã€‚
- en: 'The choice of ROPE is completely arbitrary: we can choose any value we want.
    Some choices are not very useful. If, for the coin-flipping example, we choose
    the ROPE to be [0, 1], then we will always say the coin is fair. Even more, we
    donâ€™t need to collect data or perform any analysis to reach this conclusion, this
    is a trivial example. More worrisome is to pick the ROPE after performing the
    analysis. This is problematic because we can accommodate the results to say whatever
    we want them to say. But why do we even bother to do an analysis, if we are going
    to accommodate the result to our expectations? The ROPE should be informed from
    domain knowledge.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ROPE çš„é€‰æ‹©å®Œå…¨æ˜¯ä»»æ„çš„ï¼šæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä»»ä½•æˆ‘ä»¬æƒ³è¦çš„å€¼ã€‚æœ‰äº›é€‰æ‹©å¹¶æ²¡æœ‰ä»€ä¹ˆå®é™…æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œå¯¹äºç¡¬å¸æŠ•æ·ç¤ºä¾‹ï¼Œå¦‚æœæˆ‘ä»¬é€‰æ‹© ROPE ä¸º [0, 1]ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†æ€»æ˜¯è®¤ä¸ºç¡¬å¸æ˜¯å…¬å¹³çš„ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ”¶é›†æ•°æ®æˆ–è¿›è¡Œä»»ä½•åˆ†ææ¥å¾—å‡ºè¿™ä¸ªç»“è®ºï¼Œè¿™æ˜¯ä¸€ä¸ªå¾®ä¸è¶³é“çš„ä¾‹å­ã€‚æ›´å€¼å¾—æ‹…å¿ƒçš„æ˜¯ï¼Œåœ¨è¿›è¡Œåˆ†æä¹‹åé€‰æ‹©
    ROPEã€‚è¿™æ˜¯æœ‰é—®é¢˜çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥è°ƒæ•´ç»“æœæ¥ä½¿å®ƒç¬¦åˆæˆ‘ä»¬æƒ³è¦çš„ç»“è®ºã€‚ä½†å¦‚æœæˆ‘ä»¬è¦è°ƒæ•´ç»“æœä»¥ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸï¼Œé‚£æˆ‘ä»¬ä¸ºä»€ä¹ˆè¿˜è¦åšåˆ†æå‘¢ï¼ŸROPE åº”è¯¥ç”±é¢†åŸŸçŸ¥è¯†æ¥å†³å®šã€‚
- en: 'We can use the `plot_posterior` function to plot the posterior with the HDI
    interval and the ROPE. The ROPE appears as a semi-transparent thick (gray) line:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `plot_posterior` å‡½æ•°æ¥ç»˜åˆ¶åŒ…å« HDI åŒºé—´å’Œ ROPE çš„åéªŒåˆ†å¸ƒã€‚ROPE æ˜¾ç¤ºä¸ºåŠé€æ˜çš„ç²—ï¼ˆç°è‰²ï¼‰çº¿ï¼š
- en: '**CodeÂ 2.8**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.8**'
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![PIC](img/file65.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file65.png)'
- en: '**FigureÂ 2.5**: The plot shows the posterior distribution of *Î¸* and the 94%
    HDI. The ROPE is shown as a thick light-gray line'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.5**ï¼šè¯¥å›¾å±•ç¤ºäº† *Î¸* çš„åéªŒåˆ†å¸ƒå’Œ 94% HDIã€‚ROPE ä»¥ä¸€æ¡ç²—çš„æµ…ç°è‰²çº¿è¡¨ç¤º'
- en: 'Another tool we can use to help us make a decision is to compare the posterior
    against a reference value. We can do this using `plot_posterior`. As you can see
    in *Figure [2.6](#x1-50011r6)*, we get a vertical (gray) line and the proportion
    of the posterior above and below our reference value:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥ç”¨æ¥å¸®åŠ©å†³ç­–çš„å·¥å…·æ˜¯å°†åéªŒåˆ†å¸ƒä¸å‚è€ƒå€¼è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `plot_posterior` æ¥å®ç°ã€‚å¦‚ *å›¾ [2.6](#x1-50011r6)*
    æ‰€ç¤ºï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€æ¡å‚ç›´çš„ï¼ˆç°è‰²ï¼‰çº¿ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°åéªŒåˆ†å¸ƒåœ¨å‚è€ƒå€¼ä¹‹ä¸Šå’Œä¹‹ä¸‹çš„æ¯”ä¾‹ï¼š
- en: '**CodeÂ 2.9**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.9**'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![PIC](img/file66.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file66.png)'
- en: '**FigureÂ 2.6**: The plot shows the posterior distribution of *Î¸* and the 94%
    HDI. The reference value is shown as a gray vertical line'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.6**ï¼šè¯¥å›¾å±•ç¤ºäº† *Î¸* çš„åéªŒåˆ†å¸ƒå’Œ 94% HDIã€‚å‚è€ƒå€¼æ˜¾ç¤ºä¸ºç°è‰²å‚ç›´çº¿'
- en: For a more detailed discussion on the use of the ROPE, you could read Chapter
    12 of Doing Bayesian Data Analysis by [Kruschke](Bibliography.xhtml#Xkruschke_2014)Â [[2014](Bibliography.xhtml#Xkruschke_2014)].
    That chapter also discusses how to perform hypothesis testing in a Bayesian framework
    and the caveats of hypothesis testing, whether in a Bayesian or non-Bayesian setting.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº ROPE ä½¿ç”¨çš„æ›´è¯¦ç»†è®¨è®ºï¼Œå¯ä»¥é˜…è¯» [Kruschke](Bibliography.xhtml#Xkruschke_2014) çš„ã€ŠDoing
    Bayesian Data Analysisã€‹ç¬¬12ç« [[2014](Bibliography.xhtml#Xkruschke_2014)]ã€‚è¯¥ç« èŠ‚è¿˜è®¨è®ºäº†å¦‚ä½•åœ¨è´å¶æ–¯æ¡†æ¶ä¸­è¿›è¡Œå‡è®¾æ£€éªŒåŠå…¶æ³¨æ„äº‹é¡¹ï¼Œæ— è®ºæ˜¯åœ¨è´å¶æ–¯è¿˜æ˜¯éè´å¶æ–¯è®¾ç½®ä¸‹ã€‚
- en: 2.3.3 Loss functions
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 æŸå¤±å‡½æ•°
- en: 'If you think these ROPE rules sound a little bit clunky and you want something
    more formal, loss functions are what you are looking for! To make a good decision,
    it is important to have the highest possible level of precision for the estimated
    value of the relevant parameters, but it is also important to take into account
    the cost of making a mistake. The cost/benefit trade-off can be mathematically
    formalized using loss functions. The names for loss functions or their inverses
    vary across different fields, and we could find names such as cost functions,
    objective functions, fitness functions, utility functions, and so on. No matter
    the name, the key idea is to use a function that captures how different the true
    value and the estimated value of a parameter are. The larger the value of the
    loss function, the worse the estimation is (according to the loss function). Some
    common examples of loss functions are:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è§‰å¾—è¿™äº› ROPE è§„åˆ™å¬èµ·æ¥æœ‰äº›ç¹çï¼Œå¹¶ä¸”æƒ³è¦æ›´æ­£å¼çš„ä¸œè¥¿ï¼ŒæŸå¤±å‡½æ•°å°±æ˜¯ä½ æ‰€å¯»æ‰¾çš„ï¼ä¸ºäº†åšå‡ºä¸€ä¸ªå¥½çš„å†³ç­–ï¼Œé‡è¦çš„æ˜¯å¯¹ç›¸å…³å‚æ•°çš„ä¼°è®¡å€¼å…·æœ‰å°½å¯èƒ½é«˜çš„ç²¾åº¦ï¼Œä½†åŒæ ·é‡è¦çš„æ˜¯è¦è€ƒè™‘çŠ¯é”™çš„ä»£ä»·ã€‚æˆæœ¬/æ”¶ç›Šçš„æƒè¡¡å¯ä»¥é€šè¿‡æ•°å­¦åŒ–çš„æ–¹å¼ä½¿ç”¨æŸå¤±å‡½æ•°æ¥å½¢å¼åŒ–ã€‚æŸå¤±å‡½æ•°æˆ–å…¶é€†çš„åç§°åœ¨ä¸åŒé¢†åŸŸä¸­å„ä¸ç›¸åŒï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°è¯¸å¦‚æˆæœ¬å‡½æ•°ã€ç›®æ ‡å‡½æ•°ã€é€‚åº”åº¦å‡½æ•°ã€æ•ˆç”¨å‡½æ•°ç­‰åç§°ã€‚æ— è®ºåç§°å¦‚ä½•ï¼Œå…³é”®æ€æƒ³æ˜¯ä½¿ç”¨ä¸€ä¸ªå‡½æ•°æ¥æ•æ‰å‚æ•°çš„çœŸå®å€¼å’Œä¼°è®¡å€¼ä¹‹é—´çš„å·®å¼‚ã€‚æŸå¤±å‡½æ•°çš„å€¼è¶Šå¤§ï¼Œä¼°è®¡ç»“æœè¶Šå·®ï¼ˆæ ¹æ®æŸå¤±å‡½æ•°çš„å®šä¹‰ï¼‰ã€‚ä¸€äº›å¸¸è§çš„æŸå¤±å‡½æ•°ç¤ºä¾‹å¦‚ä¸‹ï¼š
- en: The absolute loss function, |*Î¸* âˆ’ ![](img/hat_theta.png)|
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»å¯¹æŸå¤±å‡½æ•°ï¼Œ|*Î¸* âˆ’ ![](img/hat_theta.png)|
- en: The quadratic loss function, (*Î¸* âˆ’ ![](img/hat_theta.png))Â²
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äºŒæ¬¡æŸå¤±å‡½æ•°ï¼Œ(*Î¸* âˆ’ ![](img/hat_theta.png))Â²
- en: The 0-1 loss function, 1(**Î¸* â‰ * ![](img/hat_theta.png)), where ![](img/one.PNG)
    is the indicator function
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0-1 æŸå¤±å‡½æ•°ï¼Œ1(**Î¸* â‰ * ![](img/hat_theta.png))ï¼Œå…¶ä¸­ ![](img/one.PNG) æ˜¯æŒ‡ç¤ºå‡½æ•°
- en: In practice, we donâ€™t know the value of the true parameter. Instead, we have
    an estimation in the form of a posterior distribution. Thus, what we can do is
    find out the value of *Î¸* that minimizes the expected loss function. By expected
    loss function, we mean the loss function averaged over the whole posterior distribution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“çœŸå®å‚æ•°çš„å€¼ã€‚ç›¸åï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåéªŒåˆ†å¸ƒå½¢å¼çš„ä¼°è®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åšçš„æ˜¯æ‰¾å‡ºä¸€ä¸ª *Î¸*ï¼Œä½¿å…¶æœ€å°åŒ–æœŸæœ›æŸå¤±å‡½æ•°ã€‚æ‰€è°“çš„æœŸæœ›æŸå¤±å‡½æ•°æ˜¯æŒ‡åœ¨æ•´ä¸ªåéªŒåˆ†å¸ƒä¸Šå¹³å‡çš„æŸå¤±å‡½æ•°ã€‚
- en: 'In the following block of code, we have two loss functions: the absolute loss
    (`lossf_a`) and the quadratic loss (`lossf_b`). We will explore the value of over
    a grid of 200 points. We will then plot those curves and we will also include
    the value of *Î¸* that minimizes each loss function. The following block shows
    the Python code without the plotting part:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç å—ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªæŸå¤±å‡½æ•°ï¼šç»å¯¹æŸå¤±ï¼ˆ`lossf_a`ï¼‰å’ŒäºŒæ¬¡æŸå¤±ï¼ˆ`lossf_b`ï¼‰ã€‚æˆ‘ä»¬å°†æ¢ç´¢ä¸€ä¸ªåŒ…å« 200 ä¸ªç‚¹çš„ç½‘æ ¼ä¸Šçš„å€¼ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼šç»˜åˆ¶è¿™äº›æ›²çº¿ï¼Œå¹¶ä¸”è¿˜ä¼šåŒ…æ‹¬æœ€å°åŒ–æ¯ä¸ªæŸå¤±å‡½æ•°çš„
    *Î¸* å€¼ã€‚ä»¥ä¸‹ä»£ç å—å±•ç¤ºäº†æ²¡æœ‰ç»˜å›¾éƒ¨åˆ†çš„ Python ä»£ç ï¼š
- en: '**CodeÂ 2.10**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  2.10**'
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![PIC](img/file67.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file67.png)'
- en: '**FigureÂ 2.7**: The absolute (black) and quadratic (gray) loss functions applied
    to the posterior from `our_first_model`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.7**ï¼šåº”ç”¨äº `our_first_model` åéªŒçš„ç»å¯¹ï¼ˆé»‘è‰²ï¼‰å’ŒäºŒæ¬¡ï¼ˆç°è‰²ï¼‰æŸå¤±å‡½æ•°'
- en: 'What is more interesting from *Figure [2.7](#x1-51011r7)* is that the value
    we got from the absolute loss is equal to the median of the posterior and the
    one we got from the quadratic loss is equal to the mean of the posterior. You
    can check this for yourself by computing `np.mean(`*Î¸*`_pos)`, `np.median(`*Î¸*`_pos)`.
    This is no coincidence: different loss functions are related to different point
    estimates. The mean is the point estimate that minimizes the quadratic loss, the
    median, the absolute loss, and the mode, the 1-0 loss.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æœ‰è¶£çš„æ˜¯ï¼Œä»*å›¾ [2.7](#x1-51011r7)*å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ä»ç»å¯¹æŸå¤±å¾—åˆ°çš„å€¼ç­‰äºåéªŒåˆ†å¸ƒçš„ä¸­ä½æ•°ï¼Œè€Œä»äºŒæ¬¡æŸå¤±å¾—åˆ°çš„å€¼ç­‰äºåéªŒåˆ†å¸ƒçš„å‡å€¼ã€‚ä½ å¯ä»¥é€šè¿‡è®¡ç®—`np.mean(`*Î¸*`_pos)`ï¼Œ`np.median(`*Î¸*`_pos)`æ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚è¿™å¹¶éå·§åˆï¼šä¸åŒçš„æŸå¤±å‡½æ•°ä¸ä¸åŒçš„ç‚¹ä¼°è®¡æœ‰å…³ã€‚å‡å€¼æ˜¯æœ€å°åŒ–äºŒæ¬¡æŸå¤±çš„ç‚¹ä¼°è®¡ï¼Œä¸­ä½æ•°æ˜¯æœ€å°åŒ–ç»å¯¹æŸå¤±çš„ç‚¹ä¼°è®¡ï¼Œè€Œä¼—æ•°æ˜¯æœ€å°åŒ–
    1-0 æŸå¤±çš„ç‚¹ä¼°è®¡ã€‚
- en: 'If we want to be formal and we want to compute a single-point estimate, we
    must decide which loss function we want. Conversely, if we choose a point estimate,
    we are implicitly (and maybe unconsciously) choosing a loss function. The advantage
    of explicitly choosing a loss function is that we can tailor the function to our
    problem instead of using a predefined rule. It is very common to observe that
    the cost of making a decision is asymmetric; for example, vaccines can produce
    an overreaction of the immune system, but the benefit to the vaccinated persons
    and even non-vaccinated persons overcomes the risk, usually by many orders of
    magnitude. Thus, if our problem demands it, we can construct an asymmetric loss
    function. It is also important to notice that, as the posterior is in the form
    of numerical samples, we can compute complex loss functions that donâ€™t need to
    be restricted by mathematical convenience or mere simplicity. The following code,
    and *Figure [2.8](#x1-51021r8)* generated from it, is just a silly example of
    this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.11**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![PIC](img/file68.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.8**: A weird loss function applied to the posterior from `our_first_model`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have been discussing the main notions of Bayesian statistics
    and probabilistic programming using the BetaBinomial model mainly because of its
    simplicity. In our path to build more complex models, we now shift our focus to
    delve into the realm of Gaussian inference.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Gaussians all the way down
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gaussians are very appealing from a mathematical point of view. Working with
    them is relatively easy, and many operations applied to Guassians return another
    Gaussian. Additionally, many natural phenomena can be nicely approximated using
    Gaussians; essentially, almost every time that we measure the average of something,
    using a *big enough* sample size, that average will be distributed as a Gaussian.
    The details of when this is true, when this is not true, and when this is more
    or less true, are elaborated in the **central limit theorem** (CLT); you may want
    to stop reading now and search about this really *central* statistical concept
    (terrible pun intended).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Well, we were saying that many phenomena are indeed averages. Just to follow
    a clichÃ©, the height (and almost any other trait of a person, for that matter)
    is the result of many environmental factors and many genetic factors, and hence
    we get a nice Gaussian distribution for the height of adult people. Indeed, we
    get a mixture of two Gaussians, which is the result of overlapping the distribution
    of heights of women and men, but you get the idea. In summary, Gaussians are easy
    to work with and abundant in natural phenomena; hence, many of the statistical
    methods you may already know assume normality. Thus, it is important to learn
    how to build these models, and then it is equally important to learn how to relax
    the normality assumptions, something that is surprisingly easy in a Bayesian framework
    and with modern computational tools such as PyMC.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Gaussian inferences
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nuclear magnetic resonance (NMR) is a powerful technique used to study molecules
    and also living things such as humans, sunflowers, and yeast (because, after all,
    *we are just a bunch of molecules*). NMR allows you to measure different kinds
    of observable quantities related to interesting unobservable molecular properties
    [Arroyuelo etÂ al.](Bibliography.xhtml#Xarroyuelo_2021)Â [[2021](Bibliography.xhtml#Xarroyuelo_2021)].
    One of these observables is known as chemical shift, which we can only get for
    the nuclei of certain types of atoms. The details belong to quantum chemistry
    and they are irrelevant to this discussion. For all we care at the moment, we
    could have been measuring the height of a group of people, the average time to
    travel back home, or the weights of bags of oranges. In these examples the variables
    are continuous, and it makes sense to think of them as an average value plus a
    dispersion. Sometimes we can use a Gaussian model for discrete variables if the
    number of possible values is large enough; for example, bonobos are very promiscuous,
    so maybe we can model the number of sexual partners of our cousins with a Gaussian.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our example, we have 48 chemical shift values represented in a
    boxplot in *Figure [2.9](#x1-53004r9)*. We can see that the median (the line inside
    the box) is around 53 and the interquartile range (the box) is around 52 and 55\.
    We can see that there are two values far away from the rest of the data (empty
    circles).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file69.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.9**: Boxplot of the 48 chemical shift values. We observed two values
    above 60, far away from the rest of the data'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s forget about those two points for a moment and assume that a Gaussian
    distribution is a good description of the data. Since we do not know the mean
    or the standard deviation, we must set priors for both of them. Therefore, a reasonable
    model could be:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Î¼ âˆ¼ ğ’° (l,h ) Ïƒ âˆ¼ â„‹ ğ’© (ÏƒÏƒ) Y âˆ¼ ğ’© (Î¼, Ïƒ) ](img/file70.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: '![](img/U.PNG)(*l,h*) is the Uniform distribution between *l*, and *h*, ![](img/HN.PNG)(*Ïƒ*[*Ïƒ*])
    is the HalfNormal distribution with scale *Ïƒ*[*Ïƒ*], and ![](img/N.PNG)(*Î¼,Ïƒ*)
    is the Gaussian distribution with mean *Î¼* and standard deviation *Ïƒ*. A HalfNormal
    distribution considers the absolute values of Normal distribution centered around
    zero. *Figure [2.10](#x1-53005r10)* shows the graphical representation of this
    model.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file71.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.10**: Graphical representation of `model_g`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'If we do not know the possible values of *Î¼* and *Ïƒ*, we can set priors reflecting
    our ignorance. One option is to set the boundaries of the Uniform distribution
    to be *l* = 40, *h* = 75, which is a range that is larger than the range of the
    data. Alternatively, we can choose a range based on our previous knowledge. For
    instance, we may know that this is not physically possible to have values below
    0 or above 100 for this type of measurement and thus use those values as the boundaries
    of the Uniform distribution. For the HalfNormal, and in the absence of more information,
    we can choose a large value compared to the scale of the data. The PyMC code for
    the model represented in *Figure [2.10](#x1-53005r10)* is:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.12**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Letâ€™s see what the posterior looks like. *Figure [2.11](#x1-53013r11)* was generated
    with the ArviZ function `plot_trace`. It has one row for each parameter. For this
    model, the posterior is bidimensional, so each row shows one marginal distribution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file72.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.11**: Posterior from `model_g` ploted using `az.plot_trace(idata_g)`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `plot_pair` function from ArviZ to see what the bi-dimensional
    posterior looks like, together with the marginal distributions for *Î¼* and *Ïƒ*.
    See *Figure [2.12](#x1-53014r12)*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file73.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.12**: Posterior from `model_g` ploted using `az.plot_pair(idata_g,
    kind=â€™kdeâ€™, marginals=True)`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to print the summary for later use (see *Table [2.3](#x1-53018r3)*).
    We use the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.13**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|  | mean | sd | hdi_3% | hdi_97% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| *Î¼* | 53.50 | 0.52 | 52.51 | 54.44 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| *Ïƒ* | 3.52 | 0.38 | 2.86 | 4.25 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '**TableÂ 2.3**: Summary statistics for *Î¼* and *Ïƒ*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Posterior predictive checks
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the nice elements of the Bayesian toolkit is that once we have a posterior
    *p*(*Î¸*|*Y* ), it is possible to use it to generate predictions *p*(*á»¸*). Mathematically,
    this can be done by computing:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![ âˆ« p(ËœY | Y ) = p(ËœY | Î¸) p(Î¸ | Y )dÎ¸ ](img/file74.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: This distribution is known as the **posterior predictive distribution**. It
    is *predictive* because it is used to make predictions, and *posterior* because
    it is computed using the posterior distribution. So we can think of this as the
    distribution of future data given the model, and observed data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PyMC is easy to get posterior predictive samples; we donâ€™t need to compute
    any integral. We just need to call the `sample_posterior_predictive` function
    and pass the `InferenceData` object as the first argument. We also need to pass
    the `model` object, and we can use the `extend_inferencedata` argument to add
    the posterior predictive samples to the `InferenceData` object. The code is:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.14**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One common use of the posterior predictive distribution is to perform posterior
    predictive checks. These are a set of tests that can be used to check if the model
    is a good fit for the data. We can use the `plot_ppc` function from ArviZ to visualize
    the posterior predictive distribution and the observed data. The code is:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.15**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![PIC](img/file75.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.13**: Posterior predictive check for `model_g` ploted using `az.plot_ppc`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure [2.13](#x1-54009r13)*, the black line is a KDE of the data and the
    gray lines are KDEs computed from each one of the 100 posterior predictive samples.
    The gray lines reflect the uncertainty we have about the distribution of the predicted
    data. The plots look *hairy* or *wonky*; this will happen when you have very few
    data points. By default, the KDEs in ArviZ are estimated within the actual range
    of the data and assumed to be zero outside. While some might consider this a bug,
    I think itâ€™s a feature, since itâ€™s reflecting a property of the data instead of
    over-smoothing it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure [2.13](#x1-54009r13)*, we can see that the mean of the simulated
    data is slightly displaced to the right and that the variance seems to be larger
    for the simulated data than for the actual data. The source of this discrepancy
    can be attributed to the combination of our choice of likelihood and the two observations
    away from the bulk of the data (the empty dots in *Figure [2.9](#x1-53004r9)*).
    How can we interpret this plot? Is the model wrong or right? Can we use it or
    do we need a different model? Well, it depends. The interpretation of a model
    and its evaluation and criticism are always context-dependent. Based on my experience
    with this kind of measurement, I would say this model is a reasonable enough representation
    of the data and a useful one for most of my analysis. Nevertheless, it is important
    to keep in mind that we could find other models that better accommodate the whole
    dataset, including the two observations that are far from the bulk of the data.
    Letâ€™s see how we can do that.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Robust inferences
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One objection we may have with `model_g` is that we are assuming a Normal distribution,
    but we have two data points away from the bulk of the data. By using a Normal
    distribution for the likelihood, we are indirectly assuming that we are not expecting
    to see a lot of data points far away from the bulk. *Figure [2.13](#x1-54009r13)*
    shows the result of combining these assumptions with the data. Since the tails
    of the Normal distribution fall quickly as we move away from the mean, the Normal
    distribution (at least an anthropomorphized one) is *surprised by seeing* those
    two points and *reacts* in two ways, moving its mean towards those points and
    increasing its standard deviation. Another intuitive way of interpreting this
    is by saying that those points have an excessive weight in determining the parameters
    of the Normal distribution.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: So, what can we do? One option is to check for errors in the data. If we retrace
    our steps we may find an error in the code while cleaning or preprocessing the
    data, or we can relate the putative anomalous values to the malfunction of the
    measuring equipment. Unfortunately, this is not always an option. Many times,
    the data was collected by others and we donâ€™t have a good register of how it was
    collected, measured or processed. Anyway, inspecting the data before modeling
    is always a good idea, thatâ€™s a good practice in general.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option is to declare those points outliers and remove them from the
    data. Two common rules of thumb for identifying outliers in a dataset are:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the interquartile range (IQR): Any data point that falls below 1.5 times
    the IQR from the lower quartile, or above 1.5 times the IQR from the upper quartile,
    is considered an outlier.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the standard deviation: Any data point that falls below or above *N*
    times the standard deviation of the data is considered an outlier. With *N* usually
    being 2 or 3.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, itâ€™s important to note that, like any automatic method, these rules
    of thumb are not perfect and may result in discarding valid data points.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: From a modeling perspective, instead of blaming the data we can blame the model
    and change it, as explained in the next section. As a general rule, Bayesians
    prefer to encode assumptions directly into the model by using different priors
    and likelihoods rather than through ad hoc heuristics such as outlier removal
    rules.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.1 Degrees of normality
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is one distribution that looks very similar to a Normal distribution.
    It has three parameters: a location parameter *Î¼*, a scale parameter *Ïƒ*, and
    a normality parameter *Î½*. This distributionâ€™s name is Studentâ€™s t-distribution.
    *Figure [2.14](#x1-56002r14)* shows members of this family. When *Î½* = âˆ the distribution
    is the Normal distribution, *Î¼* is the mean and *Ïƒ* is the standard deviation.
    When *Î½* = 1, we get the Cauchy or Lorentz distribution. *Î½* can go from 0 to
    âˆ. The lower this number, the heavier their tails. We can also say that the lower
    the value of *Î½*, the higher the kurtosis. The kurtosis is the fourth moment,
    as you may remember from the previous chapter. By heavy tails, we mean that it
    is more probable to find values away from the mean compared to a Normal, or in
    other words, values are not as concentrated around the mean as in a lighter tail
    distribution like the Normal. For example, 95% of the values from a Studentâ€™s
    t (*Î¼* = 0*,Ïƒ* = 1*,Î½* = 1) are found between -12.7 and 12.7\. Instead, for a
    Normal (*Î¼* = 0*,Ïƒ* = 1*,Î½* = âˆ), this occurs between -1.96 and 1.96.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file76.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.14**: The Studentâ€™s t-distribution'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'A very curious feature of the Studentâ€™s t-distribution is that it has no defined
    mean value when *Î½* â‰¤ 1\. While any finite sample from a Studentâ€™s t-distribution
    is just a bunch of numbers from which it is always possible to compute an empirical
    mean, the theoretical distribution itself is the one without a defined value for
    the mean. Intuitively, this can be understood as follows: the tails of the distribution
    are so heavy that at any moment we might get a sampled value from almost anywhere
    from the real line, so if we keep getting numbers, we will never approach a fixed
    value. Instead, the estimate will keep wandering around.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Degrees of what?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: In most textbooks, the parameter *Î½* from the Studentâ€™s t-distribution is referred
    to as the degrees of freedom parameter. However, I prefer to follow Kruschkeâ€™s
    suggestion and call it the normality parameter. This name is more descriptive
    of the parameterâ€™s role in the distribution, especially as used for robust regression.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the variance of this distribution is only defined for values of *Î½
    >* 2\. So, itâ€™s important to note that the scale of the Studentâ€™s t-distribution
    isnâ€™t the same as its standard deviation. The scale and the standard deviation
    become closer and closer as *Î½* approaches infinity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.2 A robust version of the Normal model
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are going to rewrite the previous model (`model_g`) by replacing the Gaussian
    distribution with the Studentâ€™s t-distribution. Because the Studentâ€™s t-distribution
    has one more parameter, *Î½*, than the Gaussian, we need to specify one more prior,
    for this model we decided to use the exponential distribution, but other distributions
    restricted to the positive interval could also work.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Î¼ âˆ¼ ğ’°(l,h) Ïƒ âˆ¼ â„‹ ğ’© (Ïƒ ) Ïƒ Î½ âˆ¼ Exp (Î» ) Y âˆ¼ ğ’¯ (Î½,Î¼, Ïƒ) ](img/file77.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: '*Figure [2.15](#x1-57002r15)* shows the graphical representation of this model'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file78.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.15**: Graphical representation of `model_t`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s write this model in PyMC; as usual, we can (re)write models by specifying
    a few lines. The only cautionary word here is that by default the Exponential
    distribution in PyMC is parameterized with the inverse of the mean. We are going
    to set *Î½* as an Exponential distribution with a mean of 30\. From *Figure [2.14](#x1-56002r14)*,
    we can see that a Studentâ€™s t-distribution with *Î½* = 30 looks pretty similar
    to a Gaussian (even when it is not). In fact, from the same diagram, we can see
    that *most of the action* happens for relatively small values of *Î½*. Hence, we
    can say that the Exponential prior with a mean of 30 is a weakly informative prior
    telling the model we more or less think should be around 30 but can move to smaller
    and larger values with ease. In many problems, estimating *Î½* is of no direct
    interest.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.16**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Compare the trace from `model_g` (*Figure [2.11](#x1-53013r11)*) with the trace
    of `model_t` (*Figure [2.16](#x1-57011r16)*):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file79.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.16**: Posterior from `model_t` plotted using `az.plot_trace(idata_t)`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Now, print the summary of `model_t`. You should get something like *Table [2.4](#x1-57012r4)*.
    Compare the results with those from `model_g`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '|  | mean | sd | hdi_3% | hdi_97% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| *Î¼* | 53.02 | 0.39 | 52.27 | 53.71 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| *Ïƒ* | 2.21 | 0.42 | 1.46 | 3.01 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| *Î½* | 4.94 | 5.45 | 1.07 | 10.10 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '**TableÂ 2.4**: Summary statistics for *Î¼*, *Ïƒ*, and *Î½*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Before you keep reading, take a moment to compare the preceding results with
    those from `model_g` and spot the difference between both results. Did you notice
    something interesting?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: The estimation of *Î¼* between both models is similar, with a difference of â‰ˆ
    0*.*5\. The estimation of *Ïƒ* is â‰ˆ 3*.*5 for `model_g` and â‰ˆ 2*.*2 for `model_t`.
    This is a consequence of the Studentâ€™s t-distribution allocating less weight to
    values away from the mean. Loosely speaking, the Studentâ€™s t-distribution is *less
    surprised* by values away from the mean. We can also see that the mean of *Î½*
    is â‰ˆ 5, meaning that we have a heavy-tailed distribution and not a Gaussian-like
    distribution.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure [2.17](#x1-57014r17)* shows a posterior predictive check for `model_t`.
    Letâ€™s compare it with the one from `model_g` (*Figure [2.13](#x1-54009r13)*).
    Using the Studentâ€™s t-distribution in our model leads to predictive samples that
    seem to better fit the data in terms of the location of the peak of the distribution
    and also its spread. Notice how the samples extend far away from the bulk of the
    data, and how a few of the predictive samples look very flat. This is a direct
    consequence of the Studentâ€™s t-distribution expecting to see data points far away
    from the mean or bulk of the data. If you check the code used to generate *Figure
    [2.17](#x1-57014r17)* you will see that we have used `ax.set_xlim(40, 70)`.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file80.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.17**: Posterior predictive check for `model_t`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The Studentâ€™s t-distribution allows us to have a more **robust estimation**
    of the mean and standard deviation because the outliers have the effect of decreasing
    *Î½* instead of pulling the mean or increasing the standard deviation. Thus, the
    mean and the scale are estimated by weighting the data points close to the bulk
    more than those apart from it. As a rule of thumb, for values of *Î½ >* 2 and *not
    too small*, we can consider the scale of a Studentâ€™s t-distribution as a reasonable
    practical proxy for the standard deviation of the data after removing outliers.
    This is a rule of thumb because we know that the scale is not the standard deviation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 InferenceData
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: InferenceData is a rich container for the results of Bayesian inference. A modern
    Bayesian analysis potentially generates many sets of data including posterior
    samples and posterior predictive samples. But we also have observed data, samples
    from the prior, and even statistics generated by the sampler. All this data, and
    more, can be stored in an InferenceData object. To help keep all this information
    organized, each one of these sets of data has its own group. For instance, the
    posterior samples are stored in the `posterior` group. The observed data is stored
    in the `observed_data` group.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure [2.18](#x1-58002r18)* shows an HTML representation of the InferenceData
    for `model_g`. We can see 4 groups: `posterior`, `posterior_predictive`, `sample_stats`,
    and `observed_data`. All of them are collapsed except for the `posterior` group.
    We can see we have two coordinates `chain` and `draw` of dimensions 4 and 1000
    respectively. We also have 2 variables *Î¼* and *Ïƒ*.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file81.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.18**: InferenceData object for `model_g`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, PyMC has generated an InferenceData object and ArviZ has used that
    to generate plots or numerical summaries. But we can also manipulate an InferenceData
    object. Some common operations are to access specific groups. For instance, to
    access the posterior group we can write:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.17**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will return an xarray dataset. If you are not familiar with xarray [[Hoyer
    and Hamman](Bibliography.xhtml#Xxarray_2017),Â [2017](Bibliography.xhtml#Xxarray_2017)]
    ( [https://docs.xarray.dev/en/stable/](https://docs.xarray.dev/en/stable/)), imagine
    NumPy multidimensional arrays but with labels! This makes many operations easier
    as you donâ€™t have to remember the order of the dimensions. For example, the following
    code will return the first draw from chain 0 and chain 2:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.18**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We use the `sel` method to select a range of values, like the first 100 draws
    from all chains:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.19**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Additionally, the following returns the mean for *Î¼* and *Ïƒ* computed over
    all draws and chains:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.20**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Meanwhile, the following code returns the mean over the draws, i.e., this returns
    four values for *Î¼* and four values for *Ïƒ*, one per chain:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.21**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'More often than not, we donâ€™t care about chains and draws, we just want to
    get the posterior samples. In those cases, we can use the `az.extract` function:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.22**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This combines the `chain` and `draw` into a `sample` coordinate which can make
    further operations easier. By default, `az.extract` works on the posterior, but
    you can specify other groups with the `group` argument. You can also use `az.extract`
    to get a random sample of the posterior:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.23**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are going to use the InferenceData object all the time in this book, so you
    will have the time to get familiar with it and learn more about it in the coming
    pages.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 2.8 Groups comparison
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One pretty common statistical analysis is group comparison. We may be interested
    in how well patients respond to a certain drug, the reduction of car accidents
    by the introduction of new traffic regulations, student performance under different
    teaching approaches, and so on. Sometimes, this type of question is framed under
    the hypothesis testing scenario and the goal is to declare a result *statistically
    significant*. Relying only on statistical significance can be problematic for
    many reasons: on the one hand, statistical significance is not equivalent to practical
    significance; on the other hand, a really small effect can be declared significant
    just by collecting enough data.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: The idea of hypothesis testing is connected to the concept of p-values. This
    is not a fundamental connection but a cultural one; people are used to thinking
    that way mostly because thatâ€™s what they learn in most introductory statistical
    courses. There is a long record of studies and essays showing that, more often
    than not, p-values are used and interpreted the wrong way, even by people who
    are using them daily. Instead of doing hypothesis testing, we are going to take
    a different route and we are going to focus on estimating the effect size, that
    is, quantifying the difference between two groups. One advantage of thinking in
    terms of effect size is that we move away from yes-no questions like â€Does it
    work?â€ or â€Is there any effect?â€ and into the more nuanced type of questions like
    â€How well does it work?â€ or â€How large is the effect?â€.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, when comparing groups, people talk about a control group and a treatment
    group. For example, when we want to test a new drug, we want to compare the new
    drug (the treatment) against a placebo (the control group). The placebo effect
    is a psychological phenomenon where a patient experiences perceived improvements
    in their symptoms or condition after receiving an inactive substance or treatment.
    By comparing the effects of the drug with a placebo group in clinical trials,
    researchers can discern whether the drug is genuinely effective. The placebo effect
    is an example of the broader challenge in experimental design and statistical
    analysis of the difficulty of accounting for all factors in an experiment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: One interesting alternative to this design is to compare the new drug with the
    commercially available most popular or efficient drug to treat that illness. In
    such a case, the control group cannot be a placebo; it should be the other drug.
    Bogus control groups are a splendid way to lie using statistics.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine you work for a dairy product company that wants to sell
    overly sugared yogurts to kids by telling their dads and moms that this particular
    yogurt boosts the immune system or helps their kids grow stronger. One way to
    cheat with data is by using milk or even water as a control group, instead of
    another cheaper, less sugary, less marketed yogurt. It may sound silly when I
    put it this way, but I am describing actual experiments published in actual scientific
    journals. When someone says something is harder, better, faster, or stronger,
    remember to ask what the baseline used for the comparison was.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.1 The tips dataset
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To explore the subject matter of this section, we are going to use the tips
    dataset [[Bryant and Smith](Bibliography.xhtml#Xbryant_1995),Â [1995](Bibliography.xhtml#Xbryant_1995)].
    We want to study the effect of the day of the week on the tips earned at a restaurant.
    For this example, the different groups are the days. Notice there is no control
    group or treatment group. If we wish, we can arbitrarily establish one day (for
    example, Thursday) as the reference or control. For now, letâ€™s start the analysis
    by loading the dataset as a pandas DataFrame using just one line of code. If you
    are not familiar with pandas, the `tail` command is used to show the last rows
    of a DataFrame (see *Table [2.5](#x1-60007r5)*), you may want to try using `head`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.24**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|  | total_bill | tip | sex | smoker | day | time | size |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| 239 | 29.03 | 5.92 | Male | No | Sat | Dinner | 3 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| 240 | 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| 241 | 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| 242 | 17.82 | 1.75 | Male | No | Sat | Dinner | 2 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| 243 | 18.78 | 3.00 | Female | No | Thurs | Dinner | 2 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '**TableÂ 2.5**: Sample data from a restaurant'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: From this DataFrame, we are only going to use the day and tip columns. *Figure
    [2.19](#x1-60009r19)* shows the distributions of this data using ridge plots.
    This figure was done with ArviZ. Even though ArviZ is designed for Bayesian model
    analysis, some of its functions can be useful for data analysis.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file82.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.19**: Distribution of tips by day'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: We are going to do some small preprocessing of the data. First, we are going
    to create the `tip` variable representing the tips in dollars. Then we create
    the `idx` variable, a categorical dummy variable encoding the days with numbers,
    that is, `[0, 1, 2, 3]` instead of `[â€™Thurâ€™, â€™Friâ€™, â€™Satâ€™, â€™Sunâ€™]`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.25**'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The model for this problem is almost the same as `model_g`; the only difference
    is that now *Î¼* and *Ïƒ* are going to be vectors instead of scalars. PyMC syntax
    is extremely helpful for this situation: instead of writing for loops, we can
    write our model in a vectorized way.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.26**'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice how we passed a `shape` argument for the prior distribution. For *Î¼*,
    this means that we are specifying four independent ![](img/N.PNG)(0*,*10) and
    for *Ïƒ* four independent ![](img/HN.PNG)(10). Also, notice how we use the `idx`
    variable to properly index the values of *Î¼* and *Ïƒ* we pass to the likelihood.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: PyMC provides an alternative syntax, which consists of specifying coordinates
    and dimensions. The advantage of this alternative is that it allows better integration
    with ArviZ.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we have 4 values for the means and 4 for the standard deviations,
    and thatâ€™s why we use `shape=4`. The InferenceData will have 4 indices `0, 1,
    2, 3` mapping to each of the 4 days. However, it is the userâ€™s job to associate
    those numerical indices with the days. By using coordinates and dimensions we,
    and ArviZ, can use the labels `"Thur", "Fri", "Sat", "Sun"` to easily map parameters
    to their associated days.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: We are going to specify two coordinates; `"days"`, with the dimensions `"Thur",
    "Fri", "Sat", "Sun"`; and `"days_flat"`, which will contain the same labels but
    repeated according to the order and length that corresponds to each observation.
    `"days_flat"` will be useful later for posterior predictive tests.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.27**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Once the posterior distribution is computed, we can do all the analyses that
    we believe are pertinent. For instance, we can do a posterior predictive test.
    With the help of ArviZ, we can do it by calling `az.plot_ppc`. We use the `coords`
    and `flatten` parameters to get one subplot per day.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeÂ 2.28**'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: From the following figure, we can see that the model can capture the general
    shape of the distributions, but still, some details are elusive. This may be due
    to the relatively small sample size, factors other than day influencing the tips,
    or a combination of both.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file83.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.20**: Posterior predictive checks for the tips dataset'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: For now, we are going to consider that the model is good enough for us and move
    to explore the posterior. We can explain the results in terms of their average
    values and then find for which days that average is higher. But there are other
    alternatives; for instance, we may want to express the results in terms of differences
    in posterior means. In addition, we might want to use some measure of effect size
    that is popular with our audiences, such as the probability of superiority or
    Cohenâ€™s d. In the next sections, we explain these alternatives.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.2 Cohenâ€™s d
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A common way to measure the effect size is Cohenâ€™s d, which is defined as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![-Î¼2 âˆ’-Î¼1 âˆ˜ Ïƒ21+Ïƒ22- --2-- ](img/file84.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Because we have a posterior distribution we can compute a distribution of Cohenâ€™s
    d, and if we want a single value we can compute the mean or median of that distribution.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: This expression tells us that the effect size is the difference between the
    means scaled by the pooled standard deviation of both groups. By taking the pooled
    standard deviation, we are standardizing the differences of means. This is important
    because when you have a difference of 1 and a standard deviation of 0.1, the effect
    size is larger than the same difference when the standard deviation is 10\. A
    Cohenâ€™s d can be interpreted as a Z-score (a standard score). A Z-score is the
    signed number of standard deviations by which a value differs from the mean value
    of what is being observed or measured. Thus, a value of 0.5 Cohenâ€™s d could be
    interpreted as a difference of 0.5 standard deviations from one group to the other.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Even when the differences of means are standardized, we may still need to calibrate
    ourselves based on the context of a given problem to be able to say if a given
    value is big, small, medium, and so on. For instance, if we are used to performing
    several analyses for the same or similar problems, we can get used to a Cohenâ€™s
    d of say 1\. So when we get a Cohenâ€™s d of say 2, we know that we have something
    important (or someone made a mistake somewhere!). If you do not have this practice
    yet, you can ask a domain expert for their valuable input.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: A very nice web page to explore what different values of Cohenâ€™s d look like
    is [http://rpsychologist.com/d3/cohend](http://rpsychologist.com/d3/cohend). On
    that page, you will also find other ways to express an effect size; some of them
    could be more intuitive, such as the probability of superiority, which we will
    discuss next.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.3 Probability of superiority
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is another way to report the effect size, and this is defined as the probability
    that a data point taken at random from one group has a larger value than one also
    taken at random from the other group. If we assume that the data we are using
    is normally distributed, we can compute the probability of superiority from Cohenâ€™s
    d using the following expression:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![ ( Î´ ) ps = Î¦ âˆš--- 2 ](img/file85.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Î¦ is the cumulative Normal distribution and *Î´* is the Cohenâ€™s d.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: If we are OK with the normality assumption, we can use this formula to get the
    probability of superiority from the value of Cohenâ€™s d. Otherwise, we can compute
    the probability of superiority directly from the posterior samples just by taking
    random samples from two groups and counting how many times one value is larger
    than the other. To do that we donâ€™t need Cohenâ€™s d or assume normality (see the
    Exercises section). This is an example of an advantage of using **Markov Chain
    Monte Carlo** (**MCMC**) methods; once we get samples from the posterior, we can
    compute many quantities from it often in ways that are easier than with other
    methods.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 2.8.4 Posterior analysis of mean differences
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To conclude our previous discussions, letâ€™s compute the posterior distributions
    for differences in means, Cohenâ€™s d, and the probability of superiority, and integrate
    them into a single plot. *Figure [2.21](#x1-63003r21)* has a lot of information.
    Depending on the audience, the plot may be overloaded, or too crowded. Perhaps
    it is useful for a discussion within your team, but for the general public, it
    may be convenient to remove elements or distribute the information between a figure
    and a table or two figures. Anyway, here we show it precisely so you can compare
    different ways of presenting the same information, so take some time to ponder
    this figure.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file86.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: '**FigureÂ 2.21**: Posterior distributions of the differences of means, Cohenâ€™s
    d, and the probability of superiority for the tips dataset'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: One way to read *Figure [2.21](#x1-63003r21)* is to compare the reference value,
    of zero difference, with the HDI interval. We have only one case when the 94%
    HDI excludes the reference value, that is, the difference in tips between Thursday
    and Sunday. For all the other comparisons, we cannot rule out a difference of
    zero, at least according to the HDI-reference-value-overlap criteria. But even
    for that case, the average difference is â‰ˆ 0*.*5 dollars. Is that difference large
    enough? Is that difference enough to accept working on Sunday and missing the
    opportunity to spend time with family or friends? Is that difference enough to
    justify averaging the tips over the four days and giving every waitress and waiter
    the same amount of tip money?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The short answer is that those kinds of questions cannot be answered by statistics;
    they can only be informed by statistics. I hope you donâ€™t feel cheated by that
    answer, but we cannot get automatic answers unless we include in the analysis
    all the values that are important to the stakeholders. Formally, that requires
    the definition of a loss function or at least the definition of some threshold
    value for the effect size, which should be informed by those values.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 2.9 Summary
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although Bayesian statistics is conceptually simple, fully probabilistic models
    often lead to analytically intractable expressions. For many years, this was a
    huge barrier, hindering the wide adoption of Bayesian methods. Fortunately, maths,
    statistics, physics, and computer science came to the rescue in the form of numerical
    methods that are capableâ€”at least in principleâ€”of solving any inference problem.
    The possibility of automating the inference process has led to the development
    of probabilistic programming languages, allowing a clear separation between model
    definition and inference. PyMC is a Python library for probabilistic programming
    with a very simple, intuitive, and easy-to-read syntax that is also very close
    to the statistical syntax used to describe probabilistic models.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the PyMC library by revisiting the coin-flip model from *Chapter
    [1](CH01.xhtml#x1-160001)*, this time without analytically deriving the posterior.
    PyMC models are defined inside a context manager. To add a probability distribution
    to a model, we just need to write a single line of code. Distributions can be
    combined and can be used as priors (unobserved variables) or likelihoods (observed
    variables). If we pass data to a distribution, it becomes a likelihood. Sampling
    can be achieved with a single line as well. PyMC allows us to get samples from
    the posterior distribution. If everything goes right, these samples will be representative
    of the correct posterior distribution and thus they will be a representation of
    the logical consequences of our model and data. We can explore the posterior generated
    by PyMC using ArviZ, a Python library that works hand-in-hand with PyMC and can
    be used, among other tasks, to help us interpret and visualize posterior distributions.
    One way of using a posterior to help us make inference-driven decisions is by
    comparing the ROPE against the HDI interval. We also briefly mentioned the notion
    of loss functions, a formal way to quantify the trade-offs and costs associated
    with making decisions in the presence of uncertainty. We learned that loss functions
    and point estimates are intimately associated.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, the discussion was restricted to a simple one-parameter model.
    Generalizing to an arbitrary number of parameters is trivial with PyMC; we exemplify
    how to do this with the Gaussian and Studentâ€™s t models. The Gaussian distribution
    is a special case of the Studentâ€™s t-distribution and we showed you how to use
    the latter to perform robust inferences in the presence of outliers. In the next
    chapters, we will look at how these models can be used as part of linear regression
    models. We used a Gaussian model to compare groups. While this is sometimes framed
    in the context of hypothesis testing, we take another route and frame this task
    as a problem of inferring the effect size, an approach we generally consider to
    be richer and more productive. We also explored different ways to interpret and
    report effect sizes.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: With all that we have learned in this and the previous chapter, we are ready
    to study one of the most important concepts in this book, hierarchical models.
    That will be the topic of the next chapter.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 2.10 Exercises
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using PyMC, change the parameters of the prior Beta distribution in `our_first_model`
    to match those of the previous chapter. Compare the results to the previous chapter.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the model `our_first_model` with prior *Î¸* âˆ¼ Beta(1*,*1) with a model
    with prior *Î¸* âˆ¼![](img/U.PNG)(0*,*1). Are the posteriors similar or different?
    Is the sampling slower, faster, or the same? What about using a Uniform over a
    different interval such as [-1, 2]? Does the model run? What errors do you get?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PyMC has a function `pm.model_to_graphviz` that can be used to visualize the
    model. Use it to visualize the model `our_first_model`. Compare the result with
    the Kruschke diagram. Use `pm.model_to_graphviz` to visualize model `comparing_groups`.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read about the coal mining disaster model that is part of the PyMC documentation
    ( [https://shorturl.at/hyCX2](https://shorturl.at/hyCX2)). Try to implement and
    run this model by yourself.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify `model_g`, change the prior for the mean to a Gaussian distribution centered
    at the empirical mean, and play with a couple of reasonable values for the standard
    deviation of this prior. How robust/sensitive are the inferences to these changes?
    What do you think of using a Gaussian, which is an unbounded distribution (goes
    from âˆ’inf to inf), to model bounded data such as this? Remember that we said it
    is not possible to observe values below 0 or above 100.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the data from the `chemical_shifts.csv` file, compute the empirical mean
    and the standard deviation with and without outliers. Compare those results to
    the Bayesian estimation using the Gaussian and Studentâ€™s t-distribution. What
    do you observe?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the previous exercise by adding more outliers to `chemical_shifts.csv`,
    and compute new posteriors for `model_g` and `model_t` using this new data. What
    do you observe?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the InferenceData object `idata_cg`.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many groups does it contain?
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspect the posterior distribution of the parameter *Î¼* for a specific day using
    the `sel` method.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the distributions of mean differences between Thursday and Sunday. What
    are the coordinates and dimensions of the resulting DataArray?
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For the tips example compute the probability of superiority directly from the
    posterior (without computing Cohenâ€™s d first). You can use the `pm.sample_posterior_predictive()`
    function to take a sample from each group. Is it different from the calculation
    assuming normality? Can you explain the result?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our community Discord space
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG

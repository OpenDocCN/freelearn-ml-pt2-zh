- en: Building a Chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine for a moment that you're sitting alone in a quiet, spacious room. To
    your right is a small table with a stack of white printer paper and a single black
    pen. In front of you is what seems to be a large, red cube with a tiny opening—slightly
    smaller than the size of a mail slot. An inscription just above the slot invites
    you to write down a question and pass it through the slot. As it happens, you
    speak Mandarin; so, you write down your question in Mandarin on one of the sheets
    and insert it into the opening. A few moments pass, and then slowly, an answer
    emerges. It's also written in Chinese and is the just the sort of answer you might
    have expected. So, what did you ask? *Are you a person or a computer?* And the
    response? *Why yes, yes I am*.
  prefs: []
  type: TYPE_NORMAL
- en: This thought experiment is based on philosopher John Searle's Chinese Room Argument.
    The premise of the experiment is that if there were a person in the room who spoke
    no Chinese, but had a set of rules that allowed them to perfectly map English
    characters to Chinese characters, they could appear to the questioner to understand
    Chinese without actually having any understanding of it. Searle's argument was
    that algorithmic procedures that produce an intelligible output can't be said
    to have an *understanding* of that output. They lack a *mind*. His thought experiment
    was an attempt to combat the ideas of *strong AI*, or the notion that the human
    brain is essentially just a *wet machine*. Searle didn't believe that AI could
    be said to have consciousness, no matter how sophisticated its behavior appeared
    to an outside observer.
  prefs: []
  type: TYPE_NORMAL
- en: Searle published this experiment in 1980\. 31 years later, Siri would be released
    on the iPhone 4S. For anyone who has used Siri, it's clear that we have a long
    way to go before we might be confronted with uncertainty of whether the agent
    we are speaking to has a mind (though we might doubt it in people we know to be
    human). Despite the clunkiness these agents, or chatbots, have demonstrated in
    the past, the field is rapidly advancing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we're going to learn how to construct a chatbot from scratch.
    Along the way, we'll learn more about the history of the field and its future
    prospects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The Turing Test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The history of chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The design of chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Turing Test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 30 years before Searle's Chinese Room, Alan Turing posed the question, *can
    machines think?* in one of his more famous papers. Being the practical genius
    he was, he decided not to tackle that question head on, but to instead pose it
    in the framework of the *problem of other minds*. This problem asks, *how do we
    truly know that other people have minds like our own?* Since we can only observe
    their behavior—and not the inner workings of their mind—we must take it on faith
    that they are like us. Turing proposed that if a machine could behave as if it
    were intelligent, then we should view it as such. This, in a nutshell, is the
    *Turing Test*. Passing the test means convincing humans that a machine is a fellow
    human.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering at this point, *has a program ever successfully passed
    the Turing Test?* And the answer is that there really is no official Turing Test.
    The closest thing to an official test is the Loebner Prize. This is a contest
    held yearly that awards modest prizes to those chatbots that appear the most convincingly
    human to a panel of judges. If this sounds rigorous and academic, it has been
    described as anything but. Loebner himself is said to be quite the character,
    frequently professing his love of wine, marijuana, and prostitutes. His sponsorship
    for the contest is said to have been a product of his total distain for labor
    of any kind.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, by this point, you'll have realized that any claims about *passing
    the Turing Test *are more spectacle than actual science. Despite this, Turing's
    original concept is a useful construct.
  prefs: []
  type: TYPE_NORMAL
- en: The history of chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I still remember the day my father sat me down in front of his IBM XT with
    its black-and-green flickering monitor and asked me to talk to *it*. The conversation
    went something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '>Hello. How are you feeling today?'
  prefs: []
  type: TYPE_NORMAL
- en: '>Hi. Who is this?'
  prefs: []
  type: TYPE_NORMAL
- en: '>Hi there... How are you today?'
  prefs: []
  type: TYPE_NORMAL
- en: '>I''m fine. You didn''t tell me who you are.'
  prefs: []
  type: TYPE_NORMAL
- en: '>We should be discussing you, not me.'
  prefs: []
  type: TYPE_NORMAL
- en: '>We should? Why?'
  prefs: []
  type: TYPE_NORMAL
- en: '>Let''s change focus a bit... Tell me about your family.'
  prefs: []
  type: TYPE_NORMAL
- en: '>You are impossible to have a conversation with.'
  prefs: []
  type: TYPE_NORMAL
- en: '>Can you elaborate on that?'
  prefs: []
  type: TYPE_NORMAL
- en: '>You are killing me.'
  prefs: []
  type: TYPE_NORMAL
- en: '>Perhaps you would like me to be killing you.'
  prefs: []
  type: TYPE_NORMAL
- en: '>OK, you are freaking me out.'
  prefs: []
  type: TYPE_NORMAL
- en: '>Perhaps you''re really talking about yourself?'
  prefs: []
  type: TYPE_NORMAL
- en: '>What?? That makes no sense.'
  prefs: []
  type: TYPE_NORMAL
- en: '>How would an answer to that help you?'
  prefs: []
  type: TYPE_NORMAL
- en: '>END'
  prefs: []
  type: TYPE_NORMAL
- en: '>Very interesting.'
  prefs: []
  type: TYPE_NORMAL
- en: '>QUIT'
  prefs: []
  type: TYPE_NORMAL
- en: '>Thank you for talking with me.'
  prefs: []
  type: TYPE_NORMAL
- en: '>#$###$#@@....'
  prefs: []
  type: TYPE_NORMAL
- en: The *it* I was talking to was a program called **ELIZA**. Written in the mid-1960s
    at MIT's Artificial Intelligence Lab by Joseph Weizenbaum, ELIZA was created to
    mimic the responses of a Rogerian psychotherapist. Though nearly comical when
    examined in any depth, the program was capable of convincing some users that they
    were chatting to an actual human—a remarkable feat considering it was a scant
    200 lines of code that used randomization and regular expressions to parrot back
    responses. Even today, this simple program remains a staple of popular culture.
    If you ask Siri who ELIZA is, she'll tell you she's a friend and a brilliant psychiatrist.
  prefs: []
  type: TYPE_NORMAL
- en: If ELIZA was an early example of chatbots, what have we seen since that time?
    In recent years, there has been an explosion of new chatbots. The most notable
    of these is Cleverbot.
  prefs: []
  type: TYPE_NORMAL
- en: Cleverbot was released to the world using the web in 1997\. In the years since,
    the bot has racked up hundreds of millions of conversions, and, unlike early chatbots,
    Cleverbot, as its name suggests, appears to become more intelligent with each
    conversion. Though the exact details of the workings of the algorithm are difficult
    to find, it's said to work by recording all conversations in a database and finding
    the most appropriate response by identifying the most similar questions and responses
    in the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'I made up a nonsensical question, shown as follows, and you can see that it
    found something similar to the object of my question in terms of a string match:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c726d9f1-a00b-4a60-a52c-0909e423500b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I persisted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ee74215-5384-4f54-83c4-1c1df46de9ca.png)'
  prefs: []
  type: TYPE_IMG
- en: And, again, I got something... similar?
  prefs: []
  type: TYPE_NORMAL
- en: You'll also notice that topics can persist across the conversation. In response,
    I was asked to go into more detail and justify my answer. This is one of the things
    that appears to make Cleverbot, well, clever.
  prefs: []
  type: TYPE_NORMAL
- en: While chatbots that learn from humans can be quite amusing, they can also have
    a darker side.
  prefs: []
  type: TYPE_NORMAL
- en: Several years ago, Microsoft released a chatbot named Tay on to Twitter. People
    were invited to ask Tay questions, and Tay would respond in accordance with her
    *personality*. Microsoft had apparently programmed the bot to appear to be a 19-year-old
    American girl. She was intended to be your virtual *bestie*; the only problem
    was that she started tweeting out extremely racist remarks.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of these unbelievably inflammatory tweets, Microsoft was forced
    to pull Tay off Twitter and issue an apology.
  prefs: []
  type: TYPE_NORMAL
- en: '"As many of you know by now, on Wednesday we launched a chatbot called Tay.
    We are deeply sorry for the unintended offensive and hurtful tweets from Tay,
    which do not represent who we are or what we stand for, nor how we designed Tay.
    Tay is now offline and we''ll look to bring Tay back only when we are confident
    we can better anticipate malicious intent that conflicts with our principles and
    values."'
  prefs: []
  type: TYPE_NORMAL
- en: -March 25, 2016 Official Microsoft Blog
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, brands that want to release chatbots into the wild in the future should
    take a lesson from this debacle and plan for users to attempt to manipulate them
    to display the worst of human behavior.
  prefs: []
  type: TYPE_NORMAL
- en: There's no doubt that brands are embracing chatbots. Everyone from Facebook
    to Taco Bell is getting in on the game.
  prefs: []
  type: TYPE_NORMAL
- en: 'Witness the TacoBot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99d3a529-9e70-49f4-9478-89c70df123e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Yes, it's a real thing. And, despite the stumbles, like Tay, there's a good
    chance the future of UI looks a lot like TacoBot. One last example might even
    help explain why.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quartz recently launched an app that turns news into a conversation. Rather
    than lay out the day''s stories as a flat list, you are engaged in a chat as if
    you were getting news from a friend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/230650be-ef8d-4ccb-b59f-00e15d4e6267.png)'
  prefs: []
  type: TYPE_IMG
- en: 'David Gasca, a PM at Twitter, describes his experience using the app in a post
    on Medium. He describes how the conversational nature invoked feelings normally
    only triggered in human relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Unlike a simple display ad, in a conversational relationship with my app I
    feel like I owe something to it: I want to click. At the most subconscious level
    I feel the need to reciprocate and not let the app down: "The app has given me
    this content. It''s been very nice so far and I enjoyed the GIFs. I should probably
    click since it''s asking nicely."'
  prefs: []
  type: TYPE_NORMAL
- en: 'If that experience is universal—and I expect it is—this could be the next big
    thing in advertising, and I have no doubt that advertising profits will drive
    UI design:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The more the bot acts like a human, the more it will be treated like a human."'
  prefs: []
  type: TYPE_NORMAL
- en: -Mat Webb, Technologist and Co-Author of Mind Hacks
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you're probably dying to know how these things work, so let's
    get on with it!
  prefs: []
  type: TYPE_NORMAL
- en: The design of chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The original ELIZA application was 200-odd lines of code. The Python NLTK implementation
    is similarly short. An excerpt is provided from NLTK''s website ([http://www.nltk.org/_modules/nltk/chat/eliza.html](http://www.nltk.org/_modules/nltk/chat/eliza.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd034e74-d600-432f-8611-380af2fee11e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the code, input text was parsed and then matched against
    a series of regular expressions. Once the input was matched, a randomized response
    (that sometimes echoed back a portion of the input) was returned. So, something
    such as, *I need a taco* would trigger a response of, *Would it really help you
    to get a taco?* Obviously, the answer is yes, and, fortunately, we have advanced
    to the point that technology can provide you one (bless you, TacoBot), but this
    was early days still. Shockingly, some people actually believed ELIZA was a real
    human.
  prefs: []
  type: TYPE_NORMAL
- en: But what about more advanced bots? How are they built?
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, most chatbots you're likely to encounter aren't even using **machine
    learning** (**ML**); they're what's known as **retrieval-based** models. This
    means responses are predefined according to the question and the context. The
    most common architecture for these bots is something called **Artificial Intelligence
    Markup Language** (**AIML**). AIML is an XML-based schema for representing how
    the bot should interact given the user's input. It's really just a more advanced
    version of how ELIZA works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at how responses are generated using AIML. First, all input
    are preprocessed to normalize them. This means when you input *Waaazzup???* it''s
    mapped to *WHAT IS UP*. This preprocessing step funnels down the myriad ways of
    saying the same thing into one input that can run against a single rule. Punctuation
    and other extraneous input are removed as well at this point. Once that''s complete,
    the input is matched against the appropriate rule. The following is a sample template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'That is the basic setup, but you can also layer in wildcards, randomization,
    and prioritization schemes. For example, the following pattern uses wildcard matching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `*` wildcard matches one or more words before `FOR ME` and then repeats
    those back in the output template. If the user were to type in `Dance for me!`,
    the response would be `I'm a bot. I don't dance. Ever`.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, these rules don't make for anything that approximates any type
    of real intelligence, but there are a few tricks that strengthen the illusion.
    One of the better ones is the ability to generate responses conditioned on a topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here''s a rule that invokes a topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the topic is set, the rules specific to that context can be matched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what this interaction might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '>I like turtles!'
  prefs: []
  type: TYPE_NORMAL
- en: '>I feel like this whole turtle thing could be a problem. What do you like about
    them?'
  prefs: []
  type: TYPE_NORMAL
- en: '>I like how they hide in their shell.'
  prefs: []
  type: TYPE_NORMAL
- en: '>I wish, like a turtle, I could hide from this conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the continuity across the conversation adds a measure of realism.
  prefs: []
  type: TYPE_NORMAL
- en: You're probably thinking that this can't be state of the art in this age of
    deep learning, and you're right. While most bots are rule-based, the next generation
    of chatbots are emerging, and they're based on neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In 2015, Oriol Vinyas and Quoc Le of Google published a paper, [http://arxiv.org/pdf/1506.05869v1.pdf](https://arxiv.org/pdf/1506.05869v1.pdf),
    that described the construction of a neural network based on sequence-to-sequence
    models. This type of model maps an input sequence, such as *ABC*, to and output
    sequence, such as *XYZ*. These inputs and outputs might be translations from one
    language to another, for example. In the case of their work here, the training
    data was not language translation, but rather tech support transcripts and movie
    dialogues. While the results from both models are interesting, it was the interactions
    based on the movie model that stole the headlines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are sample interactions taken from the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dce3fa1-0caf-44b3-bfec-863d7afe8cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'None of this was explicitly encoded by humans or present in the training set
    as asked, and, yet, looking at this, it''s frighteningly like speaking with a
    human. But let''s see more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/591bc3ed-fb58-47f6-ac3e-77da623bd8ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the model is responding with what appears to be knowledge of gender
    (**he**, **she**), **place** (England), and career (**player**). Even questions
    of meaning, ethics, and morality are fair game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d841798-609f-4249-8237-a9b8092442e4.png)![](img/7dc81f71-3e26-4ae0-8cd0-2f5652b4ad62.png)'
  prefs: []
  type: TYPE_IMG
- en: If that transcript doesn't give you a slight chill, there's a chance you might
    already be some sort of AI.
  prefs: []
  type: TYPE_NORMAL
- en: I wholeheartedly recommend reading the entire paper. It isn't overly technical,
    and it will definitely give you a glimpse of where the technology is headed.
  prefs: []
  type: TYPE_NORMAL
- en: We've talked a lot about the history, types, and design of chatbots, but let's
    now move on to building our own. We'll take two approaches to this. This first
    will use a technique we saw in previously, cosine similarity, and the second will
    leverage sequence-to-sequence learning.
  prefs: []
  type: TYPE_NORMAL
- en: Building a chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, having seen what's possible in terms of chatbots, you most likely want
    to build the best, most state-of-the-art, Google-level bot out there, right? Well,
    just put that out of your mind for now because we're going start by doing the
    exact opposite. We're going to build the most amazingly awful bot ever!
  prefs: []
  type: TYPE_NORMAL
- en: This may sound disappointing, but if your goal is just to build something very
    cool and engaging (that doesn't take hours and hours to construct), this is a
    great place to start.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to leverage the training data derived from a set of real conversations
    with Cleverbot. The data was collected from [http://notsocleverbot.jimrule.com](http://notsocleverbot.jimrule.com).
    This site is perfect, as it has people submit the most absurd conversations they
    had with Cleverbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a sample conversation between Cleverbot and a user from
    the site:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d9fb676-13dc-4bb1-b64c-9a172256926b.png)'
  prefs: []
  type: TYPE_IMG
- en: While you are free to use the techniques for web scraping that we used in earlier
    chapters to collect the data, you can find a `.csv` of the data in the GitHub
    repo for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start again in our Jupyter Notebook. We''ll load, parse, and examine
    the data. We''ll first import pandas and the Python regular expressions library,
    `re`. We''re also going to set the option in pandas to widen our column width
    so we can see the data better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''ll load in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a48abdb7-bcff-42f5-9d55-c7da9f2aed78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we''re only interested in the first column, the conversation data, we''ll
    parse that out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63ee8de3-1c46-4190-afb9-bad34daa3a40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You should be able to make out that we have interactions between **User** and
    **Cleverbot**, and that either can initiate the conversation. To get the data
    in the format we need, we''ll have to parse it into question-and-response pairs.
    We aren''t necessarily concerned with who says what, but with matching up each
    response to each question. You''ll see why in a bit. Let''s now do a bit of regular
    expression magic on the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8069791-f4f5-4908-851f-585e4ef3b2d7.png)'
  prefs: []
  type: TYPE_IMG
- en: OK, lots of code there. What just happened? We first created a list to hold
    our question-and-response tuples. We then passed our conversations through a function
    to split them into those pairs using regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we set it all into a pandas DataFrame with columns labelled `q` and
    `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re now going to apply a bit of algorithm magic to match up the closest
    question to the one a user inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we imported our tf-idf vectorization library and the
    cosine similarity library. We then used our training data to create a tf-idf matrix.
    We can now use this to transform our own new questions and measure the similarity
    to existing questions in our training set. Let''s do that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66fa20ef-b27f-44e2-8af5-bae861983084.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What are we looking at here? This is the cosine similarity between the question
    I asked and the top-five closest questions. On the left is the index, and on the
    right is the cosine similarity. Let''s take a look at those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0f4a09b-d60c-4db2-a958-9e0b0ee6411a.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, nothing is exactly the same, but there are definitely some similarities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f020de1-b522-4912-ba1e-52f45a718d23.png)'
  prefs: []
  type: TYPE_IMG
- en: OK, so our bot seems to have an attitude already. Let's push further.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create a handy function so that we can test a number of statements easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9a792e9-1f44-42d2-946b-9b3736c2d8b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have clearly created a monster, so we''ll continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9377dd8e-c551-4e69-8e32-709f149a06c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I''m enjoying this. Let''s keep rolling with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/beab46df-ae86-4fba-9292-b72e5ce81c0d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/15082c21-bd1f-4d20-ad70-d46420b6a5db.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/265ec47e-43b4-4108-8508-f84206c682d4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4b18b671-7d88-47a2-aee5-9ee79c643ede.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/25bd083c-6922-4dd8-a00b-390058199df7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f9571b10-2ff4-4cba-bb2b-d493ba9a961e.png)'
  prefs: []
  type: TYPE_IMG
- en: Remarkably, this may be one of the best conversations I've had in a while, bot
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: Now while this was a fun little project, let's now move on to a more advanced
    modeling technique using sequence-to-sequence modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence modeling for chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this next task, we'll leverage a couple libraries discussed in [Chapter
    8](5df6fae8-a5c0-4fab-8508-baef0085b4f5.xhtml), *Classifying Images with Convolutional
    Neural Networks*, TensorFlow and Keras. Both can be `pip` installed if you haven't
    done that already.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re also going to use the type of advanced modeling discussed earlier in
    the chapter; it''s a type of deep learning called **sequence-to-sequence modeling**.
    This is frequently used in machine translation and question-answering applications,
    as it allows us to map an input sequence of any length to an output sequence of
    any length:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b04592c-6902-4403-870e-4266daf360c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html'
  prefs: []
  type: TYPE_NORMAL
- en: Francois Chollet has an excellent introduction to this type of model on the
    blog for Keras: [https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html).
    It's worth a read.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to make heavy use of his example code to build out our model.
    While his example uses machine translation, English to French, we''re going to
    repurpose it for question-answering using our Cleverbot dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the training parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We'll use these to start. We can examine the success of our model and then adjust
    as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in data processing will be to take our data, get it in the proper
    format, and then vectorize it. We''ll go step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates lists for our questions and answers (the targets) as well as sets
    for the individual characters in our questions and answers. This model will actually
    work by generating one character at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s limit our question-and-answer pairs to 50 characters or fewer. This
    will help speed up our training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up our input and target text lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code gets our data in the proper format. Note that we add a tab
    (`\t`) and a newline (`\n`) character to the target texts. This will serve as
    our start and stop tokens for the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the input texts and the target texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3318e65a-0fc7-4a07-8620-8d81902c6079.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e4cad67-bd2e-41bf-a7d0-19bb870fe629.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at those input and target-character sets now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32be9bf4-34e1-4e41-bc29-af5e999cf409.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1f54278-2513-40d3-ae69-42a7a06014d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we''ll do some additional preparation for the data that will feed into
    the model. Although data can be fed in any length and returned in any length,
    we need to add padding up to the max length of the data for the model to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34094ae5-2198-4796-afd4-066b044361a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we''ll vectorize our data using one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at one of these vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc180728-de1a-4d09-bc5d-68c4f6a3e34b.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding figure, you'll notice that we have a one-hot encoded vector
    of our character data, which will be used in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now set up our sequence-to-sequence model-encoder and -decoder LSTMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we move on to the model itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we defined our model using our encoder and decoder input
    and our decoder output. We then compile it, fit it, and save it.
  prefs: []
  type: TYPE_NORMAL
- en: We set up the model to use 1,000 samples. Here, we also split the data 80/20
    to training and validation, respectively. We also set our epochs at 100, so this
    will essentially run for 100 cycles. On a standard MacBook Pro, this may take
    around an hour to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that cell is run, the following output will be generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad208163-5986-4a4d-b6d4-95b800737727.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next step is our inference step. We''ll use the states generated from this
    model to feed into our next model to generate our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57a91f41-a7da-468e-a92a-7589eb73593c.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the results of our model are quite repetitive. But then we only
    used 1,000 samples and the responses were generated one character at a time, so
    this is actually fairly impressive.
  prefs: []
  type: TYPE_NORMAL
- en: If you want better results, rerun the model using more sample data and more
    epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I have provide some of the more humorous output I''ve noted from much
    longer training periods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e99022f7-f9da-4cf0-8a12-fb58d03c72ff.png)![](img/6936b02e-4137-4c66-ad25-1e9291112554.png)![](img/7ed8edd3-c574-482d-9d93-d184a8e3f59e.png)![](img/4096f414-dfd7-46c7-a6e0-efb90b3712bd.png)![](img/78e8de23-1a8e-488e-b326-a2e01a4bbf44.png)![](img/c60b8d01-7f73-4862-b039-3ae1cbcea5c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a full tour of the chatbot landscape. It's clear that
    we're on the cusp of an explosion of these types of applications. The *Conversational
    UI* revolution is just about to begin. Hopefully, this chapter has inspired you
    to create your own bot, but if not, we hope you have a much richer understanding
    of how these applications work and how they'll shape our future.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ll let the app say the final words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE

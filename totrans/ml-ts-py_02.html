<html><head></head><body>
  <div id="_idContainer093">
    <h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-41" class="chapterTitle">Preprocessing Time-Series</h1>
    <p class="normal">Preprocessing is a crucial step in machine learning that is nonetheless often neglected. Many books don't cover preprocessing in any depth or skip preprocessing entirely. When presenting to outsiders about a machine learning project, curiosity is naturally attracted to the algorithm rather than the dataset or the preprocessing.</p>
    <p class="normal">One reason for the relative silence on preprocessing could be that it's less glamorous than machine learning itself. It is, however, often the step that takes the most time, sometimes estimated at around 98% of the whole machine learning process. And it is often in preprocessing that relatively easy work can have a great impact on the eventual performance of the machine learning model. The quality of the data goes a long way toward determining the outcome – low-quality input, in the worst case, can invalidate the machine learning work altogether (this is summarized in the adage "garbage in, garbage out").</p>
    <p class="normal">Preprocessing includes curating and screening the data, something that overlaps with the analysis process covered in the previous chapter, <em class="chapterRef">Chapter 2</em>, <em class="italic">Time-Series Analysis with Python</em>. The expected output of the preprocessing is a dataset on which it is easier to conduct machine learning. This can mean that it is more reliable and less noisy than the original dataset.</p>
    <p class="normal">You can find the code for this chapter as a Jupyter notebook in the book's GitHub repository.</p>
    <p class="normal">We're going to cover the following topics:</p>
    <ul>
      <li class="bullet">What Is Preprocessing?</li>
      <li class="bullet">Feature Transforms</li>
      <li class="bullet">Feature Engineering</li>
      <li class="bullet">Python Practice</li>
    </ul>
    <p class="normal">We'll start off by discussing the basics of preprocessing.</p>
    <h1 id="_idParaDest-42" class="title">What Is Preprocessing?</h1>
    <p class="normal">Anyone who's ever worked in a company on a machine learning project knows that real-world data is messy. It's often aggregated<a id="_idIndexMarker159"/> from multiple sources or using multiple platforms or recording devices, and it's incomplete and inconsistent. In preprocessing, we want to improve the data quality to successfully apply a machine learning model.</p>
    <p class="normal">Data preprocessing includes<a id="_idIndexMarker160"/> the following set of techniques:</p>
    <ul>
      <li class="bullet">Feature transforms<ul>
          <li class="bullet-l2">Scaling</li>
          <li class="bullet-l2">Power/log transforms</li>
          <li class="bullet-l2">Imputation</li>
        </ul>
      </li>
      <li class="bullet">Feature engineering</li>
    </ul>
    <p class="normal">These techniques fall largely into two classes: either they tailor to the assumptions of the machine<a id="_idIndexMarker161"/> learning algorithm (feature transforms) or they are concerned with constructing more complex features from multiple underlying features (feature engineering). We'll only deal<a id="_idIndexMarker162"/> with univariate feature transforms, transforms that apply to one feature at a time. We won't discuss multivariate feature transforms (data reduction) such as variable<a id="_idIndexMarker163"/> selection or dimensionality reduction since they are not particular to time-series datasets.</p>
    <p class="normal">Missing values are a common problem in machine learning, so we will discuss replacing missing values (imputation) in this chapter as well.</p>
    <p class="normal">We'll be talking about features as the elementary units of preprocessing. We want to create input features for our machine learning process that make the model easier to train, easier to evaluate, or to improve the quality of model predictions. Our goal is to have features that are predictive of the target, and decorrelated (not redundant between themselves). Decorrelation is a requirement for linear models, but less important for more modern, for example tree-based, algorithms.</p>
    <p class="normal">Although we'll mostly deal with feature engineering, we'll also mention target transformations. We could refer to target transformations more specifically as target engineering; however, since methods applied on targets are the same methods as those applied to features, I've included them under the same headings about feature engineering or feature transformations.</p>
    <p class="normal">Please note that we define the main goal of our preprocessing as increasing the predictiveness of our features, or, in other words, we want to elevate the quality of our machine learning model <a id="_idIndexMarker164"/>predictions. We could have alternatively defined data quality in terms of accuracy, completeness, and consistency, which would have cast a much wider net including data aggregation and cleaning techniques, and methods of data quality assessment.</p>
    <p class="normal">In this chapter, we are pragmatically reducing the scope of the treatment here to usefulness in machine learning. If our model is not fit for purpose, we might want to repeat or improve data collection, do more feature engineering, or build a better model. This again emphasizes the point that data analysis, preprocessing, and machine learning are an iterative process.</p>
    <p class="normal">Binning or discretization can be a part of preprocessing as well, but can also be used for grouping data points by their similarity. We'll be discussing discretization together with other clustering techniques in <em class="chapterRef">Chapter 6</em>, <em class="italic">Unsupervised Methods for Time-Series</em>.</p>
    <p class="normal">Before we continue, let's go through some of the basics of preprocessing time-series datasets with Python. This will cover the theory behind operations with time-series data as an introduction.</p>
    <h1 id="_idParaDest-43" class="title">Feature Transforms</h1>
    <p class="normal">Many models<a id="_idIndexMarker165"/> or training processes depend on the assumption that the data is distributed according to the normal distribution. Even the most widely used descriptors, the arithmetic mean and standard deviation, are largely useless if your dataset has a skew or several peaks (multi-modal). Unfortunately, observed data often doesn't fall within the normal distribution, so that traditional algorithms can yield invalid results.</p>
    <p class="normal">When data is non-normal, transformations of data are applied to make the data as normal-like as possible and, thus, increase the validity of the associated statistical analyses.</p>
    <p class="normal">Often it can be easier to eschew traditional machine<a id="_idIndexMarker166"/> learning algorithms of dealing with time-series data and, instead, use newer, so-called non-linear methods that are not dependent on the distribution of the data.</p>
    <p class="normal">As a final remark, while all the following transformations and scaling methods can be applied to features directly, an interesting spin with time-series datasets is that they change over time, and we might not have full knowledge of the time-series. Many of these transformations have online variants, where all statistics are estimated and adjusted on the fly. You can take a look at <em class="chapterRef">Chapter 8</em>, <em class="italic">Online Learning for Time-Series</em>, for more details on this topic.</p>
    <p class="normal">In the next section, we'll look at scaling, which is a general issue in regression.</p>
    <h2 id="_idParaDest-44" class="title">Scaling</h2>
    <p class="normal">Some features<a id="_idIndexMarker167"/> have natural bounds such as the age of a person or the year of production of a product. If these ranges differ between features, some model types (again, mostly linear models) struggle with this, preferring similar ranges, and similar means.</p>
    <p class="normal">Two very<a id="_idIndexMarker168"/> common scaling methods are min-max scaling and z-score normalization.</p>
    <p class="normal"><strong class="keyword">Min-max scaling</strong> involves restricting<a id="_idIndexMarker169"/> the range of the feature within two constants, <em class="italic">a</em> and <em class="italic">b</em>. This is defined as follows:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_001.png" alt="" style="height: 3em;"/></figure>
    <p class="normal">In the special case that <em class="italic">a</em> is 0 and <em class="italic">b</em> is 1, this restricts the range of the feature within 0 and 1.</p>
    <p class="normal"><strong class="keyword">Z-score normalization</strong> is setting the mean<a id="_idIndexMarker170"/> of the feature to 0 and the variance to 1 (unit variance) like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_002.png" alt="" style="height: 3em;"/></figure>
    <p class="normal">In the case that <em class="italic">x</em> comes from a Gaussian distribution, <img src="../Images/B17577_03_003.png" alt="" style="height: 1em;"/> is a standard normal distribution.</p>
    <p class="normal">In the next section, we'll look at log and power transformations. These transformations are quite important, especially for the traditional time-series models that we'll come across in <em class="chapterRef">Chapter 5</em>, <em class="italic">Time-Series Forecasting with Moving Averages and Autoregressive Models</em>.</p>
    <h2 id="_idParaDest-45" class="title">Log and Power Transformations</h2>
    <p class="normal">Both log and power transformations can compress values that spread over large magnitudes<a id="_idIndexMarker171"/> into a narrow range of output values. A <strong class="keyword">log transformation</strong> is a feature transformation in which each value <em class="italic">x</em> gets replaced by <em class="italic">log(x)</em>.</p>
    <p class="normal">The log function<a id="_idIndexMarker172"/> is the inverse of the exponential function, and it's important to remember that the range between 0 and 1 gets mapped to negative numbers <span class="mediaobject">(<img src="../Images/B17577_03_004.png" alt="" style="height: 0.47em;"/></span> while numbers <em class="italic">x&gt;=1</em> get compressed in the positive range. The choice of the logarithm is usually between the natural and base 10 but can be anything that can help so that your feature becomes closer to the symmetric bell-shaped distribution, which is the normal distribution.</p>
    <p class="normal">Log transformation is, arguably, one of the most popular among the different types of transformations applied to take the distribution of the data closer to a Gaussian distribution. Log transformation can be used to reduce the skew of a distribution. In the best scenario, if the feature follows a log-normal distribution, then the log-transformed data follows a normal distribution. Unfortunately, your feature might not be distributed according to a log-normal distribution, so applying this transformation doesn't help.</p>
    <p class="normal">Generally, I'd recommend exercising caution with data transformations. You should always inspect your data before and after the transformation. You want the variance of your feature to capture that of the target, so you should make sure you are not losing resolution. Further, you might want to check your data conforms – as should be the goal – more closely to the normal distribution. Many statistical methods have been developed to test the normality assumption of observed data, but even a simple histogram can give a great idea of the distribution.</p>
    <p class="normal">Power transformations are often applied to transform the data from its original distribution to something that's more like a normal distribution. As discussed in the introduction, this can make a huge difference to the machine learning algorithm's ability to find a solution.</p>
    <p class="normal"><strong class="keyword">Power transforms</strong> are transformations preserving<a id="_idIndexMarker173"/> the original<a id="_idIndexMarker174"/> order (this<a id="_idIndexMarker175"/> property is called monotonicity) using power<a id="_idIndexMarker176"/> functions. A <strong class="keyword">power function</strong> is a function of this form:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_005.png" alt="" style="height: 1.5em;"/></figure>
    <p class="normal">where <img src="../Images/B17577_03_006.png" alt="" style="height: 1em;"/>.</p>
    <p class="normal">When <em class="italic">n</em> is an integer and bigger than 1, we can make two major distinctions depending on whether <em class="italic">n</em> is odd or even. If it is even, the function <img src="../Images/B17577_03_007.png" alt="" style="height: 1em;"/> will tend toward positive infinity with large <em class="italic">x</em>, either<a id="_idIndexMarker177"/> positive or negative. </p>
    <p class="normal">If it is odd, <em class="italic">f(x)</em> will tend toward positive infinity with increasing <em class="italic">x</em>, but toward negative infinity with <em class="italic">x</em>.</p>
    <p class="normal">A power transformation is generally defined like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_008.png" alt="" style="height: 4.5em;"/></figure>
    <p class="normal">where <em class="italic">GM(x)</em> is the geometric mean of <em class="italic">x</em>:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_009.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">This reduces the transformation to the optimal choice of the parameter <img src="../Images/B17577_03_010.png" alt="" style="height: 1em;"/>. For practical purposes, two power transformations are most commonly used: </p>
    <ul>
      <li class="bullet">Box-Cox transformation</li>
      <li class="bullet">Yeo–Johnson</li>
    </ul>
    <p class="normal">For <strong class="keyword">Box-Cox transformation</strong>, there are two<a id="_idIndexMarker178"/> variants, the one-parameter variant and the<a id="_idIndexMarker179"/> two-parameter variant. One-parameter Box–Cox transformations are defined like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_011.png" alt="" style="height: 3.5em;"/></figure>
    <p class="normal">The value of the parameter <img src="../Images/B17577_03_012.png" alt="" style="height: 0.38em;"/> can be via different optimization methods such as the maximum likelihood that the transformed<a id="_idIndexMarker180"/> feature is Gaussian.</p>
    <p class="normal">So the value of lambda corresponds to the exponent of the power operation, for example, <img src="../Images/B17577_03_013.png" alt="" style="height: 0.8em;"/> with <img src="../Images/B17577_03_014.png" alt="" style="height: 0.8em;"/> or <img src="../Images/B17577_03_015.png" alt="" style="height: 0.9em;"/>with <img src="../Images/B17577_03_016.png" alt="" style="height: 0.8em;"/>.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Fun fact: Box-Cox transformation is named after statisticians George E.P Box and David Cox, who decided they had to work together because Box-Cox would sound good.</p>
    </div>
    <p class="normal"><strong class="keyword">Yeo–Johnson transformation</strong> is an extension of Box-Cox transformation that allows zero and negative<a id="_idIndexMarker181"/> values of <em class="italic">x</em>. <img src="../Images/B17577_03_017.png" alt="" style="height: 1em;"/> can be any real<a id="_idIndexMarker182"/> number, where <img src="../Images/B17577_03_018.png" alt="" style="height: 1em;"/> =1 produces the identity transformation. The transformation is defined as:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_019.png" alt="" style="height: 8em;"/></figure>
    <p class="normal">Finally, <strong class="keyword">quantile transformation</strong> can map a feature to the uniform distribution based on an estimate of the cumulative distribution function. Optionally, this can then<a id="_idIndexMarker183"/> be mapped in a second step to normal distribution. The advantage of this transform, similar to other transformations that we've talked about, is that it makes features more convenient to process and plot, and easier to compare, even if they were measured at different scales.</p>
    <p class="normal">In the next section, we'll look at imputation, which literally means the assignment of values by inference, but in machine learning, often is more narrowly meant to refer to replacing missing values.</p>
    <h2 id="_idParaDest-46" class="title">Imputation</h2>
    <p class="normal">Imputation is the replacement<a id="_idIndexMarker184"/> of missing values. This is important for any machine learning algorithm that can't handle missing values. Generally, we can distinguish<a id="_idIndexMarker185"/> the following types of imputation techniques:</p>
    <ul>
      <li class="bullet">Unit imputation – where missing values are replaced<a id="_idIndexMarker186"/> by a constant such as the mean or 0</li>
      <li class="bullet">Model-based imputation – where missing values are replaced with<a id="_idIndexMarker187"/> predictions from a machine learning model</li>
    </ul>
    <p class="normal">Unit imputation is by far the most popular imputation technique, partly because it's very easy to do, and because it's less heavy on computational resources than model-based imputation.</p>
    <p class="normal">We'll do imputation in the practice section of this chapter. In the next section, we'll talk about feature engineering.</p>
    <h1 id="_idParaDest-47" class="title">Feature Engineering</h1>
    <p class="normal">Machine learning algorithms can use different representations of the input features. As we've mentioned in the introduction, the goal of feature engineering is to produce new features that can help us in the machine learning process. Some representations or augmentations of features can boost performance.</p>
    <p class="normal">We can distinguish<a id="_idIndexMarker188"/> between hand-crafted and automated feature extraction, where hand-crafted means that we look through the data and try to come up with representations that could be useful, or we can use a set of features that have been established from the work of researchers and practitioners before. An example of a set of established features is <strong class="keyword">Catch22</strong>, which includes 22 features and simple summary statistics extracted from phase-dependant intervals. The Catch22 set is a subset of the <strong class="keyword">Highly Comparative Time-Series Analysis</strong> (<strong class="keyword">HCTSA</strong>) toolbox, another set of features.</p>
    <p class="normal">Another distinction is between interpretable and non-interpretable features. Interpretable features could be summary features such as the mean, max, min, and others. These could be pooled within time periods, windows, to give us more features.</p>
    <p class="normal">In features for time-series, a few preprocessing methods come with their recommended machine learning model. For example, a ROCKET model is a linear model on top of the ROCKET features.</p>
    <p class="normal">Taken to the extreme, this can be a form of <strong class="keyword">model stacking</strong>, where the outcomes of models serve as the inputs<a id="_idIndexMarker189"/> to other models. This can be an effective way of decomposing the learning problem by training less complex (fewer features, fewer parameters) models in a supervised setting and using their outputs for training other models.</p>
    <p class="normal">Please note it is important that any new feature depends only on past and present inputs. In signal processing, this kind of operation is called a <strong class="keyword">causal filter</strong>. The word causal indicates that the filter output for a value at time <em class="italic">t</em> only uses information available at time <em class="italic">t</em> and doesn't peek into the future. Conversely, a filter whose output<a id="_idIndexMarker190"/> also depends on future inputs is non-causal. We'll discuss Temporal Convolutional Networks, basically causal convolutions, in Chapter 10, Deep Learning for Time-Series.</p>
    <p class="normal">We should take great care in training and testing that any statistics extracted and applied in preprocessing are carefully considered – at best, the model performance will be overly optimistic if it relies on data that shouldn't be available during prediction. We'll discuss leakage in the next chapter, <em class="chapterRef">Chapter 4</em>, <em class="italic">Machine Learning for Time-Series</em>.</p>
    <p class="normal">If we have many features, we might want to simplify our model building process by pruning the available features and using only a subset (feature selection), or instead, using a new set of features that describe the essential quality of the features (dimensionality reduction).</p>
    <p class="normal">We can distinguish the following types of features with time-series:</p>
    <ul>
      <li class="bullet">Date- and time-related features<ul>
          <li class="bullet-l2">Calendar features (date-related)</li>
          <li class="bullet-l2">Time-related features</li>
        </ul>
      </li>
      <li class="bullet">Window-based features</li>
    </ul>
    <p class="normal">Calendar- and time-related features are very similar, so we'll discuss them in the same section.</p>
    <p class="normal">Window-based features are features<a id="_idIndexMarker191"/> that integrate features within a (rolling) window, that is, within a time period. Examples of these are averages over 15-minute windows or sales within 7 days. Since we dealt with rolling windows in <em class="chapterRef">Chapter 2</em>, <em class="italic">Time-Series Analysis with Python</em>, in this chapter, we'll deal with more complex features such as convolutions and shapelets.</p>
    <p class="normal">Many preprocessing algorithms are implemented in <code class="Code-In-Text--PACKT-">sktime</code>. Another handy library is tsfresh, which calculates<a id="_idIndexMarker192"/> an enormous number of interpretable features for time-series. In the code in this chapter, we've accessed tsfresh features through feature tools.</p>
    <p class="normal">Let's do some more time-series preprocessing in Python! We'll discuss date- and time-related features next.</p>
    <h2 id="_idParaDest-48" class="title">Date- and Time-Related Features</h2>
    <p class="normal">Date and time variables contain information about dates, time, or a combination (datetime). We saw several<a id="_idIndexMarker193"/> examples in the previous chapter, <em class="chapterRef">Chapter 2</em>, <em class="italic">Time-Series Analysis with Python</em> – one of them was the year corresponding to pollution. Other examples could be the birth year of a person or the date of a loan being taken out.</p>
    <p class="normal">If we want to feed these<a id="_idIndexMarker194"/> fields into a machine learning model, we need to derive relevant information. We could feed the year as an integer, for example, but there are many more examples of extracted features from datetime variables, which we'll deal with in this section. We can significantly improve the performance of our machine learning model by enriching our dataset with these extracted features.</p>
    <p class="normal">Workalendar is a Python module that provides classes able to handle calendars, including lists of bank<a id="_idIndexMarker195"/> and religious holidays, and it offers working-day-related functions. Python-holidays is a similar library, but here we'll go with workalendar.</p>
    <p class="normal">In the next section, we'll discuss ROCKET features.</p>
    <h2 id="_idParaDest-49" class="title">ROCKET</h2>
    <p class="normal">The research paper <em class="italic">ROCKET: Exceptionally fast and accurate time-series classification using random convolutional kernels</em> (Angus Dempster, François Petitjean, Geoffrey I. Webb; 2019) presents<a id="_idIndexMarker196"/> a novel methodology for convolving time-series data with random kernels that can result in higher accuracy<a id="_idIndexMarker197"/> and faster training times for machine learning models. What makes this paper unique is banking on the recent successes of convolutional neural networks and transferring them to preprocessing for time-series datasets.</p>
    <p class="normal">We will go into more details of this paper. <strong class="keyword">ROCKET</strong>, short for <strong class="keyword">RandOm Convolutional KErnel Transform</strong>, is based on convolutions, so let's start with convolutions.</p>
    <p class="normal">Convolutions are a very important transformation, especially in image processing, and are one of the most important building blocks of deep neural networks in image recognition. Convolutions<a id="_idIndexMarker198"/> consist of feedforward connections, called <strong class="keyword">filters</strong> or <strong class="keyword">kernels</strong>, that are applied to rectangular<a id="_idIndexMarker199"/> patches of the image (the previous layer). Each resulting image is then the sliding window of the kernel over the whole image. Simply put, in the case of images, a kernel is a matrix used to modify the images. </p>
    <p class="normal">A sharpening kernel can look like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_020.png" alt="" style="height: 3em;"/></figure>
    <p class="normal">If we multiply<a id="_idIndexMarker200"/> this kernel to all local neighborhoods in turn, we get a sharper image as illustrated below (original on the left, sharpened on the right):</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_01.png" alt="merged.png"/></figure>
    <p class="packt_figref">Figure 3.1: Sharpening filter</p>
    <p class="normal">This picture is a gray version of "A woman divided into two, representing life and death" (owned by the Wellcome Collection, a museum and exhibition center in London; licensed under CC BY 4.0).</p>
    <p class="normal">The sharpening kernel emphasizes differences in adjacent pixel values. You can see that the picture on the right is much grainier or vivid – a result of the convolution.</p>
    <p class="normal">We can apply kernels <a id="_idIndexMarker201"/>not only to images but also to vectors or matrices, and this brings us back to ROCKET. ROCKET computes two aggregate features from each kernel and feature convolution. The two features are created using the well-known methodology global/average max pooling and a novel methodology that we'll come to in a second.</p>
    <p class="normal"><strong class="keyword">Global max pooling</strong> outputs the maximum value from the result of convolution and max pooling takes the maximum<a id="_idIndexMarker202"/> value within a pool size. For example, if the<a id="_idIndexMarker203"/> result of the convolution is 0,1,2,2,5,1,2, global max pooling returns 5, whereas <strong class="keyword">max pooling</strong> with pool size 3 outputs the maxima within windows of 3, so we'll get 2,2,5,5,5. </p>
    <p class="normal"><strong class="keyword">Positive Proportion Value</strong> (<strong class="keyword">PPV</strong>), the methodology from the paper, is the proportion (percentage) of values from the convolution<a id="_idIndexMarker204"/> that are positive (or above a bias threshold).</p>
    <p class="normal">We can improve machine learning accuracy from time-series by applying transformations<a id="_idIndexMarker205"/> with convolutional kernels. Each feature gets transformed by random kernels, the number of which is a parameter to the algorithm. This is set to 10,000 by default. The transformed features can now be fed as input into any machine learning algorithm. The authors propose to use linear algorithms like ridge regression classifier or logistic regression.</p>
    <p class="normal">The idea<a id="_idIndexMarker206"/> of ROCKET is very similar to Convolutional Neural Networks (CNNs), which we'll discuss in chapter 10, Deep Learning for Time-Series, however, two big differences are:</p>
    <ol>
      <li class="numbered">ROCKET doesn't use any hidden layers or non-linearities</li>
      <li class="numbered">The convolutions are applied independently for each feature</li>
    </ol>
    <p class="normal">In the next section, we'll be discussing shapelets.</p>
    <h2 id="_idParaDest-50" class="title">Shapelets</h2>
    <p class="normal">Shapelets for time-series were presented in the research paper <em class="italic">Time-Series Shapelets: A New Primitive for Data Mining</em> (Lexiang Ye and Eamonn Keogh, 2009). The basic idea of shapelets is decomposing<a id="_idIndexMarker207"/> time-series into discriminative subsections (shapelets). </p>
    <p class="normal">In the first step, the shapelets<a id="_idIndexMarker208"/> are learned. The algorithm calculates the information gain of possible candidates and picks the best candidates to create a shapelet dictionary of discriminating subsections. This can be quite expensive. Then, based on the shapelet decomposition of the features, a decision tree or other machine learning algorithms can be applied.</p>
    <p class="normal">Shapelets have several advantages over other methods:</p>
    <ul>
      <li class="bullet">They can provide interpretable results</li>
      <li class="bullet">The application <a id="_idIndexMarker209"/>of shapelets can be very fast – only depending on the matching of features against the dictionary of shapelets</li>
      <li class="bullet">The performance of machine learning algorithms on top of shapelets is usually very competitive</li>
    </ul>
    <p class="normal">It's time that we go through Python exercises with actual datasets.</p>
    <h1 id="_idParaDest-51" class="title">Python Practice</h1>
    <p class="normal">NumPy and SciPy offer most of the functionality that we need, but we might need a few more libraries.</p>
    <p class="normal">In this section, we'll use several libraries, which we can quickly install from the terminal, <strong class="keyword">the Jupyter Notebook</strong>, or similarly <strong class="keyword">from Anaconda Navigator</strong>:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install -U tsfresh workalendar astral "featuretools[tsfresh]" sktime
</code></pre>
    <p class="normal">All of these libraries are quite powerful and each of them deserves more than the space we can give to it in this chapter.</p>
    <p class="normal">Let's start with log and power transformations.</p>
    <h2 id="_idParaDest-52" class="title">Log and Power Transformations in Practice</h2>
    <p class="normal">Let's create a distribution<a id="_idIndexMarker210"/> that's not normal, and let's log-transform it. We'll plot the original and transformed<a id="_idIndexMarker211"/> distribution for comparison, and we'll apply a statistical test for normality.</p>
    <p class="normal">Let's first create the distribution:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> scipy.optimize <span class="hljs-keyword">import</span> minimize
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
np.random.seed(<span class="hljs-number">0</span>)
pts = <span class="hljs-number">10000</span>
vals = np.random.lognormal(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>, pts)
</code></pre>
    <p class="normal">Values are sampled from a lognormal distribution. I've added a call to the random number generator seed function to make sure the result is reproducible for readers. </p>
    <p class="normal">We can visualize our array as a histogram:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_02.png" alt="lognormal_hist.png"/></figure>
    <p class="packt_figref">Figure 3.2: An array sampled from a lognormal distribution</p>
    <p class="normal">I've used a log scale on the y-axis. We can see that the values spread over a number of orders of magnitude.</p>
    <p class="normal">We can apply the standard<a id="_idIndexMarker212"/> normalization to z-scores. We can also apply a statistical normality test<a id="_idIndexMarker213"/> on one of the transformed distributions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> normaltest
scaler = StandardScaler()
vals_ss = scaler.fit_transform(vals.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
_, p = normaltest(vals_ss)
print(<span class="hljs-string">f"significance: </span><span class="hljs-subst">{p:</span><span class="hljs-number">.2</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">The null hypothesis of this statistical test is that the sample comes from a normal distribution. Therefore significance values (p-values) lower than a threshold, typically set to 0.05 or 0.01, would let us reject the null hypothesis. </p>
    <p class="normal">We are getting this output: <code class="Code-In-Text--PACKT-">significance: 0.00</code>.</p>
    <p class="normal">We can conclude from the test that we are not getting a null distribution from our transformation by standard scaling.</p>
    <p class="normal">This should be obvious, but let's get it out of the way: we are getting the same significance for the minmax transformed values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> minmax_scale
vals_mm = minmax_scale(vals)
_, p = normaltest(vals_mm.squeeze())
print(<span class="hljs-string">f"significance: </span><span class="hljs-subst">{p:</span><span class="hljs-number">.2</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">We therefore reach the same conclusion: the minmax transformation hasn't helped us get a normal-like distribution.</p>
    <p class="normal">We can plot the original and the standard scaled distribution against each other. Unsurprisingly, visually, the two distributions look the same except for the scale.</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_03.png" alt="standard_scaled.png"/></figure>
    <p class="packt_figref">Figure 3.3: The linear transformation against the original values</p>
    <p class="normal">We can see everything lies on the diagonal.</p>
    <p class="normal">Let's use a log transformation:</p>
    <pre class="programlisting code"><code class="hljs-code">log_transformed = np.log(vals)
_, p = normaltest(log_transformed)
print(<span class="hljs-string">f"significance: </span><span class="hljs-subst">{p:.2f}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">We are getting a significance of <code class="Code-In-Text--PACKT-">0.31</code>. This lets us conclude that we can't reject the null hypothesis. Our distribution<a id="_idIndexMarker214"/> is similar to normal. In fact, we get a standard deviation<a id="_idIndexMarker215"/> close to 1.0 and a mean close to 0.0 as we would expect with a normal distribution.</p>
    <p class="normal">We can see that the log-normal distribution is a continuous probability distribution whose logarithm is normally distributed, so it's not entirely surprising that we get this result.</p>
    <p class="normal">We can plot the histogram of the log-transformed distribution:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_04.png" alt="log_transformed_hist.png"/></figure>
    <p class="packt_figref">Figure 3.4: Log-transformed lognormal array</p>
    <p class="normal">The log transform<a id="_idIndexMarker216"/> looks much more normal-like as we can appreciate.</p>
    <p class="normal">We can<a id="_idIndexMarker217"/> also apply Box-Cox transformation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> boxcox
vals_bc = boxcox(vals, <span class="hljs-number">0.0</span>)
</code></pre>
    <p class="normal">We are getting a significance of 0.46. Again, we can conclude that our Box-Cox transform is normal-like. We can also see this in a plot of the Box-Cox transformed distribution:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_041.png" alt="bc_transformed_hist.png" style="height: 9.51em;"/></figure>
    <p class="packt_figref">Figure 3.5: Box-Cox transformed lognormal array</p>
    <p class="normal">Again, this looks very<a id="_idIndexMarker218"/> much like a normal distribution. This plot looks pretty much the same as the previous one of the log transformation, which shouldn't be surprising<a id="_idIndexMarker219"/> given that the log operation corresponds to a lambda parameter of 0 in the Box-Cox transformation.</p>
    <p class="normal">This is a small selection of transformations that can help us reconcile our data with the common normality assumption in classical forecasting methods.</p>
    <p class="normal">Let's look at imputation in practice.</p>
    <h2 id="_idParaDest-53" class="title">Imputation</h2>
    <p class="normal">It is rather uncommon<a id="_idIndexMarker220"/> for machine learning algorithms to be able to deal with missing values directly. Rather, we'll either have to replace missing values with constants or infer probable values given the other features.</p>
    <p class="normal">The scikit-learn documentation<a id="_idIndexMarker221"/> lists a simple example of unit imputation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.impute <span class="hljs-keyword">import</span> SimpleImputer
imp_mean = SimpleImputer(missing_values=np.nan, strategy=<span class="hljs-string">'mean'</span>)
imp_mean.fit([[<span class="hljs-number">7</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, np.nan, <span class="hljs-number">6</span>], [<span class="hljs-number">10</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>]])
SimpleImputer()
df = [[np.nan, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">4</span>, np.nan, <span class="hljs-number">6</span>], [<span class="hljs-number">10</span>, np.nan, <span class="hljs-number">9</span>]]
print(imp_mean.transform(df))
</code></pre>
    <p class="normal">We are again using (similar to the standard scaler before) the scikit-learn transformers, which come with <code class="Code-In-Text--PACKT-">fit()</code> and <code class="Code-In-Text--PACKT-">transform()</code> methods.</p>
    <p class="normal">We get the following imputed values:</p>
    <pre class="programlisting con"><code class="hljs-con">[[ 7.   2.   3. ]
 [ 4.   3.5  6. ]
 [10.   3.5  9. ]]
</code></pre>
    <p class="normal">The missing values<a id="_idIndexMarker222"/> are replaced with the mean of the columns.</p>
    <p class="normal">Let's look at annotating holidays as derived date features.</p>
    <h2 id="_idParaDest-54" class="title">Holiday Features</h2>
    <p class="normal">If we want to get the holidays<a id="_idIndexMarker223"/> for the United Kingdom, we can do this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> workalendar.europe.united_kingdom <span class="hljs-keyword">import</span> UnitedKingdom
UnitedKingdom().holidays()
</code></pre>
    <p class="normal">We get the following holidays:</p>
    <pre class="programlisting con"><code class="hljs-con">[(datetime.date(2021, 1, 1), 'New year'),
 (datetime.date(2021, 4, 2), 'Good Friday'),
 (datetime.date(2021, 4, 4), 'Easter Sunday'),
 (datetime.date(2021, 4, 5), 'Easter Monday'),
 (datetime.date(2021, 5, 3), 'Early May Bank Holiday'),
 (datetime.date(2021, 5, 31), 'Spring Bank Holiday'),
 (datetime.date(2021, 8, 30), 'Late Summer Bank Holiday'),
 (datetime.date(2021, 12, 25), 'Christmas Day'),
 (datetime.date(2021, 12, 26), 'Boxing Day'),
 (datetime.date(2021, 12, 27), 'Christmas Shift'),
 (datetime.date(2021, 12, 28), 'Boxing Day Shift')]
</code></pre>
    <p class="normal">We can generate holidays by year and then look up holidays by date.</p>
    <p class="normal">Similarly, we can get holidays for other places, for example, California, USA. We can also extract lists of holidays, and add custom holidays:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> List
<span class="hljs-keyword">from</span> dateutil.relativedelta <span class="hljs-keyword">import</span> relativedelta, TH
<span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> workalendar.usa <span class="hljs-keyword">import</span> California
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_custom_holidays</span><span class="hljs-function">(</span><span class="hljs-params">year: </span><span class="hljs-built_in">int</span><span class="hljs-function">) -&gt; List:</span>
      custom_holidays = California().holidays()
      custom_holidays.append((
        (datetime.datetime(year, <span class="hljs-number">11</span>, <span class="hljs-number">1</span>) + relativedelta(weekday=TH(+<span class="hljs-number">4</span>)) + datetime.timedelta(days=<span class="hljs-number">1</span>)).date(),
        <span class="hljs-string">"Black Friday"</span>
      ))
      <span class="hljs-keyword">return</span> {k: v <span class="hljs-keyword">for</span> (k, v) <span class="hljs-keyword">in</span> custom_holidays}
custom_holidays = create_custom_holidays(<span class="hljs-number">2021</span>)
</code></pre>
    <p class="normal">This gives us our custom<a id="_idIndexMarker224"/> holidays for the year 2021:</p>
    <pre class="programlisting con"><code class="hljs-con">{datetime.date(2021, 1, 1): 'New year',
 datetime.date(2021, 1, 18): 'Birthday of Martin Luther King, Jr.',
 datetime.date(2021, 2, 15): "Washington's Birthday",
 datetime.date(2021, 3, 31): 'Cesar Chavez Day',
 datetime.date(2021, 5, 31): 'Memorial Day',
 datetime.date(2021, 7, 4): 'Independence Day',
 datetime.date(2021, 7, 5): 'Independence Day (Observed)',
 datetime.date(2021, 9, 6): 'Labor Day',
 datetime.date(2021, 11, 11): 'Veterans Day',
 datetime.date(2021, 11, 25): 'Thanksgiving Day',
 datetime.date(2021, 11, 26): 'Thanksgiving Friday',
 datetime.date(2021, 12, 24): 'Christmas Day (Observed)',
 datetime.date(2021, 12, 25): 'Christmas Day',
 datetime.date(2021, 12, 31): 'New Years Day (Observed)',
 datetime.date(2016, 11, 25): 'Black Friday'}
</code></pre>
    <p class="normal">Please note that we are using type hints in the code segment above. We are declaring the signature of our function like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_custom_holidays</span><span class="hljs-function">(</span><span class="hljs-params">year: </span><span class="hljs-built_in">int</span><span class="hljs-function">) -&gt; List:</span>
</code></pre>
    <p class="normal">This means we are expecting an integer called <code class="Code-In-Text--PACKT-">year</code> and we are expecting a <code class="Code-In-Text--PACKT-">List</code> as an output. Annotations are optional and they are not getting checked (if you don't invoke mypy), but they can make code in Python much clearer.</p>
    <p class="normal">Now we can implement a simple lookup like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">is_holiday</span><span class="hljs-function">(</span><span class="hljs-params">current_date: datetime.date</span><span class="hljs-function">):</span>
    <span class="hljs-string">"""Determine if we have a holiday."""</span>
    <span class="hljs-keyword">return</span> custom_holidays.get(current_date, <span class="hljs-literal">False</span>)
today = datetime.date(<span class="hljs-number">2021</span>, <span class="hljs-number">4</span>, <span class="hljs-number">11</span>)
is_holiday(today)
</code></pre>
    <p class="normal">I am getting a <code class="Code-In-Text--PACKT-">False</code> even though I wish it were a holiday.</p>
    <p class="normal">This can be a very useful feature<a id="_idIndexMarker225"/> for a machine learning model. For example, we could imagine a different profile of users who apply for loans on bank holidays or on a weekday.</p>
    <h2 id="_idParaDest-55" class="title">Date Annotation</h2>
    <p class="normal">The calendar module offers lots of methods, for example, <code class="Code-In-Text--PACKT-">monthrange() - calendar.monthrange</code> returns<a id="_idIndexMarker226"/> the first weekday of the month and the number of days in a month for a given year and month. The day of the week is given as an integer, where Monday is 0 and Sunday is 6.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> calendar
calendar.monthrange(<span class="hljs-number">2021</span>, <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">We should be getting (<code class="Code-In-Text--PACKT-">4, 31</code>). This means the first weekday of 2021 was a Friday. January 2021 had 31 days.</p>
    <p class="normal">We can also extract features relevant to the day with respect to the year. The following function provides the number of days since the end of the previous year and to the end of the current year:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> date
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">year_anchor</span><span class="hljs-function">(</span><span class="hljs-params">current_date: datetime.date</span><span class="hljs-function">):</span>
      <span class="hljs-keyword">return</span> (
        (current_date - date(current_date.year, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)).days,
        (date(current_date.year, <span class="hljs-number">12</span>, <span class="hljs-number">31</span>) - current_date).days,
      )
year_anchor(today)
</code></pre>
    <p class="normal">This feature could provide a general idea of how far into the year we are. This can be useful both for estimating a trend and for capturing cyclic variations.</p>
    <p class="normal">Similarly, we can extract the number of days from the first of the month and to the end of the month:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">month_anchor</span><span class="hljs-function">(</span><span class="hljs-params">current_date: datetime.date</span><span class="hljs-function">):</span>
      last_day = calendar.monthrange(current_date.year, current_date.month)[<span class="hljs-number">0</span>]
    
      <span class="hljs-keyword">return</span> (
        (current_date - datetime.date(current_date.year, current_date.month, <span class="hljs-number">1</span>)).days,
        (current_date - datetime.date(current_date.year, current_date.month, last_day)).days,
      )
month_anchor(today)
</code></pre>
    <p class="normal">A feature like this could also provide some useful information. I am getting <code class="Code-In-Text--PACKT-">(10, 8)</code>.</p>
    <p class="normal">In retail, it is very important<a id="_idIndexMarker227"/> to predict the spending behavior of customers. Therefore, in the next section, we'll write annotation for pay days.</p>
    <h2 id="_idParaDest-56" class="title">Paydays</h2>
    <p class="normal">We could imagine that some<a id="_idIndexMarker228"/> people get paid in the middle or at the end of the month, and would then access our website to buy our products.</p>
    <p class="normal">Most people would get paid on the last Friday of the month, so let's write a function for this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_last_friday</span><span class="hljs-function">(</span><span class="hljs-params">current_date: datetime.date, weekday=calendar.FRIDAY</span><span class="hljs-function">):</span>
      <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(week[weekday]
        <span class="hljs-keyword">for</span> week <span class="hljs-keyword">in</span> calendar.monthcalendar(
            current_date.year, current_date.month
        ))
get_last_friday(today)
</code></pre>
    <p class="normal">I am getting 30 as the last Friday.</p>
    <p class="normal">Seasons can also be predictive.</p>
    <h2 id="_idParaDest-57" class="title">Seasons</h2>
    <p class="normal">We can<a id="_idIndexMarker229"/> get the season for a specific date:</p>
    <pre class="programlisting code"><code class="hljs-code">YEAR = <span class="hljs-number">2021</span>
seasons = [
    (<span class="hljs-string">'winter'</span>, (date(YEAR,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>),  date(YEAR,  <span class="hljs-number">3</span>, <span class="hljs-number">20</span>))),
    (<span class="hljs-string">'spring'</span>, (date(YEAR,  <span class="hljs-number">3</span>, <span class="hljs-number">21</span>),  date(YEAR,  <span class="hljs-number">6</span>, <span class="hljs-number">20</span>))),
    (<span class="hljs-string">'summer'</span>, (date(YEAR,  <span class="hljs-number">6</span>, <span class="hljs-number">21</span>),  date(YEAR,  <span class="hljs-number">9</span>, <span class="hljs-number">22</span>))),
    (<span class="hljs-string">'autumn'</span>, (date(YEAR,  <span class="hljs-number">9</span>, <span class="hljs-number">23</span>),  date(YEAR, <span class="hljs-number">12</span>, <span class="hljs-number">20</span>))),
    (<span class="hljs-string">'winter'</span>, (date(YEAR, <span class="hljs-number">12</span>, <span class="hljs-number">21</span>),  date(YEAR, <span class="hljs-number">12</span>, <span class="hljs-number">31</span>)))
]
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">is_in_interval</span><span class="hljs-function">(</span><span class="hljs-params">current_date: datetime.date, seasons</span><span class="hljs-function">):</span>
      <span class="hljs-keyword">return</span> <span class="hljs-built_in">next</span>(season <span class="hljs-keyword">for</span> season, (start, end) <span class="hljs-keyword">in</span> seasons
                <span class="hljs-keyword">if</span> start &lt;= current_date.replace(year=YEAR) &lt;= end)
    
is_in_interval(today, seasons)
</code></pre>
    <p class="normal">We should<a id="_idIndexMarker230"/> be getting <code class="Code-In-Text--PACKT-">spring</code> here, but the reader is encouraged to try this with different values.</p>
    <h2 id="_idParaDest-58" class="title">The Sun and Moon</h2>
    <p class="normal">The Astral module offers information about sunrise, moon phases, and more. Let's get the hours of sunlight<a id="_idIndexMarker231"/> for a given day in London:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> astral.sun <span class="hljs-keyword">import</span> sun
<span class="hljs-keyword">from</span> astral <span class="hljs-keyword">import</span> LocationInfo
CITY = LocationInfo(<span class="hljs-string">"London"</span>, <span class="hljs-string">"England"</span>, <span class="hljs-string">"Europe/London"</span>, <span class="hljs-number">51.5</span>, -<span class="hljs-number">0.116</span>)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_sunrise_dusk</span><span class="hljs-function">(</span><span class="hljs-params">current_date: datetime.date, city_name=</span><span class="hljs-string">'London'</span><span class="hljs-function">):</span>
      s = sun(CITY.observer, date=current_date)
      sunrise = s[<span class="hljs-string">'sunrise'</span>]
      dusk = s[<span class="hljs-string">'dusk'</span>]
      <span class="hljs-keyword">return</span> (sunrise - dusk).seconds / <span class="hljs-number">3600</span>
get_sunrise_dusk(today)
</code></pre>
    <p class="normal">I am getting <code class="Code-In-Text--PACKT-">9.788055555555555</code> hours of daylight.</p>
    <p class="normal">It can often be observed that the more hours of daylight, the more business activity. We could speculate that this feature could be helpful in predicting the volume of our sales.</p>
    <h2 id="_idParaDest-59" class="title">Business Days</h2>
    <p class="normal">Similarly, if a month has more business days, we could expect more sales for our retail store. On the other hand, if we are selling windsurfing lessons, we might want to know the number of holidays in a given month. The following function extracts the number of business days<a id="_idIndexMarker232"/> and weekends/holidays in a month:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_business_days</span><span class="hljs-function">(</span><span class="hljs-params">current_date: datetime.date</span><span class="hljs-function">):</span>
      last_day = calendar.monthrange(current_date.year, current_date.month)[<span class="hljs-number">1</span>]
      rng = pd.date_range(current_date.replace(day=<span class="hljs-number">1</span>), periods=last_day, freq=<span class="hljs-string">'D'</span>)
      business_days = pd.bdate_range(rng[<span class="hljs-number">0</span>], rng[-<span class="hljs-number">1</span>])
      <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(business_days), last_day - <span class="hljs-built_in">len</span>(business_days)
get_business_days(date.today())
</code></pre>
    <p class="normal">We should be getting <code class="Code-In-Text--PACKT-">(22, 9)</code> – 22 business days and 9 weekend days and holidays.</p>
    <h2 id="_idParaDest-60" class="title">Automated Feature Extraction</h2>
    <p class="normal">We can also use automated feature extraction tools<a id="_idIndexMarker233"/> from modules like featuretools. Featuretools calculates many datetime-related functions. Here's a quick example:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> featuretools <span class="hljs-keyword">as</span> ft
<span class="hljs-keyword">from</span> featuretools.primitives <span class="hljs-keyword">import</span> Minute, Hour, Day, Month, Year, Weekday
data = pd.DataFrame(
    {<span class="hljs-string">'Time'</span>: [<span class="hljs-string">'2014-01-01 01:41:50'</span>,
              <span class="hljs-string">'2014-01-01 02:06:50'</span>,
              <span class="hljs-string">'2014-01-01 02:31:50'</span>,
              <span class="hljs-string">'2014-01-01 02:56:50'</span>,
              <span class="hljs-string">'2014-01-01 03:21:50'</span>],
     <span class="hljs-string">'Target'</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]}
)        
data[<span class="hljs-string">'index'</span>] = data.index
es = ft.EntitySet(<span class="hljs-string">'My EntitySet'</span>)
es.entity_from_dataframe(
    entity_id=<span class="hljs-string">'main_data_table'</span>,
    index=<span class="hljs-string">'index'</span>,
    dataframe=data,
    time_index=<span class="hljs-string">'Time'</span>
)
fm, features = ft.dfs(
    entityset=es, 
    target_entity=<span class="hljs-string">'main_data_table'</span>, 
    trans_primitives=[Minute, Hour, Day, Month, Year, Weekday]
)
</code></pre>
    <p class="normal">Our features are <code class="Code-In-Text--PACKT-">Minute</code>, <code class="Code-In-Text--PACKT-">Hour</code>, <code class="Code-In-Text--PACKT-">Day</code>, <code class="Code-In-Text--PACKT-">Month</code>, <code class="Code-In-Text--PACKT-">Year</code>, and <code class="Code-In-Text--PACKT-">Weekday</code>. Here's our DataFrame, <code class="Code-In-Text--PACKT-">fm</code>:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_05.png" alt="/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document Being Saved By screencaptureui 3)/Screenshot 2021-04-11 at 21.58.52.png"/></figure>
    <p class="packt_figref">Figure 3.6: Featuretools output</p>
    <p class="normal">We could extract<a id="_idIndexMarker234"/> many more features. Please see the featuretools documentation for more details.</p>
    <p class="normal">The tsfresh module also provides automated functionality for feature extraction:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tsfresh.feature_extraction <span class="hljs-keyword">import</span> extract_features
<span class="hljs-keyword">from</span> tsfresh.feature_extraction <span class="hljs-keyword">import</span> ComprehensiveFCParameters
settings = ComprehensiveFCParameters()
extract_features(data, column_id=<span class="hljs-string">'Time'</span>, default_fc_parameters=settings)
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">Please note that tsfresh optimizes features using statsmodels' autoregression, and (last I checked) still hasn't been updated to use <code class="Code-In-Text--PACKT-">statsmodels.tsa.AutoReg</code> instead of <code class="Code-In-Text--PACKT-">statsmodels.tsa.AR</code>, which has been deprecated.</p>
    </div>
    <p class="normal">We get 1,574 features that describe our <code class="Code-In-Text--PACKT-">time</code> object. These features could help us in machine learning models.</p>
    <p class="normal">Let's demonstrate how to extract ROCKET features from a time-series.</p>
    <h2 id="_idParaDest-61" class="title">ROCKET</h2>
    <p class="normal">We'll be using the implementation of ROCKET in the <code class="Code-In-Text--PACKT-">sktime</code> library.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">sktime</code> library represents<a id="_idIndexMarker235"/> data in a nested DataFrame. Each column stands for a feature, as expected, however, what may be surprising is that each row is an instance of a time-series. Each cell contains an array of all values for a given feature over time. In other words, each cell has a nested object structure, where instance-feature combinations are stored.</p>
    <p class="normal">This structure makes sense, because it allows us to store multiple instances of time-series in the same DataFrame, however, it's not intuitive at first. Fortunately, SkTime provides utility functions to unnest the SkTime datasets, as we will see.</p>
    <p class="normal">If we want to load an example time-series in SkTime, we can do this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sktime.datasets <span class="hljs-keyword">import</span> load_arrow_head
<span class="hljs-keyword">from</span> sktime.utils.data_processing <span class="hljs-keyword">import</span> from_nested_to_2d_array
X_train, y_train = load_arrow_head(split=<span class="hljs-string">"train"</span>, return_X_y=<span class="hljs-literal">True</span>)
from_nested_to_2d_array(X_train).head()
</code></pre>
    <p class="normal">We get an unnested DataFrame like this:</p>
    <figure class="mediaobject"><img src="../Images/B17577_03_06.png" alt="../../../../../../Desktop/Screenshot%202021-04-12%20at%2"/></figure>
    <p class="packt_figref">Figure 3.7: ROCKET features</p>
    <p class="normal">Again, each row is a time-series. There's only one feature, called <code class="Code-In-Text--PACKT-">dim_0</code>. The time-series has 251 measurements.</p>
    <p class="normal">We can import ROCKET, and then create the ROCKET features. We'll first have to learn the features and then apply them. This is a typical pattern for machine learning. In scitkit-learn, we'd use the <code class="Code-In-Text--PACKT-">fit()</code> and <code class="Code-In-Text--PACKT-">predict()</code> methods for models, where <code class="Code-In-Text--PACKT-">fit()</code> is applied on the training data and <code class="Code-In-Text--PACKT-">predict()</code> gives the predictions on a test set.</p>
    <p class="normal">The learning step should only ever be applied to the training set. One of the parameters in ROCKET is the number of kernels. We'll set it to 1,000 here, but we can set it to a higher number as well. 10,000 kernels is the default:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sktime.transformations.panel.rocket <span class="hljs-keyword">import</span> Rocket
rocket = Rocket(num_kernels=<span class="hljs-number">1000</span>)
rocket.fit(X_train)
X_train_transform = rocket.transform(X_train)
</code></pre>
    <p class="normal">The returned dataset<a id="_idIndexMarker236"/> is not nested, and it contains 2,000 columns. Each column describes the whole time-series but is the result from a different kernel.</p>
    <p class="normal">In the next section, we'll do a shapelets exercise.</p>
    <h2 id="_idParaDest-62" class="title">Shapelets in Practice</h2>
    <p class="normal">Let's create shapelets for the dataset<a id="_idIndexMarker237"/> we used before when we looked at ROCKET. We'll again use <code class="Code-In-Text--PACKT-">SkTime</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sktime.transformations.panel.shapelets <span class="hljs-keyword">import</span> ContractedShapeletTransform
shapelets_transform = ContractedShapeletTransform(
    time_contract_in_mins=<span class="hljs-number">1</span>,
    num_candidates_to_sample_per_case=<span class="hljs-number">10</span>,
    verbose=<span class="hljs-number">0</span>,
)
shapelets_transform.fit(X_train, y_train)
</code></pre>
    <p class="normal">The training could take a few minutes. We'll get some output about the candidates that are being examined. </p>
    <p class="normal">We can again transform our time-series using our shapelet transformer. This works as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train_transform = shapelets_transform.transform(X_train)
</code></pre>
    <p class="normal">This gives us a transformed dataset that we can use in machine learning models. We encourage the reader to play around with these feature sets.</p>
    <p class="normal">This concludes our Python practice.</p>
    <h1 id="_idParaDest-63" class="title">Summary</h1>
    <p class="normal">Preprocessing is a crucial step in machine learning that is often neglected. Many books don't cover preprocessing as a topic or skip preprocessing entirely. However, it is often in preprocessing that relatively easy wins can be achieved. The quality of the data determines the outcome.</p>
    <p class="normal">Preprocessing includes curating and screening the data. The expected output of the preprocessing is a dataset on which it is easier to conduct machine learning. This can mean that it is more reliable and less noisy than the original dataset.</p>
    <p class="normal">We've talked about feature transforms and feature engineering approaches to time-series data, and we've talked about automated approaches as well.</p>
    <p class="normal">In the next chapters, we'll explore how we can use these extracted features in a machine learning model. We'll discuss combinations of features and modeling algorithms in the next chapter, <em class="chapterRef">Chapter 4</em>, <em class="italic">Introduction to Machine Learning for Time-Series</em>. In <em class="chapterRef">Chapter 5</em>, <em class="italic">Time-Series Forecasting with Moving Averages and Autoregressive Models</em>, we'll be using machine learning pipelines, where we can connect feature extraction and machine learning models.</p>
  </div>
</body></html>
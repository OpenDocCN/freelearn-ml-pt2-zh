["```py\nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom string import punctuation\n```", "```py\n# Read the data and assign labels\nlabels = ['polarity', 'id', 'date', 'query', 'user', 'text']\ndata = pd.read_csv(\"sent140.csv\", names=labels)\n\n# Keep only text and polarity, change polarity to 0-1\ndata = data[['text', 'polarity']]\ndata.polarity.replace(4, 1, inplace=True)\n```", "```py\n# Create a list of stopwords\nstops = stopwords.words(\"english\")\n# Add stop variants without single quotes\nno_quotes = []\nfor word in stops:\n    if \"'\" in word:\n        no_quotes.append(re.sub(r'\\'', '', word))\nstops.extend(no_quotes)\n```", "```py\ndef clean_string(string):\n    # Remove HTML entities\n    tmp = re.sub(r'\\&\\w*;', '', string)\n    # Remove @user\n    tmp = re.sub(r'@(\\w+)', '', tmp)\n    # Remove links\n    tmp = re.sub(r'(http|https|ftp)://[a-zA-Z0-9\\\\./]+', '', tmp)\n    # Lowercase\n    tmp = tmp.lower()\n    # Remove Hashtags\n    tmp = re.sub(r'#(\\w+)', '', tmp)\n    # Remove repeating chars\n    tmp = re.sub(r'(.)\\1{1,}', r'\\1\\1', tmp)\n    # Remove anything that is not letters\n    tmp = re.sub(\"[^a-zA-Z]\", \" \", tmp)\n    # Remove anything that is less than two characters\n    tmp = re.sub(r'\\b\\w{1,2}\\b', '', tmp)\n    # Remove multiple spaces\n    tmp = re.sub(r'\\s\\s+', ' ', tmp)\n    return tmp\n\ndef preprocess(string):\n    stemmer = PorterStemmer()\n    # Remove any punctuation character\n    removed_punc = ''.join([char for char in string if char not in punctuation])\n    cleaned = []\n    # Remove any stopword\n    for word in removed_punc.split(' '):\n        if word not in stops:\n            cleaned.append(stemmer.stem(word.lower()))\n    return ' '.join(cleaned)\n```", "```py\ndef check_features_ngrams(features, n_grams, classifiers):\n    print(features, n_grams)\n\n    # Create the IDF feature extractor\n    tf = TfidfVectorizer(max_features=features, ngram_range=n_grams,\n                         stop_words='english')\n\n    # Create the IDF features\n    tf.fit(data.text)\n    transformed = tf.transform(data.text)\n    np.random.seed(123456)\n\n    def check_classifier(name, classifier):\n        print('--'+name+'--')\n\n        # Train the classifier\n        x_data = transformed[:train_size].toarray()\n        y_data = data.polarity[:train_size].values\n        classifier.fit(x_data, y_data)\n        i_s = metrics.accuracy_score(y_data, classifier.predict(x_data))\n\n        # Evaluate on the test set\n        x_data = transformed[test_start:test_end].toarray()\n        y_data = data.polarity[test_start:test_end].values\n        oos = metrics.accuracy_score(y_data, classifier.predict(x_data))\n\n        # Export the results\n        with open(\"outs.txt\",\"a\") as f:\n            f.write(str(features)+',')\n            f.write(str(n_grams[-1])+',')\n            f.write(name+',')\n            f.write('%.4f'%i_s+',')\n            f.write('%.4f'%oos+'\\n')\n\n    for name, classifier in classifiers:\n        check_classifier(name, classifier)\nFinally, we test for n-grams in the range of [1, 3] and for the top 500, 1000, 5000, 10000, 20000, and 30000 features.\n\n# Create csv header\nwith open(\"outs.txt\",\"a\") as f:\n    f.write('features,ngram_range,classifier,train_acc,test_acc')\n# Test all features and n-grams combinations\nfor features in [500, 1000, 5000, 10000, 20000, 30000]:\n    for n_grams in [(1, 1), (1, 2), (1, 3)]:\n    # Create the ensemble\n        voting = VotingClassifier([('LR', LogisticRegression()),\n                                   ('NB', MultinomialNB()),\n                                    ('Ridge', RidgeClassifier())])\n    # Create the named classifiers\n    classifiers = [('LR', LogisticRegression()),\n                    ('NB', MultinomialNB()),\n                    ('Ridge', RidgeClassifier()),\n                    ('Voting', voting)]\n     # Evaluate them\n     check_features_ngrams(features, n_grams, classifiers)\n```", "```py\nimport pandas as pd\nimport json\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom tweepy import OAuthHandler, Stream, StreamListener\n# Please fill your API keys as strings\nconsumer_key=\"HERE,\"\nconsumer_secret=\"HERE,\"\n\naccess_token=\"HERE,\"\naccess_token_secret=\"AND HERE\"\n```", "```py\n# Load the data\ndata = pd.read_csv('sent140_preprocessed.csv')\ndata = data.dropna()\n# Replicate our voting classifier for 30.000 features and 1-3 n-grams\ntrain_size = 10000\ntf = TfidfVectorizer(max_features=30000, ngram_range=(1, 3),\n                         stop_words='english')\ntf.fit(data.text)\ntransformed = tf.transform(data.text)\nx_data = transformed[:train_size].toarray()\ny_data = data.polarity[:train_size].values\nvoting = VotingClassifier([('LR', LogisticRegression()),\n                           ('NB', MultinomialNB()),\n                           ('Ridge', RidgeClassifier())])\nvoting.fit(x_data, y_data)\n```", "```py\n# Define the streaming classifier\nclass StreamClassifier(StreamListener):\n    def __init__(self, classifier, vectorizer, api=None):\n        super().__init__(api)\n        self.clf = classifier\n        self.vec = vectorizer\n    # What to do when a tweet arrives\n    def on_data(self, data):\n        # Create a json object\n        json_format = json.loads(data)\n        # Get the tweet's text\n        text = json_format['text']\n        features = self.vec.transform([text]).toarray()\n        print(text, self.clf.predict(features))\n        return True\n    # If an error occurs, print the status\n    def on_error(self, status):\n        print(status)\n```", "```py\n# Create the classifier and authentication handlers\nclassifier = StreamClassifier(classifier=voting, vectorizer=tf)\nauth = OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\n# Listen for specific hashtags\nstream = Stream(auth, classifier)\nstream.filter(track=['Trump'])\n```"]
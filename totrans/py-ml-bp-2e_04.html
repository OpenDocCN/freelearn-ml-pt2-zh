<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Forecast the IPO Market Using Logistic Regression</h1>
                </header>
            
            <article>
                
<p>In the late 1990s, getting in on the right <strong>Initial Public Offering</strong> (<strong>IPO</strong>) was like winning the lottery. First-day returns for some technology companies were many times their initial offering price, and if you were lucky enough to get in on an allocation, you were in for a windfall. Here are a few of the top first-day performers from the period:</p>
<ul>
<li>VA Linux up 697%, 12/09/99</li>
<li><span class="URLPACKT">Globe.com</span> up 606%, 11/13/98</li>
<li>Foundry Networks up 525%, 9/28/99</li>
</ul>
<p>While the days of dotcom mania are far behind us, IPOs can still have outsized first-day returns. Here are just a few that rose substantially on their first day of trading in the past year:</p>
<ul>
<li>Bloom Energy up 67%</li>
<li>Pinduoduo up 32%</li>
<li>Tenable up 32%</li>
</ul>
<p>As you can see, this is still a market worth paying attention to. In this chapter, we'll take a closer look at the IPO market. We'll see how we can use machine learning to help us decide which IPOs are worth a closer look and which ones we may want to take a pass on.</p>
<p>Here's what we'll cover in this chapter:</p>
<ul>
<li>The IPO market</li>
<li>Data cleansing and feature engineering</li>
<li>Binary classification with logistic regression</li>
<li>Model evaluation</li>
<li>Feature importance</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The IPO market</h1>
                </header>
            
            <article>
                
<p>Before we jump in and begin modeling, let's first discuss what an IPO, or initial public offering, is, and what research tells us about this market. After that, we'll discuss a number of strategies that we can apply.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is an IPO?</h1>
                </header>
            
            <article>
                
<p>An IPO is the process whereby a private company becomes a public company. Public offerings raise capital for the company and give the general public an opportunity to invest in the company by buying its shares.</p>
<p>Though there are variations in how this occurs, in a typical offering, a company enlists the help of one or more investment banks to underwrite their offering. This means that the banks make a guarantee to the company that they will purchase all of the shares being offered at the IPO price on the day of the IPO. The underwriters, of course, do not intend to keep all of the shares themselves. With the help of the offering company, they go on what's called a <strong>roadshow</strong> to drum up interest from institutional clients. These clients put in a <strong>subscription</strong> for the shares, which indicates their interest in buying shares on the day of the IPO. This is a non-binding contract, as the price of the offering is not finalized until the day of the IPO. The underwriter will then set the offer price, given the level of interest expressed.</p>
<p>What is interesting from our perspective is that research has consistently shown a systematic underpricing of IPOs. There are a number of theories as to why this happens, and why this level of underpricing seems to vary over time, but studies have shown that billions of dollars are left on the table every year.</p>
<p>In an IPO, <strong>money left on the table</strong>, is the difference between the offering price of shares and the first day's closing price.</p>
<p>One other point that should be mentioned before we move on is the difference between the offering price and the opening price. While you can occasionally get in on the deal through your broker and receive the IPO at its offering price, in nearly all instances, you, as a member of the general public, will have to purchase the IPO at the (typically higher) opening price. We'll build our models under this assumption.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recent IPO market performance</h1>
                </header>
            
            <article>
                
<p>Let's now take a look at the performance of the IPO market. We are going to pull down data from <kbd><span class="URLPACKT">IPOScoop.com</span></kbd>, which is a service that provides ratings for upcoming IPOs. Go to <a href="https://www.iposcoop.com/scoop-track-record-from-2000-to-present/" target="_blank"><span class="URLPACKT">https://www.iposcoop.com/scoop-track-record-from-2000-to-present/</span></a> and click on the button at the bottom of the page to download the spreadsheet. We'll load this into pandas and run a number of visualizations using our Jupyter notebook.</p>
<p>Unfortunately, the data is in a format that makes it impossible to just read into pandas with the normal <kbd>.read_csv()</kbd> method. What we'll need to do is use a library that lets us read Excel files into Python lists and then perform some preprocessing to filter out those rows that aren't of interest, primarily, header rows, and some extraneous information. Follow the steps to set up notebook:</p>
<ol>
<li>Let's now begin in our notebook by setting up the libraries we'll need:</li>
</ol>
<pre style="padding-left: 60px"><strong>import numpy as np 
import pandas as pd 
import xlrd 
import matplotlib.pyplot as plt 
%matplotlib inline</strong> </pre>
<p style="padding-left: 60px">The <kbd>xlrd</kbd> li<span>braryÂ </span>is what we'll be using to work with the Excel spreadsheet we downloaded earlier. If you don't have it installed already, it can be added to your Python distribution at the command line with <kbd>pip install xlrd</kbd>.</p>
<ol start="2">
<li>The next step is to load the workbook, as seen in the following block of code:</li>
</ol>
<pre style="padding-left: 60px"><strong>wb = xlrd.open_workbook('SCOOP-Rating-Performance.xls')</strong> </pre>
<ol start="3">
<li>Now that we have loaded the entire Excel workbook, let's target the sheet we'll be working with, in this instance, the first one:</li>
</ol>
<pre style="padding-left: 60px"><strong>ws = wb.sheet_by_index(0) </strong> </pre>
<ol start="4">
<li>Let's now check that we have the data we are expecting:</li>
</ol>
<pre style="padding-left: 60px"><strong>ws.nrows</strong> </pre>
<ol start="5">
<li>The preceding line of code generates the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1135 image-border" src="assets/3dda75ad-b2a1-4060-ac9f-fafd553a3531.png" style="width:4.42em;height:2.58em;"/></p>
<ol start="6">
<li>That number looks about right from comparing it against the spreadsheet, so let's now move on to incorporating the data row by row:</li>
</ol>
<pre style="padding-left: 60px"><strong>ipo_list = [] 
for i in range(36,ws.nrows): 
    if isinstance(ws.row(i)[0].value, float): 
        ipo_list.append([x.value for x in ws.row(i)]) 
    else: 
        print(i, ws.row(i))</strong> </pre>
<p style="padding-left: 60px">The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/48a7dd95-c7a0-439f-8fa4-9ac061ccd8dd.png"/></p>
<p style="padding-left: 60px">Let's talk about what happened in that code block. First, we create an empty list to which we will add the rows. Then, we loop over each row in the spreadsheet, checking whether the first value (the left most cell) is a float. If it is, then we add all the cells' values to our list. This works because the <kbd>Date</kbd> column, when read in, appears as a float, and we are only interested in those rows in the sheet that start with a date. Note that we also start our loop on line 36 to skip over the summary data in the sheet.</p>
<ol start="7">
<li>Let's now again check that the number of rows we expect are in our list:</li>
</ol>
<pre style="padding-left: 60px"><strong>len(ipo_list)</strong> </pre>
<p style="padding-left: 60px">The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e32104a5-8946-4314-a073-970f3b20cf9c.png" style="width:4.75em;height:2.58em;"/></p>
<p style="padding-left: 60px">After eliminating the header and other rows we aren't interested in, this looks about right.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working on the DataFrame</h1>
                </header>
            
            <article>
                
<p>Let's now move on to getting our DataFrame ready to work with:</p>
<pre><strong>df = pd.DataFrame(ipo_list) 
 
df.head()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3bfc736d-8024-4705-a0d1-c49641e15a64.png"/></p>
<p>The data looks good, so let's now add our columns:</p>
<pre><strong>df.columns = ['Date', 'Company', 'Ticker', 'Managers', \ 
              'Offer Price', 'Opening Price', '1st Day Close',\ 
              '1st Day % Chg', '$ Chg Open', '$ Chg Close',\ 
              'Star Rating', 'Performed'] 
 
df.head()</strong> </pre>
<p class="mce-root"/>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0284d547-a23d-4365-839d-3b6d69003869.png"/></p>
<p>Let's now convert that <kbd>Date</kbd> column from a float to a proper date. The <kbd>xlrd</kbd> library has some functionality that can help us with that. We'll use it in a function to get our dates in the proper format:</p>
<pre><strong>def to_date(x): 
    return xlrd.xldate.xldate_as_datetime(x, wb.datemode) 
df['Date'] = df['Date'].apply(to_date) 
df</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7f0fc1b4-4790-439d-8069-25b0ed825600.png"/></p>
<p>Now that we have dates that we can work with, let's add some additional date-related columns that can help us work with the data better:</p>
<pre><strong>df['Year'], df['Month'], df['Day'], df['Day of Week'] = \ 
df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.day, df['Date'].dt.weekday 
df</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6bbdc76b-d8ad-4758-8f33-eabf61700a67.png"/></p>
<p>Now that we've completed those steps, let's check our data in the DataFrame against the data in the original spreadsheet:</p>
<pre><strong>by_year_cnt = df.groupby('Year')[['Ticker']].count() 
 
by_year_cnt</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c309883c-42d1-4bad-8c0b-8a50b891b555.png" style="width:6.67em;height:37.00em;"/></p>
<p>Comparing this to the same values in the spreadsheet shows us that we have nearly identical values, so we should be good to continue.</p>
<p>We'll take one additional step here to eliminate what are sometimes referred to as <em>penny stocks</em>, or particularly low-priced stocks. Then, we'll check the data types to ensure they look appropriate:</p>
<pre><strong>df.drop(df[df['Offer Price'] &lt; 5].index, inplace=True) 
 
df.reset_index(drop=True, inplace=True) 
 
df.dtypes</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/370f1607-7f12-4a06-8d8c-7a7d759cad45.png" style="width:21.92em;height:23.17em;"/></p>
<p>This looks to be in line with what we expect, with the exception of the <kbd>1st Day % Chg</kbd> column. We'll correct that now by changing the data type to a float:</p>
<pre><strong>df['1st Day % Chg'] = df['1st Day % Chg'].astype(float) 
df.dtypes</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/65e42628-6df9-4efc-b3b3-de5e128c6278.png" style="width:28.00em;height:27.92em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the data</h1>
                </header>
            
            <article>
                
<p>The data types all look good now, so we'll begin our exploratory analysis by graphing the number of IPOs since 2000:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(16,8)) 
by_year_cnt.plot(kind='bar', ax=ax, color='crimson') 
ax.legend(['Ticker Count']) 
ax.set_title('IPO Count by Year', fontdict={'size': 18}, y=1.02);</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/05867657-fc25-4603-a6d9-75ee6f6c1d4e.png" style="width:57.17em;height:31.08em;"/></p>
<p>From the chart, we can see that most years have over 100 IPOs, but that in the years after and including 2001 and 2008, there was a notable reduction, mostly likely due to the aftermath of 9/11 and the financial crisis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summarizing the performance of the stocks</h1>
                </header>
            
            <article>
                
<p>We'll get a quick summary of the performance of the stocks over the past 18 years by executing the following code:</p>
<pre><strong>summary_by_year = df.groupby('Year')['1st Day % Chg'].describe() 
 
summary_by_year</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4a07bde7-f25a-4c6e-8694-c1ad1b8a207d.png" style="width:38.58em;height:38.83em;"/></p>
<p>From the table, we can see the extraordinary average return of the IPO market in 2000. At over 35%, it is more than double any other year on the list. Also notable is the fact that every year has had a positive average return for first-day performance.</p>
<p>Let's plot first-day performance to get a better feel for it:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(16,8)) 
summary_by_year['mean'].plot(kind='bar', ax=ax) 
ax.set_title('Mean First Day Percentage Change by Year', fontdict={'size': 18}, y=1.02);</strong> </pre>
<p class="mce-root"/>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1199 image-border" src="assets/8aa3123a-73f8-4315-a7c7-902a9b17dd2e.png" style="width:143.17em;height:78.17em;"/></p>
<p>The important point about these numbers is that they are not the first-day performance that the general investing public could expect to receive on that first-day. Only investors who got in on the offering could expect to see these numbers.</p>
<p>The first-day return that the general public could expect to receive would be the difference between the opening price and the closing price. This is entirely different, and much less lucrative. Let's now add a column of data to reflect that value and see the results:</p>
<pre><strong>df['1st Day Open to Close % Chg'] = ((df['1st Day Close'] - df['Opening Price'])/df['Opening Price']) 
 
df['1st Day Open to Close % Chg'].describe()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a89b4db2-aff1-4b2f-958a-a6394553c6f5.png" style="width:34.67em;height:12.92em;"/></p>
<p>This shows returns that are markedly less exciting. Let's now plot them as before:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(16,8)) 
df.groupby('Year')['1st Day Open to Close % Chg'].mean().plot(kind='bar', ax=ax) 
ax.set_title('Mean First Day Open to Close % Change by Year', fontdict={'size': 18}, y=1.02);</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/327d4037-ffc3-4ce8-a799-3cf7b6010f6d.png"/></p>
<p>Comparing the preceding chart to the earlier one, it is clear that annual average returns on the first day are displayed in the range of their order of magnitude which is lower in many cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Baseline IPO strategy</h1>
                </header>
            
            <article>
                
<p>Let's now suppose that we bought one share of every IPO at the exact opening tick and sold them at the precise closing price listed in these figures; what would our returns look like in terms of dollars earned?</p>
<p>To answer this question, let's look at the actual dollar price change from open to close:</p>
<pre><strong>df['1st Day Open to Close $ Chg'] = (df['1st Day Close'] - df['Opening Price']) 
 
df[df['Year']==2018].sum()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5b34983d-50e4-4e45-905e-4f60e13f57ce.png" style="width:30.00em;height:21.58em;"/></p>
<p>From this, we see the first day open-to-close dollar total is just above $28. That number is for over 173 IPOs so far in 2018:</p>
<pre><strong>df[df['Year']==2018]['1st Day Open to Close $ Chg'].describe()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a2d76b57-b913-4569-9139-4a971495708b.png" style="width:35.42em;height:13.08em;"/></p>
<p>That reflects a mean first-day gain of just over 16 cents per IPO. And remember, this is under ideal conditions where we ignore transaction costs and slippage.</p>
<div class="packt_infobox">Slippage is the difference between your attempted entry or exit price for a target stock and the price at which your order is actually fulfilled.</div>
<p>Let's now take a look at what the distribution of returns for these IPOs look like. This may help us understand how to improve our returns over the baseline NaÃ¯ve Bayes strategy of just buying every IPO:</p>
<pre><strong>fig, ax = plt.subplots(figsize=(16,8)) 
df[df['Year']==2018]['1st Day Open to Close % Chg'].plot(kind='hist', bins=25, ax=ax)</strong> </pre>
<p class="mce-root"/>
<p><span>The preceding code generates the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1200 image-border" src="assets/06922374-0015-4d9f-9f14-9a8b766491d7.png" style="width:144.17em;height:71.83em;"/></p>
<p>We see that returns are centered around zero, but there is a long tail to the right where there are some exceptional returns. It would be quite financially rewarding if we were able to identify some of the commonalities that these exceptional IPOs have that we could capitalize on.</p>
<p>Let's see whether we can use machine learning to help improve our results from a NaÃ¯veÂ Bayes approach. A reasonable strategy would seem to be targeting that long tail on the right, so we'll focus on feature engineering in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data cleansing and feature engineering</h1>
                </header>
            
            <article>
                
<p>What might impact the performance of an offering as it begins trading? Perhaps the performance of the market in general or the prestige of the underwriters could impact it? Perhaps the day of the week or the month that it trades is important? The consideration and inclusion of these factors in a model is called <strong>feature engineering</strong>, and modeling this is nearly as important as the data that you use to build the model. If your features aren't informative, your model simply won't have value.</p>
<p>Let's begin this process by adding a few features that we expect may influence the performance of an IPO.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding features to influence the performance of an IPO</h1>
                </header>
            
            <article>
                
<p>One measure of demand that could be informative is the <strong>opening gap</strong>. This is the difference between the offer price and the opening price of the issue. Let's add that to our DataFrame:</p>
<pre><strong>df['Opening Gap % Chg'] = (df['Opening Price'] - df['Offer Price'])/df['Offer Price']</strong> </pre>
<p>Next, let's get a count of the number of underwriters on the offering. Perhaps having more banks involved leads to better marketing of the issue? This is demonstrated in the following code block:</p>
<pre><strong>def get_mgr_count(x): 
    return len(x.split('/')) 
 
df['Mgr Count'] = df['Managers'].apply(get_mgr_count)</strong> </pre>
<p>Let's quickly see whether there might be anything to this hypothesis by means of a visualization:</p>
<pre><strong>df.groupby('Mgr Count')['1st Day Open to Close % Chg'].mean().to_frame().style.bar(align='mid', color=['#d65f5f', '#5fba7d'])</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1138 image-border" src="assets/e3847001-bd88-42f6-ae8c-37227a67f80e.png" style="width:15.25em;height:31.08em;"/></p>
<p>It's not apparent what the relationship might be from this chart, but clearly nine bankers is the sweet spot!</p>
<p>Next, let's move on to extracting the first underwriter in the list. This would be the lead, and perhaps the prestige of this bank is important to the first-day gains:</p>
<pre><strong>df['Lead Mgr'] = df['Managers'].apply(lambda x: x.split('/')[0])</strong></pre>
<p>Next, let's take a quick peek at the data in the new column that we have created:</p>
<pre><strong>df['Lead Mgr'].unique()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1139 image-border" src="assets/e94235ea-9f21-4a44-a527-a30e63bfeb41.png" style="width:43.50em;height:20.75em;"/></p>
<p>Even a cursory examination of the preceding shows us that we have some genuine issues with the data. Many names are replicated with different spellings and punctuation. We could, at this point, stop and attempt to clean up the data, and this would be the proper course of action if we were going to rely on our model for anything serious, but as this is just a toy project, we'll forge ahead and hope that the impact is minimal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Binary classification with logistic regression</h1>
                </header>
            
            <article>
                
<p>Instead of attempting to predict what the total first-day return will be, we are going to attempt to predict whether the IPO will be one we should buy for a trade or not. It is here that we should point out that this is not investment advice and is for illustrative purposes only. Please don't run out and start day trading IPOs with this model willy-nilly. It will end badly.</p>
<p>Now, to predict a binary outcome (that's a <kbd>1</kbd> or <kbd>0</kbd>/yes or no), we will start with a model calledÂ <strong>logistic regression</strong>. Logistic regression is actually a binary classification model rather than regression. But it does utilize the typical form of a linear regression; it just does so within a logistic function.</p>
<p>A typical single variable regression model takes the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d94c1fc8-a2fd-4207-b16e-74cb93920de1.png" style="width:6.00em;height:1.08em;"/></p>
<p>Here, <em>t</em> is a linear function of a single explanatory variable, <em>x</em>. This can, of course, be expanded to be a linear combination of many variables. The problem with this form for a binary outcome variable is that <em>t</em> does not naturally fall between 1 and 0.</p>
<p>The logistic function seen in the following equation has some quite favorable mathematical properties, including the fact that it can take any number as an input (<em>t</em> here) and return a result that falls between 0 and 1:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1de20958-589d-49e8-8675-5e536390b8ff.png" style="width:12.25em;height:2.75em;"/></p>
<p>The graph is represented as below:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/4c9c4dcc-4129-444c-99c2-b6be33706634.png" style="width:25.17em;height:17.08em;"/></div>
<p>By replacing <em>t</em> with our regression function, we now have a model that is able to both give us information on the importance of each predictor (the beta coefficients) and provide a form that can be used to give us a binary prediction that represents the probability of <em>success</em>, or a <em>positive result</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/51a0d4c3-1ecd-4071-bb70-e46a5d019cfc.png" style="width:12.50em;height:3.17em;"/></p>
<p>Before we can move on to modeling our data, we need to put it in a form that is appropriate for scikit-learn.</p>
<p>We'll start by importing a library that can help us with this task; it's called <kbd>patsy</kbd>. It can be pip-installed if necessary:</p>
<pre><strong>from patsy import dmatrix </strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the target for our model</h1>
                </header>
            
            <article>
                
<p>Now, we'll create the target for our model. This is the column that will tell our model whether each IPO should have been invested in or not. We are going to say that we should invest in any IPO that has a 2.5% or greater return on day one. Obviously, this is an arbitrary number, but it seems like a reasonable value for the investment to be worthy of our attention:</p>
<pre><strong>y = df['1st Day Open to Close % Chg'].apply(lambda x: 1 if x &gt; .025 else 0)</strong> </pre>
<p>Now that we have set our target column, we need to set up our predictor variables. We'll again use <kbd>patsy</kbd> for this:</p>
<pre><strong>X = dmatrix("Q('Opening Gap % Chg') + C(Q('Month'), Treatment) + C(Q('Day of Week'), Treatment)\ 
+ Q('Mgr Count') + Q('Lead Mgr') + Q('Offer Price') + C(Q('Star Rating'), Treatment)", df, return_type="dataframe")</strong> </pre>
<p>Let's discuss what's going on in that line of code. <kbd>X</kbd> here is our design matrix, or the matrix that contains our predictor variables. We have included the things we discussed earlier that could have some impact on the performance: the size of the opening gap, the month and day of the IPO, the offering price, the lead manager, the number of managers, and finally, the star rating that <kbd>IPOScoop.com</kbd> provides in advance of the IPO's listing.</p>
<p>To give some explanation regarding the Qs and Cs found in the lines of code, the Qs are simply used to provide quotes in the formula for columns that have white space in their names, and the Cs are used to indicate that the referenced column should be treated as categorical features and dummy-coded.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dummy coding</h1>
                </header>
            
            <article>
                
<p>Dummy coding is a method where, if we had one column that had a student's favorite class as a predictor variable in one column, we would turn each class into its own column and then place a <kbd>1</kbd> in that column if it was the favorite class of the student, as seen in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1140 image-border" src="assets/8b57015a-328b-48af-97e7-734144a3cf04.png" style="width:36.75em;height:12.50em;"/></p>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Source: http://www.statisticssolutions.com/dummy-coding-the-how-and-why/</div>
<p>Once that is done, then the next step is to actually drop one of those columns. The dropped column then becomes the <strong>base case</strong>. All the other cases are then compared to that case. In our IPO example using months as predictors, we will drop January, for example, and then all the other months will be judged against January's performance. The same goes for the days of the week or any other categorical predictor. This dropping of a column is to prevent multicollinearity, which would have a negative impact on the explanatory power of the model.</p>
<p>Let's take a look at what this coding looks like by running the following in a Jupyter cell:</p>
<pre><strong>X</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2576a82c-f579-492b-afc4-9bd9705c0051.png"/></p>
<p>Now that we have both our <kbd>X</kbd> and <kbd><em>y</em></kbd>, we are ready to fit our model. We are going use a very basic train/test split and simply train our model on all but the last 200 IPOs:</p>
<pre><strong>from sklearn.linear_model import LogisticRegression 
 
X_train = X[:-200] 
y_train = y[:-200] 
 
X_test = X[-200:] 
y_test = y[-200:] 
 
clf = LogisticRegression() 
clf.fit(X_train, y_train)</strong> </pre>
<p>And, with that, we have our model. Let's examine the performance of this very simple model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examining the model performance</h1>
                </header>
            
            <article>
                
<p>We'll start by making predictions on our test data and then we'll examine whether our predictions were correct:</p>
<pre><strong>y_hat = clf.predict(X_test) 
y_true = y_test 
 
pdf = pd.DataFrame({'y_true': y_true, 'y_hat': y_hat}) 
 
pdf['correct'] = pdf.apply(lambda x: 1 if x['y_true'] == x['y_hat'] else 0, axis=1) 
 
pdf </strong></pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5c6ec647-0a8f-4d55-984e-cd3266527bf4.png" style="width:14.00em;height:23.25em;"/></p>
<p>Let's now look at what percentage of the 200 IPOs in our <kbd>test</kbd> dataset we should have invested inâremember, that means they rose over 2.5% from the open to the close:</p>
<pre><strong>pdf['y_true'].value_counts(normalize=True)</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1142 image-border" src="assets/512daffe-e875-446f-a0dc-81284e18eb29.png" style="width:21.58em;height:4.58em;"/></p>
<p>So, just north of half the IPOs rose over 2.5% from their opening tick to the closing tick. Let's see how accurate our model's calls were:</p>
<pre><strong>pdf['correct'].value_counts(normalize=True)</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1143 image-border" src="assets/9138b386-ade9-4755-af1a-c2908790aed9.png" style="width:22.42em;height:4.58em;"/></p>
<p class="mce-root"/>
<p>Well, it looks like our model was about as accurate as a coin flip. That doesn't seem too promising. But with investing, what is important is not the accuracy but the expectancy. If we had a number of small losses, but a couple of huge wins, overall, the model could still be very profitable. Let's examine whether that's the case here. We'll join our results data with the first-day change data to explore this:</p>
<pre><strong>results = pd.merge(df[['1st Day Open to Close $ Chg']], pdf, left_index=True, right_index=True) 
 
results </strong></pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d010855b-3879-4bd6-8c26-094f84c74061.png" style="width:25.83em;height:22.42em;"/></p>
<p>First, let's see what our results would have looked like for one share of every one of the 200 IPOs in our test data:</p>
<pre><strong>results['1st Day Open to Close $ Chg'].sum()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/120d38b6-ac84-463f-94f6-e2543c3ec39f.png" style="width:6.92em;height:2.08em;"/></p>
<p>From this, we see that we would have gained over $215 in an ideal cost-free scenario. Now, let's examine some of the other statistics concerning these IPOs:</p>
<pre><strong>results['1st Day Open to Close $ Chg'].describe()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1d9c6402-1bcf-4a5e-b16f-e9f769afd5b2.png" style="width:30.00em;height:11.25em;"/></p>
<p>Based on the preceding, we see that the average gain was just over $1, and the largest loss was 15 times that. How does our model fare against those numbers? First, we look at the trades our model said we should take and the resulting gains:</p>
<pre><strong># ipo buys 
results[results['y_hat']==1]['1st Day Open to Close $ Chg'].sum()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1144 image-border" src="assets/feddf7fe-8c00-4af2-8e50-b34aa1a4e4c2.png" style="width:8.00em;height:1.83em;"/></p>
<p>Let's look at the other statistics as well:</p>
<pre><strong># ipo buys 
results[results['y_hat']==1]['1st Day Open to Close $ Chg'].describe()</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0f1480c1-5494-4988-942a-70e7de78c7de.png" style="width:29.75em;height:10.67em;"/></p>
<p>Here, we see that our model suggested investing in only 34 IPOs, the mean gain rose to $1.50, the largest loss was reduced to under $10, and we still were able to capture the best performing IPO. It's not stellar, but we may be on to something. We'd need to explore further to really know whether we do have something worth expanding further.</p>
<p>Now, let's move on to examine the factors that seem to influence our model's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating the importance of a feature from our modelÂ </h1>
                </header>
            
            <article>
                
<p>One of the nice features of logistic regression is that it offers predictor coefficients that can tell us the relative importance of the predictor variables or features. For categorical features, a positive sign on a feature's coefficient tells us that, when present, this feature increases the probability of a positive outcome versus the baseline. For continuous features, a positive sign tells us that an increase in the value of a feature corresponds to an increase in the probability of a positive outcome. The size of the coefficient tells us the magnitude of the increase in probability.</p>
<p>Let's generate the importance of the feature from our model, and then we can examine the impact it has:</p>
<pre><strong>fv = pd.DataFrame(X_train.columns, clf.coef_.T).reset_index() 
fv.columns = ['Coef', 'Feature'] 
fv.sort_values('Coef', ascending=0).reset_index(drop=True)</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1145 image-border" src="assets/0a7f800c-e6c7-4719-b911-69b28c47bb4c.png" style="width:23.08em;height:19.25em;"/></p>
<p>In the preceding screenshot, we see those features with the largest coefficients. Let's look at days of the week and their impact there:</p>
<pre><strong>fv[fv['Feature'].str.contains('Day')]</strong> </pre>
<p>The preceding code generates the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1146 image-border" src="assets/215fc453-b07c-458f-b65c-65c5472cb9d5.png" style="width:22.42em;height:12.25em;"/></div>
<p>Here, the first day of the week would be Monday and would be coded as <kbd>T.0</kbd>, or the base case. All the other days of the week would be compared to Monday. From the preceding screenshot, we see that Thursday appears to be the best day of the week. Saturday appears to be a terrible day of the week to have an IPO, most likely because the market is closed that day. (In all likelihood, those dates are just incorrectly recorded.)</p>
<p>Looking further at the features with the highest coefficients, we can now appreciate that extracting useful information for the predictive value of each is difficult, since many of those features are for things that no longer exist. For example, while Deutsche Bank still exists, it no longer underwrites as Deutsche Banc Alex. Brown, so that is actually conveying historical information rather than information that can be useful going forward.</p>
<p>Another issue is that features do not reflect how frequently they had an impact. The appearance of a bank that was only in business in 2000 and had 3 hugely successful IPOs would have a very large positive coefficient, but would be meaningless in our modeling efforts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest classifier method</h1>
                </header>
            
            <article>
                
<p>Another method of modeling that tells us which features have an impact on our model is the feature importance that comes out of a random forest classifier. This more accurately reflects the true impact of a given feature.</p>
<p>Let's run our data through this type of model and examine the results:</p>
<pre><strong>from sklearn.ensemble import RandomForestClassifier 
clf_rf = RandomForestClassifier(n_estimators=1000) 
clf_rf.fit(X_train, y_train) 
 
f_importances = clf_rf.feature_importances_ 
 
f_names = X_train.columns 
f_std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_], axis=0) 
 
zz = zip(f_importances, f_names, f_std) 
zzs = sorted(zz, key=lambda x: x[0], reverse=True) 
</strong> 
<strong>n_features = 10 
imps = [x[0] for x in zzs[:n_features]] 
labels = [x[1] for x in zzs[:n_features]] 
errs = [x[2] for x in zzs[:n_features]] 
 
fig, ax = plt.subplots(figsize=(16, 8)) 
ax.bar(range(n_features), imps, color="r", yerr=errs) 
plt.xticks(range(n_features), labels) 
plt.setp( ax.xaxis.get_majorticklabels(), rotation=-70, ha="left" );</strong> </pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b8c70794-d65c-417b-9ddf-c1bfdde03488.png"/></p>
<p>In the preceding code, we ran a random forest classifier, extracted and sorted the importance of the <span>features</span>, and then graphed those values with their error bars.</p>
<p>From this data, we see that what has the most impact on the model is the opening gap, the offer price, and the number of managers involved in the deal. These would all seem to make sense as having predictive values, since they indicate that there is strong demand for the deal.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We covered a lot of ground in this chapter, but we've only just scratched the surface in terms of how to build this type of model. Hopefully, you've gained a better understanding of the modeling process, from cleaning the data, to engineering the features, to testing. And hopefully, you'll use this information to extend the model on your own and improve upon it.</p>
<p>In the next chapter, we'll turn our attention to a very different domain, as we move from numeric data to text-based data.</p>


            </article>

            
        </section>
    </body></html>
- en: 'Chapter 7: Extending Machine Learning Services Using Built-In Frameworks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last three chapters, you learned how to use built-in algorithms to train
    and deploy models without having to write a line of machine learning code. However,
    these algorithms don't cover the full spectrum of machine learning problems. In
    a lot of cases, you'll need to write your own code. Thankfully, several open source
    frameworks make this reasonably easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to train and deploy models with the most
    popular open source frameworks for machine learning and deep learning. We will
    cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the built-in frameworks in Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running your framework code on Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the built-in frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and configure the AWS command-line interface for your
    account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory but
    strongly encouraged, as it includes many projects that we will need (Jupyter,
    `pandas`, `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working Docker installation. You can find installation instructions
    and the necessary documentation at [https://docs.docker.com](https://docs.docker.com).
  prefs: []
  type: TYPE_NORMAL
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the built-in frameworks in Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker lets you train and deploy your models with the following machine
    learning and deep learning frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scikit-learn**, undoubtedly the most widely used open source library for
    machine learning. If you''re new to this topic, start here: [https://scikit-learn.org](https://scikit-learn.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost**, an extremely popular and versatile open source algorithm for regression,
    classification, and ranking problems ([https://xgboost.ai](https://xgboost.ai)).
    It''s also available as a built-in algorithm, as presented in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*. Using it in framework mode will give us more
    flexibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow**, an extremely popular open source library for deep learning
    ([https://www.tensorflow.org](https://www.tensorflow.org)). SageMaker also supports
    the lovable **Keras** API ([https://keras.io](https://keras.io)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch**, another highly popular open source library for deep learning ([https://pytorch.org](https://pytorch.org)).
    Researchers, in particular, enjoy its flexibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache MXNet**, an interesting challenger for deep learning. Natively implemented
    in C++, it''s often faster and more scalable than its competitors. Its **Gluon**
    API provides rich toolkits for computer vision ([https://gluon-cv.mxnet.io](https://gluon-cv.mxnet.io)),
    **Natural Language Processing** (**NLP**) ([https://gluon-nlp.mxnet.io](https://gluon-nlp.mxnet.io)),
    and time series data ([https://gluon-ts.mxnet.io](https://gluon-ts.mxnet.io)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chainer**, another worthy challenger for deep learning ([https://chainer.org](https://chainer.org)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hugging Face**, the most popular collection of state-of-the-art tools and
    models for NLP ([https://huggingface.co](https://huggingface.co)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frameworks for **reinforcement learning**, such as **Intel Coach**, **Ray RLlib**,
    and **Vowpal Wabbit**. I won't discuss this topic here as it could take up another
    book!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark**, thanks to a dedicated SDK that lets you train and deploy models
    directly from your Spark application using either **PySpark** or **Scala** ([https://github.com/aws/sagemaker-spark](https://github.com/aws/sagemaker-spark)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You'll find plenty of examples of all of these at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll focus on the most popular ones: XGBoost, scikit-learn,
    TensorFlow, PyTorch, and Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: The best way to get started is to run a first simple example. As you will see,
    the workflow is the same as for built-in algorithms. We'll highlight a few differences
    along the way, which we'll dive into later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Running a first example with XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we'll build a binary classification model with the XGBoost
    built-in framework. At the time of writing, the latest version supported by SageMaker
    is 1.3-1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use our own training script based on the `xgboost.XGBClassifier` object
    and the Direct Marketing dataset, which we used in [*Chapter 3*](B17705_03_Final_JM_ePub.xhtml#_idTextAnchor049),
    *AutoML with Amazon SageMaker Autopilot*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we download and extract the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We import the SageMaker SDK and define an S3 prefix for the job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the dataset and apply very basic processing (as it''s not our focus
    here). Simply one-hot encode the categorical features, move the labels to the
    first column (an XGBoost requirement), shuffle the dataset, split it for training
    and validation, and save the results in two separate CSV files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We upload the two files to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define two inputs, with data in CSV format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an estimator for the training job. Of course, we could use the generic
    `Estimator` object and pass the name of the XGBoost container hosted in `XGBoost`
    estimator, which automatically selects the right container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Several parameters are familiar here: the role, the infrastructure requirements,
    and the output path. What about the other ones? `entry_point` is the path of our
    training script (available in the GitHub repository for this book). `hyperparameters`
    is passed to the training script. We also have to select a `framework_version`
    value; this is the version of XGBoost that we want to use.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We train as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also deploy as usual, creating a unique endpoint name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we load a few samples from the validation set and send them for prediction
    in CSV format. The response contains a score between 0 and 1 for each sample:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the following probabilities:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we''re done, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used XGBoost here, but the workflow would be identical for another framework.
    This standard way of training and deploying makes it really easy to switch from
    built-in algorithms to frameworks, or from one framework to the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The points that we need to focus on here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Framework containers**: What are they? Can we see how they''re built? Can
    we customize them? Can we use them to train on our local machine?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training**: How does a SageMaker training script differ from vanilla framework
    code? How does it receive hyperparameters? How should it read input data? Where
    should it save the model?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deploying**: How is the model deployed? Should the script provide some code
    for this? What''s the input format for prediction?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`entry_point` script? Can we add libraries for training and deployment?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these questions will be answered now!
  prefs: []
  type: TYPE_NORMAL
- en: Working with framework containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker contains a training and inference container for each built-in framework,
    and they are updated regularly to the latest versions. Different containers are
    also available for CPU and GPU instances. All these containers are collectively
    known as **Deep Learning Containers** ([https://aws.amazon.com/machine-learning/containers](https://aws.amazon.com/machine-learning/containers)).
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the previous example, they let you use your own code without having
    to maintain bespoke containers. In most cases, you won't need to look any further,
    and you can happily forget that these containers even exist. If this topic feels
    too advanced for now, feel free to skip it for now, and move on to the *Training
    and deploying locally* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re curious or have custom requirements, you''ll be happy to learn that
    the code for these containers is open source:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scikit-learn**: [https://github.com/aws/sagemaker-scikit-learn-container](https://github.com/aws/sagemaker-scikit-learn-container)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost**: [https://github.com/aws/sagemaker-xgboost-container](https://github.com/aws/sagemaker-xgboost-container)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow, PyTorch, Apache MXNet, and Hugging Face**: [https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chainer**: [https://github.com/aws/sagemaker-chainer-container](https://github.com/aws/sagemaker-chainer-container)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For starters, this lets you understand how these containers are built and how
    SageMaker trains and predicts with them. You could also do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Build and run them on your local machine for local experimentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and run them on your favorite managed Docker service, such as **Amazon
    ECS**, **Amazon EKS**, or **Amazon Fargate** ([https://aws.amazon.com/containers](https://aws.amazon.com/containers)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customize them, push them to Amazon ECR, and use them with the estimators present
    in the SageMaker SDK. We'll demonstrate this in [*Chapter 8*](B17705_08_Final_JM_ePub.xhtml#_idTextAnchor147),
    *Using Your Algorithms and Code*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These containers have another nice property. You can use them with the SageMaker
    SDK to train and deploy models on your local machine. Let's see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: Training and deploying locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Local mode** is the ability to train and deploy models with the SageMaker
    SDK without firing up on-demand managed infrastructure in AWS. You use your local
    machine instead. In this context, "local" means the machine running the notebook:
    it could be your laptop, a local server, or a small **notebook instance**.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, local mode is not available in SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: This is an excellent way to quickly experiment and iterate on a small dataset.
    You won't have to wait for instances to come up, and you won't have to pay for
    them either!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit our previous XGBoost example, highlighting the changes required
    to use local mode:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Explicitly set the name of the IAM role. `get_execution_role()` does not work
    on your local machine (it does on a notebook instance):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the training and validation datasets from local files. Store the model
    locally in `/tmp`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the `XGBoost` estimator, set `instance_type` to `local`. For local GPU training,
    we would use `local_gpu`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `xgb_estimator.deploy()`, set `instance_type` to `local`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's all it takes to train on your local machine using the same container
    you would use at scale on AWS. This container will be pulled once to your local
    machine and you'll be using it from then on. When you're ready to train at scale,
    just replace the `local` or `local_gpu` instance type with the appropriate AWS
    instance type and you're good to go.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting
  prefs: []
  type: TYPE_NORMAL
- en: If you see strange deployment errors, try restarting Docker (`sudo service docker
    restart`). I found that it doesn't like being interrupted during deployment, which
    it tends to do a lot when working inside Jupyter Notebooks!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see what it takes to run our own code inside these containers. This
    feature is called **script mode**.
  prefs: []
  type: TYPE_NORMAL
- en: Training with script mode
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since your training code runs inside a SageMaker container, it needs to be
    able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Receive hyperparameters passed to the estimator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read data available in input channels (training, validation, and more).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the trained model in the right place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Script mode is how SageMaker makes this possible. The name comes from the way
    your code is invoked in the container. Looking at the training log for our XGBoost
    job, we see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our code is invoked like a plain Python script (hence the name script mode).
    We can see that hyperparameters are passed as command-line arguments, which answers
    the question of what we should use inside the script to read them: `argparse`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the corresponding code snippet in our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'What about the location of the input data and the saved model? If we look at
    the log a little more closely, we''ll see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: These three environment variables define **local paths inside the container**,
    pointing to the respective locations for the training data, validation data, and
    the saved model. Does this mean we have to manually copy the datasets and the
    model from and to S3? No! SageMaker takes care of all this automatically for us.
    This is part of the support code present in the container.
  prefs: []
  type: TYPE_NORMAL
- en: Our script only needs to read these variables. I recommend using `argparse`
    again, as this will let us pass the paths to our script when we train outside
    of SageMaker (more on this soon).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the corresponding code snippet in our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Channel names
  prefs: []
  type: TYPE_NORMAL
- en: The `SM_CHANNEL_xxx` variables are named according to the channels passed to
    `fit()`. For instance, if your algorithm required a channel named `foobar`, you'd
    name it `foobar` in `fit()` and `SM_CHANNEL_FOOBAR` in your script. In your container,
    the data for that channel would automatically be available in `/opt/ml/input/data/foobar`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum things up, in order to train framework code on SageMaker, we only need
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `argparse` to read hyperparameters passed as command-line arguments. Chances
    are you're already doing this in your code anyway!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `SM_CHANNEL_xxx` environment variables and load data from there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `SM_MODEL_DIR` environment variable and save the trained model there.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's talk about deploying models trained in script mode.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general, your script needs to include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A function to load the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function to process input data before it's passed to the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function to process predictions before they're returned to the caller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount of actual work required depends on the framework and the input format
    you use. Let's see what this means for TensorFlow, PyTorch, MXNet, XGBoost, and
    scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The TensorFlow inference container relies on the **TensorFlow Serving** model
    server for model deployment ([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)).
    For this reason, your training code must save the model in this format. Model
    loading and prediction are available automatically.
  prefs: []
  type: TYPE_NORMAL
- en: JSON is the default input format for prediction, and it also works for `numpy`
    arrays thanks to automatic serialization. JSON Lines and CSV are also supported.
    For other formats, you can implement your own preprocessing and postprocessing
    functions, `input_handler()` and `output_handler()`. You'll find more information
    at [https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator](https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator).
  prefs: []
  type: TYPE_NORMAL
- en: You can also dive deeper into the TensorFlow inference container at [https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference](https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The PyTorch inference container relies on the `__call__()` method. If not, you
    should provide a `predict_fn()` function in the inference script.
  prefs: []
  type: TYPE_NORMAL
- en: For prediction, `numpy` is the default input format. JSON Lines and CSV are
    also supported. For other formats, you can implement your own preprocessing and
    postprocessing functions. You'll find more information at [https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model).
  prefs: []
  type: TYPE_NORMAL
- en: You can dive deeper into the PyTorch inference container at [https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference](https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying with Apache MXNet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Apache MXNet inference container relies on **Multi-Model Server** (**MMS**)
    for model deployment ([https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server)).
    It uses the default MXNet model format.
  prefs: []
  type: TYPE_NORMAL
- en: Models based on the `Module` API do not require a model loading function. For
    prediction, they support data in JSON, CSV, or `numpy` format.
  prefs: []
  type: TYPE_NORMAL
- en: Gluon models do require a model loading function as parameters need to be explicitly
    initialized. Data can be sent in JSON or `numpy` format.
  prefs: []
  type: TYPE_NORMAL
- en: For other data formats, you can implement your own preprocessing, prediction,
    and postprocessing functions. You can find more information at [https://sagemaker.readthedocs.io/en/stable/using_mxnet.html](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html).
  prefs: []
  type: TYPE_NORMAL
- en: You can dive deeper into the MXNet inference container at [https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker](https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying XGBoost and scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Likewise, XGBoost and scikit-learn rely on [https://github.com/aws/sagemaker-xgboost-container](https://github.com/aws/sagemaker-xgboost-container)
    and [https://github.com/aws/sagemaker-scikit-learn-container](https://github.com/aws/sagemaker-scikit-learn-container),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your script needs to provide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A `model_fn()` function to load the model. Just like for training, the location
    of the model to load is passed in the `SM_MODEL_DIR` environment variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two optional functions to deserialize and serialize prediction data, named `input_fn()`
    and `output_fn()`. These functions are only required if you need another input
    format other than JSON, CSV, or `numpy`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optional `predict_fn()` function passes deserialized data to the model and
    returns a prediction. This is only required if you need to preprocess data before
    predicting it, or to postprocess predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For XGBoost and scikit-learn, the `model_fn()` function is extremely simple
    and quite generic. Here are a couple of examples that should work in most cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: SageMaker also lets you import and export models. You can upload an existing
    model to S3 and deploy it directly on SageMaker. Likewise, you can copy a trained
    model from S3 and deploy it elsewhere. We'll look at this in detail in [*Chapter
    11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237), *Deploying Machine Learning
    Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's talk about training and deployment dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Managing dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, you'll need to add extra source files and libraries to the framework's
    containers. Let's see how we can easily do this.
  prefs: []
  type: TYPE_NORMAL
- en: Adding source files for training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By default, all estimators load the entry point script from the current directory.
    If you need additional source files for training, estimators let you pass a `source_dir`
    parameter, which points at the directory storing the extra files. Please note
    that the entry point script must be in the same directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, `myscript.py` and all additional source files must
    be placed in the `src` directory. SageMaker will automatically package the directory
    and copy it inside the training container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Adding libraries for training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use different techniques to add libraries that are required for training.
  prefs: []
  type: TYPE_NORMAL
- en: For libraries that can be installed with `pip`, the simplest technique is to
    add a `requirements.txt` file in the same folder as the entry point script. SageMaker
    will automatically install these libraries inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can use `pip` to install libraries directly in the training
    script by issuing a `pip install` command. We used this in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Natural Language Processing Models*, with LDA and NTM. This is useful
    when you don''t want to or cannot modify the SageMaker code that launches the
    training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For libraries that can't be installed with `pip`, you should use the `dependencies`
    parameter. It's available in all estimators, and it lets you list libraries to
    add to the training job. These libraries need to be present locally, in a virtual
    environment or a bespoke directory. SageMaker will package them and copy them
    inside the training container.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, `myscript.py` needs the `mylib` library. We install
    it in the `lib` local directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we pass its location to the estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The last technique is to install libraries in the Dockerfile for the container,
    rebuild the image, and push it to Amazon ECR. If you also need the libraries at
    prediction time (say, for preprocessing), this is the best option.
  prefs: []
  type: TYPE_NORMAL
- en: Adding libraries for deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you need specific libraries to be available at prediction time, you can use
    a `requirements.txt` file for libraries that can be installed with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For other libraries, the only option is to customize the framework container.
    You can pass its name to the estimator with the `image_uri` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We covered a lot of technical topics in this section. Now, let's look at the
    big picture.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The typical workflow when working with frameworks looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement script mode in your code; that is, read the necessary hyperparameters,
    input data, and output location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If required, add a `model_fn()` function to load the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test your training code locally, outside of any SageMaker container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the appropriate estimator (`XGBoost`, `TensorFlow`, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train in local mode using the estimator, with either the built-in container
    or a container you've customized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy in local mode and test your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch to a managed instance type (say, `ml.m5.large`) for training and deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This logical progression requires little work at each step. It minimizes friction,
    the risk of mistakes, and frustration. It also optimizes instance time and cost—no
    need to wait and pay for managed instances if your code crashes immediately because
    of a silly bug.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's put this knowledge to work. In the next section, we're going to run
    a simple scikit-learn example. The purpose is to make sure we understand the workflow
    we just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Running your framework code on Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start from a vanilla scikit-learn program that trains and saves a linear
    regression model on the Boston Housing dataset, which we used in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let's update it so that it runs on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing script mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will use the framework to implement script mode, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, read the hyperparameters as command-line arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the input and output paths as command-line arguments. We could decide
    to remove the splitting code and pass two input channels instead. Let''s stick
    to one channel, that is, `training`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we''re using scikit-learn, we need to add `model_fn()` to load the model
    at deployment time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, we're done. Time to test!
  prefs: []
  type: TYPE_NORMAL
- en: Testing locally
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we test our script on our local machine in a Python 3 environment, outside
    of any SageMaker container. We just need to make sure that we have `pandas` and
    scikit-learn installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the environment variables to empty values as we will pass the paths
    on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Nice. Our code runs fine with command-line arguments. We can use this for local
    development and debugging, until we're ready to move it to SageMaker local mode.
  prefs: []
  type: TYPE_NORMAL
- en: Using local mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll get started using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Still on our local machine, we configure an `SKLearn` estimator in local mode,
    setting the role according to the setup we''re using. Use local paths only:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As expected, we can see how our code is invoked in the training log. Of course,
    we get the same outcome:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We deploy locally and send some CSV samples for prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By printing the response, we will see the predicted values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With local mode, we can quickly iterate on our model. We're only limited by
    the compute and storage capabilities of the local machine. When that happens,
    we can easily move to managed infrastructure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using managed infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When it''s time to train at scale and deploy in production, all we have to
    do is make sure the input data is in S3 and replace the "local" instance type
    with an actual instance type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''re using the same container, we can be confident that training and
    deployment will work as expected. Again, I strongly recommend that you follow
    this logical progression: local work first, then SageMaker local mode, and finally,
    SageMaker managed infrastructure. It will help you focus on what needs to be done
    and when.'
  prefs: []
  type: TYPE_NORMAL
- en: For the remainder of this chapter, we're going to run additional examples.
  prefs: []
  type: TYPE_NORMAL
- en: Using the built-in frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've covered XGBoost and scikit-learn already. Now, it's time to see how we
    can use deep learning frameworks. Let's start with TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Working with TensorFlow and Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we're going to use TensorFlow 2.4.1 to train a simple convolutional
    neural network on the Fashion-MNIST dataset ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our code is split into two source files: one for the entry point script (`fmnist.py`)
    and one for the model (`model.py`, based on Keras layers). For the sake of brevity,
    I will only discuss the SageMaker steps. You can find the full code in the GitHub
    repository for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fmnist.py` starts by reading hyperparameters from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we read the environment variables, that is, the input paths for the training
    set and the validation set, the output path for the model, and the number of GPUs
    available on the instance. It''s the first time we''re using the latter. It comes
    in handy to adjust the batch size for multi-GPU training as it''s common practice
    to multiply the initial batch''s size by the number of GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the arguments in local variables. Then, load the dataset. Each channel
    provides us with a compressed `numpy` array for storing images and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, prepare the data for training by reshaping the image tensors, normalizing
    the pixel values, one-hot encoding the image labels, and creating the `tf.data.Dataset`
    objects that will feed data to the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the model, compile it, and fit it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once training is complete, save the model in TensorFlow Serving format at the
    appropriate output location. This step is important as this is the model server
    that SageMaker uses for TensorFlow models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train and deploy the model using the usual workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a notebook powered by a TensorFlow 2 kernel, we download the dataset and
    upload it to S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We configure the `TensorFlow` estimator. We also set the `source_dir` parameter
    so that our model''s file is also deployed in the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train and deploy as usual. We will go straight for managed infrastructure,
    but the same code will work fine on your local machine in local mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The validation accuracy should be 91-92%. By loading and displaying a few sample
    images from the validation dataset, we can predict their labels. The `numpy` payload
    is automatically serialized to JSON, which is the default format for prediction
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Viewing predicted classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_07_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.1 – Viewing predicted classes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When we''re done, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the combination of script mode and built-in containers makes
    it easy to run TensorFlow on SageMaker. Once you get into the routine, you'll
    be surprised at how fast you can move your models from your laptop to AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Working with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch is extremely popular for computer vision, NLP, and more.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we're going to train a **Graph Neural Network** (**GNN**).
    This category of networks works particularly well on graph-structured data, such
    as social networks, life sciences, and more. In fact, our PyTorch code will use
    the **Deep Graph Library** (**DGL**), an open source library that makes it easier
    to build and train GNNs with TensorFlow, PyTorch, and Apache MXNet ([https://www.dgl.ai/](https://www.dgl.ai/)).
    DGL is already installed in these containers, so let's get to work directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to work with the Zachary Karate Club dataset ([http://konect.cc/networks/ucidata-zachary/](http://konect.cc/networks/ucidata-zachary/)).
    The following is the graph for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – The Zachary Karate Club dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_07_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – The Zachary Karate Club dataset
  prefs: []
  type: TYPE_NORMAL
- en: Nodes 0 and 33 are teachers, while the other nodes are students. Edges represent
    ties between these people. As the story goes, the two teachers had an argument
    and the club needs to be split in two.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the training job is to find the "best" split. This can be defined
    as a semi-supervision classification task. The first teacher (node 0) is assigned
    class 0, while the second teacher (node 33) is assigned class 1\. All the other
    nodes are unlabeled, and their classes will be computed by a **graph convolutional
    network**. At the end of the last epoch, we'll retrieve the node classes and split
    the club accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is stored as a pickled Python list containing edges. Here are the
    first few edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The SageMaker code is as simple as it gets. We upload the dataset to S3, create
    a `PyTorch` estimator, and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This hardly needs any explaining at all, does it?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the abbreviated training script, where we''re using script
    mode once again. The full version is available in the GitHub repository for this
    book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following classes are predicted. Nodes 0 and 1 are class 0, node 2 is class
    1, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'By plotting them, we can see that the club has been cleanly split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Viewing predicted classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_07_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Viewing predicted classes
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the SageMaker code doesn't stand in your way. The workflow and APIs
    are consistent from one framework to the next, and you can focus on the machine
    learning problem itself. Now, let's do another example with Hugging Face, where
    we'll also see how to deploy a PyTorch model with the built-in PyTorch container.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Hugging Face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Hugging Face** ([https://huggingface.co](https://huggingface.co)) has quickly
    become the most popular collection of open source models for NLP. At the time
    of writing, they host almost 10,000 state-of-the-art models ([https://huggingface.co/models](https://huggingface.co/models)),
    pretrained on datasets ([https://huggingface.co/datasets](https://huggingface.co/datasets))
    in over 250 languages ([https://huggingface.co/languages](https://huggingface.co/languages)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easy to quickly build high-quality NLP applications, Hugging Face
    actively developed three open source libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers**: Train, fine-tune, and predict with Hugging Face models ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datasets**: Download and process Hugging Face datasets ([https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenizers**: Tokenize text for training and prediction with Hugging Face
    models ([https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face tutorial
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you are completely new to Hugging Face, please run through their tutorial
    first at [https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SageMaker added support for Hugging Face in March 2021, on both TensorFlow and
    PyTorch. As you would expect, you can use a `HuggingFace` estimator and built-in
    containers. Let's run an example where we build a sentiment analysis model for
    English language customer reviews. For this purpose, we'll fine-tune a **DistilBERT**
    model ([https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)) implemented
    with PyTorch and pretrained on two large English language datasets (Wikipedia
    and the BookCorpus dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we''ll use a Hugging Face dataset named `generated_reviews_enth`
    ([https://huggingface.co/datasets/generated_reviews_enth](https://huggingface.co/datasets/generated_reviews_enth)).
    It includes an English review, its Thai translation, a flag indicating whether
    the translation is correct or not, and a star rating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the format that the DistilBERT tokenizer expects: a `labels` variable
    (`0` for negative sentiment, `1` for positive) and a `text` variable with the
    English language review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get to work! I''ll show you the individual steps, and you''ll also find
    a **SageMaker Processing** version in the GitHub repository for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first install the `transformers` and `datasets` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We download the dataset, which is already split for training (141,369 instances)
    and validation (15,708 instances). All data is in JSON format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In each review, we create a new variable named `labels`. We set it to `1` when
    `review_star` is equal to or higher than 4, and to `0` otherwise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The reviews are nested JSON documents, making it difficult to remove variables
    we don''t need. Let''s flatten both datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now easily drop unwanted variables. We also rename the `translation.en`
    variable to `text`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The training and validation instances now have the format expected by the DistilBERT
    tokenizer. We already covered tokenization in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Natural Language Processing Models*. A significant difference is that
    we use a tokenizer that was pretrained on the same English language corpus as
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We download the tokenizer for our pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We tokenize both datasets. Words and punctuation are replaced with appropriate
    tokens. If needed, each sequence is padded or truncated to fit the input layer
    of the model (512 tokens):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We drop the `text` variable, as it''s not needed anymore:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Printing out an instance, we see the attention mask (all ones, meaning no token
    is masked in the input sequence), the inputs IDs (the sequence of tokens), and
    the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Data preparation is complete. Let's move on to training the model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a Hugging Face model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''re not going to train from scratch: it would talk far too long, and we
    probably don''t have enough data anyway. Instead, we''re going to fine-tune the
    model. Starting from a model trained on a very large text corpus, we will just
    train it for one additional epoch on our own data, so that it picks up the particular
    patterns present in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by uploading both datasets to S3\. The `datasets` library provides
    a convenient API to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define hyperparameters and configure a `HuggingFace` estimator. Note that
    we''ll fine-tune the model for just one epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the sake of brevity, I won''t discuss the training script (`train.py`),
    which is available in the GitHub repository for this book. There''s nothing particular
    about it: we use the `Trainer` Hugging Face API, as well as script mode to interface
    it with SageMaker. As we only train for a single epoch, checkpointing is disabled
    (`save_strategy=''no''`). This helps cuts down on training time (not saving checkpoints)
    and deployment time (the model artifact is smaller).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It's also worth noting that you can generate boilerplate code for your estimator
    on the Hugging Face website. As shown in the following screenshot, you can click
    on **Amazon SageMaker**, pick a task type, and copy and paste the generated code:![Figure
    7.4 – Viewing our model on the Hugging Face website
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_07_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.4 – Viewing our model on the Hugging Face website
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We launch the training job as usual, and it lasts about 42 minutes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Just like with other frameworks, we could call the `deploy()` API in order to
    deploy our model to a SageMaker endpoint. You can find an example at [https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/).
  prefs: []
  type: TYPE_NORMAL
- en: Instead, let's see how we can deploy our model with the built-in PyTorch container
    and **TorchServe**. In fact, this deployment example can generalize to any PyTorch
    model that you'd like to serve with TorchServe.
  prefs: []
  type: TYPE_NORMAL
- en: 'I find this superb blog post by my colleague Todd Escalona extremely helpful
    in understanding how to serve PyTorch models with TorchServe: [https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/](https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/).'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Hugging Face model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The only difference compared to previous examples is that we have to use the
    model artifact in S3 to create a `PyTorchModel` object, and to build a `Predictor`
    model that we can use `deploy()` and `predict()` on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from the model artifact, we define a `Predictor` object, and we create
    a `PyTorchModel` with it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Zooming in on the inference script (`torchserve-predictor.py`), we write a
    model loading function to account for Hugging Face peculiarities that the PyTorch
    container can''t handle by default:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also add a prediction function that returns a text label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The inference script also includes basic `input_fn()` and `output_fn()` functions
    to check that data is in JSON format. You'll find the code in the GitHub repository
    for the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Coming back to our notebook, we deploy the model as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the endpoint is up, we predict a text sample and print the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we delete the endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, it's really easy to work with Hugging Face models. It's also
    a cost-effective way to build high-quality NLP models, as we typically fine-tune
    them for a very small number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: To close this chapter, let's look at how SageMaker and Apache Spark can work
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the Python SageMaker SDK that we've been using so far, SageMaker
    also includes an SDK for Spark ([https://github.com/aws/sagemaker-spark](https://github.com/aws/sagemaker-spark)).
    This lets you run SageMaker jobs directly from a PySpark or Scala application
    running on a Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Spark and SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, you can decouple the **Extract-Transform-Load** (**ETL**) step and the
    machine learning step. Each usually has different infrastructure requirements
    (instance type, instance count, storage) that need to be the right size both technically
    and financially. Setting up your Spark cluster just right for ETL and using on-demand
    infrastructure in SageMaker for training and prediction is a powerful combination.
  prefs: []
  type: TYPE_NORMAL
- en: Second, although Spark's MLlib is an amazing library, you may need something
    else, such as custom algorithms in different languages or deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, deploying models for prediction on Spark clusters may not be the best
    option. SageMaker endpoints should be considered instead, especially since they
    support the **MLeap** format ([https://combust.github.io/mleap-docs/](https://combust.github.io/mleap-docs/)).
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we'll combine SageMaker and Spark to build a spam
    detection model. Data will be hosted in S3, with one text file for spam messages
    and one for non-spam ("ham") messages. We'll use Spark running on an Amazon EMR
    cluster to preprocess it. Then, we'll train and deploy a model with the XGBoost
    algorithm that's available in SageMaker. Finally, we'll predict data with it on
    our Spark cluster. For the sake of language diversity, we'll code with Scala this
    time.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, we need to build a Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Spark cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will create the cluster as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the `sagemaker-cluster`, click on **Next** again, and then click
    on **Create cluster**. You can find additional details at [https://docs.aws.amazon.com/emr/](https://docs.aws.amazon.com/emr/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the cluster is being created, we define our Git repository in the **Notebooks**
    entry in the left-hand side vertical menu. Then, we click on **Add repository**:![Figure
    7.6 – Adding a Git repository
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_07_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.6 – Adding a Git repository
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we create a Jupyter notebook connected to the cluster. Starting from the
    **Notebooks** entry in the left-hand side vertical menu, as shown in the following
    screenshot, we give it a name and select both the EMR cluster and the repository
    we just created. Then, we click on **Create notebook**:![Figure 7.7 – Creating
    a Jupyter notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_07_7.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.7 – Creating a Jupyter notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the cluster and the notebook are ready, we can click on **Open in Jupyter**,
    which takes us to the familiar Jupyter interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Everything is now ready. Let's write a spam classifier!
  prefs: []
  type: TYPE_NORMAL
- en: Building a spam classification model with Spark and SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we''re going to use the combined benefits of Spark and SageMaker
    to train, deploy, and predict with a spam classification model, thanks to just
    a few lines of Scala code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to make sure that our dataset is available in S3\. On our local
    machine, upload the two files to the default SageMaker bucket (feel free to use
    another bucket):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Back in the Jupyter notebook, make sure it's running the Spark kernel. Then,
    import the necessary objects from Spark MLlib and the SageMaker SDK.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the data from S3\. Convert all the sentences into lowercase. Then, remove
    all punctuation and numbers and trim any whitespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, split the messages into words and hash these words into 200 buckets.
    This technique is much less sophisticated than the word vectors we used in [*Chapter
    6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training Natural Language
    Processing Models*, but it should do the trick:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For example, the following message has one occurrence of a word from bucket
    15, one from bucket 83, two words from bucket 96, and two from bucket 188:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We assign a `1` label for spam messages and a `0` label for ham messages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Merge the messages and encode them in **LIBSVM** format, one of the formats
    supported by **XGBoost**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The samples now look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data for training and validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure the XGBoost estimator available in the SageMaker SDK. Here, we''re
    going to train and deploy in one single step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fire up a training job and a deployment job on the managed infrastructure,
    exactly like when we worked with built-in algorithms in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*. The SageMaker SDK automatically passes the
    Spark DataFrame to the training job, so no work is required from our end:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you would expect, these activities are visible in SageMaker Studio in the
    **Experiments** section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When the deployment is complete, transform the test set and score the model.
    This automatically invokes the SageMaker endpoint. Once again, we don''t need
    to worry about data movement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy should be around 97%, which is not too bad!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once done, delete all SageMaker resources created by the job. This will delete
    the model, the endpoint, and the endpoint configuration (an object we haven''t
    discussed yet):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Don't forget to terminate the notebook and the EMR cluster too. You can easily
    do this in the EMR console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example demonstrates how easy it is to combine the respective strengths
    of Spark and SageMaker. Another way to do this is to build MLlib pipelines with
    a mix of Spark and SageMaker stages. You'll find examples of this at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open source frameworks such as scikit-learn and TensorFlow have made it simple
    to write machine learning and deep learning code. They've become immensely popular
    in the developer community and for good reason. However, managing training and
    deployment infrastructure still requires a lot of effort and skills that data
    scientists and machine learning engineers typically do not possess. SageMaker
    simplifies the whole process. You can go quickly from experimentation to production,
    without ever worrying about infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned about the different frameworks available in SageMaker
    for machine learning and deep learning, as well as how to customize their containers.
    You also learned how to use script mode and local mode for fast iteration until
    you're ready to deploy in production. Finally, you ran several examples, including
    one that combines Apache Spark and SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to use your own custom code on SageMaker,
    without having to rely on a built-in container.
  prefs: []
  type: TYPE_NORMAL

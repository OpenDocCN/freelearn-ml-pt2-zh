<html><head></head><body>
		<div>
			<div id="_idContainer083" class="Content">
			</div>
		</div>
		<div id="_idContainer084" class="Content">
			<h1 id="_idParaDest-126"><a id="_idTextAnchor131"/>5. Supervised Learning – Key Steps</h1>
		</div>
		<div id="_idContainer099" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will dive deep into the concept of neural networks and describe the processes of forward and backpropagation. We will solve a supervised learning classification problem using a neural network and analyze the results of the neural network by performing error analysis.</p>
			<p class="callout">By the end of this chapter, you will be able to train a network to solve a classification problem and fine-tune some of the hyperparameters of the network to improve its performance.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor132"/>Introduction</h1>
			<p>In the preceding chapter, we explored three machine learning algorithms to solve supervised learning tasks, either for classification or regression. In this chapter, we will explore one of the most popular machine learning algorithms nowadays, artificial neural networks, which belong to a subgroup of machine learning called deep learning.</p>
			<p><strong class="bold">Artificial neural networks</strong> (<strong class="bold">ANNs</strong>), also known as <strong class="bold">Multilayer Perceptrons</strong> (<strong class="bold">MLPs</strong>), have become increasingly popular mostly because they present a complex algorithm that can approach almost any challenging data problem. Even though the theory was developed decades back, during the 1940s, such networks are becoming more popular now, thanks to all the improvements in technology that allow for the gathering of large amounts of data, as well as the developments in computer infrastructure that allow the training of complex algorithms with large amounts of data.</p>
			<p>Due to this, the following chapter will focus on introducing ANNs, their different types, and the advantages and disadvantages that they present. Additionally, an ANN will be used to predict the income of an individual based on demographic and financial information from the individual, as per the previous chapter, in order to present the differences in the performance of ANNs in comparison to the other supervised learning algorithms.</p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor133"/>Artificial Neural Networks</h1>
			<p>Although there are several machine learning algorithms available to solve data problems, as we have already stated, ANNs have become increasingly popular among data scientists, on account of their ability to find patterns in large and complex datasets that cannot be interpreted by humans.</p>
			<p>The <strong class="bold">neural</strong> part of the name refers to the resemblance of the architecture of the model to the anatomy of the human brain. This part is meant to replicate a human being's ability to learn from historical data by transferring bits of data from neuron to neuron until an outcome is reached.</p>
			<p>In the following diagram, a human neuron is displayed, where A represents the <strong class="bold">dendrites</strong> that receive input information from other neurons, B refers to the <strong class="bold">nucleus</strong> of the neuron that processes the information, and C represents the <strong class="bold">axon</strong> that oversees the process of passing the processed information to the next neuron:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B15781_05_01.jpg" alt="Figure 5.1: Visual representation of a human neuron&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1: Visual representation of a human neuron</p>
			<p>Moreover, the <strong class="bold">artificial</strong> part refers to the actual learning process of the model, where the main objective is to minimize the error in the model. This is an artificial learning process, considering that there is no real evidence regarding how human neurons process the information that they receive, and hence the model relies on mathematical functions that map an input to a desired output.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor134"/>How Do ANNs Work?</h2>
			<p>Before we dive into the process that is followed by an ANN, let's start by looking at its main components:</p>
			<ul>
				<li><strong class="bold">Input layer</strong>: This layer is also known as <strong class="source-inline">X</strong>, as it contains all the data from the dataset (each instance with its features).</li>
				<li><strong class="bold">Hidden layers</strong>: This layer is in charge of processing the input data in order to find patterns that are useful for making a prediction. The ANN can have as many hidden layers as desired, each with as many neurons (units) as required. The first layers are in charge of the simpler patterns, while the layers at the end search for the more complex ones.<p>The hidden layers use a set of variables that represent weights and biases in order to help train the network. The values for the weights and biases are used as the variables that change in each iteration to approximate the prediction to the ground truth. This will be explained later.</p></li>
				<li><strong class="bold">Output layer</strong>: Also known as <strong class="source-inline">Y_hat</strong>, this layer is the prediction made by the model, based on the data received from the hidden layers. This prediction is presented in the form of a probability, where the class label with a higher probability is the one selected as the prediction.</li>
			</ul>
			<p>The following diagram illustrates the architecture of the preceding three layers, where the circles under 1 denote the neurons in the input layer, the ones under 2 represent the neurons of 2 hidden layers (each layer represented by a column of circles), and finally, the circles under 3 are the neurons of the output layer: </p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B15781_05_02.jpg" alt="Figure 5.2: Basic architecture of an ANN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2: Basic architecture of an ANN</p>
			<p>As an analogy, consider a manufacturing process for building car parts. Here, the input layer consists of the raw materials, which, in this case, may be aluminum. The initial steps of the process involve polishing and cleaning the material, which can be seen as the first couple of hidden layers. Next, the material is bent to achieve the shape of the car part, which is handled by the deeper hidden layers. Finally, the part is delivered to the client, which can be considered to be the output layer.</p>
			<p>Considering these steps, the main objective of the manufacturing process is to achieve a final part that highly resembles the part that the process aimed to build, meaning that the output, <strong class="source-inline">Y_hat</strong>, should maximize its similarity to <strong class="source-inline">Y</strong> (the ground truth) for a model to be considered a good fit to the data.</p>
			<p>The actual methodology to train an ANN is an iterative process comprised of the following steps: forward propagation, calculation of the cost function, backpropagation, and weights and biases updates. Once the weights and biases are updated, the process starts again until the number of iterations is met. </p>
			<p>Let's explore each of the steps of the iteration process in detail.</p>
			<h3 id="_idParaDest-130"><a id="_idTextAnchor135"/>Forward Propagation</h3>
			<p>The input layer feeds the initial information to the ANN. The processing of the data is done by propagating data bits through the depth (number of hidden layers) and width (number of units in each layer) of the network. The information is processed by each neuron in each layer using a linear function, coupled with an activation function that aims to break the linearity, as follows:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B15781_05_03.jpg" alt="Figure 5.3: The linear and activation functions used by an ANN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3: The linear and activation functions used by an ANN</p>
			<p>Here, <em class="italic">W</em><span class="subscript">1</span> and <em class="italic">b</em><span class="subscript">1</span> are a matrix and a vector containing the weights and biases, respectively, and serve as the variables that can be updated through the iterations to train the model. <em class="italic">Z</em><span class="subscript">1</span> is the linear function for a given neuron, and <em class="italic">A</em><span class="subscript">1</span> is the outcome from the unit after applying an activation function (represented by the sigma symbol) to the linear one.</p>
			<p>The preceding two formulas are calculated for each neuron in each layer, where the value of <em class="italic">X</em> for the hidden layers (other than the input layer) is replaced by the output of the previous layer (<em class="italic">A</em><span class="subscript">n</span>), as follows:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B15781_05_04.jpg" alt="Figure 5.4: The values calculated for the second layer of the ANN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4: The values calculated for the second layer of the ANN</p>
			<p>Finally, the output from the last hidden layer is fed to the output layer, where the linear function is once again calculated, along with an activation function. The outcome from this layer, after some processing as required, is the one that will be compared against the ground truth in order to evaluate the performance of the algorithm before moving on to the next iteration.</p>
			<p>The values of the weights for the first iteration are randomly initialized between 0 and 1, while the values for the biases can be set to 0 initially. Once the first iteration is run, the values will be updated, so that the process can start again.</p>
			<p>The activation function can be of different types. Some of the most common ones are the <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>), the <strong class="bold">Hyperbolic tangent</strong> (<strong class="bold">tanh</strong>), and the <strong class="bold">Sigmoid</strong> and <strong class="bold">Softmax</strong> functions, which will be explained in a subsequent section.</p>
			<h3 id="_idParaDest-131"><a id="_idTextAnchor136"/>Cost Function</h3>
			<p>Considering that the final objective of the training process is to build a model based on a given set of data that maps an expected output, it is particularly important to measure the model's ability to estimate a relation between <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> by comparing the differences between the predicted value (<strong class="source-inline">Y_hat</strong>) and the ground truth (<strong class="source-inline">Y</strong>). This is accomplished by calculating the cost function (also known as the <strong class="bold">loss function</strong>) to determine how poor the model's predictions are. The cost function is calculated for each iteration to measure the progress of the model along the iteration process, with the objective of finding the values for the weights and biases that minimize the cost function. </p>
			<p>For classification tasks, the cost function most commonly used is the <strong class="bold">cross-entropy cost function</strong>, where the higher the value of the cost function, the greater the divergence between the predicted and actual values. </p>
			<p>For a binary classification task, that is, tasks with only two class output labels, the cross-entropy cost function is calculated as follows:</p>
			<p class="source-code">cost = -(y * log(y<span class="subscript">hat</span>) + (1-y) *(1-y<span class="subscript">hat</span>))</p>
			<p>Here, <em class="italic">y</em> would be either 1 or 0 (either of the two class labels), <em class="italic">y</em><span class="subscript">hat</span> would be the probability calculated by the model, and <em class="italic">log</em> would be the natural logarithm.</p>
			<p>For a multiclass classification task, the formula is as follows:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B15781_05_05.jpg" alt="Figure 5.5: The cost function for a multiclass classification task&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5: The cost function for a multiclass classification task</p>
			<p>Here, <em class="italic">c</em> represents a class label and <em class="italic">M</em> refers to the total number of class labels.</p>
			<p>Once the cost function is calculated, the training process proceeds to perform the backpropagation step, which will be explained in the following section.</p>
			<p>Moreover, for regression tasks, the cost function would be the RMSE, which was explained in <em class="italic">Chapter 3</em>, <em class="italic">Supervised Learning – Key Steps</em>.</p>
			<h3 id="_idParaDest-132"><a id="_idTextAnchor137"/>Backpropagation</h3>
			<p>The backpropagation procedure was introduced as part of the training process of ANNs to make learning faster. It basically involves calculating the partial derivatives of the cost function with respect to the weights and biases along the network. The objective of this is to minimize the cost function by changing the weights and the biases.</p>
			<p>Considering that the weights and biases are not directly contained in the cost function, a chain rule is used to propagate the error from the cost function backward until it reaches the first layers of the network. Next, a weighted average of the derivatives is calculated, which is used as the value to update the weights and biases before running a new iteration. </p>
			<p>There are several algorithms that can be used to perform backpropagation, but the most common one is <strong class="bold">gradient descent</strong>. Gradient descent is an optimization algorithm that tries to find some local or global minimum of a function, which, in this case, is the cost function. It does so by determining the direction in which the model should move to reduce the error. </p>
			<p>For instance, the following diagram displays an example of the training process of an ANN through the different iterations, where the job of backpropagation is to determine the direction in which the weights and biases should be updated, so that the error can continue to be minimized until it reaches a minimum point:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B15781_05_06.jpg" alt="Figure 5.6: Example of the iterative process of training an ANN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6: Example of the iterative process of training an ANN</p>
			<p>It is important to highlight that backpropagation does not always find the global minima, since it stops updating once it has reached the lowest point in a slope, regardless of any other regions. For instance, consider the following diagram:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B15781_05_07.jpg" alt="Figure 5.7: Examples of minimum points&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7: Examples of minimum points</p>
			<p>Although all three points can be considered minimum points when compared to the points to their left and right, only one of them is the global minima.</p>
			<h3 id="_idParaDest-133"><a id="_idTextAnchor138"/>Updating the Weights and Biases</h3>
			<p>Taking the derivatives' average that was calculated during backpropagation, the final step of an iteration is to update the values of the weights and biases. This process is done using the following formula for updating weights and biases:</p>
			<p class="source-code">New weight = old weight – derivative rate * learning rate</p>
			<p class="source-code">New bias = old bias – derivative rate * learning rate</p>
			<p>Here, the old values are those used to perform the forward propagation step, the derivative rate is the value obtained from the backpropagation step, which is different for the weights and the biases, and the learning rate is a constant that is used to neutralize the effect of the derivative rate, so that the changes in the weights and biases are small and smooth. This has been proven to help reach the lowest point more quickly.</p>
			<p>Once the weights and the biases have been updated, the entire process starts again.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor139"/>Understanding the Hyperparameters</h2>
			<p>Hyperparameters, as you have seen so far, are parameters that can be fine-tuned to improve the accuracy of a model. For neural networks, hyperparameters can be classified into two main groups: </p>
			<ul>
				<li>Those that alter the structure of the network</li>
				<li>Those that modify the process to train it</li>
			</ul>
			<p>An important part of building an ANN is the process of fine-tuning the hyperparameters by performing error analysis and by playing around with the hyperparameters that help to solve the condition that is affecting the network. As a general reminder, networks suffering from high bias can usually be improved by creating bigger networks or training for longer durations of time (that is, more iterations), whereas networks suffering from high variance can benefit from the addition of more training data or by introducing a regularization technique, which will be explained in a subsequent section.</p>
			<p>Considering that the number of hyperparameters that can be changed for training an ANN is large, the most commonly used ones will be explained in the following sections.</p>
			<h3 id="_idParaDest-135"><a id="_idTextAnchor140"/>Number of Hidden Layers and Units</h3>
			<p>The number of hidden layers and the number of units in each layer can be set by the researcher, as mentioned previously. Again, there is no exact science to select this number, and, on the contrary, the selection of this number is part of the fine-tuning process to test different approximations.</p>
			<p>Nonetheless, when selecting the number of hidden layers, some data scientists lean toward an approach wherein multiple networks are trained, each with an extra layer. The model with the lowest error is the one with the correct number of hidden layers. Unfortunately, this approach does not always work well, as more complex data problems do not really show a difference in performance through simply changing the number of hidden layers, regardless of the other hyperparameters. </p>
			<p>On the other hand, there are several techniques that you can use to choose the number of units in a hidden layer. It is common for data scientists to choose the initial values for both of these hyperparameters based on similar research papers that are available online. This means that a good starting point would be copying the architecture of networks that have been successfully used for projects in a similar field, and then, through error analysis, fine-tuning the hyperparameters to improve performance.</p>
			<p>Nonetheless, it is important to consider the fact that based on research activity, deeper networks (networks with many hidden layers) outperform wider networks (networks with many units in each layer).</p>
			<h3 id="_idParaDest-136"><a id="_idTextAnchor141"/>Activation Functions</h3>
			<p>As mentioned previously, the activation function is used to introduce non-linearity to the model. The most commonly used activation functions are the following:</p>
			<ul>
				<li><strong class="bold">ReLU</strong>: The output of this function is either 0 or the number derived from the linear function, whichever is higher. This means that the output will be the raw number it receives whenever this number is above 0, otherwise, the output would be 0.</li>
				<li><strong class="bold">Tanh</strong>: This function consists of the division of the hyperbolic sine by the hyperbolic cosine of the input. The output is a number between -1 and 1.</li>
				<li><strong class="bold">Sigmoid</strong>: The function has an S-shape. It takes the input and converts it into a probability. The output from this function is between 0 and 1.</li>
				<li><strong class="bold">Softmax</strong>: Similar to the sigmoid function, this calculates the probability of the input, with the difference being that the Softmax function can be used for multiclass classification tasks as it is capable of calculating the probability of a class label in reference to the others.</li>
			</ul>
			<p>The selection of an activation function should be done by considering that, conventionally, both the ReLU and the Hyperbolic tangent (tanh) activation functions are used for all of the hidden layers, with ReLU being the most popular one among scientists due to its performance in relation to the majority of data problems.</p>
			<p>Moreover, the Sigmoid and the Softmax activation functions should be used for the output layer, as their outcome is in the form of a probability. The Sigmoid activation function is used for binary classification problems, as it only outputs the probability for two class labels, whereas the Softmax activation function can be used for either binary or multiclass classification problems.</p>
			<h3 id="_idParaDest-137"><a id="_idTextAnchor142"/>Regularization</h3>
			<p>Regularization is a technique used in machine learning to improve a model that is suffering from overfitting, which means that this hyperparameter is mostly used when it is strictly required, and its main objective is to increase the generalization ability of the model.</p>
			<p>There are different regularization techniques, but the most common ones are the L1, L2, and dropout techniques. Although scikit-learn only supports L2 for its MLP classifier, brief explanations of the three forms of regularization are as follows:</p>
			<ul>
				<li>The L1 and L2 techniques add a regularization term to the cost function as a way of penalizing high weights that may be affecting the performance of the model. The main difference between these approaches is that the regularization term for L1 is the absolute value of the magnitude of the weights, while for L2, it is the squared magnitude of the weights. For regular data problems, L2 has proven to work better, while L1 is mainly popular for feature extraction tasks since it creates sparse models.</li>
				<li>Dropout, on the other hand, refers to the model's ability to drop out some units in order to ignore their output during a step in the iteration, which simplifies the neural network. The dropout value is set between 0 and 1, and it represents the percentage of units that will be ignored. The units that are ignored are different in each iteration step.</li>
			</ul>
			<h3 id="_idParaDest-138"><a id="_idTextAnchor143"/>Batch Size</h3>
			<p>Another hyperparameter to be tuned during the construction of an ANN is the batch size. This refers to the number of instances to be fed to the neural network during an iteration, which will be used to perform a forward and a backward pass through the network. For the next iteration, a new set of instances will be used.</p>
			<p>This technique also helps to improve the model's ability to generalize to the training data because, in each iteration, it is fed with new combinations of instances, which is useful when dealing with an overfitted model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">As per the result of many years of research, a good practice is to set the batch size to a value that is a multiple of 2. Some of the most common values are 32, 64, 128, and 256.</p>
			<h3 id="_idParaDest-139"><a id="_idTextAnchor144"/>Learning Rate</h3>
			<p>The learning rate, as explained previously, is introduced to help determine the size of the steps that the model will take to get to the local or global minima in each iteration. The lower the learning rate, the slower the learning process of the network, but this results in better models. On the other hand, the larger the learning rate, the faster the learning process of the model; however, this may result in a model not converging.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The default learning rate value is usually set to 0.001.</p>
			<h3 id="_idParaDest-140"><a id="_idTextAnchor145"/>Number of Iterations</h3>
			<p>A neural network is trained through an iterative process, as mentioned previously. Therefore, it is necessary to set the number of iterations that the model will perform. The best way to set up the ideal number of iterations is to start low, between 200 and 500, and increase it, in the event that the plot of the cost function over each iteration shows a decreasing line. Needless to say, the larger the number of iterations, the longer it takes to train a model.</p>
			<p>Additionally, increasing the number of iterations is a technique known to address underfitted networks. This is because it gives the network more time to find the right weights and biases that generalize to the training data.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor146"/>Applications of Neural Networks</h2>
			<p>In addition to the preceding architecture, a number of new architectures have emerged over time, thanks to the popularity of neural networks. Some of the most popular ones are <strong class="bold">convolutional neural networks</strong>, which can handle the processing of images by using filters as layers, and <strong class="bold">recurrent neural networks</strong>, which are used to process sequences of data such as text translations. </p>
			<p>On account of this, the applications of neural networks extend to almost any data problem, ranging from simple to complex. While a neural network is capable of finding patterns in really large datasets (either for classification or regression tasks), they are also known for effectively handling challenging problems, such as the autonomous abilities of self-driving cars, the construction of chatbots, and the recognition of faces.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor147"/>Limitations of Neural Networks</h2>
			<p>Some of the limitations of training neural networks are as follows:</p>
			<ul>
				<li>The training process takes time. Regardless of the hyperparameters used, they generally take time to converge.</li>
				<li>They need very large datasets in order to work better. Neural networks are meant for larger datasets, as their main advantage is their ability to find patterns within millions of values.</li>
				<li>They are considered a black box as there is no actual knowledge of how the network arrives at a result. Although the math behind the training process is clear, it is not possible to know what assumptions the model makes while being trained.</li>
				<li>The hardware requirements are large. Again, the greater the complexity of the problem, the larger the hardware requirements.</li>
			</ul>
			<p>Although ANNs can be applied to almost any data problem, due to their limitations, it is always a good practice to test other algorithms when dealing with simpler data problems. This is important because applying neural networks to data problems that can be solved by simpler models makes the costs outweigh the benefits.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor148"/>Applying an Artificial Neural Network</h1>
			<p>Now that you know the components of an ANN, as well as the different steps that it follows to train a model and make predictions, let's train a simple network using the scikit-learn library.</p>
			<p>In this topic, scikit-learn's neural network module will be used to train a network using the datasets used in the previous chapter's exercises and activities (that is, the Fertility Dataset and the Processed Census Income Dataset). It is important to mention that scikit-learn is not the most appropriate library for neural networks, as it does not currently support many types of neural networks, and its performance over deeper networks is not as good as other neural network specialized libraries, such as TensorFlow and PyTorch.</p>
			<p>The neural network module in scikit-learn currently supports an MLP for classification, an MLP for regression, and a Restricted Boltzmann Machine architecture. Considering that the case study consists of a classification task, the MLP for classifications will be used.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor149"/>Scikit-Learn's Multilayer Perceptron</h2>
			<p>An MLP is a supervised learning algorithm that, as the name indicates, uses multiple layers (hidden layers) to learn a non-linear function that translates the input values into output, either for classification or regression. As we explained previously, the job of each unit of a layer is to transform the data received from the previous layer by calculating a linear function and then applying an activation function to break the linearity.</p>
			<p>It is important to mention the fact that an MLP has a non-convex loss function that, as mentioned previously, signifies that there may be multiple local minima. This means that different initializations of the weights and biases will result in different trained models, which, in turn, indicates different accuracy levels.</p>
			<p>The MLP classifier in scikit-learn has around 20 different hyperparameters associated with the architecture or the learning process, which can be altered in order to modify the training process of the network. Fortunately, all of these hyperparameters have set default values, which allows us to run an initial model without much effort. The results from this model can then be used to tune the hyperparameters as required.</p>
			<p>To train an MLP classifier, it is required that you input two arrays: first, the <strong class="source-inline">X</strong> input of dimensions (<strong class="source-inline">n_samples</strong>, <strong class="source-inline">n_features</strong>) containing the training data, and then the <strong class="source-inline">Y</strong> input of dimensions (<strong class="source-inline">n_sample</strong>) that contains the label values for each sample.</p>
			<p>Similar to the algorithms that we looked at in the previous chapter, the model is trained using the <strong class="source-inline">fit</strong> method, and then predictions can be obtained by using the <strong class="source-inline">predict</strong> method on the trained model.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor150"/>Exercise 5.01: Applying the MLP Classifier Class</h2>
			<p>In this exercise, you will train a model using scikit-learn's MLP to solve a classification task that consists of determining whether the fertility of the subjects has been affected by their demographics, their environmental conditions, and their previous medical conditions.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For the exercises and activities within this chapter, you will need to have Python 3.7, NumPy, Jupyter, pandas, and scikit-learn installed on your system.</p>
			<ol>
				<li>Open a Jupyter Notebook to implement this exercise. Import all the necessary elements to read the dataset and to calculate a model's accuracy, as well as scikit-learn's <strong class="source-inline">MLPClassifier</strong> class:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.neural_network import MLPClassifier</p><p class="source-code">from sklearn.metrics import accuracy_score</p></li>
				<li>Using the Fertility Dataset from the previous chapter, read the <strong class="source-inline">.csv</strong> file. Make sure that you add the <strong class="source-inline">header</strong> argument equal to <strong class="source-inline">None</strong> to the <strong class="source-inline">read_csv</strong> function, considering that the dataset does not contain a header row:<p class="source-code">data = pd.read_csv("fertility_Diagnosis.csv", header=None)</p></li>
				<li>Split the dataset into <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> sets in order to separate the features data from the label values:<p class="source-code">X = data.iloc[:,:9]</p><p class="source-code">Y = data.iloc[:,9]</p></li>
				<li>Instantiate the <strong class="source-inline">MLPClassifier</strong> class from the <strong class="source-inline">neural_network</strong> module of scikit-learn and use the <strong class="source-inline">fit</strong> method to train a model. When instantiating the model, leave all the hyperparameters at their default values, but add a <strong class="source-inline">random_state</strong> argument equal to <strong class="source-inline">101</strong> to ensure that you get the same results as the one shown in this exercise:<p class="source-code">model = MLPClassifier(random_state=101)</p><p class="source-code">model = model.fit(X, Y)</p><p>Address the warning that appears after running the <strong class="source-inline">fit</strong> method:</p><div id="_idContainer092" class="IMG---Figure"><img src="image/B15781_05_08.jpg" alt="Figure 5.8: Warning message displayed after running the fit method&#13;&#10;"/></div><p class="figure-caption">Figure 5.8: Warning message displayed after running the fit method</p><p>As you can see, the warning specifies that after running the default number of iterations, which is <strong class="source-inline">200</strong>, the model has not reached convergence. </p></li>
				<li>To address this issue, try higher values for the iterations until the warning stops appearing. To change the number of iterations, add the <strong class="source-inline">max_iter</strong> argument inside the parentheses during the instantiation of the model:<p class="source-code">model = MLPClassifier(random_state=101, max_iter =1200)</p><p class="source-code">model = model.fit(X, Y)</p><p>Furthermore, the output beneath the warning explains the values used for all of the hyperparameters of the MLP.</p></li>
				<li>Finally, perform a prediction by using the model that you trained previously, for a new instance with the following values for each feature: <strong class="source-inline">−0.33</strong>, <strong class="source-inline">0.69</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.8</strong>, <strong class="source-inline">0</strong>, <strong class="source-inline">0.88</strong>.<p>Use the following code:</p><p class="source-code">pred = model.predict([[-0.33,0.69,0,1,1,0,0.8,0,0.88]])</p><p class="source-code">print(pred)</p><p>The model's prediction is equal to <strong class="source-inline">N</strong>, that is, the model predicts the person with the specified features to have a normal diagnosis.</p></li>
				<li>Calculate the accuracy of your model, based on the predictions it achieves over the <strong class="source-inline">X</strong> variable, as follows:<p class="source-code">pred = model.predict(X)</p><p class="source-code">score = accuracy_score(Y, pred)</p><p class="source-code">print(score)</p><p>The accuracy of your model is equal to <strong class="source-inline">98%</strong>.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2BaKHRe">https://packt.live/2BaKHRe</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/37tTxpv">https://packt.live/37tTxpv</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>You have successfully trained and evaluated the performance of an MLP model.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor151"/>Activity 5.01: Training an MLP for Our Census Income Dataset</h2>
			<p>With the objective of comparing the performance of the algorithms trained in the previous chapter with the performance of a neural network, for this activity, we will continue to work with the Preprocessed Census Income Dataset. Consider the following scenario: your company is continually offering a course for employees to improve their abilities, and you have recently learned about neural networks and their power. You have decided to build a network to model the dataset that you were given previously in order to test whether a neural network is better at predicting a person's income based on their demographic data. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Start this activity using the preprocessed dataset from the previous chapter: <strong class="source-inline">census_income_dataset_preprocessed.csv</strong>. You can also find the preprocessed dataset on this book's GitHub repository at <a href="https://packt.live/2UQIthA">https://packt.live/2UQIthA</a>.</p>
			<p>Perform the following steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the elements required to load and split a dataset, train an MLP, and to measure accuracy.</li>
				<li>Using the preprocessed Census Income Dataset, separate the features from the target, creating the variables <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong>.</li>
				<li>Divide the dataset into training, validation, and testing sets, using a split ratio of 10%.<p class="callout-heading">Note</p><p class="callout">Remember to continue using a <strong class="source-inline">random_state</strong> argument equal to <strong class="source-inline">101</strong> when performing the dataset split in order to set a seed to arrive at the same results as the ones in this book.</p></li>
				<li>Instantiate the <strong class="source-inline">MLPClassifier</strong> class from scikit-learn and train the model with the training data. <p>Leave all the hyperparameters at their default values. Again, use a <strong class="source-inline">random_state</strong> equal to 101.</p><p>Although a warning will appear specifying that, with the given iterations, no convergence was reached, leave the warning unaddressed, since hyperparameter fine-tuning will be explored in the following sections of this chapter.</p></li>
				<li>Calculate the accuracy of the model for all three sets (training, validation, and testing).<p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found on page 240.</p><p>The accuracy score for the three sets should be as follows:</p><p>Train sets = 0.8465</p><p>Dev sets = 0.8246</p><p>Test sets = 0.8415</p></li>
			</ol>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor152"/>Performance Analysis</h1>
			<p>In the following section, we will first perform error analysis using the accuracy metric as a tool to determine the condition that is affecting (in greater proportion) the performance of the algorithm. Once the model is diagnosed, the hyperparameters can be tuned to improve the overall performance of the algorithm. The final model will be compared to those that were created during the previous chapter in order to determine whether a neural network outperforms the other models. </p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor153"/>Error Analysis</h2>
			<p>Using the accuracy score calculated in <em class="italic">Activity 5.01</em>, <em class="italic">Training an MLP for Our Census Income Dataset</em>, we can calculate the error rates for each of the sets and compare them against one another to diagnose the condition that is affecting the model. To do so, a Bayes error equal to 1% will be assumed, considering that other models in the previous chapter were able to achieve an accuracy level of over 97%:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B15781_05_09.jpg" alt="Figure 5.9: Accuracy score and error rate of the network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9: Accuracy score and error rate of the network</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Considering <em class="italic">Figure 5.9</em>, remember that in order to detect the condition that is affecting the network, it is necessary to take an error rate and, from that, subtract the value of the error rate above it. The biggest positive difference is the one that we use to diagnose the model.</p>
			<p>According to the column of differences, it is evident that the biggest difference is found between the error rate in the training set and the Bayes error. Based on this, it is possible to conclude that the model is suffering from <em class="italic">high bias</em>, which, as explained in previous chapters, can be handled by training a bigger network and/or training for longer periods of time (a higher number of iterations).</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor154"/>Hyperparameter Fine-Tuning</h2>
			<p>Through error analysis, it was possible to determine that the network is suffering from high bias. This is highly important as it indicates the actions that need to be taken in order to improve the performance of the model in greater proportion. </p>
			<p>Considering that both the number of iterations and the size of the network (number of layers and units) should be changed using a trial-and-error approach, the following experiments will be performed:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B15781_05_10.jpg" alt="Figure 5.10: Suggested experiments to tune the hyperparameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10: Suggested experiments to tune the hyperparameters</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Some experiments may take longer to run due to their complexity. For instance, Experiment 3 will take longer than Experiment 2.</p>
			<p>The idea behind these experiments is to be able to test different values for the different hyperparameters in order to find out whether an improvement can be achieved. If the improvements achieved through these experiments are significant, further experiments should be considered.</p>
			<p>Similar to adding the <strong class="source-inline">random_state</strong> argument to the initialization of the MLP, the change in the values of the number of iterations and the size of the network can be achieved using the following code, which shows the values for Experiment 3:</p>
			<p class="source-code">from sklearn.neural_network import MLPClassifier</p>
			<p class="source-code">model = MLPClassifier(random_state=101, max_iter = 500, \</p>
			<p class="source-code">                      hidden_layer_sizes=(100,100,100))</p>
			<p class="source-code">model = model.fit(X_train, Y_train)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To find what term to use in order to change each hyperparameter, visit scikit-learn's <strong class="source-inline">MLPClassifier</strong> page at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</a>.</p>
			<p>As you can see in the preceding snippet, the <strong class="source-inline">max_iter</strong> argument is used to set the number of iterations to run during the training of the network. The <strong class="source-inline">hidden_layer_sizes</strong> argument is used to both set the number of hidden layers and set the number of units in each. For instance, in the preceding example, by setting the argument to <strong class="source-inline">(100,100,100)</strong>, the architecture of the network is of 3 hidden layers, each with 100 units. Of course, this architecture also includes the required input and output layers.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Using the example to train a network with the configurations of Experiment 3, you are encouraged to try to execute the training process for the configurations of Experiment 1 and 2.</p>
			<p>The accuracy scores from running the preceding experiments can be seen in the following table:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B15781_05_11.jpg" alt="Figure 5.11: Accuracy scores for all experiments&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11: Accuracy scores for all experiments</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Keep in mind that the main purpose behind tuning the hyperparameters is to decrease the difference between the error rate of the training set and the Bayes error, which is why most of the analysis is done by considering only this value.</p>
			<p>Through an analysis of the accuracy scores of the experiments, it can be concluded that the best configuration of hyperparameters is the one used during Experiment 2. Additionally, it is possible to conclude that there is most likely no point in trying other values for the number of iterations, considering that increasing the number of iterations did not have a positive effect on the performance of the algorithm.</p>
			<p>Nonetheless, in order to test the width of the hidden layers, the following experiments will be considered, using the selected values for the number of iterations and the number of hidden layers of Experiment 2, but varying the number of units in each layer:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B15781_05_12.jpg" alt="Figure 5.12: Suggested experiments to vary the width of the network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12: Suggested experiments to vary the width of the network</p>
			<p>The accuracy score of the two experiments is shown, followed by an explanation of the logic behind them:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B15781_05_13.jpg" alt="Figure 5.13: Accuracy scores for the second round of experiments&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13: Accuracy scores for the second round of experiments</p>
			<p>It can be seen that the accuracy for both experiments decreases for all sets of data, in comparison to the initial model. By observing these values, it can be concluded that the performance of Experiment 2 is the highest in terms of testing sets, which leaves us with a network that iterates for 500 steps, with one input and output layer and two hidden layers with 100 units each.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">There is no ideal way to test the different configurations of hyperparameters. The only important thing to consider is that the focus is centered on those hyperparameters that solve the condition that is affecting the network in a greater proportion. Feel free to try more experiments if you wish.</p>
			<p>Considering the accuracy scores of all three sets of Experiment 2 to calculate the error rate, the biggest difference is still between the training set error and the Bayes error. This means that the model may not be the best fit for the dataset, considering that the training set error could not be brought closer to the minimum possible error margin.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3e2O8bS">https://packt.live/3e2O8bS</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor155"/>Model Comparison</h2>
			<p>When more than one model has been trained, the final step related to the process of creating a model is a comparison between the models in order to choose the one that best represents the training data in a generalized way, so that it works well over unseen data.</p>
			<p>The comparison, as mentioned previously, must be done by using only the metric that was selected to measure the performance of the models for the data problem. This is important, considering that one model can perform very differently for each metric, so the model that maximizes the performance with the ideal metric should be selected.</p>
			<p>Although the metric is calculated on all three sets of data (training, validation, and testing) in order to be able to perform error analysis, for most cases, comparison and selection should be done by prioritizing the results obtained with the testing set. This is mainly due to the purpose of the sets, considering that the training set is used to create the model, the validation set is used to fine-tune the hyperparameters, and finally, the testing set is used to measure the overall performance of the model on unseen data.</p>
			<p>Taking this into account, the model with a superior performance on the testing set, after having improved all models to their fullest potential, will be the one that performs best on unseen data.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor156"/>Activity 5.02: Comparing Different Models to Choose the Best Fit for the Census Income Data Problem</h2>
			<p>Consider the following scenario: after training four different models with the available data, you have been asked to perform an analysis to choose the model that best suits the case study. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The following activity is mainly analytical. Use the results obtained from the activities in the previous chapter, as well as the activity in the current chapter.</p>
			<p>Perform the following steps to compare the different models:</p>
			<ol>
				<li value="1">Open the Jupyter Notebooks that you used to train the models.</li>
				<li>Compare the four models, based only on their accuracy scores. Fill in the details in the following table:<div id="_idContainer098" class="IMG---Figure"><img src="image/B15781_05_14.jpg" alt="Figure 5.14: Accuracy scores of all four models for the Census Income Dataset&#13;&#10;"/></div><p class="figure-caption">Figure 5.14: Accuracy scores of all four models for the Census Income Dataset</p></li>
				<li>On the basis of the accuracy scores, identify the model with the best performance.<p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found on page 242.</p></li>
			</ol>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor157"/>Summary</h1>
			<p>This chapter mainly focused on ANNs (the MLP, in particular), which have become increasingly important in the field of machine learning due to their ability to tackle highly complex data problems that usually use extremely large datasets with patterns that are impossible to see with the human eye.</p>
			<p>The main objective is to emulate the architecture of the human brain by using mathematical functions to process data. The process that is used to train an ANN consists of a forward propagation step, the calculation of a cost function, a backpropagation step, and the updating of the different weights and biases that help to map the input values to an output.</p>
			<p>In addition to the variables of the weights and biases, ANNs have multiple hyperparameters that can be tuned to improve the performance of the network, which can be done by modifying the architecture or training process of the algorithm. Some of the most popular hyperparameters are the size of the network (in terms of hidden layers and units), the number of iterations, the regularization term, the batch size, and the learning rate.</p>
			<p>Once these concepts were covered, we created a simple network to tackle the Census Income Dataset problem that was introduced in the previous chapter. Next, by performing error analysis, we fine-tuned some of the hyperparameters of the network to improve its performance. </p>
			<p>In the next chapter, we will learn how to develop an end-to-end machine learning solution, starting from the understanding of the data and training of the model, as seen thus far, and ending with the process of saving a trained model in order to be able to make future use of it.</p>
		</div>
	</body></html>
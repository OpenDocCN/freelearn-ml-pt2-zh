<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Lazy Learning &#x2013; Classification Using Nearest Neighbors"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Lazy Learning – Classification Using Nearest Neighbors</h1></div></div></div><p>An interesting new type of dining experience has been appearing in cities around the world. Patrons are served in a completely darkened restaurant by waiters who move carefully around memorized routes using only their sense of touch and sound. The allure of these establishments is the belief that depriving oneself of visual sensory input will enhance the sense of taste and smell, and foods will be experienced in new ways. Each bite provides a sense of wonder while discovering the flavors the chef has prepared.</p><p>Can you imagine how a diner experiences the unseen food? Upon first bite, the senses are overwhelmed. What are the dominant flavors? Does the food taste savory or sweet? Does it taste similar to something eaten previously? Personally, I imagine this process of discovery in terms of a slightly modified adage: if it smells like a duck and tastes like a duck, then you are probably eating duck.</p><p>This illustrates an idea that can be used for machine learning—as does another maxim involving poultry: "birds of a feather flock together." Stated differently, things that are alike are likely to have properties that are alike. Machine learning uses this principle to classify data by placing it in the same category as similar or "nearest" neighbors. This chapter is devoted to the classifiers that use this approach. You will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The key concepts that define <span class="strong"><strong>nearest neighbor</strong></span> classifiers, and why they are considered "lazy" learners</li><li class="listitem" style="list-style-type: disc">Methods to measure the similarity of two examples using distance</li><li class="listitem" style="list-style-type: disc">To apply a popular nearest neighbor classifier called k-NN</li></ul></div><p>If all these talks about food is making you hungry, feel free to grab a snack. Our first task will be to understand the k-NN approach by putting it to use by settling a long-running culinary debate.</p><div class="section" title="Understanding nearest neighbor classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Understanding nearest neighbor classification</h1></div></div></div><p>In a single <a id="id199" class="indexterm"/>sentence, <span class="strong"><strong>nearest neighbor</strong></span> classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples. Despite the simplicity of this idea, nearest neighbor methods are extremely powerful. They have been used successfully for:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Computer vision applications, including optical character recognition and facial recognition in both still images and video</li><li class="listitem" style="list-style-type: disc">Predicting whether a person will enjoy a movie or music recommendation</li><li class="listitem" style="list-style-type: disc">Identifying patterns in genetic data, perhaps to use them in detecting specific proteins or diseases</li></ul></div><p>In general, nearest neighbor classifiers are well-suited for classification tasks, where relationships among the features and the target classes are numerous, complicated, or extremely difficult to understand, yet the items of similar class type tend to be fairly homogeneous. Another way of putting it would be to say that if a concept is difficult to define, but you know it when you see it, then nearest neighbors might be appropriate. On the other hand, if the data is noisy and thus no clear distinction exists among the groups, the nearest neighbor algorithms may struggle to identify the class boundaries.</p><div class="section" title="The k-NN algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec31"/>The k-NN algorithm</h2></div></div></div><p>The nearest <a id="id200" class="indexterm"/>neighbors approach to classification is exemplified by the <span class="strong"><strong>k-nearest neighbors algorithm</strong></span> (<span class="strong"><strong>k-NN</strong></span>). Although this is perhaps one of the simplest machine learning algorithms, it is still used widely.</p><p>The strengths and weaknesses of this algorithm are as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Simple and effective</li><li class="listitem" style="list-style-type: disc">Makes no assumptions about the underlying data distribution</li><li class="listitem" style="list-style-type: disc">Fast training phase</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Does <a id="id201" class="indexterm"/>not produce a model, limiting the ability to understand how the features are related to the class</li><li class="listitem" style="list-style-type: disc">Requires selection of an appropriate <span class="emphasis"><em>k</em></span></li><li class="listitem" style="list-style-type: disc">Slow classification phase</li><li class="listitem" style="list-style-type: disc">Nominal features and missing data require additional processing</li></ul></div>
</td></tr></tbody></table></div><p>The k-NN algorithm gets its name from the fact that it uses information about an example's k-nearest neighbors to classify unlabeled examples. The letter <span class="emphasis"><em>k</em></span> is a variable term implying that any number of nearest neighbors could be used. After choosing <span class="emphasis"><em>k</em></span>, the algorithm requires a training dataset made up of examples that have been classified into several categories, as labeled by a nominal variable. Then, for each unlabeled record in the test dataset, k-NN <a id="id202" class="indexterm"/>identifies <span class="emphasis"><em>k</em></span> records in the training data that are the "nearest" in similarity. The unlabeled test instance is assigned the class of the majority of the k nearest neighbors.</p><p>To illustrate this process, let's <a id="id203" class="indexterm"/>revisit the blind tasting experience described in the introduction. Suppose that prior to eating the mystery meal we had created a dataset in which we recorded our impressions of a number of ingredients we tasted previously. To keep things simple, we rated only two features of each ingredient. The first is a measure from 1 to 10 of how crunchy the ingredient is and the second is a 1 to 10 score of how sweet the ingredient tastes. We then labeled each ingredient as one of the three types of food: fruits, vegetables, or proteins. The first few rows of such a dataset might be structured as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Ingredient</p>
</th><th style="text-align: left" valign="bottom">
<p>Sweetness</p>
</th><th style="text-align: left" valign="bottom">
<p>Crunchiness</p>
</th><th style="text-align: left" valign="bottom">
<p>Food type</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>apple</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>fruit</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>bacon</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>protein</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>banana</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>fruit</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>carrot</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>vegetable</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>celery</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>vegetable</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>cheese</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>protein</p>
</td></tr></tbody></table></div><p>The k-NN algorithm treats the features as coordinates in a multidimensional feature space. As our dataset includes only two features, the feature space is two-dimensional. We can plot two-dimensional data on a scatter plot, with the <span class="emphasis"><em>x</em></span> dimension indicating the ingredient's sweetness and the <span class="emphasis"><em>y</em></span> dimension, the crunchiness. After adding a few more ingredients to the taste dataset, the scatter plot might look similar to this:</p><div class="mediaobject"><img src="graphics/3905_03_01.jpg" alt="The k-NN algorithm"/></div><p>Did you notice the pattern? Similar types of food tend to be grouped closely together. As illustrated in the next <a id="id204" class="indexterm"/>diagram, vegetables tend to be crunchy but not sweet, fruits tend to be sweet and either crunchy or not crunchy, while proteins tend to be neither crunchy nor sweet:</p><div class="mediaobject"><img src="graphics/3905_03_02.jpg" alt="The k-NN algorithm"/></div><p>Suppose that after constructing this dataset, we decide to use it to settle the age-old question: is tomato a fruit or vegetable? We can use the nearest neighbor approach to determine which class <a id="id205" class="indexterm"/>is a better fit, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/3905_03_03.jpg" alt="The k-NN algorithm"/></div><div class="section" title="Measuring similarity with distance"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec10"/>Measuring similarity with distance</h3></div></div></div><p>Locating the <a id="id206" class="indexterm"/>tomato's nearest neighbors requires a <span class="strong"><strong>distance function</strong></span>, or a formula that measures the similarity between the two instances.</p><p>There are many different ways to calculate distance. Traditionally, the k-NN algorithm uses <span class="strong"><strong>Euclidean distance</strong></span>, which is the distance one would measure if it were possible to use a ruler to connect two points, illustrated in the previous figure by the dotted lines connecting the tomato to its neighbors.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip34"/>Tip</h3><p>Euclidean distance is measured "as the crow flies," implying the shortest direct route. Another common distance measure is Manhattan distance, which is based on the paths a pedestrian would take by walking city blocks. If you are interested in learning more about other distance measures, you can read the documentation for R's distance function (a useful tool in its own right), using the <code class="literal">?dist</code> command.</p></div></div><p>Euclidean <a id="id207" class="indexterm"/>distance is specified by the following formula, where <span class="emphasis"><em>p</em></span> and <span class="emphasis"><em>q</em></span> are the examples to be compared, each having <span class="emphasis"><em>n</em></span> features. The term <span class="emphasis"><em>p<sub>1</sub></em></span> refers to the value of the first feature of example <span class="emphasis"><em>p</em></span>, while <span class="emphasis"><em>q<sub>1</sub></em></span> refers to the value of the first feature of example <span class="emphasis"><em>q</em></span>:</p><div class="mediaobject"><img src="graphics/3905_03_04.jpg" alt="Measuring similarity with distance"/></div><p>The distance formula involves comparing the values of each feature. For example, to calculate the distance between the tomato (<span class="emphasis"><em>sweetness = 6</em></span>, <span class="emphasis"><em>crunchiness = 4</em></span>), and the green bean (<span class="emphasis"><em>sweetness = 3</em></span>, <span class="emphasis"><em>crunchiness = 7</em></span>), we can use the formula as follows:</p><div class="mediaobject"><img src="graphics/3905_03_05.jpg" alt="Measuring similarity with distance"/></div><p>In a similar vein, we can calculate the distance between the tomato and several of its closest neighbors as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Ingredient</p>
</th><th style="text-align: left" valign="bottom">
<p>Sweetness</p>
</th><th style="text-align: left" valign="bottom">
<p>Crunchiness</p>
</th><th style="text-align: left" valign="bottom">
<p>Food type</p>
</th><th style="text-align: left" valign="bottom">
<p>Distance to the tomato</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>grape</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>fruit</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>sqrt((6 - 8)^2 + (4 - 5)^2) = 2.2</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>green bean</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>vegetable</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>sqrt((6 - 3)^2 + (4 - 7)^2) = 4.2</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>nuts</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>protein</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>sqrt((6 - 3)^2 + (4 - 6)^2) = 3.6</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>orange</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>fruit</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>sqrt((6 - 7)^2 + (4 - 3)^2) = 1.4</em></span>
</p>
</td></tr></tbody></table></div><p>To classify the tomato as a vegetable, protein, or fruit, we'll begin by assigning the tomato, the food type of its single nearest neighbor. This is called 1-NN classification because <span class="emphasis"><em>k = 1</em></span>. The orange is the nearest neighbor to the tomato, with a distance of 1.4. As orange is a fruit, the 1-NN algorithm would classify tomato as a fruit.</p><p>If we use the k-NN algorithm with <span class="emphasis"><em>k = 3</em></span> instead, it performs a vote among the three nearest <a id="id208" class="indexterm"/>neighbors: orange, grape, and nuts. Since the majority class among these neighbors is fruit (two of the three votes), the tomato again is classified as a fruit.</p></div><div class="section" title="Choosing an appropriate k"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec11"/>Choosing an appropriate k</h3></div></div></div><p>The decision of <a id="id209" class="indexterm"/>how many neighbors to use for k-NN determines how well the model will generalize to future data. The balance between overfitting and underfitting the training data is a problem known as <span class="strong"><strong>bias-variance tradeoff</strong></span>. Choosing a large <span class="emphasis"><em>k</em></span> reduces the impact or variance caused by noisy data, but can bias <a id="id210" class="indexterm"/>the learner so that it runs the risk of ignoring small, but important patterns.</p><p>Suppose we took the extreme stance of setting a very large <span class="emphasis"><em>k</em></span>, as large as the total number of observations in the training data. With every training instance represented in the final vote, the most common class always has a majority of the voters. The model would consequently always predict the majority class, regardless of the nearest neighbors.</p><p>On the opposite extreme, using a single nearest neighbor allows the noisy data or outliers to unduly influence the classification of examples. For example, suppose some of the training examples were accidentally mislabeled. Any unlabeled example that happens to be nearest to the incorrectly labeled neighbor will be predicted to have the incorrect class, even if nine other nearest neighbors would have voted differently.</p><p>Obviously, the best <span class="emphasis"><em>k</em></span> value is somewhere between these two extremes.</p><p>The following figure illustrates, more generally, how the decision boundary (depicted by a dashed line) is affected by larger or smaller <span class="emphasis"><em>k</em></span> values. Smaller values allow more complex decision boundaries that more carefully fit the training data. The problem is that we do not know whether the straight boundary or the curved boundary better represents the true underlying concept to be learned.</p><div class="mediaobject"><img src="graphics/3905_03_06.jpg" alt="Choosing an appropriate k"/></div><p>In practice, choosing <span class="emphasis"><em>k</em></span> depends on the difficulty of the concept to be learned, and the number of records in the training data. One common practice is to begin with <span class="emphasis"><em>k</em></span> equal to the square root of the number of training examples. In the food classifier we developed previously, we might set <span class="emphasis"><em>k = 4</em></span> because there were 15 example ingredients in the training data and the square root of 15 is 3.87.</p><p>However, such rules <a id="id211" class="indexterm"/>may not always result in the single best <span class="emphasis"><em>k</em></span>. An alternative approach is to test several <span class="emphasis"><em>k</em></span> values on a variety of test datasets and choose the one that delivers the best classification performance. That said, unless the data is very noisy, a large training dataset can make the choice of <span class="emphasis"><em>k</em></span> less important. This is because even subtle concepts will have a sufficiently large pool of examples to vote as nearest neighbors.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip35"/>Tip</h3><p>A less common, but interesting solution to this problem is to choose a larger <span class="emphasis"><em>k</em></span>, but apply a <span class="strong"><strong>weighted voting</strong></span> process in which the vote of the closer neighbors is considered more authoritative than the vote of the far away neighbors. Many k-NN implementations offer this option.</p></div></div></div><div class="section" title="Preparing data for use with k-NN"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec12"/>Preparing data for use with k-NN</h3></div></div></div><p>Features are typically <a id="id212" class="indexterm"/>transformed to a standard range prior to applying the k-NN algorithm. The rationale for this step is that the distance formula is highly dependent on how features are measured. In particular, if certain features have a much larger range of values than the others, the distance measurements will be strongly dominated by the features with larger ranges. This wasn't a problem for food tasting example as both sweetness and crunchiness were measured on a scale from 1 to 10.</p><p>However, suppose we added an additional feature to the dataset for a food's spiciness, which was measured using the <a id="id213" class="indexterm"/>Scoville scale. If you are not familiar with this metric, it is a standardized measure of spice heat, ranging from zero (not at all spicy) to over a million (for the hottest chili peppers). Since the difference between spicy and non-spicy foods can be over a million, while the difference between sweet and non-sweet or crunchy and non-crunchy foods is at most 10, the difference in scale allows the spice level to impact the distance function much more than the other two factors. Without adjusting our data, we might find that our distance measures only differentiate foods by their spiciness; the impact of crunchiness and sweetness would be dwarfed by the contribution of spiciness.</p><p>The solution is to rescale the features by shrinking or expanding their range such that each one contributes relatively equally to the distance formula. For example, if sweetness and crunchiness are both measured on a scale from 1 to 10, we would also like spiciness to be measured on a scale from 1 to 10. There are several common ways to accomplish such scaling.</p><p>The traditional method of <a id="id214" class="indexterm"/>rescaling features for k-NN is <span class="strong"><strong>min-max </strong></span><a id="id215" class="indexterm"/>
<span class="strong"><strong>normalization</strong></span>. This process transforms a feature such that all of its values fall in a range between 0 and 1. The formula for normalizing a feature is as follows:</p><div class="mediaobject"><img src="graphics/3905_03_07.jpg" alt="Preparing data for use with k-NN"/></div><p>Essentially, the formula subtracts the minimum of feature <span class="emphasis"><em>X</em></span> from each value and divides by the range of <span class="emphasis"><em>X</em></span>.</p><p>Normalized feature values can be interpreted as indicating how far, from 0 percent to 100 percent, the original value fell along the range between the original minimum and maximum.</p><p>Another common transformation is called <span class="strong"><strong>z-score standardization</strong></span>. The following formula subtracts the <a id="id216" class="indexterm"/>mean value of feature <span class="emphasis"><em>X</em></span>, and divides the outcome by the standard deviation of <span class="emphasis"><em>X</em></span>:</p><div class="mediaobject"><img src="graphics/3905_03_08.jpg" alt="Preparing data for use with k-NN"/></div><p>This formula, which is based on the properties of the normal distribution covered in <a class="link" href="ch02.html" title="Chapter 2. Managing and Understanding Data">Chapter 2</a>, <span class="emphasis"><em>Managing and Understanding Data</em></span>, rescales each of the feature's values in terms of how many standard deviations they fall above or below the mean value. The resulting value is called a <a id="id217" class="indexterm"/>
<span class="strong"><strong>z-score</strong></span>. The z-scores fall in an unbound range of negative and positive numbers. Unlike the normalized values, they have no predefined minimum and maximum.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip36"/>Tip</h3><p>The same rescaling method used on the k-NN training dataset must also be applied to the examples the algorithm will later classify. This can lead to a tricky situation for min-max normalization, as the minimum or maximum of future cases might be outside the range of values observed in the training data. If you know the plausible minimum or maximum value ahead of time, you can use these constants rather than the observed values. Alternatively, you can use z-score standardization under the assumption that the future examples will have similar mean and standard deviation as the training examples.</p></div></div><p>The Euclidean distance formula is not defined for nominal data. Therefore, to calculate the distance between <a id="id218" class="indexterm"/>nominal features, we need to convert them into a <a id="id219" class="indexterm"/>numeric format. A typical solution utilizes <span class="strong"><strong>dummy coding</strong></span>, where a value of <span class="emphasis"><em>1</em></span> indicates one category, and <span class="emphasis"><em>0</em></span>, the other. For instance, dummy coding for a gender variable could be constructed as:</p><div class="mediaobject"><img src="graphics/3905_03_09.jpg" alt="Preparing data for use with k-NN"/></div><p>Notice how the dummy coding of the two-category (binary) gender variable results in a single new feature named male. There is no need to construct a separate feature for female; since the two sexes are mutually exclusive, knowing one or the other is enough.</p><p>This is true more generally as well. An <span class="emphasis"><em>n</em></span>-category nominal feature can be dummy coded by creating the binary indicator variables for (<span class="emphasis"><em>n - 1</em></span>) levels of the feature. For example, the dummy coding for a three-category temperature variable (for example, hot, medium, or cold) could be set up as <span class="emphasis"><em>(3 - 1) = 2</em></span> features, as shown here:</p><div class="mediaobject"><img src="graphics/3905_03_10.jpg" alt="Preparing data for use with k-NN"/></div><p>Knowing that hot and medium are both <span class="emphasis"><em>0</em></span> is enough to know that the temperature is cold. We, therefore, do not need a third feature for the cold category.</p><p>A convenient aspect of dummy coding is that the distance between dummy coded features is always one or <a id="id220" class="indexterm"/>zero, and thus, the values fall on the same scale as min-max normalized numeric data. No additional transformation is necessary.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip37"/>Tip</h3><p>If a nominal feature is ordinal (one could make such an argument for temperature), an alternative to dummy coding is to number the categories and apply normalization. For instance, cold, warm, and hot could be numbered as 1, 2, and 3, which normalizes to 0, 0.5, and 1. A caveat to this approach is that it should only be used if the steps between the categories are equivalent. For instance, although income categories for poor, middle class, and wealthy are ordered, the difference between the poor and middle class may be different than the difference between the middle class and wealthy. Since the steps between groups are not equal, dummy coding is a safer approach.</p></div></div></div></div><div class="section" title="Why is the k-NN algorithm lazy?"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec32"/>Why is the k-NN algorithm lazy?</h2></div></div></div><p>Classification <a id="id221" class="indexterm"/>algorithms based on the nearest neighbor methods are considered <span class="strong"><strong>lazy learning</strong></span> algorithms because, technically speaking, no abstraction occurs. The <a id="id222" class="indexterm"/>abstraction and generalization processes are skipped altogether, and this undermines the definition of learning, proposed in <a class="link" href="ch01.html" title="Chapter 1. Introducing Machine Learning">Chapter 1</a>, <span class="emphasis"><em>Introducing Machine Learning</em></span>.</p><p>Under the strict definition of learning, a lazy learner is not really learning anything. Instead, it merely stores the training data verbatim. This allows the training phase, which is not actually training anything, to occur very rapidly. Of course, the downside is that the process of making <a id="id223" class="indexterm"/>predictions tends to be relatively slow in comparison to training. Due to the heavy reliance on the training instances rather than an abstracted <a id="id224" class="indexterm"/>model, lazy learning is also known as <span class="strong"><strong>instance-based learning</strong></span> or <span class="strong"><strong>rote learning</strong></span>.</p><p>As instance-based learners do not build a model, the method is said to be in a class of <span class="strong"><strong>non-parametric</strong></span> learning methods—no parameters are learned about the data. Without generating theories about the underlying data, non-parametric methods limit our ability to understand how the classifier <a id="id225" class="indexterm"/>is using the data. On the other hand, this allows the learner to find natural patterns rather than trying to fit the data into a preconceived and potentially biased functional form.</p><div class="mediaobject"><img src="graphics/3905_03_11.jpg" alt="Why is the k-NN algorithm lazy?"/></div><p>Although k-NN classifiers may be considered lazy, they are still quite powerful. As you will soon see, the simple principles of nearest neighbor learning can be used to automate the process of screening for cancer.</p></div></div></div>
<div class="section" title="Example &#x2013; diagnosing breast cancer with the k-NN algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Example – diagnosing breast cancer with the k-NN algorithm</h1></div></div></div><p>Routine breast <a id="id226" class="indexterm"/>cancer screening allows the <a id="id227" class="indexterm"/>disease to be diagnosed and treated prior to it causing noticeable symptoms. The process of early detection involves examining the breast tissue for abnormal lumps or masses. If a lump is found, a fine-needle aspiration biopsy is performed, which uses a hollow needle to extract a small sample of cells from the mass. A clinician then examines the cells under a microscope to determine whether the mass is likely to be malignant or benign.</p><p>If machine learning could automate the identification of cancerous cells, it would provide considerable benefit to the health system. Automated processes are likely to improve the efficiency of the detection process, allowing physicians to spend less time diagnosing and more time treating the disease. An automated screening system might also provide greater detection accuracy by removing the inherently subjective human component from the process.</p><p>We will investigate the <a id="id228" class="indexterm"/>utility of machine learning for <a id="id229" class="indexterm"/>detecting cancer by applying the k-NN algorithm to measurements of biopsied cells from women with abnormal breast masses.</p><div class="section" title="Step 1 – collecting data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec33"/>Step 1 – collecting data</h2></div></div></div><p>We will utilize the Wisconsin Breast Cancer Diagnostic dataset from the UCI Machine Learning Repository at <a class="ulink" href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>. This data was donated by researchers of the <a id="id230" class="indexterm"/>University of Wisconsin and includes the measurements from digitized images of fine-needle aspirate of a breast mass. The values represent the characteristics of the cell nuclei present in the digital image.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>To read more about this dataset, refer to: Mangasarian OL, Street WN, Wolberg WH. Breast cancer diagnosis and prognosis via linear programming. <span class="emphasis"><em>Operations Research</em></span>. 1995; 43:570-577.</p></div></div><p>The breast cancer data includes 569 examples of cancer biopsies, each with 32 features. One feature is an identification number, another is the cancer diagnosis, and 30 are numeric-valued laboratory measurements. The diagnosis is coded as <code class="literal">"M"</code> to indicate malignant or <code class="literal">"B"</code> to indicate benign.</p><p>The other 30 numeric measurements comprise the mean, standard error, and worst (that is, largest) value for 10 different characteristics of the digitized cell nuclei. These include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Radius</li><li class="listitem" style="list-style-type: disc">Texture</li><li class="listitem" style="list-style-type: disc">Perimeter</li><li class="listitem" style="list-style-type: disc">Area</li><li class="listitem" style="list-style-type: disc">Smoothness</li><li class="listitem" style="list-style-type: disc">Compactness</li><li class="listitem" style="list-style-type: disc">Concavity</li><li class="listitem" style="list-style-type: disc">Concave points</li><li class="listitem" style="list-style-type: disc">Symmetry</li><li class="listitem" style="list-style-type: disc">Fractal dimension</li></ul></div><p>Based on these names, all the features seem to relate to the shape and size of the cell nuclei. Unless you are an <a id="id231" class="indexterm"/>oncologist, you are unlikely to know how each relates to benign or malignant masses. These patterns will be revealed as we continue in the machine learning process.</p></div><div class="section" title="Step 2 – exploring and preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec34"/>Step 2 – exploring and preparing the data</h2></div></div></div><p>Let's explore the data <a id="id232" class="indexterm"/>and see whether we can shine some light on <a id="id233" class="indexterm"/>the relationships. In doing so, we will prepare the data for use with the k-NN learning method.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip38"/>Tip</h3><p>If you plan on following along, download the <code class="literal">wisc_bc_data.csv</code> file from the Packt website and save it to your R working directory. The dataset was modified very slightly from its original form for this book. In particular, a header line was added and the rows of data were randomly ordered.</p></div></div><p>We'll begin by importing the CSV data file, as we have done in previous chapters, saving the Wisconsin breast cancer data to the <code class="literal">wbcd</code> data frame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd &lt;- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)</strong></span>
</pre></div><p>Using the <code class="literal">str(wbcd)</code> command, we can confirm that the data is structured with 569 examples and 32 features as we expected. The first several lines of output are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>'data.frame':  569 obs. of  32 variables:</strong></span>
<span class="strong"><strong> $ id               : int  87139402 8910251 905520 ...</strong></span>
<span class="strong"><strong> $ diagnosis        : chr  "B" "B" "B" "B" ...</strong></span>
<span class="strong"><strong> $ radius_mean      : num  12.3 10.6 11 11.3 15.2 ...</strong></span>
<span class="strong"><strong> $ texture_mean     : num  12.4 18.9 16.8 13.4 13.2 ...</strong></span>
<span class="strong"><strong> $ perimeter_mean   : num  78.8 69.3 70.9 73 97.7 ...</strong></span>
<span class="strong"><strong> $ area_mean        : num  464 346 373 385 712 ...</strong></span>
</pre></div><p>The first variable is an integer variable named <code class="literal">id</code>. As this is simply a unique identifier (ID) for each patient in the data, it does not provide useful information, and we will need to exclude it from the model.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip39"/>Tip</h3><p>Regardless of the machine learning method, ID variables should always be excluded. Neglecting to do so can lead to erroneous findings because the ID can be used to uniquely "predict" each example. Therefore, a model that includes an identifier will suffer from overfitting, and is unlikely to generalize well to other data.</p></div></div><p>Let's drop the <code class="literal">id</code> feature altogether. As it is located in the first column, we can exclude it by making a copy of the <code class="literal">wbcd</code> data frame without column <code class="literal">1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd &lt;- wbcd[-1]</strong></span>
</pre></div><p>The next variable, <code class="literal">diagnosis</code>, is of particular interest as it is the outcome we hope to predict. This feature indicates whether the example is from a benign or malignant mass. The <code class="literal">table()</code> output indicates that 357 masses are benign while 212 are malignant:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(wbcd$diagnosis)</strong></span>
<span class="strong"><strong>  B   M </strong></span>
<span class="strong"><strong>357 212</strong></span>
</pre></div><p>Many R machine <a id="id234" class="indexterm"/>learning classifiers require that the target <a id="id235" class="indexterm"/>feature is coded as a factor, so we will need to recode the <code class="literal">diagnosis</code> variable. We will also take this opportunity to give the <code class="literal">"B"</code> and <code class="literal">"M"</code> values more informative labels using the <code class="literal">labels</code> parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd$diagnosis&lt;- factor(wbcd$diagnosis, levels = c("B", "M"),</strong></span>
<span class="strong"><strong>        labels = c("Benign", "Malignant"))</strong></span>
</pre></div><p>Now, when we look at the <code class="literal">prop.table()</code> output, we notice that the values have been labeled <code class="literal">Benign</code> and <code class="literal">Malignant</code> with 62.7 percent and 37.3 percent of the masses, respectively:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)</strong></span>
<span class="strong"><strong>   Benign Malignant </strong></span>
<span class="strong"><strong>     62.7      37.3</strong></span>
</pre></div><p>The remaining 30 features are all numeric, and as expected, they consist of three different measurements of ten characteristics. For illustrative purposes, we will only take a closer look at three of these features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])</strong></span>
<span class="strong"><strong>  radius_mean       area_mean      smoothness_mean  </strong></span>
<span class="strong"><strong> Min.   : 6.981   Min.   : 143.5   Min.   :0.05263  </strong></span>
<span class="strong"><strong> 1st Qu.:11.700   1st Qu.: 420.3   1st Qu.:0.08637  </strong></span>
<span class="strong"><strong> Median :13.370   Median : 551.1   Median :0.09587  </strong></span>
<span class="strong"><strong> Mean   :14.127   Mean   : 654.9   Mean   :0.09636  </strong></span>
<span class="strong"><strong> 3rd Qu.:15.780   3rd Qu.: 782.7   3rd Qu.:0.10530  </strong></span>
<span class="strong"><strong> Max.   :28.110   Max.   :2501.0   Max.   :0.16340</strong></span>
</pre></div><p>Looking at the features <a id="id236" class="indexterm"/>side-by-side, do you notice anything <a id="id237" class="indexterm"/>problematic about the values? Recall that the distance calculation for k-NN is heavily dependent upon the measurement scale of the input features. Since smoothness ranges from 0.05 to 0.16 and area ranges from <code class="literal">143.5</code> to <code class="literal">2501.0</code>, the impact of area is going to be much larger than the smoothness in the distance calculation. This could potentially cause problems for our classifier, so let's apply normalization to rescale the features to a standard range of values.</p><div class="section" title="Transformation – normalizing numeric data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec13"/>Transformation – normalizing numeric data</h3></div></div></div><p>To normalize these <a id="id238" class="indexterm"/>features, we need to create a <code class="literal">normalize()</code> function in R. This function takes a vector <code class="literal">x</code> of numeric values, and for each value in <code class="literal">x</code>, subtracts the minimum value in <code class="literal">x</code> and divides by the range of values in <code class="literal">x</code>. Finally, the resulting vector is returned. The code for this function is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; normalize &lt;- function(x) {</strong></span>
<span class="strong"><strong>       return ((x - min(x)) / (max(x) - min(x)))</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>After executing the preceding code, the <code class="literal">normalize()</code> function is available for use in R. Let's test the function on a couple of vectors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; normalize(c(1, 2, 3, 4, 5))</strong></span>
<span class="strong"><strong>[1] 0.00 0.25 0.50 0.75 1.00</strong></span>
<span class="strong"><strong>&gt; normalize(c(10, 20, 30, 40, 50))</strong></span>
<span class="strong"><strong>[1] 0.00 0.25 0.50 0.75 1.00</strong></span>
</pre></div><p>The function appears to be working correctly. Despite the fact that the values in the second vector are 10 times larger than the first vector, after normalization, they both appear exactly the same.</p><p>We can now apply the <code class="literal">normalize()</code> function to the numeric features in our data frame. Rather than normalizing each of the 30 numeric variables individually, we will use one of R's functions to automate the process.</p><p>The <code class="literal">lapply()</code> function takes a list and applies a specified function to each list element. As a data frame is a list of equal-length vectors, we can use <code class="literal">lapply()</code> to apply <code class="literal">normalize()</code> to each feature in the data frame. The final step is to convert the list returned by <code class="literal">lapply()</code> to a data frame, using the <code class="literal">as.data.frame()</code> function. The full process looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd_n &lt;- as.data.frame(lapply(wbcd[2:31], normalize))</strong></span>
</pre></div><p>In plain English, this command applies the <code class="literal">normalize()</code> function to columns 2 through 31 in the <code class="literal">wbcd</code> data frame, converts the resulting list to a data frame, and assigns it the name <code class="literal">wbcd_n</code>. The <code class="literal">_n</code> suffix is used here as a reminder that the values in <code class="literal">wbcd</code> have been normalized.</p><p>To confirm that the transformation was applied correctly, let's look at one variable's summary statistics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(wbcd_n$area_mean)</strong></span>
<span class="strong"><strong>Min. 1st Qu.Median    Mean 3rd Qu.    Max. </strong></span>
<span class="strong"><strong>0.0000  0.1174  0.1729  0.2169  0.2711  1.0000  </strong></span>
</pre></div><p>As expected, the <code class="literal">area_mean</code> <a id="id239" class="indexterm"/>variable, which originally ranged from 143.5 to 2501.0, now ranges from 0 to 1.</p></div><div class="section" title="Data preparation – creating training and test datasets"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec14"/>Data preparation – creating training and test datasets</h3></div></div></div><p>Although all the <a id="id240" class="indexterm"/>569 biopsies are labeled with a benign or malignant status, it is not very interesting to predict what we <a id="id241" class="indexterm"/>already know. Additionally, any performance measures we obtain during the training may be misleading as we do not know the extent to which cases have been overfitted or how well the learner will generalize to unseen cases. A more interesting question is how well our learner performs on a dataset of unlabeled data. If we had access to a laboratory, we could apply our learner to the measurements taken from the next 100 masses of unknown cancer status, and see how well the machine learner's predictions compare to the diagnoses obtained using conventional methods.</p><p>In the absence of such data, we can simulate this scenario by dividing our data into two portions: a training dataset that will be used to build the k-NN model and a test dataset that will be used to estimate the predictive accuracy of the model. We will use the first 469 records for the training dataset and the remaining 100 to simulate new patients.</p><p>Using the data extraction methods given in <a class="link" href="ch02.html" title="Chapter 2. Managing and Understanding Data">Chapter 2</a>, <span class="emphasis"><em>Managing and Understanding Data</em></span>, we will split the <code class="literal">wbcd_n</code> data frame into <code class="literal">wbcd_train</code> and <code class="literal">wbcd_test</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd_train &lt;- wbcd_n[1:469, ]</strong></span>
<span class="strong"><strong>&gt; wbcd_test &lt;- wbcd_n[470:569, ]</strong></span>
</pre></div><p>If the preceding commands are confusing, remember that data is extracted from data frames using the <code class="literal">[row, column]</code> syntax. A blank value for the row or column value indicates that all the rows or columns should be included. Hence, the first line of code takes rows 1 to 469 and all columns, and the second line takes 100 rows from 470 to 569 and all columns.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip40"/>Tip</h3><p>When constructing training and test datasets, it is important that each dataset is a representative subset of the full set of data. The <code class="literal">wbcd</code> records were already randomly ordered, so we could simply extract 100 consecutive records to create a test dataset. This would not be appropriate if the data was ordered chronologically or in groups of similar values. In these cases, random sampling methods would be needed. Random sampling will be discussed in <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span>.</p></div></div><p>When we constructed our normalized training and test datasets, we excluded the target variable, <code class="literal">diagnosis</code>. For training the k-NN model, we will need to store these class labels in factor <a id="id242" class="indexterm"/>vectors, split between the training and test datasets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd_train_labels &lt;- wbcd[1:469, 1]</strong></span>
<span class="strong"><strong>&gt; wbcd_test_labels &lt;- wbcd[470:569, 1]</strong></span>
</pre></div><p>This code takes <a id="id243" class="indexterm"/>the <code class="literal">diagnosis</code> factor in the first column of the <code class="literal">wbcd</code> data frame, and creates the vectors <code class="literal">wbcd_train_labels</code> and <code class="literal">wbcd_test_labels</code>. We will use these in the next steps of training and evaluating our classifier.</p></div></div><div class="section" title="Step 3 – training a model on the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/>Step 3 – training a model on the data</h2></div></div></div><p>Equipped with <a id="id244" class="indexterm"/>our training data and labels vector, we are now ready to classify our unknown records. For the k-NN algorithm, the training phase actually involves no model building; the process of training a lazy learner like k-NN simply involves storing the input data in a structured format.</p><p>To classify our test instances, we will use a k-NN implementation from the <code class="literal">class</code> package, which provides a set of basic R functions for classification. If this package is not already installed on your system, you can install it by typing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; install.packages("class")</strong></span>
</pre></div><p>To load the package during any session in which you wish to use the functions, simply enter the <code class="literal">library(class)</code> command.</p><p>The <code class="literal">knn()</code> function in the <code class="literal">class</code> package provides a standard, classic implementation of the k-NN algorithm. For each instance in the test data, the function will identify the k-Nearest Neighbors, using Euclidean distance, where <span class="emphasis"><em>k</em></span> is a user-specified number. The test instance is classified by taking a "vote" among the k-Nearest Neighbors—specifically, this involves assigning the class of the majority of the <span class="emphasis"><em>k</em></span> neighbors. A tie vote is broken at random.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip41"/>Tip</h3><p>There are several other k-NN functions in other R packages, which provide more sophisticated or more efficient implementations. If you run into limits with <code class="literal">knn()</code>, search for k-NN at the <span class="strong"><strong>Comprehensive R Archive Network</strong></span> (<span class="strong"><strong>CRAN</strong></span>).</p></div></div><p>Training and classification using the <code class="literal">knn()</code> function is performed in a single function call, using four <a id="id245" class="indexterm"/>parameters, as shown in the following table:</p><div class="mediaobject"><img src="graphics/3905_03_12.jpg" alt="Step 3 – training a model on the data"/></div><p>We now have nearly everything that we need to apply the k-NN algorithm to this data. We've split our data into training and test datasets, each with exactly the same numeric features. The labels for the training data are stored in a separate factor vector. The only remaining parameter is <code class="literal">k</code>, which specifies the number of neighbors to include in the vote.</p><p>As our training data includes 469 instances, we might try <code class="literal">k = 21</code>, an odd number roughly equal to the square root of 469. With a two-category outcome, using an odd number eliminates the chance of ending with a tie vote.</p><p>Now we can use the <code class="literal">knn()</code> function to classify the <code class="literal">test</code> data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test,</strong></span>
<span class="strong"><strong>                        cl = wbcd_train_labels, k = 21)</strong></span>
</pre></div><p>The <code class="literal">knn()</code> function returns a factor vector of predicted labels for each of the examples in the <code class="literal">test</code> dataset, which we have assigned to <code class="literal">wbcd_test_pred</code>.</p></div><div class="section" title="Step 4 – evaluating model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/>Step 4 – evaluating model performance</h2></div></div></div><p>The next <a id="id246" class="indexterm"/>step of the process is to evaluate how well the predicted classes in the <code class="literal">wbcd_test_pred</code> vector match up with the known values in the <code class="literal">wbcd_test_labels</code> vector. To do this, we can use the <code class="literal">CrossTable()</code> function in the <code class="literal">gmodels</code> package, which was introduced in <a class="link" href="ch02.html" title="Chapter 2. Managing and Understanding Data">Chapter 2</a>, <span class="emphasis"><em>Managing and Understanding Data</em></span>. If you haven't done so already, please install this package, using the <code class="literal">install.packages("gmodels")</code> command.</p><p>After loading the package with the <code class="literal">library(gmodels)</code> command, we can create a cross tabulation indicating the agreement between the two vectors. Specifying <code class="literal">prop.chisq = FALSE</code> will remove the unnecessary chi-square values from the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,</strong></span>
<span class="strong"><strong>                       prop.chisq=FALSE)</strong></span>
</pre></div><p>The resulting table looks like this:</p><div class="mediaobject"><img src="graphics/3905_03_13.jpg" alt="Step 4 – evaluating model performance"/></div><p>The cell percentages in the table indicate the proportion of values that fall into four categories. The top-left cell indicates the <span class="strong"><strong>true negative</strong></span> results. These 61 of 100 values are cases where the mass was benign and the k-NN algorithm correctly identified it as such. The bottom-right cell indicates the <span class="strong"><strong>true positive</strong></span> results, where the classifier and the clinically determined label agree that the mass is malignant. A total of 37 of 100 predictions were true positives.</p><p>The cells <a id="id247" class="indexterm"/>falling on the other diagonal contain counts of examples where the k-NN approach disagreed with the true label. The two examples in the lower-left cell are <span class="strong"><strong>false negative</strong></span> results; in this case, the predicted value was benign, but the tumor was actually malignant. Errors in this direction could be extremely costly as they might lead a patient to believe that she is cancer-free, but in reality, the disease may continue to spread. The top-right cell would contain the <span class="strong"><strong>false positive</strong></span> results, if there were any. These values occur when the model classifies a mass as malignant, but in reality, it was benign. Although such errors are less dangerous than a false negative result, they should also be avoided as they could lead to additional financial burden on the health care system or additional stress for the patient as additional tests or treatment may have to be provided.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip42"/>Tip</h3><p>If we desired, we could totally eliminate false negatives by classifying every mass as malignant. Obviously, this is not a realistic strategy. Still, it illustrates the fact that prediction involves striking a balance between the false positive rate and the false negative rate. In <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>, you will learn more sophisticated methods for measuring predictive accuracy that can be used to identify places where the error rate can be optimized depending on the costs of each type of error.</p></div></div><p>A total of 2 out of 100, or 2 percent of masses were incorrectly classified by the k-NN approach. While 98 percent accuracy seems impressive for a few lines of R code, we might try another iteration of the model to see whether we can improve the performance and reduce the number of values that have been incorrectly classified, particularly because the errors were dangerous false negatives.</p></div><div class="section" title="Step 5 – improving model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>Step 5 – improving model performance</h2></div></div></div><p>We will <a id="id248" class="indexterm"/>attempt two simple variations on our previous classifier. First, we will employ an alternative method for rescaling our numeric features. Second, we will try several different values for <span class="emphasis"><em>k</em></span>.</p><div class="section" title="Transformation – z-score standardization"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec15"/>Transformation – z-score standardization</h3></div></div></div><p>Although <a id="id249" class="indexterm"/>normalization is traditionally used for k-NN classification, it may not always be the most appropriate way to <a id="id250" class="indexterm"/>rescale features. Since the z-score standardized values have no predefined minimum and maximum, extreme values are not compressed towards the center. One might suspect that with a malignant tumor, we might see some very extreme outliers as the tumors grow uncontrollably. It might, therefore, be reasonable to allow the outliers to be weighted more heavily in the distance calculation. Let's see whether z-score standardization can improve our predictive accuracy.</p><p>To standardize a vector, we can use the R's built-in <code class="literal">scale()</code> function, which, by default, rescales values using the z-score standardization. The <code class="literal">scale()</code> function offers the additional benefit that it can be applied directly to a data frame, so we can avoid the use of the <code class="literal">lapply()</code> function. To create a z-score standardized version of the <code class="literal">wbcd</code> data, we can use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd_z &lt;- as.data.frame(scale(wbcd[-1]))</strong></span>
</pre></div><p>This command rescales all the features, with the exception of <code class="literal">diagnosis</code> and stores the result as the <code class="literal">wbcd_z</code> data frame. The <code class="literal">_z</code> suffix is a reminder that the values were z-score transformed.</p><p>To confirm that the transformation was applied correctly, we can look at the summary statistics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(wbcd_z$area_mean)</strong></span>
<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </strong></span>
<span class="strong"><strong>-1.4530 -0.6666 -0.2949  0.0000  0.3632  5.2460</strong></span>
</pre></div><p>The mean of a z-score standardized variable should always be zero, and the range should be fairly compact. A z-score greater than 3 or less than -3 indicates an extremely rare value. With this in mind, the transformation seems to have worked.</p><p>As we had done earlier, we need to divide the data into training and test sets, and then classify the test instances using the <code class="literal">knn()</code> function. We'll then compare the predicted labels to the actual labels using <code class="literal">CrossTable()</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; wbcd_train &lt;- wbcd_z[1:469, ]</strong></span>
<span class="strong"><strong>&gt; wbcd_test &lt;- wbcd_z[470:569, ]</strong></span>
<span class="strong"><strong>&gt; wbcd_train_labels &lt;- wbcd[1:469, 1]</strong></span>
<span class="strong"><strong>&gt; wbcd_test_labels &lt;- wbcd[470:569, 1]</strong></span>
<span class="strong"><strong>&gt; wbcd_test_pred &lt;- knn(train = wbcd_train, test = wbcd_test,</strong></span>
<span class="strong"><strong>                        cl = wbcd_train_labels, k = 21)</strong></span>
<span class="strong"><strong>&gt; CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,</strong></span>
<span class="strong"><strong>                       prop.chisq = FALSE)</strong></span>
</pre></div><p>Unfortunately, in the following table, the results of our new transformation show a slight decline in accuracy. The instances where we had correctly classified 98 percent of examples <a id="id251" class="indexterm"/>previously, we <a id="id252" class="indexterm"/>classified only 95 percent correctly this time. Making matters worse, we did no better at classifying the dangerous false negatives:</p><div class="mediaobject"><img src="graphics/3905_03_14.jpg" alt="Transformation – z-score standardization"/></div></div><div class="section" title="Testing alternative values of k"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec16"/>Testing alternative values of k</h3></div></div></div><p>We may <a id="id253" class="indexterm"/>be able do even better by examining performance across various <span class="emphasis"><em>k</em></span> values. Using the normalized training and test datasets, the same 100 records were classified using several different <span class="emphasis"><em>k</em></span> values. The number of false negatives and false positives are shown for each iteration:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>k value</p>
</th><th style="text-align: left" valign="bottom">
<p>False negatives</p>
</th><th style="text-align: left" valign="bottom">
<p>False positives</p>
</th><th style="text-align: left" valign="bottom">
<p>Percent classified incorrectly</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>4 percent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>2 percent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>3 percent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>15</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>3 percent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>2 percent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>27</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>4 percent</p>
</td></tr></tbody></table></div><p>Although the classifier was never perfect, the 1-NN approach was able to avoid some of the false negatives at the expense of adding false positives. It is important to keep in mind, however, that it would be unwise to tailor our approach too closely to our test data; after all, a different set of 100 patient records is likely to be somewhat different from those used to <a id="id254" class="indexterm"/>measure our performance.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip43"/>Tip</h3><p>If you need to be certain that a learner will generalize to future data, you might create several sets of 100 patients at random and repeatedly retest the result. The methods to carefully evaluate the performance of machine learning models will be discussed further in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span>.</p></div></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Summary</h1></div></div></div><p>In this chapter, we learned about classification using k-NN. Unlike many classification algorithms, k-NN does not do any learning. It simply stores the training data verbatim. Unlabeled test examples are then matched to the most similar records in the training set using a distance function, and the unlabeled example is assigned the label of its neighbors.</p><p>In spite of the fact that k-NN is a very simple algorithm, it is capable of tackling extremely complex tasks, such as the identification of cancerous masses. In a few simple lines of R code, we were able to correctly identify whether a mass was malignant or benign 98 percent of the time.</p><p>In the next chapter, we will examine a classification method that uses probability to estimate the likelihood that an observation falls into certain categories. It will be interesting to compare how this approach differs from k-NN. Later on, in <a class="link" href="ch09.html" title="Chapter 9. Finding Groups of Data – Clustering with k-means">Chapter 9</a>, <span class="emphasis"><em>Finding Groups of Data – Clustering with k-means</em></span>, we will learn about a close relative to k-NN, which uses distance measures for a completely different learning task.</p></div></body></html>
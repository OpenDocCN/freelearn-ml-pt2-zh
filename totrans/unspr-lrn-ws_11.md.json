["```py\n    from sklearn.datasets import make_blobs\n    from sklearn.cluster import KMeans\n    from sklearn.metrics import accuracy_score, silhouette_score\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import numpy as np\n    from scipy.spatial.distance import cdist\n    import math\n    np.random.seed(0)\n    %matplotlib inline\n    ```", "```py\n    seeds = pd.read_csv('Seed_Data.csv')\n    ```", "```py\n    seeds.head()\n    ```", "```py\n    X = seeds[['A','P','C','LK','WK','A_Coef','LKG']]\n    y = seeds['target']\n    ```", "```py\n    X.head()\n    ```", "```py\n    Activity 1.01.ipynb\n    def k_means(X, K):\n        # Keep track of history so you can see K-Means in action\n        centroids_history = []\n        labels_history = []\n\n        # Randomly initialize Kcentroids\n        rand_index = np.random.choice(X.shape[0], K)  \n        centroids = X[rand_index]\n        centroids_history.append(centroids)\n    The complete code for this step can be found at https://packt.live/2JPZ4M8.\n    ```", "```py\n    X_mat = X.values\n    ```", "```py\n    centroids, labels, centroids_history, labels_history = \\\n    k_means(X_mat, 3)\n    ```", "```py\n    print(labels)\n    ```", "```py\n    plt.scatter(X['A'], X['LK'])\n    plt.title('Wheat Seeds - Area vs Length of Kernel')\n    plt.show()\n    plt.scatter(X['A'], X['LK'], c=labels, cmap='tab20b')\n    plt.title('Wheat Seeds - Area vs Length of Kernel')\n    plt.show()\n    ```", "```py\n    silhouette_score(X[['A','LK']], labels)\n    ```", "```py\n    0.5875704550892767\n    ```", "```py\n    from sklearn.cluster import KMeans\n    from sklearn.cluster import AgglomerativeClustering\n    from sklearn.metrics import silhouette_score\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    ```", "```py\n    wine_df = pd.read_csv(\"wine_data.csv\")\n    print(wine_df.head())\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], wine_df.values[:,1])\n    plt.title(\"Wine Dataset\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    km = KMeans(3)\n    km_clusters = km.fit_predict(wine_df)\n    ```", "```py\n    ac = AgglomerativeClustering(3, linkage='average')\n    ac_clusters = ac.fit_predict(wine_df)\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], \\\n                wine_df.values[:,1], c=km_clusters)\n    plt.title(\"Wine Clusters from K-Means Clustering\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], \\\n                wine_df.values[:,1], c=ac_clusters)\n    plt.title(\"Wine Clusters from Agglomerative Clustering\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    print(\"Silhouette Scores for Wine Dataset:\\n\")\n    print(\"K-Means Clustering: \", silhouette_score\\\n         (wine_df, km_clusters))\n    print(\"Agg Clustering: \", silhouette_score(wine_df, ac_clusters))\n    ```", "```py\n    Silhouette Scores for Wine Dataset:\n    K-Means Clustering:  0.5809421087616886\n    Agg Clustering:  0.5988495817462\n    ```", "```py\n    from sklearn.cluster import DBSCAN\n    from sklearn.datasets import make_blobs\n    import matplotlib.pyplot as plt\n    import numpy as np\n    %matplotlib inline\n    X_blob, y_blob = make_blobs(n_samples=500, \\\n                                centers=4, n_features=2, \\\n                                random_state=800)\n    ```", "```py\n    plt.scatter(X_blob[:,0], X_blob[:,1])\n    plt.show()\n    ```", "```py\n    Activity3.01.ipynb\n    def scratch_DBSCAN(x, eps, min_pts): \n        \"\"\"\n        param x (list of vectors): your dataset to be clustered\n        param eps (float): neighborhood radius threshold \n        param min_pts (int): minimum number of points threshold for \n        a neighborhood to be a cluster\n        \"\"\"\n        # Build a label holder that is comprised of all 0s\n        labels = [0]* x.shape[0]\n        # Arbitrary starting \"current cluster\" ID\n        C = 0\n    The complete code for this step can be found at https://packt.live/3c1rONO.\n    ```", "```py\n    labels = scratch_DBSCAN(X_blob, 0.6, 5)\n    ```", "```py\n    plt.scatter(X_blob[:,0], X_blob[:,1], c=labels)\n    plt.title(\"DBSCAN from Scratch Performance\")\n    plt.show()\n    ```", "```py\n    from sklearn.cluster \\\n    import KMeans, AgglomerativeClustering, DBSCAN\n    from sklearn.metrics import silhouette_score\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    ```", "```py\n    # Load Wine data set\n    wine_df = pd.read_csv(\"wine_data.csv\")\n    # Show sample of data set\n    print(wine_df.head())\n    ```", "```py\n    plt.scatter(wine_df.values[:,0], wine_df.values[:,1])\n    plt.title(\"Wine Dataset\")\n    plt.xlabel(\"OD Reading\")\n    plt.ylabel(\"Proline\")\n    plt.show()\n    ```", "```py\n    # Generate clusters from K-Means\n    km = KMeans(3)\n    km_clusters = km.fit_predict(wine_df)\n    # Generate clusters using Agglomerative Hierarchical Clustering\n    ac = AgglomerativeClustering(3, linkage='average')\n    ac_clusters = ac.fit_predict(wine_df)\n    ```", "```py\n    db_param_options = [[20,5],[25,5],[30,5],[25,7],[35,7],[40,5]]\n    for ep,min_sample in db_param_options:\n        # Generate clusters using DBSCAN\n        db = DBSCAN(eps=ep, min_samples = min_sample)\n        db_clusters = db.fit_predict(wine_df)\n        print(\"Eps: \", ep, \"Min Samples: \", min_sample)\n        print(\"DBSCAN Clustering: \", \\\n              silhouette_score(wine_df, db_clusters))\n    ```", "```py\n    # Generate clusters using DBSCAN\n    db = DBSCAN(eps=40, min_samples = 5)\n    db_clusters = db.fit_predict(wine_df)\n    ```", "```py\n    plt.title(\"Wine Clusters from K-Means\")\n    plt.scatter(wine_df['OD_read'], wine_df['Proline'], \\\n                c=km_clusters,s=50, cmap='tab20b')\n    plt.show()\n    plt.title(\"Wine Clusters from Agglomerative Clustering\")\n    plt.scatter(wine_df['OD_read'], wine_df['Proline'], \\\n                c=ac_clusters,s=50, cmap='tab20b')\n    plt.show()\n    plt.title(\"Wine Clusters from DBSCAN\")\n    plt.scatter(wine_df['OD_read'], wine_df['Proline'], \\\n                c=db_clusters,s=50, cmap='tab20b')\n    plt.show()\n    ```", "```py\n    # Calculate Silhouette Scores\n    print(\"Silhouette Scores for Wine Dataset:\\n\")\n    print(\"K-Means Clustering: \", \\\n           silhouette_score(wine_df, km_clusters))\n    print(\"Agg Clustering: \", \\\n          silhouette_score(wine_df, ac_clusters))\n    print(\"DBSCAN Clustering: \", \\\n          silhouette_score(wine_df, db_clusters))\n    ```", "```py\n    Silhouette Scores for Wine Dataset:\n    K-Means Clustering:  0.5809421087616886\n    Agg Clustering:  0.5988495817462\n    DBSCAN Clustering:  0.5739675293567901\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')\n    df = df[['A', 'LK']]\n    df.head()\n    ```", "```py\n    cov = np.cov(df.values.T)\n    cov\n    ```", "```py\n    array([[8.46635078, 1.22470367],\n           [1.22470367, 0.19630525]])\n    ```", "```py\n    model = PCA(n_components=1)\n    sklearn_pca = model.fit_transform(df.values)\n    ```", "```py\n    eigenvectors, eigenvalues, _ = \\\n    np.linalg.svd(cov, full_matrices=False)\n    P = eigenvectors[0]\n    manual_pca = P.dot(df.values.T)\n    ```", "```py\n    plt.figure(figsize=(10, 7)) \n    plt.plot(sklearn_pca, label='Scikit-learn PCA')\n    plt.plot(manual_pca, label='Manual PCA', linestyle='--')\n    plt.xlabel('Sample')\n    plt.ylabel('Transformed Value')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    model.components_\n    ```", "```py\n    array([[0.98965371, 0.14347657]])\n    ```", "```py\n    P\n    ```", "```py\n    array([-0.98965371, -0.14347657])\n    ```", "```py\n    manual_pca *= -1\n    plt.figure(figsize=(10, 7))\n    plt.plot(sklearn_pca, label='Scikit-learn PCA')\n    plt.plot(manual_pca, label='Manual PCA', linestyle='--')\n    plt.xlabel('Sample')\n    plt.ylabel('Transformed Value')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    mean_vals = np.mean(df.values, axis=0)\n    offset_vals = df.values - mean_vals\n    manual_pca = P.dot(offset_vals.T)\n    ```", "```py\n    manual_pca *= -1\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.plot(sklearn_pca, label='Scikit-learn PCA')\n    plt.plot(manual_pca, label='Manual PCA', linestyle='--')\n    plt.xlabel('Sample')\n    plt.ylabel('Transformed Value')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from mpl_toolkits.mplot3d import Axes3D #Required for 3D plotting\n    ```", "```py\n    df = pd.read_csv('../Seed_Data.csv')[['A', 'LK', 'C']]\n    df.head()\n    ```", "```py\n    fig = plt.figure(figsize=(10, 7))\n    # Where Axes3D is required\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(df['A'], df['LK'], df['C'])\n    ax.set_xlabel('Area of Kernel')\n    ax.set_ylabel('Length of Kernel')\n    ax.set_zlabel('Compactness of Kernel')\n    ax.set_title('Expanded Seeds Dataset')\n    plt.show()\n    ```", "```py\n    model = PCA()\n    ```", "```py\n    model.fit(df.values)\n    ```", "```py\n    PCA(copy=True, iterated_power='auto', n_components=None, \n        random_state=None,\n        svd_solver='auto', tol=0.0, whiten=False)\n    ```", "```py\n    model.explained_variance_ratio_\n    ```", "```py\n    array([9.97794495e-01, 2.19418709e-03, 1.13183333e-05])\n    ```", "```py\n    model = PCA(n_components=1)\n    ```", "```py\n    data_transformed = model.fit_transform(df.values)\n    ```", "```py\n    data_restored = model.inverse_transform(data_transformed)\n    ```", "```py\n    fig = plt.figure(figsize=(10, 14))\n    # Original Data\n    ax = fig.add_subplot(211, projection='3d')\n    ax.scatter(df['A'], df['LK'], df['C'], label='Original Data');\n    ax.set_xlabel('Area of Kernel');\n    ax.set_ylabel('Length of Kernel');\n    ax.set_zlabel('Compactness of Kernel');\n    ax.set_title('Expanded Seeds Dataset');\n    # Transformed Data\n    ax = fig.add_subplot(212, projection='3d')\n    ax.scatter(data_restored[:,0], data_restored[:,1], \\\n               data_restored[:,2], label='Restored Data');\n    ax.set_xlabel('Area of Kernel');\n    ax.set_ylabel('Length of Kernel');\n    ax.set_zlabel('Compactness of Kernel');\n    ax.set_title('Restored Seeds Dataset');\n    ```", "```py\n    import pickle\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from keras.models import Sequential\n    from keras.layers import Dense\n    import tensorflow.python.util.deprecation as deprecation\n    deprecation._PRINT_DEPRECATION_WARNINGS = False\n    ```", "```py\n    with open('mnist.pkl', 'rb') as f:\n        data = pickle.load(f)\n    images = data['images']\n    labels = data['labels']\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    for i in range(10):\n        plt.subplot(2, 5, i + 1)\n        plt.imshow(images[i], cmap='gray')\n        plt.title(labels[i])\n        plt.axis('off')\n    ```", "```py\n    one_hot_labels = np.zeros((images.shape[0], 10))\n    for idx, label in enumerate(labels):\n        one_hot_labels[idx, label] = 1\n    one_hot_labels\n    ```", "```py\n    array([[0., 0., 0., ..., 0., 0., 0.],\n           [1., 0., 0., ..., 0., 0., 0.],\n           [0., 0., 0., ..., 0., 0., 0.],\n           ...,\n           [0., 0., 0., ..., 0., 0., 0.],\n           [0., 0., 0., ..., 0., 0., 1.],\n           [0., 0., 0., ..., 1., 0., 0.]])\n    ```", "```py\n    images = images.reshape((-1, 28 ** 2))\n    images = images / 255.\n    ```", "```py\n    model = Sequential([Dense(600, input_shape=(784,), \\\n                        activation='relu'), \\\n                        Dense(10, activation='softmax'),])\n    ```", "```py\n    model.compile(loss='categorical_crossentropy', \\\n                  optimizer='sgd', metrics=['accuracy'])\n    ```", "```py\n    model.fit(images, one_hot_labels, epochs=20)\n    ```", "```py\n    import pickle\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from keras.models import Model\n    from keras.layers import Input, Dense\n    import tensorflow.python.util.deprecation as deprecation\n    deprecation._PRINT_DEPRECATION_WARNINGS = False\n    ```", "```py\n    with open('mnist.pkl', 'rb') as f:\n        images = pickle.load(f)['images']\n    ```", "```py\n    images = images.reshape((-1, 28 ** 2))\n    images = images / 255.\n    ```", "```py\n    input_stage = Input(shape=(784,))\n    encoding_stage = Dense(100, activation='relu')(input_stage)\n    decoding_stage = Dense(784, activation='sigmoid')(encoding_stage)\n    autoencoder = Model(input_stage, decoding_stage)\n    ```", "```py\n    autoencoder.compile(loss='binary_crossentropy', \\\n                        optimizer='adadelta')\n    ```", "```py\n    autoencoder.fit(images, images, epochs=100)\n    ```", "```py\n    encoder_output = Model(input_stage, encoding_stage)\\\n                     .predict(images[:5])\n    ```", "```py\n    encoder_output = encoder_output.reshape((-1, 10, 10)) * 255\n    ```", "```py\n    decoder_output = autoencoder.predict(images[:5])\n    ```", "```py\n    decoder_output = decoder_output.reshape((-1, 28, 28)) * 255\n    ```", "```py\n    images = images.reshape((-1, 28, 28))\n    plt.figure(figsize=(10, 7))\n    for i in range(5):\n        plt.subplot(3, 5, i + 1)\n        plt.imshow(images[i], cmap='gray')\n        plt.axis('off')\n        plt.subplot(3, 5, i + 6)\n        plt.imshow(encoder_output[i], cmap='gray')\n        plt.axis('off')\n        plt.subplot(3, 5, i + 11)\n        plt.imshow(decoder_output[i], cmap='gray')\n        plt.axis('off')\n    ```", "```py\n    import pickle\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from keras.models import Model\n    from keras.layers \\\n    import Input, Conv2D, MaxPooling2D, UpSampling2D\n    import tensorflow.python.util.deprecation as deprecation\n    deprecation._PRINT_DEPRECATION_WARNINGS = False\n    ```", "```py\n    with open('mnist.pkl', 'rb') as f:\n        images = pickle.load(f)['images']\n    ```", "```py\n    images = images / 255.\n    ```", "```py\n    images = images.reshape((-1, 28, 28, 1))\n    ```", "```py\n    input_layer = Input(shape=(28, 28, 1,))\n    ```", "```py\n    hidden_encoding = \\\n    Conv2D(16, # Number of layers or filters in the weight matrix \\\n           (3, 3), # Shape of the weight matrix \\\n           activation='relu', \\\n           padding='same', # How to apply the weights to the images \\\n           )(input_layer)\n    ```", "```py\n    encoded = MaxPooling2D((2, 2))(hidden_encoding)\n    ```", "```py\n    hidden_decoding = \\\n    Conv2D(16, # Number of layers or filters in the weight matrix \\\n           (3, 3), # Shape of the weight matrix \\\n           activation='relu', \\\n           padding='same', # How to apply the weights to the images \\\n           )(encoded)\n    ```", "```py\n    upsample_decoding = UpSampling2D((2, 2))(hidden_decoding)\n    ```", "```py\n    decoded = \\\n    Conv2D(1, # Number of layers or filters in the weight matrix \\\n           (3, 3), # Shape of the weight matrix \\\n           activation='sigmoid', \\\n           padding='same', # How to apply the weights to the images \\\n           )(upsample_decoding)\n    ```", "```py\n    autoencoder = Model(input_layer, decoded)\n    ```", "```py\n    autoencoder.summary()\n    ```", "```py\n    autoencoder.compile(loss='binary_crossentropy', \\\n                        optimizer='adadelta')\n    ```", "```py\n    autoencoder.fit(images, images, epochs=20)\n    ```", "```py\n    encoder_output = Model(input_layer, encoded).predict(images[:5])\n    ```", "```py\n    encoder_output = encoder_output.reshape((-1, 14 * 14, 16))\n    ```", "```py\n    decoder_output = autoencoder.predict(images[:5])\n    ```", "```py\n    decoder_output = decoder_output.reshape((-1, 28, 28))\n    ```", "```py\n    images = images.reshape((-1, 28, 28))\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    for i in range(5):\n        # Plot the original digit images\n        plt.subplot(3, 5, i + 1)\n        plt.imshow(images[i], cmap='gray')\n        plt.axis('off')\n        # Plot the encoder output\n        plt.subplot(3, 5, i + 6)\n        plt.imshow(encoder_output[i], cmap='gray')\n        plt.axis('off')\n        # Plot the decoder output\n        plt.subplot(3, 5, i + 11)\n        plt.imshow(decoder_output[i], cmap='gray')\n        plt.axis('off')\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    ```", "```py\n    df = pd.read_csv('wine.data', header=None)\n    df.head()\n    ```", "```py\n    labels = df[0]\n    del df[0]\n    ```", "```py\n    model_pca = PCA(n_components=6)\n    wine_pca = model_pca.fit_transform(df)\n    ```", "```py\n    np.sum(model_pca.explained_variance_ratio_)\n    ```", "```py\n    0.99999314824536\n    ```", "```py\n    tsne_model = TSNE(random_state=0, verbose=1)\n    tsne_model\n    ```", "```py\n    wine_tsne = tsne_model.fit_transform\\\n                (wine_pca.reshape((len(wine_pca), -1)))\n    ```", "```py\n    wine_tsne.shape\n    ```", "```py\n    (178, 2)\n    ```", "```py\n    plt.figure(figsize=(10, 7))\n    plt.scatter(wine_tsne[:,0], wine_tsne[:,1])\n    plt.title('Low Dimensional Representation of Wine')\n    plt.show()\n    ```", "```py\n    MARKER = ['o', 'v', '^',]\n    plt.figure(figsize=(10, 7))\n    plt.title('Low Dimensional Representation of Wine')\n    for i in range(1, 4):\n        selections = wine_tsne[labels == i]\n        plt.scatter(selections[:,0], selections[:,1], \\\n                    marker=MARKER[i-1], label=f'Wine {i}', s=30)\n        plt.legend()\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    ```", "```py\n    df = pd.read_csv('wine.data', header=None)\n    df.head()\n    ```", "```py\n    labels = df[0]\n    del df[0]\n    ```", "```py\n    model_pca = PCA(n_components=6)\n    wine_pca = model_pca.fit_transform(df)\n    wine_pca = wine_pca.reshape((len(wine_pca), -1))\n    ```", "```py\n    MARKER = ['o', 'v', '^',]\n    for perp in [1, 5, 20, 30, 80, 160, 320]:\n        tsne_model = TSNE(random_state=0, verbose=1, perplexity=perp)\n        wine_tsne = tsne_model.fit_transform(wine_pca)\n        plt.figure(figsize=(10, 7))\n        plt.title(f'Low Dimensional Representation of Wine. \\\n                  Perplexity {perp}');\n        for i in range(1, 4):\n            selections = wine_tsne[labels == i]\n            plt.scatter(selections[:,0], selections[:,1], \\\n                        marker=MARKER[i-1], label=f'Wine {i}', s=30)\n            plt.legend()\n    plt.show()\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from sklearn.manifold import TSNE\n    ```", "```py\n    df = pd.read_csv('wine.data', header=None)\n    df.head()\n    ```", "```py\n    labels = df[0]\n    del df[0]\n    ```", "```py\n    model_pca = PCA(n_components=6)\n    wine_pca = model_pca.fit_transform(df)\n    wine_pca = wine_pca.reshape((len(wine_pca), -1))\n    ```", "```py\n    MARKER = ['o', 'v', '1', 'p' ,'*', '+', 'x', 'd', '4', '.']\n    for iterations in [250, 500, 1000]:\n        model_tsne = TSNE(random_state=0, verbose=1, \\\n                          n_iter=iterations, \\\n                          n_iter_without_progress=iterations)\n        wine_tsne = model_tsne.fit_transform(wine_pca)\n    ```", "```py\n        plt.figure(figsize=(10, 7))\n        plt.title(f'Low Dimensional Representation of Wine \\\n    (iterations = {iterations})')\n        for i in range(10):\n            selections = wine_tsne[labels == i]\n            plt.scatter(selections[:,0], selections[:,1], \\\n                        alpha=0.7, marker=MARKER[i], s=10);\n            x, y = selections.mean(axis=0)\n            plt.text(x, y, str(i), \\\n                     fontdict={'weight': 'bold', 'size': 30})\n    plt.show()\n    ```", "```py\n    import warnings\n    warnings.filterwarnings('ignore')\n    import langdetect \n    import matplotlib.pyplot \n    import nltk\n    nltk.download('wordnet')\n    nltk.download('stopwords')\n    import numpy \n    import pandas \n    import pyLDAvis \n    import pyLDAvis.sklearn \n    import regex \n    import sklearn \n    ```", "```py\n    path = 'latimeshealth.txt' \n    df = pandas.read_csv(path, sep=\"|\", header=None)\n    df.columns = [\"id\", \"datetime\", \"tweettext\"]\n    ```", "```py\n    def dataframe_quick_look(df, nrows):\n        print(\"SHAPE:\\n{shape}\\n\".format(shape=df.shape))\n        print(\"COLUMN NAMES:\\n{names}\\n\".format(names=df.columns))\n        print(\"HEAD:\\n{head}\\n\".format(head=df.head(nrows)))\n    dataframe_quick_look(df, nrows=2)\n    ```", "```py\n    raw = df['tweettext'].tolist() \n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=raw[:5])) \n    print(\"LENGTH:\\n{length}\\n\".format(length=len(raw))) \n    ```", "```py\n    Activity7.01-Activity7.03.ipynb\n    def do_language_identifying(txt): \n        try: \n            the_language = langdetect.detect(txt) \n        except: \n            the_language = 'none' \n        return the_language \n    def do_lemmatizing(wrd): \n        out = nltk.corpus.wordnet.morphy(wrd)\n        return (wrd if out is None else out)\n    The complete code for this step can be found at https://packt.live/3e3VifV.\n    ```", "```py\n    clean = list(map(do_tweet_cleaning, raw)) \n    ```", "```py\n    clean = list(filter(None.__ne__, clean)) \n    print(\"HEADLINES:\\n{lines}\\n\".format(lines=clean[:5]))\n    print(\"LENGTH:\\n{length}\\n\".format(length=len(clean)))\n    ```", "```py\n    clean_sentences = [\" \".join(i) for i in clean]\n    print(clean_sentences[0:10])\n    ```", "```py\n    number_words = 10\n    number_docs = 10\n    number_features = 1000\n    ```", "```py\n    vectorizer1 = sklearn.feature_extraction.text\\\n                  .CountVectorizer(analyzer=\"word\", \\\n                                   max_df=0.95, \\\n                                   min_df=10, \\\n                                   max_features=number_features)\n    clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n    print(clean_vec1[0]) \n    feature_names_vec1 = vectorizer1.get_feature_names()\n    ```", "```py\n    (0, 320)    1 \n    ```", "```py\n    Activity7.01-Activity7.03.ipynb\n    def perplexity_by_ntopic(data, ntopics): \n        output_dict = {\"Number Of Topics\": [], \\\n                       \"Perplexity Score\": []}\n        for t in ntopics: \n            lda = sklearn.decomposition\\\n                  .LatentDirichletAllocation(n_components=t, \\\n                                             learning_method=\"online\", \\\n                                             random_state=0)\n            lda.fit(data)\n            output_dict[\"Number Of Topics\"].append(t) \n            output_dict[\"Perplexity Score\"]\\\n            .append(lda.perplexity(data))\n    The complete code for this step can be found at https://packt.live/3e3VifV.\n    ```", "```py\n    lda = sklearn.decomposition.LatentDirichletAllocation\\\n          (n_components=optimal_num_topics, \\\n           learning_method=\"online\", \\\n           random_state=0)\n    lda.fit(clean_vec1) \n    ```", "```py\n    Activity7.01-Activity7.03.ipynb\n    def get_topics(mod, vec, names, docs, ndocs, nwords):\n        # word to topic matrix \n        W = mod.components_ \n        W_norm = W / W.sum(axis=1)[:, numpy.newaxis] \n        # topic to document matrix \n        H = mod.transform(vec) \n        W_dict = {} \n        H_dict = {} \n    The complete code for this step can be found at https://packt.live/3e3VifV.\n    ```", "```py\n    print(H_df)\n    ```", "```py\n    lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, \\\n                                        vectorizer1, R=10)\n    pyLDAvis.display(lda_plot)\n    ```", "```py\n    vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer\\\n                  (analyzer=\"word\", \\\n                   max_df=0.5,\\\n                   min_df=20,\\\n                   max_features=number_features,\\\n                   smooth_idf=False)\n    clean_vec2 = vectorizer2.fit_transform(clean_sentences)\n    print(clean_vec2[0]) \n    feature_names_vec2 = vectorizer2.get_feature_names() \n    ```", "```py\n    nmf = sklearn.decomposition.NMF(n_components=optimal_num_topics, \\\n                                    init=\"nndsvda\", \\\n                                    solver=\"mu\", \\\n                                    beta_loss=\"frobenius\", \\\n                                    random_state=0, \\\n                                    alpha=0.1, \\\n                                    l1_ratio=0.5)\n    nmf.fit(clean_vec2) \n    ```", "```py\n    W_df, H_df = get_topics(mod=nmf, vec=clean_vec2, \\\n                            names=feature_names_vec2, \\\n                            docs=raw, \\\n                            ndocs=number_docs, \\\n                            nwords=number_words)\n    print(W_df)\n    ```", "```py\n    print(H_df)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    import mlxtend.frequent_patterns\n    import mlxtend.preprocessing\n    import numpy\n    import pandas\n    ```", "```py\n    online = pandas.read_excel(io=\"./Online Retail.xlsx\", \\\n                               sheet_name=\"Online Retail\", \\\n                               header=0)\n    ```", "```py\n    online['IsCPresent'] = (online['InvoiceNo'].astype(str)\\\n                            .apply(lambda x: 1 \\\n                                   if x.find('C') != -1 else 0))\n    online1 = (online.loc[online[\"Quantity\"] > 0]\\\n                     .loc[online['IsCPresent'] != 1]\\\n                     .loc[:, [\"InvoiceNo\", \"Description\"]].dropna())\n    invoice_item_list = []\n    for num in list(set(online1.InvoiceNo.tolist())):\n        tmp_df = online1.loc[online1['InvoiceNo'] == num]\n        tmp_items = tmp_df.Description.tolist()\n        invoice_item_list.append(tmp_items)\n    ```", "```py\n    online_encoder = mlxtend.preprocessing.TransactionEncoder()\n    online_encoder_array = \\\n    online_encoder.fit_transform(invoice_item_list)\n    online_encoder_df = pandas.DataFrame(\\\n                        online_encoder_array, \\\n                        columns=online_encoder.columns_)\n    online_encoder_df.loc[20125:20135, \\\n                          online_encoder_df.columns.tolist()\\\n                          [100:110]]\n    ```", "```py\n    mod_colnames_minsupport = mlxtend.frequent_patterns\\\n                              .apriori(online_encoder_df, \\\n                                       min_support=0.01, \\\n                                       use_colnames=True)\n    mod_colnames_minsupport.loc[0:6]\n    ```", "```py\n    mod_colnames_minsupport[mod_colnames_minsupport['itemsets'] \\\n    == frozenset({'10 COLOUR SPACEBOY PEN'})]\n    ```", "```py\n    mod_colnames_minsupport['length'] = (mod_colnames_minsupport\\\n                                         ['itemsets']\\\n                                         .apply(lambda x: len(x)))\n    mod_colnames_minsupport[(mod_colnames_minsupport['length'] == 2) \\\n                            & (mod_colnames_minsupport['support'] \\\n                               >= 0.02)\\\n                            &(mod_colnames_minsupport['support'] \\\n                               < 0.021)]\n    ```", "```py\n    mod_colnames_minsupport.hist(\"support\", grid=False, bins=30)\n    plt.xlabel(\"Support of item\")\n    plt.ylabel(\"Number of items\")\n    plt.title(\"Frequency distribution of Support\")\n    plt.show()\n    ```", "```py\n    rules = mlxtend.frequent_patterns\\\n            .association_rules(mod_colnames_minsupport, \\\n                               metric=\"confidence\", \\\n                               min_threshold=0.6, \\\n                               support_only=False)\n    rules.loc[0:6]\n    ```", "```py\n    print(\"Number of Associations: {}\".format(rules.shape[0]))\n    ```", "```py\n    rules.plot.scatter(\"support\", \"confidence\", \\\n                       alpha=0.5, marker=\"*\")\n    plt.xlabel(\"Support\")\n    plt.ylabel(\"Confidence\")\n    plt.title(\"Association Rules\")\n    plt.show()\n    ```", "```py\n    rules.hist(\"lift\", grid=False, bins=30)\n    plt.xlabel(\"Lift of item\")\n    plt.ylabel(\"Number of items\")\n    plt.title(\"Frequency distribution of Lift\")\n    plt.show()\n    ```", "```py\nrules.hist(\"leverage\", grid=False, bins=30)\nplt.xlabel(\"Leverage of item\")\nplt.ylabel(\"Number of items\")\nplt.title(\"Frequency distribution of Leverage\")\nplt.show()\n```", "```py\nplt.hist(rules[numpy.isfinite(rules['conviction'])]\\\n         .conviction.values, bins = 3)\nplt.xlabel(\"Conviction of item\")\nplt.ylabel(\"Number of items\")\nplt.title(\"Frequency distribution of Conviction\")\nplt.show()\n```", "```py\n    get_ipython().run_line_magic('matplotlib', 'inline')\n    import matplotlib.pyplot as plt\n    import numpy\n    import pandas\n    import seaborn\n    import sklearn.model_selection\n    import sklearn.neighbors\n    seaborn.set()\n    ```", "```py\n    rand = numpy.random.RandomState(100)\n    vals = rand.randn(1000)  # standard normal\n    vals[375:] += 3.5\n    ```", "```py\n    fig, ax = plt.subplots(figsize=(14, 10))\n    ax.hist(vals, bins=50, density=True, label='Sampled Values')\n    ax.plot(vals, -0.005 - 0.01 * numpy.random.random(len(vals)), \\\n            '+k', label='Individual Points')\n    ax.legend(loc='upper right')\n    plt.show()\n    ```", "```py\n    bandwidths = 10 ** numpy.linspace(-1, 1, 100)\n    grid = sklearn.model_selection.GridSearchCV\\\n           (estimator=sklearn.neighbors.KernelDensity(kernel=\"gaussian\"),\n            param_grid={\"bandwidth\": bandwidths}, cv=10)\n    grid.fit(vals[:, None])\n    ```", "```py\n    best_bandwidth = grid.best_params_[\"bandwidth\"]\n    print(\"Best Bandwidth Value: {}\".format(best_bandwidth))\n    ```", "```py\n    fig, ax = plt.subplots(figsize=(14, 10))\n    ax.hist(vals, bins=50, density=True, alpha=0.75, \\\n            label='Sampled Values')\n    x_vec = numpy.linspace(-4, 8, 10000)[:, numpy.newaxis]\n    log_density = numpy.exp(grid.best_estimator_.score_samples(x_vec))\n    ax.plot(x_vec[:, 0], log_density, \\\n            '-', linewidth=4, label='Kernel = Gaussian')\n    ax.legend(loc='upper right')\n    plt.show()\n    ```", "```py\n    # define the file base path\n    base_path = \"./metro-jul18-dec18/{yr_mon}/{yr_mon}\\\n    -metropolitan-street.csv\"\n    print(base_path)\n    ```", "```py\n    ./metro-jul18-dec18/{yr_mon}/{yr_mon}-metropolitan-street.csv\n    ```", "```py\n    yearmon_list = [\"2018-0\" + str(i) if i <= 9 else \"2018-\" + str(i) \\\n                    for i in range(7, 13)]\n    print(yearmon_list)\n    ```", "```py\n    ['2018-07', '2018-08', '2018-09', \\\n     '2018-10', '2018-11', '2018-12']\n    ```", "```py\n    data_yearmon_list = []\n    # read each year month file individually\n    #print summary statistics\n    for idx, i in enumerate(yearmon_list):\n        df = pandas.read_csv(base_path.format(yr_mon=i), \\\n                             header=0)\n        data_yearmon_list.append(df)\n        if idx == 0:\n            print(\"Month: {}\".format(i))\n            print(\"Dimensions: {}\".format(df.shape))\n            print(\"Head:\\n{}\\n\".format(df.head(2)))\n    # concatenate the list of year month data frames together\n    london = pandas.concat(data_yearmon_list)\n    ```", "```py\n    Activity9.01-Activity9.02.ipynb\n    print(\"Dimensions - Full Data:\\n{}\\n\".format(london.shape))\n    print(\"Unique Months - Full Data:\\n{}\\n\".format(london[\"Month\"].unique()))\n    print(\"Number of Unique Crime Types - Full Data:\\n{}\\n\"\\\n          .format(london[\"Crime type\"].nunique()))\n    The complete code for this step can be found at https://packt.live/2wmh5yj.\n    ```", "```py\n    london_subset = london[[\"Month\", \"Longitude\", \"Latitude\", \\\n                            \"Crime type\"]]\n    london_subset.head(5)\n    ```", "```py\n    crime_bicycle_jul = london_subset\\\n                        [(london_subset[\"Crime type\"] \\\n                          == \"Bicycle theft\") \\\n                         & (london_subset[\"Month\"] == \"2018-07\")]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                      crime_bicycle_jul, kind=\"kde\")\n    ```", "```py\n    crime_bicycle_sept = london_subset\\\n                         [(london_subset[\"Crime type\"] \n                           == \"Bicycle theft\") \n                          & (london_subset[\"Month\"] == \"2018-09\")]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                      crime_bicycle_sept, kind=\"kde\")\n    ```", "```py\n    crime_bicycle_dec = london_subset\\\n                        [(london_subset[\"Crime type\"] \\\n                          == \"Bicycle theft\") \n                         & (london_subset[\"Month\"] == \"2018-12\")]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                      crime_bicycle_dec, kind=\"kde\")\n    ```", "```py\n    crime_shoplift_aug = london_subset\\\n                         [(london_subset[\"Crime type\"] \\\n                           == \"Shoplifting\") \n                          & (london_subset[\"Month\"] == \"2018-08\")]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                      crime_shoplift_aug, kind=\"kde\")\n    ```", "```py\n    crime_shoplift_oct = london_subset\\\n                         [(london_subset[\"Crime type\"] \\\n                           == \"Shoplifting\") \\\n                          & (london_subset[\"Month\"] == \"2018-10\")]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                      crime_shoplift_oct, kind=\"kde\")\n    ```", "```py\n    crime_shoplift_nov = london_subset\\\n                         [(london_subset[\"Crime type\"] \\\n                           == \"Shoplifting\") \\\n                          & (london_subset[\"Month\"] == \"2018-11\")]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                      crime_shoplift_nov, kind=\"kde\")\n    ```", "```py\n    crime_burglary_jul = london_subset\\\n                        [(london_subset[\"Crime type\"] == \"Burglary\") \\\n                         & (london_subset[\"Month\"] == \"2018-07\")]\n    seaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                      crime_burglary_jul, kind=\"kde\")\n    ```", "```py\ncrime_burglary_oct = london_subset\\\n                     [(london_subset[\"Crime type\"] == \"Burglary\")\\\n                      & (london_subset[\"Month\"] == \"2018-10\")]\nseaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                  crime_burglary_oct, kind=\"kde\")\n```", "```py\ncrime_burglary_dec = london_subset\\\n                     [(london_subset[\"Crime type\"] == \"Burglary\")\\\n                      & (london_subset[\"Month\"] == \"2018-12\")]\nseaborn.jointplot(\"Longitude\", \"Latitude\", \\\n                  crime_burglary_dec, kind=\"kde\")\n```"]
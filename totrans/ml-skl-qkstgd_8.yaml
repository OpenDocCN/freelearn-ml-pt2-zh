- en: Performance Evaluation Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your method of performance evaluation will vary by the type of machine learning
    algorithm that you choose to implement. In general, there are different metrics
    that can potentially determine how well your model is performing at its given
    task for classification, regression, and unsupervised machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore how the different performance evaluation methods
    can help you to better understand your model. The chapter will be split into three
    sections, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance evaluation for classification algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance evaluation for regression algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance evaluation for unsupervised algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, NumPy ≥ 1.15.1, Matplotlib ≥ 3.0.0, and Scikit-plot ≥ 0.3.7 installed
    on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the code in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2EY4nJU](http://bit.ly/2EY4nJU)'
  prefs: []
  type: TYPE_NORMAL
- en: Why is performance evaluation critical?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is key for you to understand why we need to evaluate the performance of
    a model in the first place. Some of the potential reasons why performance evaluation
    is critical are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It prevents overfitting**:Overfitting occurs when your algorithm hugs the
    data too tightly and makes predictions that are specific to only one dataset.
    In other words, your model cannot generalize its predictions outside of the data
    that it was trained on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It prevents underfitting**:This is the exact opposite of overfitting. In
    this case, the model is very generic in nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Understanding predictions**:Performance evaluation methods will help you
    to understand, in greater detail, how your model makes predictions, along with
    the nature of those predictions and other useful information, such as the accuracy
    of your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance evaluation for classification algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to evaluate the performance of classification, let''s consider the
    two classification algorithms that we have built in this book: k-nearest neighbors
    and logistic regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step will be to implement both of these algorithms in the fraud detection
    dataset. We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we read the fraud detection dataset into our notebook
    and split the data into the features and target variables, as usual. We then split
    the data into training and test sets, and build the k-nearest neighbors and logistic
    regression models in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to evaluate the performance of a single
    model: k-nearest neighbors. You will also learn how to compare and contrast multiple
    models. Therefore, you will learn about the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalized confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area under the curve (`auc` score)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cumulative gains curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lift curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-S statistic plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calibration plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validated box plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the visualizations in this section will require a package titled `scikit-plot`.
    The `scikit-plot` package is very effective, and it is used to visualize the various
    performance measures of machine learning models. It was specifically made for
    models that are built using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to install `scikit-plot` on your local machine, using `pip` in Terminal,
    we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using the Anaconda distribution to manage your Python packages,
    you can install `scikit-plot` by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we have used the accuracy as the sole measure of model performance.
    That was fine, because we have a balanced dataset. A balanced dataset is a dataset
    in which there are almost the same numbers of labels for each category. In the
    dataset that we are working with, 8,000 labels belong to the fraudulent transactions,
    while 12,000 belong to the non-fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a situation in which 90% of our data had non-fraudulent transactions,
    while only 10% of the transactions had fraudulent cases. If the classifier reported
    an accuracy of 90%, it wouldn't make sense, because most of the data that it has
    seen thus far were the non-fraudulent cases and it has seen very little of the
    fraudulent cases. So, even if it classified 90% of the cases accurately, it would
    mean that most of the cases that it classified would belong to the non-fraudulent
    cases. That would provide no value to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **confusion matrix** is a performance evaluation technique that can be used
    in such cases, which do not involve a balanced dataset. The confusion matrix for
    our dataset would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fb4aee3-f8bc-4ef3-810d-e6102797028e.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix for fraudulent transactions
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the confusion matrix is to maximize the number of true positives
    and true negatives, as this gives the correct predictions; it also minimizes the
    number of false negatives and false positives, as they give us the wrong predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your problem, the false positives might be more problematic than
    the false negatives (and vice versa), and thus, the goal of building the right
    classifier should be to solve your problem in the best possible way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement the confusion matrix in scikit-learn, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f160e07-2447-4a3f-a8e2-a6c2076082f2.png)'
  prefs: []
  type: TYPE_IMG
- en: The confusion matrix output from our classifier for fraudulent transactions
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we create a set of predictions using the `.predict()`method
    on the test training data, and then we use the `confusion_matrix()`function on
    the test set of the target variable and the predictions that were created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding confusion matrix looks almost perfect, as most cases are classified
    into the true positive and true negative categories, along the main diagonal.
    Only 46 cases are classified incorrectly, and this number is almost equal. This
    means that the numbers of false positives and false negatives are minimal and
    balanced, and one does not outweigh the other. This is an example of the ideal
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Three other metrics that can be derived from the confusion matrix are **precision**,
    **recall,** and **F1-score**. A high value of precision indicates that not many
    non-fraudulent transactions are classified as fraudulent, while a high value of
    recall indicates that most of the fraudulent cases were predicted correctly.
  prefs: []
  type: TYPE_NORMAL
- en: The F1-score is the weighted average of the precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compute the precision and recall by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a13ae6ab-fe62-495e-896f-a5130759e7f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification report
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, we use the `classificiation_report()` function with
    two arguments: the test set of the target variable and the prediction variable
    that we created for the confusion matrix earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: In the output, the precision, recall, and F1-score are all high, because we
    have built the ideal machine learning model. These values range from 0 to 1, with
    1 being the highest.
  prefs: []
  type: TYPE_NORMAL
- en: The normalized confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **normalized confusion matrix** makes it easier for the data scientist to
    visually interpret how the labels are being predicted. In order to construct a
    normalized confusion matrix, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following normalized confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3c87197-7a32-4095-a4f8-7ea43559918d.png)'
  prefs: []
  type: TYPE_IMG
- en: Normalized confusion matrix for the K-NN model
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, the predicted labels are along the *x* axis, while the
    true (or actual) labels are along the *y* axis. We can see that the model has
    0.01, or 1%, of the predictions for the fraudulent transactions incorrect, while
    0.99, or 99%, of the fraudulent transactions have been predicted correctly. We
    can also see that the K-NN model predicted 100% of the non-fraudulent transactions
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can compare the performance of the logistic regression model by using
    a normalized confusion matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following normalized confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/125d0f78-4a87-415d-a3d0-f383eac558fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Normalized confusion matrix for the logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding confusion matrix, it is clear that the logistic regression
    model only predicted 42% of the non-fraudulent transactions correctly. This indicates,
    almost instantly, that the k-nearest neighbor model performed better.
  prefs: []
  type: TYPE_NORMAL
- en: Area under the curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The curve, in this case, is the **receiver operator characteristics** (**ROC**)
    curve. This is a plot between the true positive rate and the false positive rate.
    We can plot this curve as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7bfbb951-cb4e-4052-b9fe-4ca056717412.png)'
  prefs: []
  type: TYPE_IMG
- en: ROC curve
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, first, we create a set of probabilities for each of the
    predicted labels. For instance, the predicted label of 1 would have a certain
    set of probabilities associated with it, while the label 0 would have a certain
    set of probabilities associated with it. Using these probabilities, we use the
    `roc_curve()`function, along with the target test set, to generate the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding curve is an example of a perfect ROC curve. The preceding curve
    has a true positive rate of 1.0, which indicates accurate predictions, while it
    has a false positive rate of 0, which indicates a lack of wrong predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a curve also has the most area under the curve, as compared to the curves
    of models that have a lower accuracy. In order to compute the area under the curve
    score, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This produces a score of 0.99\. A higher `auc` score indicates a better performing
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Cumulative gains curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building multiple machine learning models, it is important to understand
    which of the models in question produces the type of predictions that you want
    it to generate. The **cumulative gains curve** helps you with the process of model
    comparison, by telling you about the percentage of a category/class that appears
    within a percentage of the sample population for a particular model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simple terms, in the fraud detection dataset, we might want to pick a model
    that can predict a larger number of fraudulent transactions, as opposed to a model
    that cannot. In order to construct the cumulative gains plot for the k-nearest
    neighbors model, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8db348f7-5c94-4fe9-9382-216276efc920.png)'
  prefs: []
  type: TYPE_IMG
- en: Cumulative gains plot for the k-nearest neighbors model
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the `scikit-plot` package, which generates the preceding plot.
    We then compute the probabilities for the target variable, which, in this case,
    are the probabilities if a particular mobile transaction is fraudulent or not
    on the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we use the `plot_cumulative_gain()`function on these probabilities
    and the test data target labels, in order to generate the preceding plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How do we interpret the preceding plot? We simply look for the point at which
    a certain percentage of the data contains 100% of the target class. This is illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f432ac3-7149-40a5-9ed4-d7fb0c139012.png)'
  prefs: []
  type: TYPE_IMG
- en: Point at which 100% of the target class exists
  prefs: []
  type: TYPE_NORMAL
- en: The point defined in the preceding diagram corresponds to a value of 0.3 on
    the *x* axis and 1.0 on the *y* axis. This means that 0.3 to 1.0 (or 30% to 100%)
    of the data will consist of the target class, 1, which are the fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can also be interpreted as follows: 70% of the total data will contain
    100% of the fraudulent transaction predictions if you use the k-nearest neighbors
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s compute the cumulative gains curve for the logistic regression
    model, and see if it is different. In order to do this, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/438c000a-eafd-4c9e-b1c2-b604b65f299e.png)'
  prefs: []
  type: TYPE_IMG
- en: Cumulative gains plot for the logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plot is similar to the cumulative gains plot that was previously
    produced by the K-NN model, in that 70% of the data contains 100% of the target
    class. Therefore, using either the K-NN or the logistic regression model will
    yield similar results.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is a good practice to compare how different models behave by using
    the cumulative gains chart, in order to gain a fundamental understanding of how
    your model makes predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Lift curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **lift curve** gives you information about how well you can make predictions
    by using a machine learning model, as opposed to when you are not using one. In
    order to construct a lift curve for the k-nearest neighbor model, we use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cba26353-d293-4a22-9cae-f6e6315a92bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Lift curve for the K-NN model
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we interpret the preceding lift curve? We have to look for the point
    at which the curve dips. This is illustrated for you in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94c581f3-70aa-49f6-8d32-298e88da3e6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Point of interest in the lift curve
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding plot, the point that is highlighted is the point that we want
    to look for in any lift curve. The point tells us that 0.3, or 30%, of our total
    data will perform 3.5 times better when using the K-NN predictive model, as opposed
    to when we do not use any model at all to predict the fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can construct the lift curve for the logistic regression model, in
    order to compare and contrast the performance of the two models. We can do this
    by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a99e0447-e440-49ac-9f06-98e3c4779cce.png)'
  prefs: []
  type: TYPE_IMG
- en: Lift curve for the logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: Although the plot tells us that 30% of the data will see an improved performance
    (similar to that of the K-NN model that we built earlier in order to predict the
    fraudulent transactions), there is a difference when it comes to predicting the
    non-fraudulent transactions (the blue line).
  prefs: []
  type: TYPE_NORMAL
- en: For a small percentage of the data, the lift curve for the non-fraudulent transactions
    is actually lower than the baseline (the dotted line). This means that the logistic
    regression model does worse than not using a predictive model for a small percentage
    of the data when it comes to predicting the non-fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: K-S statistic plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **K-S statistic plot**, or the **Kolmogorov Smirnov** statistic plot, is
    a plot that tells you whether the model gets confused when it comes to predicting
    the different labels in your dataset. In order to illustrate what the term *confused*
    means in this case, we will construct the K-S statistic plot for the K-NN model
    by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d914ee1-1c8e-406d-8137-0e1885305642.png)'
  prefs: []
  type: TYPE_IMG
- en: K-S statistic plot for the K-NN model
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding plot, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: The dotted line is the distance between the predictions for the fraudulent transactions
    (the yellow line at the bottom) and the non-fraudulent transactions (the blue
    line at the top). This distance is 0.985, as indicated by the plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A K-S statistic score that is close to 1 is usually a good indication that the
    model does not get confused between predicting the two different target labels,
    and can make a clear distinction when it comes to predicting the labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding plot, the score of 0.985 can be observed as the difference
    between the two classes of predictions, for up to 70% (0.7) of the data. This
    can be observed along the *x* axis, as a threshold of 0.7 still has the maximum
    separation distance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now compute the K-S statistic plot for the logistic regression model,
    in order to compare which of the two models provides a better distinction in predictions
    between the two class labels. We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acbae8fc-3ed1-4f58-b477-86449a66fcf2.png)'
  prefs: []
  type: TYPE_IMG
- en: K-S statistic plot for the logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: Although the two models have the same separation score of 0.985, the threshold
    at which the separation occurs is quite different. In the case of logistic regression,
    this distance only occurs for the bottom 43% of the data, since the maximum separation
    starts at a threshold of 0.57, along the *x* axis.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the k-nearest neighbors model, which has a large distance for
    about 70% of the total data, is much better at making predictions about fraudulent
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Calibration plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **calibration plot**, as the name suggests, tells you how well calibrated
    your model is. A well-calibrated model will have a prediction score equal to the
    fraction of the positive class (in this case, the fraudulent transactions). In
    order to plot a calibration plot, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following calibration plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ce52083-1671-4729-b9f4-f13b2696bca9.png)'
  prefs: []
  type: TYPE_IMG
- en: Calibration plot for the two models
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we compute the probability that the positive class (fraudulent transactions)
    will be predicted for each model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we store these probabilities and the model names in a list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we use the `plot_calibration_curve()`function from the `scikit-plot`
    package with these probabilities, the test labels, and the model names, in order
    to create the calibration plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This results in the preceding calibration plot, which can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The dotted line represents the perfect calibration plot. This is because the
    mean prediction value has the exact value of the fraction of the positive class
    at each and every point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the plot, it is clear that the k-nearest neighbors model is much better
    calibrated than the calibration plot of the logistic regression model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is because the calibration plot of the k-nearest neighbors model follows
    that of the ideal calibration plot much more closely than the calibration plot
    of the logistic regression model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **learning curve** is a plot that compares how the training accuracy scores
    and the test accuracy scores vary as the number of samples/rows added to the data
    increases. In order to construct the learning curve for the k-nearest neighbors
    model, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07066563-bdff-4d65-b444-a13ba26b829f.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning curve for the K-NN model
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding curve, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: The training score and the test score are only the highest when the number of
    samples is 15,000\. This suggests that even if we had only 15,000 samples (instead
    of the 17,500), we would still get the best possible results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Anything under the 15,000 samples will mean that the test cross-validated scores
    will be much lower than the training scores, suggesting that the model is overfit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-validated box plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this plot, we compare the cross-validated accuracy scores of multiple models
    by making use of box plots. In order to do so, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c850faa-94c3-4070-90a1-5d19b28b7899.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-validated box plot
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we store the models that we want to compare in a list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we initialize two empty lists, in order to store the results of the cross-validated
    accuracy scores and the names of the models, so that we can use them later, in
    order to create the box plots.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then iterate over each model in the list of models, and use the `model_selection.KFold()`function
    in order to split the data into a five-fold cross-validated set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we extract the five-fold cross-validated scores by using the
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`model_selection.cross_val_scores()`function and append the scores, along with
    the model names, into the lists that we initialized at the beginning of the code.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, a box plot is created, displaying the cross-validated scores in a box
    plot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The list that we created consists of the five cross-validated scores, along
    with the model names. A box plot takes these five scores for each model and computes
    the min, max, median, first, and third quartiles, in the form of a box plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding plot, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that the K-NN model has the highest value of accuracy, with the
    lowest difference between the minimum and maximum values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The logistic regression model, on the other hand, has the greatest difference
    between the minimum and maximum values, and has an outlier in its accuracy score,
    as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performance evaluation for regression algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three main metrics that you can use to evaluate the performance of
    the regression algorithm that you built, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Root mean squared error** (**RMSE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, you will learn what the three metrics are, how they work,
    and how you can implement them using scikit-learn. The first step is to build
    the linear regression algorithm. We can do this by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Mean absolute error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mean absolute error is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8406d412-d72f-4741-886f-84610b68299e.png)'
  prefs: []
  type: TYPE_IMG
- en: MAE formula
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding formula, ![](img/6b9d56b7-d0be-4784-bc16-c611b85cc303.png)
    represents the true (or actual) value of the output, while the ![](img/bfcb7fbb-92c2-4a74-b7d8-a723cfe02682.png)
    hat represents the predicted output values. Therefore, by computing the summation
    of the difference between the true value and the predicted value of the output
    for each row in your data, and then dividing it by the total number of observations,
    you get the mean value of the absolute error.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement the MAE in scikit-learn, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `mean_absolute_error()`function from the `metrics`
    module in scikit-learn is used to compute the MAE. It takes in two arguments:
    the real/true output, which is the target, and the predictions, which are the
    predicted outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mean squared error is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c21cb85-d073-4e73-8dc4-bee43b5bc301.png)'
  prefs: []
  type: TYPE_IMG
- en: MSE formula
  prefs: []
  type: TYPE_NORMAL
- en: The preceding formula is similar to the formula that we saw for the mean absolute
    error, except that instead of computing the absolute difference between the true
    and predicted output values, we compute the square of the difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement the MSE in scikit-learn, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We use the `mean_squared_error()`function from the `metrics` module, with the
    real/true output values and the predictions as arguments. The mean squared error
    is better at detecting larger errors, because we square the errors, instead of
    depending on only the difference.
  prefs: []
  type: TYPE_NORMAL
- en: Root mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The root mean squared error is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1960f41c-0b17-4d04-86de-f8af9e2b6b77.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding formula is very similar to that of the mean squared error, except
    for the fact that we take the square root of the MSE formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compute the RMSE in scikit-learn, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we use the `mean_squared_error()`function with the true/real
    output and the predictions, and then we take the square root of this answer by
    using the `np.sqrt()`function from the `numpy` package.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the MAE and the MSE, the RMSE is the best possible metric that you
    can use in order to evaluate the linear regression model, since this detects large
    errors and gives you the value in terms of the output units. The key takeaway
    from using any one of the three metrics is that the value that these `metrics`
    gives you should be as low as possible, indicating that the model has relatively
    low error values.
  prefs: []
  type: TYPE_NORMAL
- en: Performance evaluation for unsupervised algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn how to evaluate the performance of an unsupervised
    machine learning algorithm, such as the k-means algorithm. The first step is to
    build a simple k-means model. We can do so by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a simple k-means model with two clusters, we can proceed to
    evaluate the model''s performance. The different visual performance charts that
    can be deployed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Elbow plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silhouette analysis plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you will learn how to create and interpret each of the preceding
    plots.
  prefs: []
  type: TYPE_NORMAL
- en: Elbow plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to construct an elbow plot, we use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8acf8d3f-2174-46de-a8e1-d9ab51207753.png)'
  prefs: []
  type: TYPE_IMG
- en: Elbow plot
  prefs: []
  type: TYPE_NORMAL
- en: The elbow plot is a plot between the number of clusters that the model takes
    into consideration along the *x* axis and the sum of the squared errors along
    the *y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `plot_elbow_curve()`function with the k-means model, the data, and
    the number of clusters that we want to evaluate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we define a range of 1 to 19 clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding plot, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that the elbow point, or the point at which the sum of the squared
    errors (*y* axis) starts decreasing very slowly, is where the number of clusters
    is 4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plot also gives you another interesting metric on the *y* axis (right-hand
    side), which is the clustering duration (in seconds). This indicates the amount
    of time it took for the algorithm to create the clusters, in seconds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to evaluate the performances of the three
    different types of machine learning algorithms: classification, regression, and
    unsupervised.'
  prefs: []
  type: TYPE_NORMAL
- en: For the classification algorithms, you learned how to evaluate the performance
    of a model by using a series of visual techniques, such as the confusion matrix,
    normalized confusion matrix, area under the curve, K-S statistic plot, cumulative
    gains plot, lift curve, calibration plot, learning curve, and cross-validated
    box plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the regression algorithms, you learned how to evaluate the performance
    of a model by using three metrics: the mean squared error, mean absolute error,
    and root mean squared error.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for the unsupervised machine learning algorithms, you learned how to
    evaluate the performance of a model by using the elbow plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Congratulations! You have now made it to the end of your machine learning journey
    with scikit-learn. You''ve made your way through eight chapters, which gave you
    the quickest entry point into the wonderful world of machine learning with one
    of the world''s most popular machine learning frameworks: scikit-learn.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, you learned about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What machine learning is (in a nutshell) and the different types and applications
    of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised machine learning algorithms, such as K-NN, logistic regression, Naive
    Bayes, support vector machines, and linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised machine learning algorithms, such as the k-means algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms that can perform both classification and regression, such as decision
    trees, random forests, and gradient-boosted trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope that you can make the best possible use of the application based on the
    knowledge that this book has given you, allowing you to solve many real-world
    problems by using machine learning as your tool!
  prefs: []
  type: TYPE_NORMAL

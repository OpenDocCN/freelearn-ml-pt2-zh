<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Voting</h1>
                </header>
            
            <article>
                
<p>The most intuitive of all ensemble learning methods is <strong>majority voting</strong>. It is intuitive, as the aim is to output the most popular (or most voted for) of the base learner's predictions. This chapter covers the basic theory as well as practical implementations concerning majority voting. By the end of this chapter, you will be able to do the following:</p>
<ul>
<li>Understand majority voting</li>
<li><span>Understand </span>the difference between hard and soft majority voting and their respective strengths and weaknesses</li>
<li>Implement both versions in Python</li>
<li>Utilize the voting technique to improve the performance of classifiers on the breast cancer dataset</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter03">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter03</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2M52VY7">http://bit.ly/2M52VY7</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hard and soft voting</h1>
                </header>
            
            <article>
                
<p>Majority voting is the simplest ensemble learning technique that allows the combination of multiple base learner's predictions. Similar to how elections work, the algorithm assumes that each base learner is a voter and each class is a contender. The algorithm takes votes into consideration in order to elect a contender as the winner. There are two main approaches to combining multiple predictions with voting: one is hard voting and the other is soft voting. We present both approaches here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hard voting</h1>
                </header>
            
            <article>
                
<p>Hard voting combines a number of predictions by assuming that the most voted class is the winner. In a simple case of two classes and three base learners, if a target class has at least two votes, it becomes the ensemble's output, as shown in the following diagram. Implementing a hard voting classifier is as simple as counting the votes for each target class:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-587 image-border" src="assets/cf089773-85dd-4f2a-84de-a4c10f12254f.png" style="width:25.17em;height:22.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Voting with two classes and three base learners</div>
<p>For example, let's say that there are three different base learners, who are predicting whether a sample belongs to one of three classes with a certain probability (<em>Table 1</em>).</p>
<p>In the following table, each learner predicts the probability that the instance belongs to a certain class:</p>
<div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Class A</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Class B</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Class C</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Learner 1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Learner 2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.48</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.52</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Learner 3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="packt_figref CDPAlignCenter CDPAlign">Assigned class probabilities</div>
<p>In this example, class A has two votes, while class C has only one. According to hard voting, class A will be the prediction of the ensemble. It's a fairly robust method of combining many base learners, although it doesn't take into account that some classes may be chosen by a base learner only because they are marginally better than the others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Soft voting</h1>
                </header>
            
            <article>
                
<p>Soft voting takes into account the probability of the predicted classes. In order to combine the predictions, soft voting calculates the average probability of each class and assumes that the winner is the class with the highest average probability.In the simple case of three base learners and two classes, we must take into consideration the predicted probability for each class and average them across the three learners:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-940 image-border" src="assets/9a69ad7d-0f1d-4116-abd2-572423922f69.png" style="width:21.50em;height:19.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Soft voting with two classes and three base learners</span></span></div>
<p>Using our previous example, and by taking the average of each column for <em>Table 1</em>, we can expand it, adding a row for the average probability.</p>
<p>The following table shows the predicted probabilities for each class by each learner, as well as the average probability:</p>
<div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Class A</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Class B</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Class C</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Learner 1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.5</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Learner 2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.48</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.52</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Learner 3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.4</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Average</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.3</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.36</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.34</p>
</td>
</tr>
</tbody>
</table>
</div>
<div class="CDPAlignCenter CDPAlign packt_figref">Predicted probabilities for each class by each learner, as well as the average probability</div>
<p>As we can see, class A has an average probability of 0.3, class B has an average probability of 0.36, and class C has an average probability of 0.34, making class B the winner. Note that class B is not selected by any base learner as the predicted class, but by combining the predicted probabilities, class B arises as the best compromise between the predictions.</p>
<p>In order for soft voting to be more effective than hard voting, the base classifiers must produce good estimates regarding the probability that a sample belongs to a specific class. If the probabilities are meaningless (for example, if they are always 100% for one class and 0% for all others), soft voting could be even worse than hard voting.</p>
<div class="packt_infobox">A note on voting: it is impossible to have a perfect voting system, as has been proved by Dr. Kenneth Arrow with his impossibility theorem. Nonetheless, certain types of voting systems can better reflect the preferences of a population. Soft voting better reflects the individual learner's preferences, as it takes into account the rating (probabilities) instead of the ranking (predicted class).<br/>
<br/>
For more on the impossibility theorem, see<span> A difficulty in the concept of social welfare.</span><span> <em>Arrow, K.J., 1950</em>. <em>Journal of political economy</em>, </span>58<span>(4), pp.328-346</span><span>.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">​Python implementation</h1>
                </header>
            
            <article>
                
<p>The simplest way to implement hard voting in Python is to use scikit-l<span>earn</span><span> </span>to create base learners, train them on some data, and combine their predictions on test data. In order to do so, we will go through the following steps:</p>
<ol>
<li>Load the data and split it into train and test sets</li>
<li>Create some base learners</li>
<li>Train them on the train data</li>
<li>Produce predictions for the test data</li>
<li>Combine predictions using hard voting</li>
<li>Compare the individual learner's predictions as well as the combined predictions with the ground truth (actual correct classes)</li>
</ol>
<p>Although scikit-learn has implementations for voting, by creating a custom implementation, it will be easier to understand how the algorithm works. Furthermore, it will enable us to better understand how to process and analyze a base learner's outputs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom hard voting implementation</h1>
                </header>
            
            <article>
                
<p class="mce-root">In order to implement a custom hard voting solution, we will use three base learners: a <strong>Perceptron</strong> (a neural network with a single neuron), a <strong>Support Vector Machine</strong> (<strong>SVM</strong>), and a <strong>Nearest Neighbor</strong>. These are contained in the <kbd>sklearn.linear_model</kbd>, <kbd>sklearn.svm</kbd>, and <kbd>sklearn.neighbors</kbd> packages. Furthermore, we will use the <kbd>argmax</kbd> function from NumPy. This function returns the index of an array's (or array-like data structure) element with the highest value. Finally, <kbd>accuracy_score</kbd> will calculate the accuracy of each classifier on our test data:</p>
<pre class="mce-root"># --- SECTION 1 ---<br/># Import the required libraries<br/>from sklearn import datasets, linear_model, svm, neighbors<br/>from sklearn.metrics import accuracy_score<br/>from numpy import argmax<br/># Load the dataset<br/>breast_cancer = datasets.load_breast_cancer()<br/>x, y = breast_cancer.data, breast_cancer.target</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We then instantiate our base learners. We hand-picked their hyperparameters to ensure that they are diverse in order to produce a well-performing ensemble. As <kbd>breast_cancer</kbd> is a classification dataset, we use <kbd>SVC</kbd>, the classification version of SVM, along with <kbd>KNeighborsClassifier</kbd> and <kbd>Perceptron</kbd>. Furthermore, we set the random state of <kbd>Perceptron</kbd> to 0 in order to ensure the reproducibility of our example:</p>
<pre># --- SECTION 2 ---<br/># Instantiate the learners (classifiers)<br/>learner_1 = neighbors.KNeighborsClassifier(n_neighbors=5)<br/>learner_2 = linear_model.Perceptron(tol=1e-2, random_state=0)<br/>learner_3 = svm.SVC(gamma=0.001)</pre>
<p>We split the data into train and test sets, using 100 instances for our test set and train our base learners on the train set:</p>
<pre># --- SECTION 3 ---<br/># Split the train and test samples<br/>test_samples = 100<br/>x_train, y_train = x[:-test_samples], y[:-test_samples]<br/>x_test, y_test = x[-test_samples:], y[-test_samples:]<br/><br/># Fit learners with the train data<br/>learner_1.fit(x_train, y_train)<br/>learner_2.fit(x_train, y_train)<br/>learner_3.fit(x_train, y_train)</pre>
<p>By storing each base learner's prediction in <kbd>predictions_1</kbd>, <span><kbd>predictions_2</kbd>, and <kbd>predictions_3</kbd></span>, we can further analyze and combine them into our ensemble. Note that we trained each classifier individually; additionally, as well as that each classifier produces predictions for the test data autonomously. As mentioned in <span class="cdp-organizer-chapter-number"><a href="d7921006-351e-4c21-ab54-f1dc834557dc.xhtml"/></span><span class="cdp-organizer-chapter-number"><a href="d7921006-351e-4c21-ab54-f1dc834557dc.xhtml">Chapter 2</a></span><span class="cdp-organizer-chapter-number"><a href="d7921006-351e-4c21-ab54-f1dc834557dc.xhtml"/>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Getting Started with Ensemble Learning</span></span></em>, this is the main characteristic of non-generative ensemble methods:</p>
<pre>#--- SECTION 4 ---<br/># Each learner predicts the classes of the test data<br/>predictions_1 = learner_1.predict(x_test)<br/>predictions_2 = learner_2.predict(x_test)<br/>predictions_3 = learner_3.predict(x_test)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Following the predictions, we combine the predictions of each base learner for each test instance. The<span> </span><kbd>hard_predictions</kbd><span> list will contain the ensemble's predictions (output). By iterating over every test sample with </span><kbd>for i in range(test_samples)</kbd><span>, we count the total number of votes that each class has received from the three base learners. As the dataset contains only two classes, we need a list of two elements: </span><kbd>counts = [0 for _ in range(2)]</kbd><span>. In </span><kbd># --- SECTION 3 ---</kbd><span>, we stored each base learner's predictions in an array. Each one of those array's elements contains the index of the instance's predicted class (in our case, 0 and 1). Thus, we increase the corresponding element's value in </span><kbd>counts[predictions_1[i]]</kbd><span> by one to count the base learner's vote. Then, <kbd>argmax(counts)</kbd> returns the element (class) with the highest number of votes:</span></p>
<pre># --- SECTION 5 ---<br/># We combine the predictions with hard voting<br/>hard_predictions = []<br/># For each predicted sample<br/>for i in range(test_samples):<br/>    # Count the votes for each class<br/>    counts = [0 for _ in range(2)]<br/>    counts[predictions_1[i]] = counts[predictions_1[i]]+1<br/>    counts[predictions_2[i]] = counts[predictions_2[i]]+1<br/>    counts[predictions_3[i]] = counts[predictions_3[i]]+1<br/>    # Find the class with most votes<br/>    final = argmax(counts)<br/>    # Add the class to the final predictions<br/>    hard_predictions.append(final) </pre>
<p>Finally, we calculate the accuracy of the individual base learners as well as the ensemble with <kbd>accuracy_score</kbd>, and print them on screen:</p>
<pre># --- SECTION 6 ---<br/># Accuracies of base learners<br/>print('L1:', accuracy_score(y_test, predictions_1))<br/>print('L2:', accuracy_score(y_test, predictions_2))<br/>print('L3:', accuracy_score(y_test, predictions_3))<br/># Accuracy of hard voting<br/>print('-'*30)<br/>print('Hard Voting:', accuracy_score(y_test, hard_predictions))</pre>
<p>The final output is as follows:</p>
<pre>L1: 0.94<br/>L2: 0.93<br/>L3: 0.88<br/>------------------------------<br/>Hard Voting: 0.95</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing our results using Python</h1>
                </header>
            
            <article>
                
<p>The final accuracy achieved is 1% better than the best of the three classifiers (the <strong>k-Nearest Neighbors</strong> (<strong>k-NN</strong>) classifier). We can visualize the learner's errors in order to examine why the ensemble performs in this specific way.</p>
<p>First, we <kbd>import matplotlib</kbd> and use a specific <kbd>seaborn-paper</kbd> plotting style with <kbd>mpl.style.use('seaborn-paper')</kbd>:</p>
<pre># --- SECTION 1 ---<br/># Import the required libraries<br/>import matplotlib as mpl<br/>import matplotlib.pyplot as plt<br/>mpl.style.use('seaborn-paper')</pre>
<p>Then, we calculate the errors by subtracting our prediction from the actual target. Thus, w<span>e get a -1 each time the learner predicts a positive (1) when the true class is negative (0), and a 1 when it predicts a negative (0) while the true class is positive (1). If the prediction is correct, we get a zero (0):</span></p>
<pre># --- SECTION 2 ---<br/># Calculate the errors <br/>errors_1 = y_test-predictions_1<br/>errors_2 = y_test-predictions_2<br/>errors_3 = y_test-predictions_3</pre>
<p>For each base learner, we plot the instances where they have predicted the wrong class. Our aim is to scatter plot the <kbd>x</kbd> and <kbd>y</kbd> lists. These lists will contain the instance number (the <kbd>x</kbd> list) and the type of error (the <kbd>y</kbd> list). With <kbd>plt.scatter</kbd>, we can specify the coordinates of our points using the aforementioned lists, as well as specify how these points are depicted. This is important in order to ensure that we can simultaneously visualize all the errors of the classifiers as well as the relationship between them.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The default shape for each point is a circle. By specifying the <kbd>marker</kbd> parameter, we can alter this shape. Furthermore, with the <kbd>s</kbd> <span>parameter, </span>we can specify the marker's size. Thus, the first learner (k-NN) will have a round shape of size 120, the second learner (Perceptron) <span>will have an <kbd>x</kbd> shape of size 60, and the third learner </span>(SVM) will have a round shape of size 20. The <kbd>if not errors_*[i] == 0</kbd> <span>guard </span>ensures that we will not store correctly classified instances:</p>
<pre># --- SECTION 3 ---<br/># Discard correct predictions and plot each learner's errors<br/>x=[]<br/>y=[]<br/>for i in range(len(errors_1)):<br/>    if not errors_1[i] == 0:<br/>        x.append(i)<br/>        y.append(errors_1[i])<br/>plt.scatter(x, y, s=120, label='Learner 1 Errors')<br/><br/>x=[]<br/>y=[]<br/>for i in range(len(errors_2)):<br/>     if not errors_2[i] == 0:<br/>         x.append(i)<br/>         y.append(errors_2[i])<br/>plt.scatter(x, y, marker='x', s=60, label='Learner 2 Errors')<br/><br/>x=[]<br/>y=[]<br/>for i in range(len(errors_3)):<br/>    if not errors_3[i] == 0:<br/>        x.append(i)<br/>        y.append(errors_3[i])<br/>plt.scatter(x, y, s=20, label='Learner 3 Errors')</pre>
<p> Finally, we specify the figure's title and labels, and plot the legend:</p>
<pre>plt.title('Learner errors')<br/>plt.xlabel('Test sample')<br/>plt.ylabel('Error')<br/>plt.legend()<br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As the following shows, there are five samples where at least two learners predict the wrong class. These are the 5 cases out of the 100 that the ensemble predicts wrong, as the most voted class is wrong, thus producing a 95% accuracy. In all other cases, two out of three learners predict the correct class, thus the ensemble predicts the correct class as it is the most voted:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-589 image-border" src="assets/0ba4d397-fc7a-43f0-8009-ef44003bcf9a.png" style="width:41.25em;height:28.25em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Learner errors on the test set</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using scikit-learn</h1>
                </header>
            
            <article>
                
<p><span>The scikit-learn library includes many ensemble learning algorithms, including voting. In order to implement hard voting, we will follow the same procedure as we did previously, except this time, we will not implement the individual fitting, predicting, and voting ourselves. Instead, we will use the provided implementation, which enables quick and easy training and testing.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hard voting implementation</h1>
                </header>
            
            <article>
                
<p>Similarly to our custom implementation, we import the required libraries, split our train and test data, and instantiate our base learners. Furthermore, we import scikit-learn's <kbd>VotingClassifier</kbd> voting implementation from the <kbd>sklearn.ensemble</kbd> package, as follows:</p>
<pre># --- SECTION 1 ---<br/># Import the required libraries<br/>from sklearn import datasets, linear_model, svm, neighbors<br/>from sklearn.ensemble import VotingClassifier<br/>from sklearn.metrics import accuracy_score<br/># Load the dataset<br/>breast_cancer = datasets.load_breast_cancer()<br/>x, y = breast_cancer.data, breast_cancer.target<br/><br/># Split the train and test samples<br/>test_samples = 100<br/>x_train, y_train = x[:-test_samples], y[:-test_samples]<br/>x_test, y_test = x[-test_samples:], y[-test_samples:]<br/><br/># --- SECTION 2 ---<br/># Instantiate the learners (classifiers)<br/>learner_1 = neighbors.KNeighborsClassifier(n_neighbors=5)<br/>learner_2 = linear_model.Perceptron(tol=1e-2, random_state=0)<br/>learner_3 = svm.SVC(gamma=0.001)</pre>
<p>Following the above code, we instantiate the <kbd><span>VotingClassifier</span></kbd> <span>class,</span> passing as a parameter a list of tuples with the names and objects of our base classifiers. Note that passing the parameters outside of a list will result in an error:</p>
<pre># --- SECTION 3 ---<br/># Instantiate the voting classifier<br/>voting = VotingClassifier([('KNN', learner_1),<br/>                           ('Prc', learner_2),<br/>                           ('SVM', learner_3)])</pre>
<p>Now, having instantiated the classifier, we can use it in the same way as any other classifier, without having to tend to each base learner individually. The following two sections execute the fitting and prediction for all base learners as well as the calculation of the most voted class for each test instance:</p>
<pre># --- SECTION 4 ---<br/># Fit classifier with the training data<br/>voting.fit(x_train, y_train)<br/><br/># --- SECTION 5 ---<br/># Predict the most voted class<br/>hard_predictions = voting.predict(x_test)</pre>
<p>Finally, we can print the accuracy of the ensemble:</p>
<pre># --- SECTION 6 ---<br/># Accuracy of hard voting<br/>print('-'*30)<br/>print('Hard Voting:', accuracy_score(y_test, hard_predictions))</pre>
<p>This is the same as our custom implementation:</p>
<pre>------------------------------<br/>Hard Voting: 0.95</pre>
<div class="mce-root packt_tip"><span><br/>
Note that </span><kbd>VotingClassifier</kbd> <span>will not fit the objects that you pass as parameters, but will, instead, clone them and fit the cloned objects. Thus, if you try to print the accuracy of each individual base learner on the test set, you will get <kbd>NotFittedError</kbd>, as the objects that you have access to are, in fact, not fitted. This is the only drawback of using scikit-learn's implementation over a custom one.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Soft voting implementation</h1>
                </header>
            
            <article>
                
<p>Scikit-learn's implementation allows for soft voting as well. The only requirement is that the base learners implement the <kbd>predict_proba</kbd> function. In our example, <kbd>Perceptron</kbd> does not implement the function at all, while <kbd>SVC</kbd> only produces probabilities when it is passed the <kbd>probability=True</kbd> argument. Having these limitations in mind, we swap our <kbd>Perceptron</kbd> with a Naive Bayes classifier implemented in the <kbd>sklearn.naive_bayes</kbd> package.</p>
<p>To actually use soft voting, the <kbd>VotingClassifier</kbd> object must be initialized with the <kbd>voting='soft'</kbd> argument. Except for the changes mentioned here, the majority of the code remains the same. Load the libraries and datasets, and produce a train/test split as follows:</p>
<pre># --- SECTION 1 ---<br/># Import the required libraries<br/>from sklearn import datasets, naive_bayes, svm, neighbors<br/>from sklearn.ensemble import VotingClassifier<br/>from sklearn.metrics import accuracy_score<br/># Load the dataset<br/>breast_cancer = datasets.load_breast_cancer()<br/>x, y = breast_cancer.data, breast_cancer.target<br/><br/># Split the train and test samples<br/>test_samples = 100<br/>x_train, y_train = x[:-test_samples], y[:-test_samples]<br/>x_test, y_test = x[-test_samples:], y[-test_samples:]</pre>
<p>Instantiate the base learners and voting classifier. We use a Gaussian Naive Bayes implemented as <kbd>GaussianNB</kbd>. <span>Note that we use <kbd>probability=True</kbd> in order for the <kbd>GaussianNB</kbd> object to be able to produce probabilities:</span></p>
<pre># --- SECTION 2 ---<br/># Instantiate the learners (classifiers)<br/>learner_1 = neighbors.KNeighborsClassifier(n_neighbors=5)<br/>learner_2 = naive_bayes.GaussianNB()<br/>learner_3 = svm.SVC(gamma=0.001, probability=True)<br/><br/># --- SECTION 3 ---<br/># Instantiate the voting classifier<br/>voting = VotingClassifier([('KNN', learner_1),<br/>                           ('NB', learner_2),<br/>                           ('SVM', learner_3)],<br/>                            voting='soft')</pre>
<p>We fit both <kbd>VotingClassifier</kbd> and the individual learners. We want to analyze our results, and, as mentioned earlier, the classifier will not fit the objects that we pass as arguments, but will instead clone them. Thus, we have to manually fit our learners as follows:</p>
<pre># --- SECTION 4 ---<br/># Fit classifier with the training data<br/>voting.fit(x_train, y_train)<br/>learner_1.fit(x_train, y_train)<br/>learner_2.fit(x_train, y_train)<br/>learner_3.fit(x_train, y_train)</pre>
<p>We predict the test set's targets using both the voting ensemble and the individual learners:</p>
<pre># --- SECTION 5 ---<br/># Predict the most probable class<br/>hard_predictions = voting.predict(x_test)<br/><br/># --- SECTION 6 ---<br/># Get the base learner predictions<br/>predictions_1 = learner_1.predict(x_test)<br/>predictions_2 = learner_2.predict(x_test)<br/>predictions_3 = learner_3.predict(x_test)</pre>
<p>Finally, we print the accuracy of each base learner and the soft voting ensemble's accuracy:</p>
<pre># --- SECTION 7 ---<br/># Accuracies of base learners<br/>print('L1:', accuracy_score(y_test, predictions_1))<br/>print('L2:', accuracy_score(y_test, predictions_2))<br/>print('L3:', accuracy_score(y_test, predictions_3))<br/># Accuracy of hard voting<br/>print('-'*30)<br/>print('Hard Voting:', accuracy_score(y_test, hard_predictions))</pre>
<p>The final output is as follows:</p>
<pre>L1: 0.94<br/>L2: 0.96<br/>L3: 0.88<br/>------------------------------<br/>Hard Voting: 0.94</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing our results</h1>
                </header>
            
            <article>
                
<p>As is evident, the accuracy achieved by soft voting is 2% worse than the best learner and on par with the second-best learner. We would like to analyze our results similarly to how we analyzed the performance of our hard voting custom implementation. But as soft voting takes into account the predicted class probabilities, we cannot use the same approach. Instead, we will plot the predicted probability for each instance to be classified as positive by each base learner as well as the average probability of the ensemble.</p>
<p>Again, we <kbd>import matplotlib</kbd> <span>and set the plotting style:</span></p>
<pre># --- SECTION 1 ---<br/># Import the required libraries<br/>import matplotlib as mpl<br/>import matplotlib.pyplot as plt<br/>mpl.style.use('seaborn-paper')</pre>
<p>We calculate the ensemble's errors with <span><kbd>errors = y_test-hard_predictions</kbd> and get the predicted probabilities of each base learner with the <kbd>predict_proba(x_test)</kbd> function. All base learners implement this function, as it is a requirement for utilizing them in a soft voting ensemble:</span></p>
<pre><br/># --- SECTION 2 ---<br/># Get the wrongly predicted instances<br/># and the predicted probabilities for the whole test set<br/>errors = y_test-hard_predictions<br/><br/>probabilities_1 = learner_1.predict_proba(x_test)<br/>probabilities_2 = learner_2.predict_proba(x_test)<br/>probabilities_3 = learner_3.predict_proba(x_test)</pre>
<p>Following this, for each wrongly classified instance, <span>we store the predicted probability that the instance belongs to in class 0. We also implement this for each base learner, as well as their average</span>. Each <kbd>probabilities_*</kbd> array,  is a two-dimensional array. Each row contains the predicted probability that the corresponding instance belongs to class 0 or class 1. Thus, storing one of the two is sufficient. In the case of a dataset with <em>N</em> classes, we would have to store at least <em>N</em>-1 probabilities in order to get a clear picture:</p>
<pre># --- SECTION 2 ---<br/># Store the predicted probability for <br/># each wrongly predicted instance, for each base learner<br/># as well as the average predicted probability<br/>#<br/>x=[]<br/>y_1=[]<br/>y_2=[]<br/>y_3=[]<br/>y_avg=[]<br/><br/>for i in range(len(errors)):<br/>    if not errors[i] == 0:<br/>         x.append(i)<br/>         y_1.append(probabilities_1[i][0])<br/>         y_2.append(probabilities_2[i][0])<br/>         y_3.append(probabilities_3[i][0])<br/>         y_avg.append((probabilities_1[i][0]+<br/>                       probabilities_2[i][0]+probabilities_3[i][0])/3)</pre>
<p>Finally, we plot the probabilities as bars of different widths with <kbd>plt.bar</kbd>. This ensures that any overlapping bars will still be visible. The third <span><kbd>plt.bar</kbd> argument dictates the bar's width. We scatter plot the average probability as a black 'X' and ensure that it will be plotted over any bar with <kbd>zorder=10</kbd>. Finally, we plot a threshold line at 0.5 probability with <kbd>plt.plot(y, c='k', linestyle='--')</kbd>, ensuring that it will be a black dotted line with <kbd>c='k', linestyle='--'</kbd>. If the average probability is above the line, the sample is classified as positive, as follows:</span></p>
<pre><br/># --- SECTION 3 ---<br/># Plot the predicted probaiblity of each base learner as <br/># a bar and the average probability as an X<br/>plt.bar(x, y_1, 3, label='KNN')<br/>plt.bar(x, y_2, 2, label='NB')<br/>plt.bar(x, y_3, 1, label='SVM')<br/>plt.scatter(x, y_avg, marker='x', c='k', s=150, <br/>            label='Average Positive', zorder=10)<br/><br/>y = [0.5 for x in range(len(errors))]<br/>plt.plot(y, c='k', linestyle='--')<br/><br/>plt.title('Positive Probability')<br/>plt.xlabel('Test sample')<br/>plt.ylabel('probability')<br/>plt.legend()<br/>plt.show()</pre>
<p>The preceding code outputs the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-590 image-border" src="assets/557d63bc-fadb-42d6-bdea-ace7257b02da.png" style="width:40.67em;height:27.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Predicted and average probabilities for the test set</span></div>
<p class="mce-root">As we can see, only two samples have an extreme average probability (sample 22 with p = 0.98 and 67 with p = 0.001). The other four are quite close to 50%. For three out of these four samples, SVM seems to assign a very high probability to the wrong class, thus greatly affecting the average probability. If SVM did not overestimate the probability of these samples as much, the ensemble could well out perform each individual learner. For the two extreme cases, nothing can be done, as all three learners agree on the miss classification. We can try to swap our SVM for another k-NN with a significantly higher number of neighbors. In this case, <kbd>(learner_3 = neighbors.KNeighborsClassifier(n_neighbors=50) )</kbd>, we can see that the ensemble's accuracy is greatly increased. The ensemble's accuracies and errors are as follows:</p>
<pre>L1: 0.94<br/>L2: 0.96<br/>L3: 0.95<br/>------------------------------<br/>Hard Voting: 0.97</pre>
<p>Take a look at the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-591 image-border" src="assets/f7a6f54d-3272-4c1a-bfc4-430a293f2a76.png" style="width:41.92em;height:28.75em;"/></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span>Predicted and average probabilities for the test set with two k-NNs</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented the most basic ensemble learning method: voting. Although it is quite simple, it can prove to be effective and an easy way to combine many machine learning models. We presented hard and soft voting, a custom implementation for hard voting, and scikit-learn implementations for both hard and soft voting. Finally, we presented a way to analyze the ensemble's performance by plotting each base learner's errors using <kbd>matplotlib</kbd>. The chapter's key points are summarized below.</p>
<p><strong>Hard voting</strong> assumes that the most voted class is the winner. <span><strong>Soft voting</strong> assumes that the class with the highest average probability is the winner. </span><span><strong>Soft voting</strong> requires that the base classifiers predict the <strong>probability</strong> of each class for every instance with a relatively high accuracy. </span><span>Scikit-learn implements voting ensembles using the</span> <kbd>VotingClassifier</kbd> <span>class. </span><span>An array of tuples in the form of</span> <kbd>[(learner_name, learner_object),…]</kbd> <span>is passed to </span><kbd>VotingClassifier</kbd><span>. The </span><kbd>VotingClassifier</kbd> <span>does not train the objects passed as arguments. Instead, a copy is generated and trained. </span><span>The default mode of <kbd><span>VotingClassifier </span></kbd>implements hard voting. </span><span>To use soft voting, pass the </span><kbd>voting='soft'</kbd><span> </span><span>argument </span><span>to the constructor. </span><span>Soft voting requires that the base learners return probabilities for each prediction. </span><span>If a base learner greatly takes over or underestimates the probabilities, the ensemble's predictive ability will suffer.</span></p>
<p><span>In the next chapter, we will discuss about another non-generative method, Stacking, and how it can be utilized in both regression and classification problems.</span></p>


            </article>

            
        </section>
    </body></html>
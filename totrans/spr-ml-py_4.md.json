["```py\nfrom scipy import sparse\nimport numpy as np\n\nrows = np.array([0,0,0,0,1,1,1,2,2,2,2,3,3,3])\ncols = np.array([0,1,4,5,2,3,4,0,4,6,7,1,4,7])\ndata = np.array([5.,1.,2.5,4.5,3.5,2.,3.,1.5,\n                 4.,4.5,4.,1.,1.,5.])\n# Make a sparse matrix\nR = sparse.csr_matrix((data, (rows, cols)), shape = (4, 8))\nprint(R.todense())\n```", "```py\n[[5\\. 1\\. 0\\. 0\\. 2.5 4.5 0\\. 0\\. ]\n [0\\. 0\\. 3.5 2\\. 3\\. 0\\. 0\\. 0\\. ]\n [1.5 0\\. 0\\. 0\\. 4\\. 0\\. 4.5 4\\. ]\n [0\\. 1\\. 0\\. 0\\. 1\\. 0\\. 0\\. 5\\. ]]\n```", "```py\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute the sim matrix\nsim = cosine_similarity(R.T).round(3)\nsim\ntop_k = 3\nuser_3 = np.array([0., 1., 0., 0., 1., 0., 0., 5.])\n\n# compute dot product btwn user vec and the sim matrix\nrecommendations = user_3.dot(sim)\nitem_indices = np.arange(recommendations.shape[0])\n\n# now arg sort descending (most similar items first)\norder = np.argsort(-recommendations)[:top_k]\nitems = item_indices[order]\n\n# zip them together (item, predicted rating)\nlist(zip(items, recommendations[order]))\n```", "```py\n[(7, 6.130000000000001), (4, 4.326), (1, 4.196)]\n```", "```py\nclass RecommenderMixin(six.with_metaclass(ABCMeta)):\n    \"\"\"Mixin interface for recommenders.\n\n    This class should be inherited by recommender algorithms. It provides an\n    abstract interface for generating recommendations for a user, and a\n    function for creating recommendations for all users.\n    \"\"\"\n    @abstractmethod\n    def recommend_for_user(self, R, user, n=10, filter_previously_seen=False,\n                           return_scores=True, **kwargs):\n        \"\"\"Generate recommendations for a user.\n\n        A method that should be overridden by subclasses to create\n        recommendations via their own prediction strategy.\n        \"\"\"\n\n    def recommend_for_all_users(self, R, n=10,\n                                filter_previously_seen=False,\n                                return_scores=True, **kwargs):\n        \"\"\"Create recommendations for all users.\"\"\"\n        return (\n            self.recommend_for_user(\n                R, user, n=n, filter_previously_seen=filter_previously_seen,\n                return_scores=return_scores, **kwargs)\n            for user in xrange(R.shape[0]))\n```", "```py\n def __init__(self, R, k=10):\n        # check the array, but don't copy if not needed\n        R = check_array(R, dtype=np.float32, copy=False) # type: np.ndarray\n\n        # save the hyper param for later use later\n        self.k = k\n        self.similarity = self._compute_sim(R, k)\n\n    def _compute_sim(self, R, k):\n        # compute the similarity between all the items. This calculates the\n        # similarity between each ITEM\n        sim = cosine_similarity(R.T)\n\n        # Only keep the similarities of the top K, setting all others to zero\n        # (negative since we want descending)\n        not_top_k = np.argsort(-sim, axis=1)[:, k:] # shape=(n_items, k)\n\n        if not_top_k.shape[1]: # only if there are cols (k < n_items)\n            # now we have to set these to zero in the similarity matrix\n            row_indices = np.repeat(range(not_top_k.shape[0]),\n                                    not_top_k.shape[1])\n            sim[row_indices, not_top_k.ravel()] = 0.\n\n        return sim\n\n    def recommend_for_user(self, R, user, n=10,\n                           filter_previously_seen=False,\n                           return_scores=True, **kwargs):\n        \"\"\"Generate predictions for a single user.\n```", "```py\n# check the array and get the user vector\nR = check_array(R, dtype=np.float32, copy=False)\nuser_vector = R[user, :]\n```", "```py\n# compute the dot product between the user vector and the similarity\n# matrix\nrecommendations = user_vector.dot(self.similarity) # shape=(n_items,)\n\n# if we're filtering previously-seen items, now is the time to do that\nitem_indices = np.arange(recommendations.shape[0])\nif filter_previously_seen:\n    rated_mask = user_vector != 0.\n    recommendations = recommendations[~rated_mask]\n    item_indices = item_indices[~rated_mask]\n```", "```py\n# now arg sort descending (most similar items first)\norder = np.argsort(-recommendations)[:n]\nitems = item_indices[order]\n\nif return_scores:\n   return items, recommendations[order]\nreturn items\n```", "```py\n    return (np.array([\n        # user 0 is a classic 30-yo millennial who is nostalgic for the 90s\n        [5.0, 3.5, 5.0, 0.0, 0.0, 0.0, 4.5, 3.0,\n         0.0, 2.5, 4.0, 4.0, 0.0, 1.5, 3.0],\n\n        # user 1 is a 40-yo who only likes action\n        [1.5, 0.0, 0.0, 1.0, 0.0, 4.0, 5.0, 0.0,\n         2.0, 0.0, 3.0, 3.5, 0.0, 4.0, 0.0],\n\n        # user 2 is a 12-yo whose parents are strict about what she watches.\n        [4.5, 4.0, 5.0, 0.0, 0.0, 0.0, 0.0, 4.0,\n         3.5, 5.0, 0.0, 0.0, 0.0, 0.0, 5.0],\n\n        # user 3 has just about seen it all, and doesn't really care for\n        # the goofy stuff. (but seriously, who rates the Goonies 2/5???)\n        [2.0, 1.0, 2.0, 1.0, 2.5, 4.5, 4.5, 0.5,\n         1.5, 1.0, 2.0, 2.5, 3.5, 3.5, 2.0],\n\n        # user 4 has just opened a netflix account and hasn't had a chance\n        # to watch too much\n        [0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0,\n         0.0, 0.0, 0.0, 1.5, 4.0, 0.0, 0.0],\n    ]), np.array([\"Ghost Busters\", \"Ghost Busters 2\",\n                  \"The Goonies\", \"Big Trouble in Little China\",\n                  \"The Rocky Horror Picture Show\", \"A Clockwork Orange\",\n                  \"Pulp Fiction\", \"Bill & Ted's Excellent Adventure\",\n                  \"Weekend at Bernie's\", \"Dumb and Dumber\", \"Clerks\",\n                  \"Jay & Silent Bob Strike Back\", \"Tron\", \"Total Recall\",\n                  \"The Princess Bride\" ]))\n```", "```py\n# #############################################################################\n# Use our fabricated data set\nR, titles = get_completely_fabricated_ratings_data()\n\n# #############################################################################\n# Fit an item-item recommender, predict for user 0\nrec = ItemItemRecommender(R, k=3)\nuser0_rec, user_0_preds = rec.recommend_for_user(\n    R, user=0, filter_previously_seen=True,\n    return_scores=True)\n```", "```py\nimport numpy as np\nfrom numpy.linalg import solve\n\nnan = np.nan\nQ = np.array([[5.0, 1.0, nan, nan, 2.5, 4.5, nan, nan],\n              [nan, nan, 3.5, 2.0, 3.0, nan, nan, nan],\n              [1.5, nan, nan, nan, 4.0, nan, 4.5, 4.0],\n              [nan, 1.0, nan, nan, 1.0, nan, nan, 5.0]])\n\nnan_mask = np.isnan(Q) # mask applied when computing loss\nQ[nan_mask] = 0.\n\nf = 3 # num factors\nn_iter = 5 # num iterations\nI_lambda = np.eye(f) * 0.01 # regularizing term\nrandom_state = np.random.RandomState(42)\n\n# initialize X, Y randomly\nX = random_state.rand(Q.shape[0], f)\nY = random_state.rand(f, Q.shape[1])\nW = nan_mask.astype(int) # weights for calculating loss (0/1)\n\n# iterate:\nerrors = []\nfor i in range(n_iter):\n    X = solve(Y.dot(Y.T) + I_lambda, Y.dot(Q.T)).T\n    Y = solve(X.T.dot(X) + I_lambda, X.T.dot(Q))\n    errors.append(((W * (Q - X.dot(Y))) ** 2).sum())\n\nX.dot(Y).round(3)\n```", "```py\ndef __init__(self, R, factors=0.25, n_iter=10, lam=0.001,\n random_state=None):\n```", "```py\n# check the array\nR = check_array(R, dtype=np.float32) # type: np.ndarray\nn_users, n_items = R.shape\n# get the random state\nrandom_state = check_random_state(random_state)\n```", "```py\n# get the number of factors. If it's a float, compute it\nif isinstance(factors, float):\n    factors = min(np.ceil(factors * n_items).astype(int), n_items)\n```", "```py\nW = (R > 0.).astype(np.float32)\n```", "```py\n# initialize the first array, Y, and X to None\n Y = random_state.rand(factors, n_items)\n X = None\n# the identity matrix (time lambda) is added to the XX or YY product\n# at each iteration.\n I = np.eye(factors) * lam\n```", "```py\n# for each iteration, iteratively solve for X, Y, and compute the\n # updated MSE\n for i in xrange(n_iter):\n X = solve(Y.dot(Y.T) + I, Y.dot(R.T)).T\n Y = solve(X.T.dot(X) + I, X.T.dot(R))\n# update the training error\n train_err.append(mse(R, X, Y, W))\n# now we have X, Y, which are our user factors and item factors\n self.X = X\n self.Y = Y\n self.train_err = train_err\n self.n_factors = factors\n self.lam = lam\n```", "```py\ndef predict(self, R, recompute_users=False):\n        \"\"\"Generate predictions for the test set.\n\n        Computes the predicted product of ``XY`` given the fit factors.\n        If recomputing users, will learn the new user factors given the\n        existing item factors.\n        \"\"\"\n        R = check_array(R, dtype=np.float32, copy=False) # type: np.ndarray\n        Y = self.Y # item factors\n        n_factors, _ = Y.shape\n\n        # we can re-compute user factors on their updated ratings, if we want.\n        # (not always advisable, but can be useful for offline recommenders)\n        if recompute_users:\n            I = np.eye(n_factors) * self.lam\n            X = solve(Y.dot(Y.T) + I, Y.dot(R.T)).T\n        else:\n            X = self.X\n\n        return X.dot(Y)\n```", "```py\ndef recommend_for_user(self, R, user, n=10, recompute_user=False,\n                       filter_previously_seen=False,\n                       return_scores=True):\n```", "```py\nR = check_array(R, dtype=np.float32, copy=False)\n# compute the new user vector. Squeeze to make sure it's a vector\n user_vec = self.predict(R, recompute_users=recompute_user)[user, :]\n item_indices = np.arange(user_vec.shape[0])\n# if we are filtering previously seen, remove the prior-rated items\n if filter_previously_seen:\n rated_mask = R[user, :] != 0.\n user_vec = user_vec[~rated_mask]\n item_indices = item_indices[~rated_mask]\norder = np.argsort(-user_vec)[:n] # descending order of computed scores\n items = item_indices[order]\n if return_scores:\n return items, user_vec[order]\n return items\n```", "```py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\n\nfrom packtml.recommendation import ALS\nfrom packtml.recommendation.data import get_completely_fabricated_ratings_data\nfrom packtml.metrics.ranking import mean_average_precision\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport sys\n\n# #############################################################################\n# Use our fabricated data set\nR, titles = get_completely_fabricated_ratings_data()\n\n# #############################################################################\n# Fit an item-item recommender, predict for user 0\nn_iter = 25\nrec = ALS(R, factors=5, n_iter=n_iter, random_state=42, lam=0.01)\nuser0_rec, user_0_preds = rec.recommend_for_user(\n    R, user=0, filter_previously_seen=True,\n    return_scores=True)\n\n# print some info about user 0\ntop_rated = np.argsort(-R[0, :])[:3]\nprint(\"User 0's top 3 rated movies are: %r\" % titles[top_rated].tolist())\nprint(\"User 0's top 3 recommended movies are: %r\"\n      % titles[user0_rec[:3]].tolist())\n```", "```py\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nratings = np.array(([5.0, 1.0, 0.0, 0.0, 2.5, 4.5, 0.0, 0.0],\n                    [0.0, 0.0, 3.5, 2.0, 3.0, 0.0, 0.0, 0.0],\n                    [1.5, 0.0, 0.0, 0.0, 4.0, 0.0, 4.5, 4.0],\n                    [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 5.0]))\n# content vector\n\ncategories = ['Alcohol license',\n              'Healthy options',\n              'Burgers on menu',\n              'Located in downtown',\n              '$', '$$', '$$$', '$$$$',\n              'Full bar', 'Southern cooking',\n              'Grilled food']\n# categories        a1   he  bu  dt  1$  2$  3$  4$  fb  sc  gf\ncontent = np.array([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n                    [1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.],\n                    [0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.],\n                    [1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.],\n                    [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n                    [1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.],\n                    [1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.],\n                    [1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.]\n                   ])\nsim = cosine_similarity(content)\nratings.dot(sim).round(3)\n```", "```py\narray([[6.337, 4.381, 6.169, 6.738, 5.703, 5.545, 4.813, 6.872],\n [2.997, 1.797, 7.232, 5.294, 6.904, 4.03 , 4.078, 5.587],\n [5.697, 4.539, 8.515, 8.305, 8.799, 5.876, 9.01 , 9.005],\n [2.306, 3\\. , 4.444, 5.169, 3.582, 4.658, 3.758, 5.916]])\n```", "```py\nfrom packtml.recommendation import ItemItemRecommender\n\nrec = ItemItemRecommender(ratings, k=5)\n\nzero_mask = rec.similarity == 0\nrec.similarity[zero_mask] = sim[zero_mask]\nrec.similarity\n```", "```py\narray([[0.99999994, 0.67728543, 0.35355338, 0.26726124, 0.62405604,\n        0.95782626, 0.28734788, 0.31622776],\n       [0.67728543, 0.99999994, 0.2236068 , 0.50709254, 0.43580094,\n        0.70710677, 0.5477226 , 0.5521576 ],\n       [0.35355338, 0.2236068 , 1\\. , 1\\. , 0.52827054,\n        0.4472136 , 0.4082483 , 0.6708204 ],\n       [0.26726124, 0.50709254, 1\\. , 1\\. , 0.52827054,\n        0.8451542 , 0.6172134 , 0.8451542 ],\n       [0.62405604, 0\\. , 0.52827054, 0.4364358 , 1\\. ,\n        0.2581989 , 0.7043607 , 0.577514 ],\n       [0.95782626, 0.70710677, 0.4472136 , 0.8451542 , 0.44022545,\n        1\\. , 0.36514837, 0.8 ],\n       [0.28734788, 0.5477226 , 0.4082483 , 0.6172134 , 0.7043607 ,\n        0.36514837, 1\\. , 0.62469506],\n       [0.1795048 , 0.5521576 , 0.6708204 , 0.8451542 , 0.577514 ,\n        0.8 , 0.62469506, 0.99999994]], dtype=float32)\n```", "```py\nimport numpy as np\n\n# define activation function\nf = (lambda v: 1./ (1\\. + np.exp(-v)))\nlam = 0.01\n# input matrix\nX = np.array([[1.5, 5.0, 2.5],\n             [0.6, 3.5, 2.8],\n             [2.4, 5.6, 5.6]])\n\ny = np.array([1, 1, 0])\n# initialize hidden layers, bias\n\nrs = np.random.RandomState(42)\nH1 = rs.rand(3, 4)\nH2 = rs.rand(4, 2)\nb1, b2 = np.ones(4), np.ones(2)\n\n# feed forward\nH1_res = f(X.dot(H1) + b1)\noutput = f(H1_res.dot(H2) + b2)\n```", "```py\n# back prop\nout_delta = output.copy() # get a copy of the output\nout_delta[range(X.shape[0]), y] -= 1.\nH2_d = H1_res.T.dot(out_delta)\nb2_d = H2_d.sum(axis=0)\ndelta2 = out_delta.dot(H2.T) * (1\\. - np.power(H1_res, 2.))\nH1_d = X.T.dot(delta2)\nb1_d = delta2.sum(axis=0)\n\n# update weights, bias\n\nH1 += -lam * H1_d\nb1 += -lam * b1_d\nH2 += -lam * H2_d\nb2 += -lam * b2_d\n```", "```py\ndef tanh(X):\n    \"\"\"Hyperbolic tangent.\n\n    Compute the tan-h (Hyperbolic tangent) activation function.\n    This is a very easily-differentiable activation function.\n\n    Parameters\n    ----------\n    X : np.ndarray, shape=(n_samples, n_features)\n        The transformed X array (X * W + b).\n    \"\"\"\n    return np.tanh(X)\n\nclass NeuralMixin(six.with_metaclass(ABCMeta)):\n    \"\"\"Abstract interface for neural network classes.\"\"\"\n    @abstractmethod\n    def export_weights_and_biases(self, output_layer=True):\n        \"\"\"Return the weights and biases of the network\"\"\"\n```", "```py\ndef _calculate_loss(truth, preds, weights, l2):\n    \"\"\"Compute the log loss.\n\n    Calculate the log loss between the true class labels and the predictions\n    generated by the softmax layer in our neural network.\n\n    Parameters\n    ----------\n    truth : np.ndarray, shape=(n_samples,)\n        The true labels\n\n    preds : np.ndarray, shape=(n_samples, n_classes)\n        The predicted class probabilities\n\n    weights : list\n        The list of weights matrices. Used for computing the loss\n        with the L2 regularization.\n\n    l2 : float\n        The regularization parameter\n    \"\"\"\n    # get the log probs of the prediction for the true class labels\n    n_samples = truth.shape[0]\n    logprobs = -np.log(preds[range(n_samples), truth])\n\n    # compute the sum of log probs\n    sum_logprobs = logprobs.sum()\n\n    # add the L2 regularization term\n    sum_logprobs += l2 / 2\\. * sum(np.square(W).sum() for W in weights)\n    return 1\\. / n_samples * sum_logprobs\n```", "```py\n def __init__(self, X, y, hidden=(25,), n_iter=10, learning_rate=0.001,\n              regularization=0.01, random_state=42):\n```", "```py\nself.hidden = hidden\nself.random_state = random_state\nself.n_iter = n_iter\nself.learning_rate = learning_rate\nself.regularization = regularization\n# initialize weights, biases, etc.\nX, y, weights, biases = self._init_weights_biases(\n    X, y, hidden, random_state, last_dim=None)\n```", "```py\n    def _forward_step(X, weights, biases):\n        # track the intermediate products\n        intermediate_results = [X]\n\n        # progress through all the layers EXCEPT the very last one.\n        for w, b in zip(weights[:-1], biases[:-1]):\n\n            # apply the activation function to the product of X and the weights\n            # (after adding the bias vector)\n            X = tanh(X.dot(w) + b)\n\n            # append this layer result\n            intermediate_results.append(X)\n\n        # we handle the very last layer a bit differently, since it's out\n        # output layer. First compute the product...\n        X = X.dot(weights[-1]) + biases[-1]\n\n        # then rather than apply the activation function (tanh), we apply\n        # the softmax, which is essentially generalized logistic regression.\n        return softmax(X), intermediate_results\n```", "```py\n        # for each iteration, feed X through the network, compute the loss,\n        # and back-propagate the error to correct the weights.\n        for _ in xrange(n_iter):\n            # compute the product of X on the hidden layers (the output of\n            # the network)\n            out, layer_results = self._forward_step(X, weights, biases)\n\n            # compute the loss on the output\n            loss = _calculate_loss(truth=y, preds=out, weights=weights,\n                                   l2=self.regularization)\n            train_loss.append(loss)\n\n            # now back-propagate to correct the weights and biases via\n            # gradient descent\n            self._back_propagate(y, out, layer_results, weights,\n                                 biases, learning_rate,\n                                 self.regularization)\n```", "```py\nprobas[range(n_samples), truth] -= 1.\n# iterate back through the layers computing the deltas (derivatives)\n last_delta = probas\n for next_weights, next_biases, layer_res in \\\n zip(weights[::-1], biases[::-1], layer_results[::-1]):\n# the gradient for this layer is equivalent to the previous delta\n# multiplied by the intermittent layer result\n d_W = layer_res.T.dot(last_delta)\n# column sums of the (just-computed) delta is the derivative\n# of the biases\n d_b = np.sum(last_delta, axis=0)\n# set the next delta for the next iter\n last_delta = last_delta.dot(next_weights.T) * \\\n (1\\. - np.power(layer_res, 2.))\n# update the weights gradient with the L2 regularization term\n d_W += l2 * next_weights\n# update the weights in this layer. The learning rate governs how\n# quickly we descend the gradient\n next_weights += -learning_rate * d_W\n next_biases += -learning_rate * d_b\n```", "```py\n# Fit a simple neural network\nn_iter = 4\nhidden = (10,)\nclf = NeuralNetClassifier(X_train, y_train, hidden=hidden, n_iter=n_iter,\n                          learning_rate=0.001, random_state=42)\nprint(\"Loss per training iteration: %r\" % clf.train_loss)\n\npred = clf.predict(X_test)\nclf_accuracy = accuracy_score(y_test, pred)\nprint(\"Test accuracy (hidden=%s): %.3f\" % (str(hidden), clf_accuracy))\n\n# #############################################################################\n# Fit a more complex neural network\nn_iter2 = 150\nhidden2 = (25, 25)\nclf2 = NeuralNetClassifier(X_train, y_train, hidden=hidden2, n_iter=n_iter2,\n                           learning_rate=0.001, random_state=42)\n\npred2 = clf2.predict(X_test)\nclf_accuracy2 = accuracy_score(y_test, pred2)\nprint(\"Test accuracy (hidden=%s): %.3f\" % (str(hidden2), clf_accuracy2))\n```", "```py\n    def __init__(self, X, y, pretrained, hidden=(25,), n_iter=10,\n                 regularization=0.01, learning_rate=0.001, random_state=42):\n\n        # initialize via the NN static method\n        self.hidden = hidden\n        self.random_state = random_state\n        self.n_iter = n_iter\n        self.learning_rate = learning_rate\n        self.regularization = regularization\n\n        # this is the previous model\n        self.model = pretrained\n\n        # assert that it's a neural net or we'll break down later\n        assert isinstance(pretrained, NeuralMixin), \\\n            \"Pre-trained model must be a neural network!\"\n\n        # initialize weights, biases, etc. for THE TRAINABLE LAYERS ONLY!\n        pt_w, pt_b = pretrained.export_weights_and_biases(output_layer=False)\n        X, y, weights, biases = NeuralNetClassifier._init_weights_biases(\n            X, y, hidden, random_state,\n\n            # use as the last dim the column dimension of the last weights\n            # (the ones BEFORE the output layer, that is)\n            last_dim=pt_w[-1].shape[1])\n```", "```py\n        train_loss = []\n        for _ in xrange(n_iter):\n            # first, pass the input data through the pre-trained model's\n            # hidden layers. Do not pass it through the last layer, however,\n            # since we don't want its output from the softmax layer.\n            X_transform = _pretrained_forward_step(X, pt_w, pt_b)\n\n            # NOW we complete a forward step on THIS model's\n            # untrained weights/biases\n            out, layer_results = NeuralNetClassifier._forward_step(\n                X_transform, weights, biases)\n\n            # compute the loss on the output\n            loss = _calculate_loss(truth=y, preds=out, weights=pt_w + weights,\n                                   l2=self.regularization)\n            train_loss.append(loss)\n\n            # now back-propagate to correct THIS MODEL's weights and biases via\n            # gradient descent. NOTE we do NOT adjust the pre-trained model's\n            # weights!!!\n            NeuralNetClassifier._back_propagate(\n                truth=y, probas=out, layer_results=layer_results,\n                weights=weights, biases=biases,\n                learning_rate=learning_rate,\n                l2=self.regularization)\n```", "```py\n    def predict(self, X):\n        # compute the probabilities and then get the argmax for each class\n        probas = self.predict_proba(X)\n\n        # we want the argmaxes of each row\n        return np.argmax(probas, axis=1)\n\n    def predict_proba(self, X):\n        # Compute a forward step with the pre-trained model first:\n        pt_w, pt_b = self.model.export_weights_and_biases(output_layer=False)\n        X_transform = _pretrained_forward_step(X, pt_w, pt_b)\n\n        # and then complete a forward step with the trained weights and biases\n        return NeuralNetClassifier._forward_step(\n            X_transform, self.weights, self.biases)[0]\n\n    def export_weights_and_biases(self, output_layer=True):\n        pt_weights, pt_biases = \\\n            self.model.export_weights_and_biases(output_layer=False)\n        w = pt_weights + self.weights\n        b = pt_biases + self.biases\n\n        if output_layer:\n            return w, b\n        return w[:-1], b[:-1]\n```", "```py\n# these are the majority classes\nn_obs = 1250\nx1 = rs.multivariate_normal(mean=[0, 0], cov=covariance, size=n_obs)\nx2 = rs.multivariate_normal(mean=[1, 5], cov=covariance, size=n_obs)\n\n# this is the minority class\nx3 = rs.multivariate_normal(mean=[0.85, 3.25], cov=[[1., .5], [1.25, 0.85]],\n                            size=n_obs // 3)\n\n# this is what the FIRST network will be trained on\nn_first = int(0.8 * n_obs)\nX = np.vstack((x1[:n_first], x2[:n_first])).astype(np.float32)\ny = np.hstack((np.zeros(n_first), np.ones(n_first))).astype(int)\n\n# this is what the SECOND network will be trained on\nX2 = np.vstack((x1[n_first:], x2[n_first:], x3)).astype(np.float32)\ny2 = np.hstack((np.zeros(n_obs - n_first),\n                np.ones(n_obs - n_first),\n                np.ones(x3.shape[0]) * 2)).astype(int)\n```", "```py\n# Fit the transfer network - train one more layer with a new class\nt_hidden = (15,)\nt_iter = 25\ntransfer = TransferLearningClassifier(X2_train, y2_train, pretrained=clf,\n                                      hidden=t_hidden, n_iter=t_iter,\n                                      random_state=42)\n\nt_pred = transfer.predict(X2_test)\ntrans_accuracy = accuracy_score(y2_test, t_pred)\nprint(\"Test accuracy (hidden=%s): %.3f\" % (str(hidden + t_hidden),\n                                           trans_accuracy))\n```", "```py\n# Fit the transfer network - train one more layer with a new class\nt_hidden = (15,)\nt_iter = 25\ntransfer = TransferLearningClassifier(X2_train, y2_train, pretrained=clf,\n                                      hidden=t_hidden, n_iter=t_iter,\n                                      random_state=42)\n\nt_pred = transfer.predict(X2_test)\ntrans_accuracy = accuracy_score(y2_test, t_pred)\nprint(\"Test accuracy (hidden=%s): %.3f\" % (str(hidden + t_hidden),\n                                           trans_accuracy))\n```"]
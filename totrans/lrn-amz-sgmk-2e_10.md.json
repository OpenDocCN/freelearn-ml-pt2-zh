["```py\nFROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.4.1-cpu-py37-ubuntu18.04\n. . .\n```", "```py\n    $ sudo yum -y install docker\n    $ sudo usermod -a -G docker ec2-user\n    ```", "```py\n    $ service docker start\n    $ docker login\n    ```", "```py\n    $ sudo yum -y install git python3-devel python3-pip\n    ```", "```py\n    $ git clone https://github.com/aws/deep-learning-containers.git\n    $ cd deep-learning-containers\n    ```", "```py\n    $ export ACCOUNT_ID=123456789012\n    $ export REGION=eu-west-1\n    $ export REPOSITORY_NAME=my-pt-dlc\n    ```", "```py\n    $ aws ecr create-repository \n    --repository-name $REPOSITORY_NAME --region $REGION\n    $ aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n    ```", "```py\n    $ python3 -m venv dlc\n    $ source dlc/bin/activate\n    $ pip install -r src/requirements.txt\n    ```", "```py\n    && conda install -c dglteam -y dgl==0.6.1 \\\n    ```", "```py\n    BuildCPUPTTrainPy3DockerImage:\n        tag: !join [ *VERSION, \"-\", *DEVICE_TYPE, \"-\", *TAG_PYTHON_VERSION, \"-\", *OS_VERSION, \"-training\" ]\n    BuildGPUPTTrainPy3DockerImage:\n        tag: !join [ *VERSION, \"-\", *DEVICE_TYPE, \"-\", *TAG_PYTHON_VERSION, \"-\", *CUDA_VERSION, \"-\", *OS_VERSION, \"-training\" ]\n    BuildCPUPTInferencePy3DockerImage:\n        tag: !join [ *VERSION, \"-\", *DEVICE_TYPE, \"-\", *TAG_PYTHON_VERSION, \"-\", *OS_VERSION, \"-inference\" ]\n    BuildGPUPTInferencePy3DockerImage:\n        tag: !join [ *VERSION, \"-\", *DEVICE_TYPE, \"-\", *TAG_PYTHON_VERSION, \"-\", *CUDA_VERSION, \"-\", *OS_VERSION, \"-inference\"]\n    ```", "```py\n    $ bash src/setup.sh pytorch\n    $ python src/main.py --buildspec pytorch/buildspec.yml --framework pytorch --device_types cpu,gpu --image_types training,inference\n    ```", "```py\n    $ docker images\n    123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-gpu-py36-cu111-ubuntu18.04-example-2021-05-28-10-14-15     \n    123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-gpu-py36-cu111-ubuntu18.04-training-2021-05-28-10-14-15    \n    123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-gpu-py36-cu111-ubuntu18.04-inference-2021-05-28-10-14-15\n    123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-cpu-py36-ubuntu18.04-inference-2021-05-28-10-14-15         \n    123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc   1.8.1-cpu-py36-ubuntu18.04-training-2021-05-28-10-14-15          \n    ```", "```py\n    Estimator = PyTorch(\n        image_uri='123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-pt-dlc:1.8.1-cpu-py36-ubuntu18.04-training-2021-05-28-10-14-15',\n        role=sagemaker.get_execution_role(),\n        entry_point='karate_club_sagemaker.py',\n        hyperparameters={'node_count': 34, 'epochs': 30},\n        instance_count=1,\n        instance_type='ml.m5.large')\n    ```", "```py\n    FROM python:3.7\n    RUN pip3 install --no-cache scikit-learn numpy pandas joblib sagemaker-training\n    ```", "```py\n    $ docker build -t sklearn-custom:sklearn -f Dockerfile .\n    ```", "```py\n    $ docker images\n    REPOSITORY          TAG         IMAGE ID   \n    sklearn-custom      sklearn     bf412a511471         \n    ```", "```py\n    $ aws ecr create-repository --repository-name sklearn-custom --region eu-west-1\n    $ aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:latest\n    ```", "```py\n    $ docker tag bf412a511471 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:sklearn\n    ```", "```py\n    $ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:sklearn\n    ```", "```py\n    sk = SKLearn(\n        role=sagemaker.get_execution_role(),\n        entry_point='sklearn-boston-housing.py',\n        image_name='123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:sklearn',\n        instance_count=1,\n        instance_type='ml.m5.large',\n        output_path=output,\n        hyperparameters={\n             'normalize': True,\n             'test-size': 0.1\n        }\n    )\n    ```", "```py\n    /usr/local/bin/python -m sklearn-boston-housing \n    --normalize True --test-size 0.1\n    ```", "```py\n#!/usr/bin/env python\nimport pandas as pd\nimport joblib, os, json\nif __name__ == '__main__':\n    config_dir = '/opt/ml/input/config'\n    training_dir = '/opt/ml/input/data/training'\n    model_dir = '/opt/ml/model'\n    with open(os.path.join(config_dir, \n    'hyperparameters.json')) as f:\n        hp = json.load(f)\n        normalize = hp['normalize']\n        test_size = float(hp['test-size'])\n        random_state = int(hp['random-state'])\n    filename = os.path.join(training_dir, 'housing.csv')\n    data = pd.read_csv(filename)\n    # Train model\n    . . . \n    joblib.dump(regr, \n                os.path.join(model_dir, 'model.joblib'))\n```", "```py\nFROM python:3.7\nRUN pip3 install --no-cache scikit-learn numpy pandas joblib\nCOPY sklearn-boston-housing-generic.py /usr/bin/train\nRUN chmod 755 /usr/bin/train\n```", "```py\n    $ docker build -t sklearn-custom:estimator -f Dockerfile-generic .\n    $ docker tag <IMAGE_ID> 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:estimator\n    $ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:estimator\n    ```", "```py\n    from sagemaker.estimator import Estimator\n    sk = Estimator(\n        role=sagemaker.get_execution_role(),\n        image_name='123456789012.dkr.ecr.eu-west-1.amazonaws.com/sklearn-custom:estimator',\n        instance_count=1,\n        instance_type='ml.m5.large',\n        output_path=output,\n        hyperparameters={\n             'normalize': True,\n             'test-size': 0.1,\n             'random-state': 123\n        }\n    )\n    ```", "```py\nFROM python:3.7\nRUN pip3 install --no-cache scikit-learn numpy pandas joblib\nRUN pip3 install --no-cache flask\nCOPY sklearn-boston-housing-generic.py /usr/bin/train\nCOPY sklearn-boston-housing-serve.py /usr/bin/serve\nRUN chmod 755 /usr/bin/train /usr/bin/serve\nEXPOSE 8080\n```", "```py\n    #!/usr/bin/env python\n    import joblib, os\n    import pandas as pd\n    from io import StringIO\n    import flask\n    from flask import Flask, Response\n    model_dir = '/opt/ml/model'\n    model = joblib.load(os.path.join(model_dir, \n                        'model.joblib'))\n    app = Flask(__name__)\n    ```", "```py\n    @app.route(\"/ping\", methods=[\"GET\"])\n    def ping():\n        return Response(response=\"\\n\", status=200)\n    ```", "```py\n    @app.route(\"/invocations\", methods=[\"POST\"])\n    def predict():\n        if flask.request.content_type == 'text/csv':\n            data = flask.request.data.decode('utf-8')\n            s = StringIO(data)\n            data = pd.read_csv(s, header=None)\n            response = model.predict(data)\n            response = str(response)\n        else:\n            return flask.Response(\n                response='CSV data only', \n                status=415, mimetype='text/plain')\n        return Response(response=response, status=200)\n    ```", "```py\n    if __name__ == \"__main__\":\n        app.run(host=\"0.0.0.0\", port=8080)\n    ```", "```py\n    sk_predictor = sk.deploy(instance_type='ml.t2.medium',\n                             initial_instance_count=1)\n    ```", "```py\n    test_samples = ['0.00632, 18.00, 2.310, 0, 0.5380, 6.5750, 65.20, 4.0900, 1,296.0, 15.30, 396.90, 4.98',             \n    '0.02731, 0.00, 7.070, 0, 0.4690, 6.4210, 78.90, 4.9671, 2,242.0, 17.80, 396.90, 9.14']\n    sk_predictor.serializer =\n        sagemaker.serializers.CSVSerializer()\n    response = sk_predictor.predict(test_samples)\n    print(response)\n    ```", "```py\n    b'[[29.801388899699845], [24.990809475886078]]'\n    ```", "```py\n    sk_predictor.delete_endpoint()\n    ```", "```py\n    # train_function.R\n    library(\"rjson\")\n    train <- function() {\n        hp <- fromJSON(file = \n              '/opt/ml/input/config/hyperparameters.json')\n        normalize <- hp$normalize\n        data <- read.csv(file = \n                '/opt/ml/input/data/training/housing.csv', \n                header=T)\n        if (normalize) {\n            data <- as.data.frame(scale(data))\n        }\n    ```", "```py\n        model = lm(medv~., data)\n        saveRDS(model, '/opt/ml/model/model.rds')\n    }\n    ```", "```py\n    # serve_function.R\n    #' @get /ping\n    function() {\n      return('')\n    }\n    #' @post /invocations\n    function(req) {\n        model <- readRDS('/opt/ml/model/model.rds')\n        conn <- textConnection(gsub('\\\\\\\\n', '\\n', \n                               req$postBody))\n        data <- read.csv(conn)\n        close(conn)\n        medv <- predict(model, data)\n        return(medv)\n    }\n    ```", "```py\n    library('plumber')\n    source('train_function.R')\n    serve <- function() {\n        app <- plumb('serve_function.R')\n        app$run(host='0.0.0.0', port=8080)}\n    args <- commandArgs()\n    if (any(grepl('train', args))) {\n        train()\n    }\n    if (any(grepl('serve', args))) {\n        serve()\n    }\n    ```", "```py\n    FROM r-base:latest\n    WORKDIR /opt/ml/\n    RUN apt-get update\n    RUN apt-get install -y libcurl4-openssl-dev libsodium-dev\n    RUN R -e \"install.packages(c('rjson', 'plumber')) \"\n    ```", "```py\n    COPY main.R train_function.R serve_function.R /opt/ml/\n    ENTRYPOINT [\"/usr/bin/Rscript\", \"/opt/ml/main.R\", \"--no-save\"]\n    ```", "```py\n    $ aws ecr create-repository --repository-name r-custom --region eu-west-1\n    $ aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest\n    $ docker build -t r-custom:latest -f Dockerfile .\n    $ docker tag <IMAGE_ID> 123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest\n    $ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest\n    ```", "```py\n    r_estimator = Estimator(\n        role = sagemaker.get_execution_role(),\n        image_uri='123456789012.dkr.ecr.eu-west-1.amazonaws.com/r-custom:latest',\n        instance_count=1,\n        instance_type='ml.m5.large',\n        output_path=output,\n        hyperparameters={'normalize': False}\n    )\n    r_estimator.fit({'training':training})\n    ```", "```py\n    r_predictor = r_estimator.deploy(\n        initial_instance_count=1, \n        instance_type='ml.t2.medium')\n    ```", "```py\n    import pandas as pd\n    data = pd.read_csv('housing.csv')\n    data.drop(['medv'], axis=1, inplace=True)\n    data = data.to_csv(index=False)\n    r_predictor.serializer = \n        sagemaker.serializers.CSVSerializer()\n    response = r_predictor.predict(data)\n    print(response)\n    ```", "```py\n    b'[30.0337,25.0568,30.6082,28.6772,27.9288\\. . .\n    ```", "```py\n    r_predictor.delete_endpoint()\n    ```", "```py\n    $ virtualenv mlflow-example\n    $ source mlflow-example/bin/activate\n    ```", "```py\n    $ pip install mlflow gunicorn pandas sklearn xgboost boto3\n    ```", "```py\n    $ wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n    $ unzip -o bank-additional.zip\n    ```", "```py\n# train-xgboost.py\nimport mlflow.xgboost\nimport xgboost as xgb\nfrom load_dataset import load_dataset\nif __name__ == '__main__':\n    mlflow.set_experiment('dm-xgboost')\n    with mlflow.start_run(run_name='dm-xgboost-basic') \n    as run:\n        x_train, x_test, y_train, y_test = load_dataset(\n            'bank-additional/bank-additional-full.csv')\n        cls = xgb.XGBClassifier(\n                  objective='binary:logistic', \n                  eval_metric='auc')\n        cls.fit(x_train, y_train)\n        auc = cls.score(x_test, y_test)\n        mlflow.log_metric('auc', auc)\n        mlflow.xgboost.log_model(cls, 'dm-xgboost-model')\n        mlflow.end_run()\n```", "```py\n# load_dataset.py\nimport mlflow\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\ndef load_dataset(path, test_size=0.2, random_state=123):\n    data = pd.read_csv(path)\n    data = pd.get_dummies(data)\n    data = data.drop(['y_no'], axis=1)\n    x = data.drop(['y_yes'], axis=1)\n    y = data['y_yes']\n    mlflow.log_param(\"dataset_path\", path)\n    mlflow.log_param(\"dataset_shape\", data.shape)\n    mlflow.log_param(\"test_size\", test_size)\n    mlflow.log_param(\"random_state\", random_state)\n    mlflow.log_param(\"one_hot_encoding\", True)\n    return train_test_split(x, y, test_size=test_size, \n                            random_state=random_state)\n```", "```py\n    $ python train-xgboost.py\n    INFO: 'dm-xgboost' does not exist. Creating a new experiment\n    AUC  0.91442097596504\n    ```", "```py\n    $ mlflow ui &\n    ```", "```py\n$ mlflow sagemaker build-and-push-container\n```", "```py\n    $ mlflow sagemaker run-local -p 8888 -m runs:/d08ab8383ee84f72a92164d3ca548693/dm-xgboost-model\n    ```", "```py\n    [2021-05-26 20:21:23 +0000] [370] [INFO] Starting gunicorn 20.1.0\n    [2021-05-26 20:21:23 +0000] [370] [INFO] Listening at: http://127.0.0.1:8000 (370)\n    [2021-05-26 20:21:23 +0000] [370] [INFO] Using worker: gevent\n    [2021-05-26 20:21:23 +0000] [381] [INFO] Booting worker with pid: 381 \n    ```", "```py\n    # predict-xgboost-local.py \n    import json\n    import requests\n    from load_dataset import load_dataset\n    port = 8888\n    if __name__ == '__main__':\n        x_train, x_test, y_train, y_test = load_dataset(\n            'bank-additional/bank-additional-full.csv')\n        input_data = x_test[:10].to_json(orient='split')\n        endpoint = 'http://localhost:{}/invocations'\n                   .format(port)\n        headers = {'Content-type': 'application/json; \n                    format=pandas-split'}\n        prediction = requests.post(\n            endpoint, \n            json=json.loads(input_data),\n            headers=headers)\n        print(prediction.text)\n    ```", "```py\n    $ source mlflow-example/bin/activate\n    $ python predict-xgboost-local.py\n    [0.00046298891538754106, 0.10499032586812973, . . . \n    ```", "```py\n    $ mlflow sagemaker deploy \\\n    --region-name eu-west-1 \\\n    -t ml.t2.medium \\\n    -a mlflow-xgb-demo \\\n    -m runs:/d08ab8383ee84f72a92164d3ca548693/dm-xgboost-model \\\n    -e arn:aws:iam::123456789012:role/Sagemaker-fullaccess\n    ```", "```py\n    # predict-xgboost.py \n    import boto3\n    from load_dataset import load_dataset\n    app_name = 'mlflow-xgb-demo'\n    region = 'eu-west-1'\n    if __name__ == '__main__':\n        sm = boto3.client('sagemaker', region_name=region)\n        smrt = boto3.client('runtime.sagemaker', \n                            region_name=region)\n        endpoint = sm.describe_endpoint(\n                  EndpointName=app_name)\n        print(\"Status: \", endpoint['EndpointStatus'])\n        x_train, x_test, y_train, y_test = load_dataset(\n            'bank-additional/bank-additional-full.csv')\n        input_data = x_test[:10].to_json(orient=\"split\")\n        prediction = smrt.invoke_endpoint(\n            EndpointName=app_name,\n            Body=input_data,\n            ContentType='application/json;\n                         format=pandas-split')\n        prediction = prediction['Body']\n                     .read().decode(\"ascii\")\n        print(prediction)\n    ```", "```py\n    $ python3 predict-xgboost.py\n    Status:  InService\n    [0.00046298891538754106, 0.10499032586812973, 0.016391035169363022, . . .\n    ```", "```py\n    $ mlflow sagemaker delete -a mlflow-xgb-demo –region-name eu-west-1\n    ```", "```py\n    FROM python:3.7-slim\n    RUN pip3 install --no-cache gensim nltk sagemaker\n    RUN python3 -m nltk.downloader stopwords wordnet\n    ADD preprocessing-lda-ntm.py /\n    ENTRYPOINT [\"python3\", \"/preprocessing-lda-ntm.py\"]\n    ```", "```py\n    python:3.7 instead of python:3.7-slim. This makes it faster to push and download.\n    ```", "```py\n    $ aws ecr create-repository --repository-name sm-processing-custom --region eu-west-1\n    $ aws ecr get-login-password | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest\n    ```", "```py\n    $ docker tag <IMAGE_ID> 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest\n    ```", "```py\n    $ docker push 123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest\n    ```", "```py\n    from sagemaker.processing import Processor\n    sklearn_processor = Processor( \n        image_uri='123456789012.dkr.ecr.eu-west-1.amazonaws.com/sm-processing-custom:latest',\n        role=sagemaker.get_execution_role(),\n        instance_type='ml.c5.2xlarge',\n        instance_count=1)\n    ```", "```py\n    from sagemaker.processing import ProcessingInput, ProcessingOutput\n    sklearn_processor.run(\n        inputs=[\n            ProcessingInput(\n                source=input_data,\n                destination='/opt/ml/processing/input')\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name='train_data',\n                source='/opt/ml/processing/train/')\n        ],\n        arguments=[\n            '--filename', 'abcnews-date-text.csv.gz'\n        ]\n    )\n    ```"]
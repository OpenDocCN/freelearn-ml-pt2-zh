["```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nimport numpy as np\ndiabetes = load_diabetes()\n\ntrain_x, train_y = diabetes.data[:400], diabetes.target[:400]\ntest_x, test_y = diabetes.data[400:], diabetes.target[400:]\n```", "```py\n# --- SECTION 2 ---\n# Create the ensemble's base learners and meta-learner\n# Append base learners to a list for ease of access\nbase_learners = []\nknn = KNeighborsRegressor(n_neighbors=5)\n\nbase_learners.append(knn)\ndtr = DecisionTreeRegressor(max_depth=4 , random_state=123456)\n\nbase_learners.append(dtr)\nridge = Ridge()\n\nbase_learners.append(ridge)\nmeta_learner = LinearRegression()\n```", "```py\n# --- SECTION 3 ---\n# Create the training metadata\n\n# Create variables to store metadata and their targets\nmeta_data = np.zeros((len(base_learners), len(train_x)))\nmeta_targets = np.zeros(len(train_x))\n\n# Create the cross-validation folds\nKF = KFold(n_splits=5)\nmeta_index = 0\nfor train_indices, test_indices in KF.split(train_x):\n  # Train each learner on the K-1 folds \n  # and create metadata for the Kth fold\n  for i in range(len(base_learners)):\n    learner = base_learners[i]\n    learner.fit(train_x[train_indices], train_y[train_indices])\n    predictions = learner.predict(train_x[test_indices])\n    meta_data[i][meta_index:meta_index+len(test_indices)] = \\\n                              predictions\n\n  meta_targets[meta_index:meta_index+len(test_indices)] = \\\n                          train_y[test_indices]\n  meta_index += len(test_indices)\n\n# Transpose the metadata to be fed into the meta-learner\nmeta_data = meta_data.transpose()\n```", "```py\n# --- SECTION 4 ---\n# Create the metadata for the test set and evaluate the base learners\ntest_meta_data = np.zeros((len(base_learners), len(test_x)))\nbase_errors = []\nbase_r2 = []\nfor i in range(len(base_learners)):\n  learner = base_learners[i]\n  learner.fit(train_x, train_y)\n  predictions = learner.predict(test_x)\n  test_meta_data[i] = predictions\n\n  err = metrics.mean_squared_error(test_y, predictions)\n  r2 = metrics.r2_score(test_y, predictions)\n\n  base_errors.append(err)\n  base_r2.append(r2)\n\ntest_meta_data = test_meta_data.transpose()\n```", "```py\n# --- SECTION 5 ---\n# Fit the meta-learner on the train set and evaluate it on the test set\nmeta_learner.fit(meta_data, meta_targets)\nensemble_predictions = meta_learner.predict(test_meta_data)\n\nerr = metrics.mean_squared_error(test_y, ensemble_predictions)\nr2 = metrics.r2_score(test_y, ensemble_predictions)\n\n# --- SECTION 6 ---\n# Print the results \nprint('ERROR R2 Name')\nprint('-'*20)\nfor i in range(len(base_learners)):\n  learner = base_learners[i]\n  print(f'{base_errors[i]:.1f} {base_r2[i]:.2f} {learner.__class__.__name__}')\nprint(f'{err:.1f} {r2:.2f} Ensemble')\n```", "```py\nERROR R2 Name\n--------------------\n2697.8 0.51 KNeighborsRegressor\n3142.5 0.43 DecisionTreeRegressor\n2564.8 0.54 Ridge\n2066.6 0.63 Ensemble\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nimport numpy as np\nbc = load_breast_cancer()\n\ntrain_x, train_y = bc.data[:400], bc.target[:400]\ntest_x, test_y = bc.data[400:], bc.target[400:]\n```", "```py\n# --- SECTION 2 ---\n# Create the ensemble's base learners and meta-learner\n# Append base learners to a list for ease of access\nbase_learners = []\n\nknn = KNeighborsClassifier(n_neighbors=2)\nbase_learners.append(knn)\n\ndtr = DecisionTreeClassifier(max_depth=4, random_state=123456)\nbase_learners.append(dtr)\n\nmlpc = MLPClassifier(hidden_layer_sizes =(100, ), \n           solver='lbfgs', random_state=123456)\nbase_learners.append(mlpc)\n\nmeta_learner = LogisticRegression(solver='lbfgs')\n```", "```py\n# --- SECTION 3 ---\n# Create the training metadata\n\n# Create variables to store metadata and their targets\nmeta_data = np.zeros((len(base_learners), len(train_x)))\nmeta_targets = np.zeros(len(train_x))\n\n# Create the cross-validation folds\nKF = KFold(n_splits=5)\nmeta_index = 0\nfor train_indices, test_indices in KF.split(train_x):\n   # Train each learner on the K-1 folds and create \n   # metadata for the Kth fold\n   for i in range(len(base_learners)):\n   learner = base_learners[i]\n\n   learner.fit(train_x[train_indices], train_y[train_indices])\n   predictions = learner.predict_proba(train_x[test_indices])[:,0]\n\n   meta_data[i][meta_index:meta_index+len(test_indices)] = predictions\n\n   meta_targets[meta_index:meta_index+len(test_indices)] = \\\n                           train_y[test_indices]\n   meta_index += len(test_indices)\n\n# Transpose the metadata to be fed into the meta-learner\nmeta_data = meta_data.transpose()\n```", "```py\n# --- SECTION 4 ---\n# Create the metadata for the test set and evaluate the base learners\ntest_meta_data = np.zeros((len(base_learners), len(test_x)))\nbase_acc = []\nfor i in range(len(base_learners)):\n  learner = base_learners[i]\n  learner.fit(train_x, train_y)\n  predictions = learner.predict_proba(test_x)[:,0]\n  test_meta_data[i] = predictions\n\n  acc = metrics.accuracy_score(test_y, learner.predict(test_x))\n  base_acc.append(acc)\ntest_meta_data = test_meta_data.transpose()\n```", "```py\n# --- SECTION 5 ---\n# Fit the meta-learner on the train set and evaluate it on the test set\nmeta_learner.fit(meta_data, meta_targets)\nensemble_predictions = meta_learner.predict(test_meta_data)\n\nacc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# --- SECTION 6 ---\n# Print the results\nprint('Acc Name')\nprint('-'*20)\nfor i in range(len(base_learners)):\n  learner = base_learners[i]\n  print(f'{base_acc[i]:.2f} {learner.__class__.__name__}')\nprint(f'{acc:.2f} Ensemble')\n```", "```py\nAcc Name\n--------------------\n0.86 KNeighborsClassifier\n0.88 DecisionTreeClassifier\n0.92 MLPClassifier  \n0.93 Ensemble\n```", "```py\nAcc Name\n--------------------\n0.86 KNeighborsClassifier\n0.88 DecisionTreeClassifier\n0.92 MLPClassifier\n0.91 Ensemble\n```", "```py\n# --- SECTION 1 ---\n# Libraries\nimport numpy as np\n\nfrom sklearn.model_selection import KFold\nfrom copy import deepcopy\n\nclass StackingRegressor():\n  # --- SECTION 2 ---\n  # The constructor \n  def __init__(self, learners):\n    # Create a list of sizes for each stacking level\n    # And a list of deep copied learners \n    self.level_sizes = []\n    self.learners = []\n    for learning_level in learners:\n      self.level_sizes.append(len(learning_level))\n      level_learners = []\n      for learner in learning_level:\n        level_learners.append(deepcopy(learner))\n      self.learners.append(level_learners)\n```", "```py\n  # --- SECTION 3 ---\n  # The fit function. Creates training metadata for every level\n  # and trains each level on the previous level's metadata\n  def fit(self, x, y):\n    # Create a list of training metadata, one for each stacking level\n    # and another one for the targets. For the first level, \n    # the actual data is used.\n    meta_data = [x]\n    meta_targets = [y]\n    for i in range(len(self.learners)):\n      level_size = self.level_sizes[i]\n\n      # Create the metadata and target variables for this level\n      data_z = np.zeros((level_size, len(x)))\n      target_z = np.zeros(len(x))\n\n      train_x = meta_data[i]\n      train_y = meta_targets[i]\n\n      # Create the cross-validation folds\n      KF = KFold(n_splits=5)\n      meta_index = 0\n      for train_indices, test_indices in KF.split(x):\n        # Train each learner on the K-1 folds and create\n        # metadata for the Kth fold\n        for j in range(len(self.learners[i])):\n\n          learner = self.learners[i][j]\n          learner.fit(train_x[train_indices], \n                train_y[train_indices])\n          predictions = learner.predict(train_x[test_indices])\n\n          data_z[j][meta_index:meta_index+len(test_indices)] = \\\n                              predictions\n\n        target_z[meta_index:meta_index+len(test_indices)] = \\\n                          train_y[test_indices]\n        meta_index += len(test_indices)\n\n      # Add the data and targets to the metadata lists\n      data_z = data_z.transpose()\n      meta_data.append(data_z)\n      meta_targets.append(target_z)\n\n      # Train the learner on the whole previous metadata\n      for learner in self.learners[i]:\n        learner.fit(train_x, train_y)\n```", "```py\n\n  # --- SECTION 4 ---\n  # The predict function. Creates metadata for the test data and returns\n  # all of them. The actual predictions can be accessed with \n  # meta_data[-1]\n  def predict(self, x):\n\n    # Create a list of training metadata, one for each stacking level\n    meta_data = [x]\n    for i in range(len(self.learners)):\n      level_size = self.level_sizes[i]\n\n      data_z = np.zeros((level_size, len(x)))\n\n      test_x = meta_data[i]\n\n      # Create the cross-validation folds\n      KF = KFold(n_splits=5)\n      for train_indices, test_indices in KF.split(x):\n        # Train each learner on the K-1 folds and create\n        # metadata for the Kth fold\n        for j in range(len(self.learners[i])):\n\n          learner = self.learners[i][j]\n          predictions = learner.predict(test_x)\n          data_z[j] = predictions\n\n      # Add the data and targets to the metadata lists\n      data_z = data_z.transpose()\n      meta_data.append(data_z)\n\n    # Return the meta_data the final layer's prediction can be accessed\n    # With meta_data[-1]\n    return meta_data\n```", "```py\n# --- SECTION 5 ---\n# Use the classifier\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn import metrics\ndiabetes = load_diabetes()\n\ntrain_x, train_y = diabetes.data[:400], diabetes.target[:400]\ntest_x, test_y = diabetes.data[400:], diabetes.target[400:]\n\nbase_learners = []\n\nknn = KNeighborsRegressor(n_neighbors=5)\nbase_learners.append(knn)\n\ndtr = DecisionTreeRegressor(max_depth=4, random_state=123456)\nbase_learners.append(dtr)\n\nridge = Ridge()\nbase_learners.append(ridge)\n\nmeta_learner = LinearRegression()\n\n# Instantiate the stacking regressor\nsc = StackingRegressor([[knn,dtr,ridge],[meta_learner]])\n\n# Fit and predict\nsc.fit(train_x, train_y)\nmeta_data = sc.predict(test_x)\n\n# Evaluate base learners and meta-learner\nbase_errors = []\nbase_r2 = []\nfor i in range(len(base_learners)):\n  learner = base_learners[i]\n  predictions = meta_data[1][:,i]\n  err = metrics.mean_squared_error(test_y, predictions)\n  r2 = metrics.r2_score(test_y, predictions)\n  base_errors.append(err)\n  base_r2.append(r2)\n\nerr = metrics.mean_squared_error(test_y, meta_data[-1])\nr2 = metrics.r2_score(test_y, meta_data[-1])\n\n# Print the results\nprint('ERROR R2 Name')\nprint('-'*20)\nfor i in range(len(base_learners)):\n  learner = base_learners[i]\n  print(f'{base_errors[i]:.1f} {base_r2[i]:.2f} \n      {learner.__class__.__name__}')\nprint(f'{err:.1f} {r2:.2f} Ensemble')\n```", "```py\nERROR R2 Name\n--------------------\n2697.8 0.51 KNeighborsRegressor\n3142.5 0.43 DecisionTreeRegressor\n2564.8 0.54 Ridge\n2066.6 0.63 Ensemble\n```"]
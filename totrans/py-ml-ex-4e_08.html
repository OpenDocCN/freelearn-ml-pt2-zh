<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer281">
    <h1 class="chapterNumber">8</h1>
    <h1 class="chapterTitle" id="_idParaDest-184">Discovering Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling</h1>
    <p class="normal">In the previous chapter, we went through a text visualization using t-SNE. t-SNE, or any dimensionality reduction algorithm, is a type of unsupervised learning. In this chapter, we will be continuing our unsupervised learning journey, specifically focusing on clustering and topic modeling. We will start with how unsupervised learning learns without guidance and how it is good at discovering hidden information underneath data.</p>
    <p class="normal">Next, we will talk about clustering as an important branch of unsupervised learning, which identifies different groups of observations from data. For instance, clustering is useful for market segmentation, where consumers of similar behaviors are grouped into one segment for marketing purposes. We will perform clustering on the 20 newsgroups text dataset and see what clusters will be produced.</p>
    <p class="normal">Another unsupervised learning route we will take is topic modeling, which is the process of extracting themes hidden in the dataset. You will be amused by how many interesting themes we are able to mine from the 20 newsgroups dataset.</p>
    <p class="normal">We will cover the following topics:</p>
    <ul>
      <li class="bulletList">Leaning without guidance – unsupervised learning</li>
      <li class="bulletList">Getting started with k-means clustering</li>
      <li class="bulletList">Clustering newsgroups data</li>
      <li class="bulletList">Discovering underlying topics in newsgroups</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-185">Learning without guidance – unsupervised learning</h1>
    <p class="normal">In the previous chapter, we applied t-SNE to visualize the newsgroup text data, reduced to two dimensions. t-SNE, or dimensionality reduction in general, is a type of <strong class="keyWord">unsupervised learning</strong>. Instead of <a id="_idIndexMarker771"/>being guided by predefined labels or categories, such as a class or membership (classification), and a continuous value (regression), unsupervised learning identifies inherent structures or commonalities in the input data. Since there is no guidance in unsupervised learning, there is no clear answer on what is a right or wrong result. Unsupervised learning has the freedom to discover hidden information underneath input data.</p>
    <p class="normal">An easy way to understand unsupervised learning is to think of going through many practice questions for an exam. In supervised learning, you are given answers to those practice questions. You basically figure out the relationship between the questions and answers and learn how to map the questions to the answers. Hopefully, you will do well in the actual exam in the end by giving the correct answers. However, in unsupervised learning, you are not provided with the answers to those practice questions. What you might do in this instance could include the following:</p>
    <ul>
      <li class="bulletList">Grouping similar practice questions so that you can later study related questions together at one time</li>
      <li class="bulletList">Finding questions that are highly repetitive so that you don’t have to waste time working out the answer for each one individually</li>
      <li class="bulletList">Spotting rare questions so that you can be better prepared for them</li>
      <li class="bulletList">Extracting the key chunk of each question by removing boilerplate text so you can cut to the point</li>
    </ul>
    <p class="normal">You will notice that the outcomes of all these tasks are pretty open-ended. They are correct as long as they are able to describe the commonality and the structure underneath the data.</p>
    <p class="normal">Practice <a id="_idIndexMarker772"/>questions are the <strong class="keyWord">features</strong> in machine learning, which are also often <a id="_idIndexMarker773"/>called <strong class="keyWord">attributes</strong>, <strong class="keyWord">observations</strong>, or <strong class="keyWord">predictive variables</strong>. Answers to <a id="_idIndexMarker774"/>questions are <a id="_idIndexMarker775"/>the labels in machine learning, which <a id="_idIndexMarker776"/>are also called <strong class="keyWord">targets</strong> or <strong class="keyWord">target variables</strong>. Practice questions <a id="_idIndexMarker777"/>with answers provided are called <strong class="keyWord">labeled data</strong>, while practice <a id="_idIndexMarker778"/>questions without answers are called <strong class="keyWord">unlabeled data</strong>. Unsupervised learning works with unlabeled data and acts on that information without guidance.</p>
    <p class="normal">Unsupervised learning can include the following types:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Clustering</strong>: This means grouping data based on commonality, which is often used for <a id="_idIndexMarker779"/>exploratory data analysis. Grouping similar practice questions, as mentioned earlier, is an example of clustering. Clustering techniques are widely used in customer segmentation or for grouping similar online behaviors for a marketing campaign. We will learn more about the popular algorithm k-means clustering in this chapter.</li>
      <li class="bulletList"><strong class="keyWord">Association</strong>: This <a id="_idIndexMarker780"/>explores the co-occurrence of particular values of two or more features. Outlier detection (also called anomaly detection) is a typical case, where rare observations are identified. Spotting rare questions in the preceding example can be achieved using outlier detection techniques.</li>
      <li class="bulletList"><strong class="keyWord">Projection</strong>: This maps the original feature space to a reduced dimensional space <a id="_idIndexMarker781"/>retaining or extracting a set of principal variables. Extracting the key chunk of practice questions is an example projection or, specifically, a dimensionality reduction. The t-SNE we learned about previously is a good example.</li>
    </ul>
    <p class="normal">Unsupervised learning is extensively employed in the area of NLP mainly because of the difficulty of obtaining labeled text data. Unlike numerical data (such as house prices, stock data, and online click streams), labeling text can sometimes be subjective, manual, and tedious. Unsupervised learning algorithms that do not require labels become effective when it comes to mining text data.</p>
    <p class="normal">In <em class="chapterRef">Chapter 7</em>, <em class="italic">Mining the 20 Newsgroups Dataset with Text Analysis Techniques</em>, you experienced using t-SNE to reduce the dimensionality of text data. Now, let’s explore text mining with clustering algorithms and topic modeling techniques. We will start with clustering the newsgroups data.</p>
    <h1 class="heading-1" id="_idParaDest-186">Getting started with k-means clustering</h1>
    <p class="normal">The newsgroups data comes with labels, which are the categories of the newsgroups, and a number <a id="_idIndexMarker782"/>of categories that are closely related or even overlapping, for instance, the five computer groups: <code class="inlineCode">comp.graphics</code>, <code class="inlineCode">comp.os.ms-windows.misc</code>, <code class="inlineCode">comp.sys.ibm.pc.hardware</code>, <code class="inlineCode">comp.sys.mac.hardware</code>, and <code class="inlineCode">comp.windows.x</code>, and the two religion-related ones: <code class="inlineCode">alt.atheism</code> and <code class="inlineCode">talk.religion.misc</code>.</p>
    <p class="normal">Let’s now pretend we don’t know those labels or they don’t exist. Will samples from related topics be clustered together? We will now resort to the k-means clustering algorithm.</p>
    <h2 class="heading-2" id="_idParaDest-187">How does k-means clustering work?</h2>
    <p class="normal">The goal of the k-means algorithm is to partition the data into k groups based on feature similarities. <em class="italic">k</em> is a <a id="_idIndexMarker783"/>predefined property of a <em class="italic">k</em>-means clustering model. Each of the <em class="italic">k</em> clusters is specified by a centroid (center of a cluster) and each data sample belongs to the cluster with the nearest centroid. During training, the algorithm iteratively updates the <em class="italic">k</em> centroids based on the data provided. Specifically, it involves the following steps:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Specifying k</strong>: The algorithm needs to know how many clusters to generate as an end result.</li>
      <li class="numberedList"><strong class="keyWord">Initializing centroids</strong>: The algorithm starts with randomly selecting k samples from the dataset as centroids.</li>
      <li class="numberedList"><strong class="keyWord">Assigning clusters</strong>: Now that we have <em class="italic">k</em> centroids, samples that share the same closest centroid constitute one cluster. <em class="italic">k</em> clusters are created as a result. Note that <a id="_idIndexMarker784"/>closeness is usually measured by the <strong class="keyWord">Euclidean distance</strong>. Other <a id="_idIndexMarker785"/>metrics can also be used, such <a id="_idIndexMarker786"/>as the <strong class="keyWord">Manhattan distance</strong> and <strong class="keyWord">Chebyshev distance</strong>, which are listed in the following table:</li>
    </ol>
    <figure class="mediaobject"><img alt="A picture containing text, screenshot, font, number  Description automatically generated" src="../Images/B21047_08_01.png"/></figure>
    <p class="packt_figref">Figure 8.1: Distance metrics</p>
    <ol>
      <li class="numberedList" value="4"><strong class="keyWord">Updating centroids</strong>: For each cluster, we need to recalculate its center point, which is <a id="_idIndexMarker787"/>the mean of all the samples in the cluster. <em class="italic">k</em> centroids are updated to be the means of corresponding clusters. This is why the algorithm is called <strong class="keyWord">k-means</strong>.</li>
      <li class="numberedList"><strong class="keyWord">Repeating steps 3 and 4</strong>: We keep repeating assigning clusters and updating centroids until the model converges when no or a small enough update of centroids can be done, or enough iterations have been completed.</li>
    </ol>
    <p class="normal">The outputs of a <a id="_idIndexMarker788"/>trained k-means clustering model include the following:</p>
    <ul>
      <li class="bulletList">The cluster ID of each training sample, ranging from 1 to <em class="italic">k</em></li>
      <li class="bulletList"><em class="italic">k</em> centroids, which can be used to cluster new samples—a new sample will belong to the cluster of the closest centroid</li>
    </ul>
    <p class="normal">It is easy to understand the k-means clustering algorithm and its implementation is also straightforward, as you will discover next.</p>
    <h2 class="heading-2" id="_idParaDest-188">Implementing k-means from scratch</h2>
    <p class="normal">We will <a id="_idIndexMarker789"/>use the <code class="inlineCode">iris</code> dataset from scikit-learn as an example. Let’s first load the data and visualize it. We herein only use two features out of the original four for simplicity:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn </span><span class="hljs-con-keyword">import</span><span class="language-python"> datasets</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">iris = datasets.load_iris()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X = iris.data[:, </span><span class="hljs-con-number">2</span><span class="language-python">:</span><span class="hljs-con-number">4</span><span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y = iris.target</span>
</code></pre>
    <p class="normal">Since the dataset contains three iris classes, we plot it in three different colors, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> numpy </span><span class="hljs-con-keyword">as</span><span class="language-python"> np</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> matplotlib </span><span class="hljs-con-keyword">import</span><span class="language-python"> pyplot </span><span class="hljs-con-keyword">as</span><span class="language-python"> plt</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.scatter(X[:,</span><span class="hljs-con-number">0</span><span class="language-python">], X[:,</span><span class="hljs-con-number">1</span><span class="language-python">], c=y)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">This will <a id="_idIndexMarker790"/>give us the following output for the original data plot:</p>
    <figure class="mediaobject"><img alt="A chart of different colored dots  Description automatically generated" src="../Images/B21047_08_02.png"/></figure>
    <p class="packt_figref">Figure 8.2: Plot of the original iris dataset</p>
    <p class="normal">Assuming we know nothing about the label <em class="italic">y</em>, we try to cluster the data into three groups, as there seem to be three clusters in the preceding plot (or you might say two, which we will come back to later). Let’s perform <em class="italic">step 1</em>, <em class="italic">specifying k</em>, and <em class="italic">step 2</em>, <em class="italic">initializing centroids</em>, by randomly selecting three samples as the initial centroids:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">k = </span><span class="hljs-con-number">3</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">np.random.seed(</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">random_index = np.random.choice(</span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-built_in">len</span><span class="language-python">(X)), k)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">centroids = X[random_index]</span>
</code></pre>
    <p class="normal">We visualize the data (without labels anymore) along with the initial random centroids:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">visualize_centroids</span><span class="language-python">(</span><span class="hljs-con-params">X, centroids</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.scatter(X[:, </span><span class="hljs-con-number">0</span><span class="language-python">], X[:, </span><span class="hljs-con-number">1</span><span class="language-python">])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.scatter(centroids[:, </span><span class="hljs-con-number">0</span><span class="language-python">], centroids[:, </span><span class="hljs-con-number">1</span><span class="language-python">], marker=</span><span class="hljs-con-string">'*'</span><span class="language-python">,</span>
                                             s=200, c='#050505')
<span class="hljs-con-meta">...</span> <span class="language-python">    plt.show()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">visualize_centroids(X, centroids)</span>
</code></pre>
    <p class="normal">Refer to <a id="_idIndexMarker791"/>the following screenshot for the data, along with the initial random centroids:</p>
    <figure class="mediaobject"><img alt="A diagram of a line of dots  Description automatically generated" src="../Images/B21047_08_03.png"/></figure>
    <p class="packt_figref">Figure 8.3: Data points with random centroids</p>
    <p class="normal">Now we perform <em class="italic">step 3</em>, which entails assigning clusters based on the nearest centroids. First, we need to define a function calculating distance, which is measured by the Euclidean distance, as demonstrated here:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">dist</span><span class="language-python">(</span><span class="hljs-con-params">a, b</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">return</span><span class="language-python"> np.linalg.norm(a - b, axis=</span><span class="hljs-con-number">1</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">Then, we develop a function that assigns a sample to the cluster of the nearest centroid:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">assign_cluster</span><span class="language-python">(</span><span class="hljs-con-params">x, centroids</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    distances = dist(x, centroids)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    cluster = np.argmin(distances)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">return</span><span class="language-python"> cluster</span>
</code></pre>
    <p class="normal">With the clusters assigned, we perform <em class="italic">step 4</em>, which involves updating the centroids to the mean of all samples in the individual clusters:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">update_centroids</span><span class="language-python">(</span><span class="hljs-con-params">X, centroids, clusters</span><span class="language-python">):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(k):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        cluster_i = np.where(clusters == i)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        centroids[i] = np.mean(X[cluster_i], axis=</span><span class="hljs-con-number">0</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">Finally, we have <em class="italic">step 5</em>, which involves repeating <em class="italic">step 3</em> and <em class="italic">step 4</em> until the model converges <a id="_idIndexMarker792"/>and whichever of the following occurs:</p>
    <ul>
      <li class="bulletList">Centroids move less than the pre-specified threshold</li>
      <li class="bulletList">Sufficient iterations have been taken</li>
    </ul>
    <p class="normal">We set the tolerance of the first condition and the maximum number of iterations as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tol = </span><span class="hljs-con-number">0.0001</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">max_iter = </span><span class="hljs-con-number">100</span>
</code></pre>
    <p class="normal">Initialize the clusters’ starting values, along with the starting clusters for all samples, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">iter</span><span class="language-python"> = </span><span class="hljs-con-number">0</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">centroids_diff = </span><span class="hljs-con-number">100000</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">clusters = np.zeros(</span><span class="hljs-con-built_in">len</span><span class="language-python">(X))</span>
</code></pre>
    <p class="normal">With all the components ready, we can train the model iteration by iteration where it first checks convergence before performing <em class="italic">steps 3</em> and <em class="italic">4</em>, and then visualizes the latest centroids:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> copy </span><span class="hljs-con-keyword">import</span><span class="language-python"> deepcopy</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">while</span><span class="language-python"> </span><span class="hljs-con-built_in">iter</span><span class="language-python"> &lt; max_iter </span><span class="hljs-con-keyword">and</span><span class="language-python"> centroids_diff &gt; tol:</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-built_in">len</span><span class="language-python">(X)):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        clusters[i] = assign_cluster(X[i], centroids)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    centroids_prev = deepcopy(centroids)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    update_centroids(X, centroids, clusters)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">iter</span><span class="language-python"> += </span><span class="hljs-con-number">1</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    centroids_diff = np.linalg.norm(centroids -</span>
                                       centroids_prev)
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'Iteration:'</span><span class="language-python">, </span><span class="hljs-con-built_in">str</span><span class="language-python">(</span><span class="hljs-con-built_in">iter</span><span class="language-python">))</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'Centroids:\n'</span><span class="language-python">, centroids)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'Centroids move: </span><span class="hljs-con-subst">{centroids_diff:</span><span class="hljs-con-number">5.4</span><span class="hljs-con-subst">f}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    visualize_centroids(X, centroids)</span>
</code></pre>
    <p class="normal">Let’s <a id="_idIndexMarker793"/>look at the following outputs generated from the preceding commands:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Iteration 1</strong>: Take a look at the following output of iteration 1:
        <pre class="programlisting con-one"><code class="hljs-con">Iteration: 1
Centroids:
[[1.462      0.246     ]
[5.80285714 2.11142857]
[4.42307692 1.44153846]]
Centroids move: 0.8274
</code></pre>
      </li>
    </ul>
    <p class="normal-one">The plot of centroids after iteration 1 is as follows:</p>
    <figure class="mediaobject"><img alt="A diagram of a map  Description automatically generated" src="../Images/B21047_08_04.png"/></figure>
    <p class="packt_figref">Figure 8.4: k-means clustering result after the first round</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Iteration 2</strong>: Take a <a id="_idIndexMarker794"/>look at the following output of iteration 2:
        <pre class="programlisting con-one"><code class="hljs-con">Iteration: 2
Centroids:
[[1.462      0.246     ]
[5.73333333 2.09487179]
[4.37704918 1.40819672]]
Centroids move: 0.0913
</code></pre>
      </li>
    </ul>
    <p class="normal-one">The plot of centroids after iteration 2 is as follows:</p>
    <figure class="mediaobject"><img alt="A diagram of a map  Description automatically generated" src="../Images/B21047_08_05.png"/></figure>
    <p class="packt_figref">Figure 8.5: k-means clustering result after the second round</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Iteration 6</strong>: Take <a id="_idIndexMarker795"/>a look at the following output of iteration 6 (we herein skip iterations 3 to 5 to avoid tedium):
        <pre class="programlisting con-one"><code class="hljs-con">Iteration: 6
Centroids:
[[1.462      0.246     ]
[5.62608696 2.04782609]
[4.29259259 1.35925926]]
Centroids move: 0.0225
</code></pre>
      </li>
    </ul>
    <p class="normal-one">The plot of centroids after iteration 6 is as follows:</p>
    <figure class="mediaobject"><img alt="A diagram of a map  Description automatically generated" src="../Images/B21047_08_06.png"/></figure>
    <p class="packt_figref">Figure 8.6: k-means clustering result after the sixth round</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Iteration 7</strong>: Take <a id="_idIndexMarker796"/>a look at the following output of iteration 7:
        <pre class="programlisting con-one"><code class="hljs-con">Iteration: 7
Centroids:
[[1.462      0.246     ]
[5.62608696 2.04782609]
[4.29259259 1.35925926]]
Centroids move: 0.0000
</code></pre>
      </li>
    </ul>
    <p class="normal-one">The plot of centroids after iteration 7 is as follows:</p>
    <figure class="mediaobject"><img alt="A diagram of a map  Description automatically generated" src="../Images/B21047_08_07.png"/></figure>
    <p class="packt_figref">Figure 8.7: k-means clustering result after the seventh round</p>
    <p class="normal">The model <a id="_idIndexMarker797"/>converges after seven iterations. The resulting centroids look promising, and we can also plot the clusters:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.scatter(X[:, </span><span class="hljs-con-number">0</span><span class="language-python">], X[:, </span><span class="hljs-con-number">1</span><span class="language-python">], c=clusters)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.scatter(centroids[:, </span><span class="hljs-con-number">0</span><span class="language-python">], centroids[:, </span><span class="hljs-con-number">1</span><span class="language-python">], marker=</span><span class="hljs-con-string">'*'</span><span class="language-python">,</span>
                                           s=200, c='r')
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">Refer to the following screenshot for the end result:</p>
    <figure class="mediaobject"><img alt="A diagram of different colored dots  Description automatically generated" src="../Images/B21047_08_08.png"/></figure>
    <p class="packt_figref">Figure 8.8: Data samples along with learned cluster centroids</p>
    <p class="normal">As you <a id="_idIndexMarker798"/>can see, samples around the same centroid form a cluster. After seven iterations (you might see slightly more or fewer iterations in your case if you change the random seed in <code class="inlineCode">np.random.seed(0)</code>), the model converges and the centroids will no longer be updated.</p>
    <h2 class="heading-2" id="_idParaDest-189">Implementing k-means with scikit-learn</h2>
    <p class="normal">Having <a id="_idIndexMarker799"/>developed our own k-means clustering model, we will now discuss how to use scikit-learn for a quicker solution by performing the following steps:</p>
    <ol>
      <li class="numberedList" value="1">First, import the <code class="inlineCode">KMeans</code> class and initialize a model with three clusters, as follows:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.cluster </span><span class="hljs-con-keyword">import</span><span class="language-python"> KMeans</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">kmeans_sk = KMeans(n_clusters=</span><span class="hljs-con-number">3</span><span class="language-python">, n_init=</span><span class="hljs-con-string">'auto'</span><span class="language-python">, random_state=</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The <code class="inlineCode">KMeans</code> class takes in the following important parameters:</p>
    <table class="table-container" id="table001-3">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Constructor parameter</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Default value</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Example values</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Description</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">n_clusters</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">8</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">3, 5, 10</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><em class="italic">k</em> clusters</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">max_iter</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">300</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">10, 100, 500</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Maximum number of iterations</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">tol</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">1e-4</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">1e-5, 1e-8</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Tolerance to declare convergence</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">random_state</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">0, 42</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Random seed for program reproducibility</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 8.1: Parameters of the KMeans class</p>
    <ol>
      <li class="numberedList" value="2">We then <a id="_idIndexMarker800"/>fit the model on the data:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">kmeans_sk.fit(X)</span>
</code></pre>
      </li>
      <li class="numberedList">After that, we can obtain the clustering results, including the clusters for data samples and centroids of individual clusters:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">clusters_sk = kmeans_sk.labels_</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">centroids_sk = kmeans_sk.cluster_centers_</span>
</code></pre>
      </li>
      <li class="numberedList">Similarly, we plot the clusters along with the centroids:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.scatter(X[:, </span><span class="hljs-con-number">0</span><span class="language-python">], X[:, </span><span class="hljs-con-number">1</span><span class="language-python">], c=clusters_sk)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.scatter(centroids_sk[:, </span><span class="hljs-con-number">0</span><span class="language-python">], centroids_sk[:, </span><span class="hljs-con-number">1</span><span class="language-python">], marker=</span><span class="hljs-con-string">'*'</span><span class="language-python">, s=</span><span class="hljs-con-number">200</span><span class="language-python">, c=</span><span class="hljs-con-string">'r'</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
      </li>
    </ol>
    <p class="normal">This will result in the following output:</p>
    <figure class="mediaobject"><img alt="A diagram of different colored dots  Description automatically generated" src="../Images/B21047_08_09.png"/></figure>
    <p class="packt_figref">Figure 8.9: Data samples along with learned cluster centroids using scikit-learn</p>
    <p class="normal">We get a <a id="_idIndexMarker801"/>similar result to the previous one using the model we implemented from scratch.</p>
    <h2 class="heading-2" id="_idParaDest-190">Choosing the value of k</h2>
    <p class="normal">Let’s return to our earlier discussion on what the right value for <em class="italic">k</em> is. In the preceding example, it is <a id="_idIndexMarker802"/>more intuitive to set it to <code class="inlineCode">3</code> since we know there are three classes in total. However, in most cases, we don’t know how many groups are sufficient or efficient, and meanwhile, the algorithm needs a specific value of <em class="italic">k</em> to start with. So, how can we choose the value of <em class="italic">k</em>? There is a famous heuristic <a id="_idIndexMarker803"/>approach called the <strong class="keyWord">elbow method</strong>.</p>
    <p class="normal">In the elbow method, different values of <em class="italic">k</em> are chosen and corresponding models are trained; for <a id="_idIndexMarker804"/>each trained model, the <strong class="keyWord">sum of squared errors</strong>, or <strong class="keyWord">SSE </strong>(also called the <strong class="keyWord">sum of within-cluster distances</strong>), of centroids is calculated and is plotted against <em class="italic">k</em>. Note that for one cluster, the squared error (or the within-cluster distance) is computed as the sum of the squared distances from individual samples in the cluster to the centroid. The optimal <em class="italic">k</em> is chosen where the marginal drop of SSE starts to decrease dramatically. This means that further clustering does not provide any substantial gain.</p>
    <p class="normal">Let’s apply the elbow method to the example we covered in the previous section (learning by example is what this book is all about). We perform k-means clustering under different values of <em class="italic">k</em> on the <code class="inlineCode">iris</code> data:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">X = iris.data</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">y = iris.target</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">k_list = </span><span class="hljs-con-built_in">list</span><span class="language-python">(</span><span class="hljs-con-built_in">range</span><span class="language-python">(</span><span class="hljs-con-number">1</span><span class="language-python">, </span><span class="hljs-con-number">7</span><span class="language-python">))</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">sse_list = [</span><span class="hljs-con-number">0</span><span class="language-python">] * </span><span class="hljs-con-built_in">len</span><span class="language-python">(k_list)</span>
</code></pre>
    <p class="normal">We use the <a id="_idIndexMarker805"/>whole feature space and <code class="inlineCode">k</code> ranges from <code class="inlineCode">1</code> to <code class="inlineCode">6</code>. Then, we train individual models and record the resulting SSE, respectively:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> k_ind, k </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">enumerate</span><span class="language-python">(k_list):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    kmeans = KMeans(n_clusters=k, n_init=</span><span class="hljs-con-string">'auto'</span><span class="language-python">, random_state=</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    kmeans.fit(X)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    clusters = kmeans.labels_</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    centroids = kmeans.cluster_centers_</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    sse = </span><span class="hljs-con-number">0</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">range</span><span class="language-python">(k):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        cluster_i = np.where(clusters == i)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        sse += np.linalg.norm(X[cluster_i] - centroids[i])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'k=</span><span class="hljs-con-subst">{k}</span><span class="hljs-con-string">, SSE=</span><span class="hljs-con-subst">{sse}</span><span class="hljs-con-string">'</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    sse_list[k_ind] = sse</span>
k=1, SSE=26.103076447039722
k=2, SSE=16.469773740281195
k=3, SSE=15.089477089696558
k=4, SSE=15.0307321707491
k=5, SSE=14.858930749063735
k=6, SSE=14.883090350867239
</code></pre>
    <p class="normal">Finally, we plot the SSE versus the various <code class="inlineCode">k</code> ranges, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.plot(k_list, sse_list)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">plt.show()</span>
</code></pre>
    <p class="normal">This will result in the following output:</p>
    <figure class="mediaobject"><img alt="A picture containing screenshot, line, rectangle, plot  Description automatically generated" src="../Images/B21047_08_10.png"/></figure>
    <p class="packt_figref">Figure 8.10: k-means elbow – SSE versus k</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong></p>
      <p class="normal">Choosing the right similarity measure for distance calculation in k-means clustering depends on the nature of your data and the specific goals of your analysis. Some common similarity measures include the following:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Euclidean distance</strong>: This <a id="_idIndexMarker806"/>default measure is suitable for continuous data where the difference between feature values matters.</li>
        <li class="bulletList"><strong class="keyWord">Manhattan distance (also known as L1 norm)</strong>: This <a id="_idIndexMarker807"/>calculates the sum of the <a id="_idIndexMarker808"/>absolute differences between the coordinates of two points. It is suitable for high-dimensional data and when the dimensions are not directly comparable.</li>
        <li class="bulletList"><strong class="keyWord">Cosine similarity</strong>: This is <a id="_idIndexMarker809"/>useful for text data or data represented as vectors where the magnitude of the vectors is less important than the orientation.</li>
        <li class="bulletList"><strong class="keyWord">Jaccard similarity</strong>: This <a id="_idIndexMarker810"/>measures the similarity between two sets by comparing their intersection to their union. It is commonly used for binary or categorical data.</li>
      </ul>
    </div>
    <p class="normal">Apparently, the elbow point is <code class="inlineCode">k=3</code>, since the drop in SSE slows down dramatically right after <code class="inlineCode">3</code>. Hence, <code class="inlineCode">k=3</code> is an optimal solution in this case, which is consistent with the fact that there are three classes of flowers.</p>
    <h1 class="heading-1" id="_idParaDest-191">Clustering newsgroups dataset</h1>
    <p class="normal">You should now be very familiar with k-means clustering. Next, let’s see what we are able to <a id="_idIndexMarker811"/>mine from the newsgroups dataset using this algorithm. We will use all the data from four categories, <code class="inlineCode">'alt.atheism'</code>, <code class="inlineCode">'talk.religion.misc'</code>, <code class="inlineCode">'comp.graphics'</code>, and <code class="inlineCode">'sci.space'</code>, as an example. We will then use ChatGPT to describe the generated newsgroup clusters. ChatGPT can generate natural language descriptions of the clusters formed by k-means clustering. This can help in understanding the characteristics and themes of each cluster.</p>
    <h2 class="heading-2" id="_idParaDest-192">Clustering newsgroups data using k-means</h2>
    <p class="normal">We first <a id="_idIndexMarker812"/>load the data from <a id="_idIndexMarker813"/>those newsgroups and preprocess it as we did in <em class="chapterRef">Chapter 7</em>, <em class="italic">Mining the 20 Newsgroups Dataset with Text Analysis Techniques</em>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.datasets </span><span class="hljs-con-keyword">import</span><span class="language-python"> fetch_20newsgroups</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">categories = [</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-string">'alt.atheism'</span><span class="language-python">,</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-string">'talk.religion.misc'</span><span class="language-python">,</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-string">'comp.graphics'</span><span class="language-python">,</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-string">'sci.space'</span><span class="language-python">,</span>
<span class="hljs-con-meta">...</span> <span class="language-python">]</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">groups = fetch_20newsgroups(subset=</span><span class="hljs-con-string">'</span><span class="hljs-con-string">all'</span><span class="language-python">,</span>
                                categories=categories)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">labels = groups.target</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">label_names = groups.target_names</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> nltk.corpus </span><span class="hljs-con-keyword">import</span><span class="language-python"> names</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> nltk.stem </span><span class="hljs-con-keyword">import</span><span class="language-python"> WordNetLemmatizer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">all_names = </span><span class="hljs-con-built_in">set</span><span class="language-python">(names.words())</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">lemmatizer = WordNetLemmatizer()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">get_cleaned_data</span><span class="language-python">(</span><span class="hljs-con-params">groups, lemmatizer, remove_words</span><span class="language-python">):</span>
        data_cleaned = []
        for doc in groups.data:
<span class="hljs-con-meta">...</span> <span class="language-python">        doc = doc.lower()</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        doc_cleaned = </span><span class="hljs-con-string">' '</span><span class="language-python">.join(lemmatizer.lemmatize(word)</span>
                                  for word in doc.split()
                                  if word.isalpha() and
                                  word not in remove_words)
<span class="hljs-con-meta">...</span> <span class="language-python">        data_cleaned.append(doc_cleaned)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">return</span><span class="language-python"> data_cleaned</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_cleaned = get_cleaned_data(groups, lemmatizer, all_names)</span>
</code></pre>
    <p class="normal">We then convert the cleaned text data into count vectors using <code class="inlineCode">CountVectorizer</code> from scikit-learn:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.feature_extraction.text </span><span class="hljs-con-keyword">import</span><span class="language-python"> CountVectorizer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">count_vector = CountVectorizer(stop_words=</span><span class="hljs-con-string">"english"</span><span class="language-python">,</span>
                        max_features=None, max_df=0.5, min_df=2)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_cv = count_vector.fit_transform(data_cleaned)</span>
</code></pre>
    <p class="normal">Note <a id="_idIndexMarker814"/>that the vectorizer we <a id="_idIndexMarker815"/>use here does not limit the number of features (word tokens), but the minimum and maximum document frequency (<code class="inlineCode">min_df</code> and <code class="inlineCode">max_df</code>), which are 2% and 50% of the dataset, respectively. The<strong class="keyWord"> document frequency</strong> of a word is measured by the fraction of documents (samples) in the dataset that contain this word. This helps filter out rare or spurious terms that may not be relevant to the analysis.</p>
    <p class="normal">With the input data ready, we will now try to cluster them into four groups as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">k = </span><span class="hljs-con-number">4</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">kmeans = KMeans(n_clusters=k, n_init=</span><span class="hljs-con-string">'auto'</span><span class="language-python">, random_state=</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">kmeans.fit(data_cv)</span>
</code></pre>
    <p class="normal">Let’s do a quick check on the sizes of the resulting clusters:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">clusters = kmeans.labels_</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> collections </span><span class="hljs-con-keyword">import</span><span class="language-python"> Counter</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(Counter(clusters))</span>
Counter({3: 3360, 0: 17, 1: 7, 2: 3})
</code></pre>
    <p class="normal">The clusters don’t look absolutely correct, with most samples (<code class="inlineCode">3360</code> samples) congested in one big cluster (cluster 3). What could have gone wrong? It turns out that our count-based features are not sufficiently representative. A better numerical representation for text data <a id="_idIndexMarker816"/>is the <strong class="keyWord">term frequency-inverse document frequency</strong> (<strong class="keyWord">tf-idf</strong>). Instead of simply using the token count, or the <a id="_idIndexMarker817"/>so-called <strong class="keyWord">term frequency</strong> (<strong class="keyWord">tf</strong>), it assigns each term frequency a weighting factor that is inversely proportional to the document frequency. In practice, the <strong class="keyWord">idf</strong> factor of a term <em class="italic">t</em> in documents <em class="italic">D</em> is calculated as follows:</p>
    <p class="center"><img alt="" role="presentation" src="../Images/B21047_08_001.png"/></p>
    <p class="normal">Here, <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">D</sub> is the total number of documents, <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">t</sub> is the number of documents containing the term <em class="italic">t</em>, and <em class="italic">1</em> is added to avoid division by 0.</p>
    <p class="normal">With the <code class="inlineCode">idf</code> factor incorporated, the <code class="inlineCode">tf-idf</code> representation diminishes the weight of common <a id="_idIndexMarker818"/>terms (such as <em class="italic">get</em> and <em class="italic">make</em>) and emphasizes terms that rarely occur but convey an important meaning.</p>
    <p class="normal">To use <a id="_idIndexMarker819"/>the <code class="inlineCode">tf-idf</code> representation, we just need to replace <code class="inlineCode">CountVectorizer</code> with <code class="inlineCode">TfidfVectorizer</code> from scikit-learn as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.feature_extraction.text </span><span class="hljs-con-keyword">import</span><span class="language-python"> TfidfVectorizer</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">tfidf_vector = TfidfVectorizer(stop_words=</span><span class="hljs-con-string">'english'</span><span class="language-python">,</span>
                                  max_features=None, max_df=0.5, min_df=2)
</code></pre>
    <p class="normal">The parameter <code class="inlineCode">max_df</code> is used to ignore terms that have a document frequency higher than the given threshold. In this case, terms that appear in more than 50% of the documents will be ignored during the vectorization process. <code class="inlineCode">min_df</code> specifies the minimum document frequency required for a term to be included in the output. Terms that appear in fewer than two documents will be ignored.</p>
    <p class="normal">Now, redo feature extraction using the <code class="inlineCode">tf-idf</code> vectorizer and the k-means clustering algorithm on the resulting feature space:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">data_tv = tfidf_vector.fit_transform(data_cleaned)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">kmeans.fit(data_tv)</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">clusters = kmeans.labels_</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(Counter(clusters))</span>
Counter({1: 1478, 2: 797, 3: 601, 0: 511})
</code></pre>
    <p class="normal">The clustering result becomes more reasonable.</p>
    <p class="normal">We <a id="_idIndexMarker820"/>also take a closer look <a id="_idIndexMarker821"/>at the clusters by examining what they contain and the top 10 terms (the terms with the 10 highest tf-idf scores) representing each cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">cluster_label = {i: labels[np.where(clusters == i)] </span><span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span>
                                                        range(k)}
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">terms = tfidf_vector.get_feature_names_out()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">centroids = kmeans.cluster_centers_</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> cluster, index_list </span><span class="hljs-con-keyword">in</span><span class="language-python"> cluster_label.items():</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    counter = Counter(cluster_label[cluster])</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span>(<span class="hljs-con-string">f'cluster_{cluster}: {len(index_list)} samples'</span>)
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">for</span><span class="language-python"> label_index, count </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">sorted</span><span class="language-python">(counter.items(),</span>
                               key=lambda x: x[1], reverse=True):
<span class="hljs-con-meta">...</span> <span class="language-python">        </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">f'- {label_names[label_index]}: {count} samples'</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">'Top 10 terms:'</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-keyword">for</span><span class="language-python"> ind </span><span class="hljs-con-keyword">in</span><span class="language-python"> centroids[cluster].argsort()[-</span><span class="hljs-con-number">10</span><span class="language-python">:]:</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">' %s'</span><span class="language-python"> % terms[ind], end=</span><span class="hljs-con-string">""</span><span class="language-python">)</span>
<span class="hljs-con-meta">...</span> <span class="language-python">    </span><span class="hljs-con-built_in">print</span><span class="language-python">()</span>
cluster_0: 601 samples
- sci.space: 598 samples
- alt.atheism: 1 samples
- talk.religion.misc: 1 samples
- comp.graphics: 1 samples
Top 10 terms: just orbit moon hst nasa mission launch wa shuttle space
cluster_1: 1478 samples
- alt.atheism: 522 samples
- talk.religion.misc: 387 samples
- sci.space: 338 samples
- comp.graphics: 231 samples
Top 10 terms: say people know like think ha just university wa article
cluster_2: 797 samples
- comp.graphics: 740 samples
- sci.space: 49 samples
- talk.religion.misc: 5 samples
- alt.atheism: 3 samples
Top 10 terms: computer need know looking thanks university program file graphic image
cluster_3: 511 samples
- alt.atheism: 273 samples
- talk.religion.misc: 235 samples
- sci.space: 2 samples
- comp.graphics: 1 samples
Top 10 terms: doe bible think believe say people christian jesus wa god
</code></pre>
    <p class="normal">From <a id="_idIndexMarker822"/>what we observe <a id="_idIndexMarker823"/>in the preceding results:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">cluster_0</code> is obviously about space and includes almost all <code class="inlineCode">sci.space</code> samples and related terms such as <code class="inlineCode">orbit</code>, <code class="inlineCode">moon</code>, <code class="inlineCode">nasa</code>, <code class="inlineCode">launch</code>, <code class="inlineCode">shuttle</code>, and <code class="inlineCode">space</code></li>
      <li class="bulletList"><code class="inlineCode">cluster_1</code> is more of a generic topic</li>
      <li class="bulletList"><code class="inlineCode">cluster_2</code> is more about computer graphics and related terms, such as <code class="inlineCode">computer</code>, <code class="inlineCode">program</code>, <code class="inlineCode">file</code>, <code class="inlineCode">graphic</code>, and <code class="inlineCode">image</code></li>
      <li class="bulletList"><code class="inlineCode">cluster_3</code> is an interesting one, which successfully brings together two overlapping topics, atheism and religion, with key terms including <code class="inlineCode">bible</code>, <code class="inlineCode">believe</code>, <code class="inlineCode">jesus</code>, <code class="inlineCode">christian</code>, and <code class="inlineCode">god</code></li>
    </ul>
    <p class="normal">Feel free to try different values of <code class="inlineCode">k</code>, or use the elbow method to find the optimal one (this is actually an exercise later in this chapter).</p>
    <p class="normal">It is quite interesting to find key terms for each text group via clustering. It will be more fun if we can describe each cluster based on its key terms. Let’s see how we do so with ChatGPT in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-193">Describing the clusters using GPT</h2>
    <p class="normal"><strong class="keyWord">ChatGPT</strong> (<a href="https://chat.openai.com/"><span class="url">https://chat.openai.com/</span></a>) is an <a id="_idIndexMarker824"/>AI language <a id="_idIndexMarker825"/>model developed by <strong class="keyWord">OpenAI</strong> (<a href="https://openai.com/"><span class="url">https://openai.com/</span></a>). It is <a id="_idIndexMarker826"/>part of the <strong class="keyWord">Generative Pre-trained Transformer</strong> (<strong class="keyWord">GPT</strong>) family <a id="_idIndexMarker827"/>of models, specifically <a id="_idIndexMarker828"/>based on GPT-3.5 (GPT-4 is in beta at the time of writing) architecture. ChatGPT is designed to engage in natural language conversations with users and provide human-like responses.</p>
    <p class="normal">The model is trained on a vast amount of diverse text data from the internet, allowing it to understand and generate human-like text across a wide range of topics and contexts. ChatGPT can comprehend questions, prompts, and instructions given by users and generate coherent responses based on its training.</p>
    <p class="normal">ChatGPT <a id="_idIndexMarker829"/>has been used in various applications, including chatbots, virtual assistants, content generation, language translation, and more. Users interact with ChatGPT through <a id="_idIndexMarker830"/>API calls or interactive interfaces, and the model generates responses in real time. However, it is essential to note that while ChatGPT can produce impressive and contextually relevant responses, it may also occasionally generate incorrect or nonsensical answers due to the limitations of current language models. ChatGPT responses should be sense-checked to improve the quality and reliability of the generated text and minimize the risk of misinformation.</p>
    <p class="normal">We will ask ChatGPT to describe the clusters we just generated in the following steps.</p>
    <p class="normal">First, we obtain the top 100 terms as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">keywords = </span><span class="hljs-con-string">'</span><span class="hljs-con-string"> '</span><span class="language-python">.join(</span>
                      terms[ind] for ind in centroids[0].argsort()[-100:])
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(keywords)</span>
big power vehicle using alaska look mass money marketing company loss pluto russian scheduled office express probably research software funding billboard online pat access doe telescope april jet usa digest light want prize forwarded way large mar project sci center command technology air government commercial good work servicing know going comet world propulsion people idea design data university day international use orbital long science need time sky program thing make spencer new year earth spacecraft flight henry billion rocket think ha station lunar solar like cost satellite article toronto zoology just orbit moon hst nasa mission launch wa shuttle space
</code></pre>
    <p class="normal">After signing up (or logging in if you have an account) at <a href="https://chat.openai.com"><span class="url">https://chat.openai.com</span></a>, we ask ChatGPT to describe the topic based on these keywords using the prompt <code class="inlineCode">Describe a common topic based on the following keywords:</code>. Refer to the following screenshot for the entire question and answer:</p>
    <figure class="mediaobject"> <img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21047_08_11.png"/></figure>
    <p class="packt_figref">Figure 8.11: Asking ChatGPT to describe the topic of cluster 0</p>
    <p class="normal">As ChatGPT pointed out correctly, <code class="inlineCode">"the common topic revolves around the various aspects of space exploration, research, technology, and missions, with mentions of key players and celestial bodies in the field."</code> Feel free to repeat the same <a id="_idIndexMarker831"/>process for other clusters. You can <a id="_idIndexMarker832"/>also achieve the same using the ChatGPT API in Python by following these steps:</p>
    <ol>
      <li class="numberedList" value="1">Install the OpenAI library with <code class="inlineCode">pip</code>:
        <pre class="programlisting con-one"><code class="hljs-con">pip install openai
</code></pre>
      </li>
    </ol>
    <p class="normal-one">You can also do this with <code class="inlineCode">conda</code>:</p>
    <pre class="programlisting con-one"><code class="hljs-con">conda install openai
</code></pre>
    <ol>
      <li class="numberedList" value="2">Generate an API key at <a href="https://platform.openai.com/account/api-keys"><span class="url">https://platform.openai.com/account/api-keys</span></a>. Note that you will need to log in or sign up to do this.</li>
      <li class="numberedList">Import the library and set your API key:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">import</span><span class="language-python"> openai</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">openai.api_key = </span><span class="hljs-con-string">'</span><span class="hljs-con-string">&lt;YOUR API KEY&gt;'</span>
</code></pre>
      </li>
      <li class="numberedList">Create a function that allows you to obtain a response from ChatGPT:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">def</span><span class="language-python"> </span><span class="hljs-con-title">get_completion</span><span class="language-python">(</span><span class="hljs-con-params">prompt, model=</span><span class="hljs-con-string">"text-davinci-003"</span><span class="language-python">):</span>
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0
    )
    return response.choices[0].message["content"]
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Here, we use the <code class="inlineCode">text-davinci-003</code> model. Check out the page at <a href="https://platform.openai.com/docs/models"><span class="url">https://platform.openai.com/docs/models</span></a> for more information on the various models available.</p>
    <ol>
      <li class="numberedList" value="5">Query the API:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">response = get_completion(</span><span class="hljs-con-string">f"Describe a common topic based on the </span>
<span class="hljs-con-string">    following keywords: {keywords}"</span>)
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-string">print(response)</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">This will yield a response similar to what you previously read in the web interface. Note that the API call is subject to your plan quota.</p>
    <p class="normal">Up to <a id="_idIndexMarker833"/>this point, we have produced topical keywords by first grouping documents into <a id="_idIndexMarker834"/>clusters and subsequently extracting the top <a id="_idIndexMarker835"/>terms within each cluster. <strong class="keyWord">Topic modeling</strong> is another approach to produce topical keywords but in a much more direct way. It does not simply search for the key terms in individual clusters generated beforehand. What it does is directly extract collections of key terms from documents. You will see how this works in the next section.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Density-Based Spatial Clustering of Applications with Noise</strong> (<strong class="keyWord">DBSCAN</strong>) is another <a id="_idIndexMarker836"/>popular clustering algorithm used for identifying clusters in spatial data. Unlike centroid-based algorithms like k-means, DBSCAN does not require specifying the number of clusters in advance and can discover clusters of arbitrary shapes. It works by partitioning the dataset into clusters of contiguous high-density regions, separated by regions of low density, while also identifying outliers as noise.</p>
      <p class="normal">The algorithm requires two parameters: epsilon (<img alt="" role="presentation" src="../Images/B21047_08_002.png"/>), which defines the maximum distance between two samples for them to be considered as part of the same neighborhood, and <code class="inlineCode">min_samples</code>, which specifies the minimum number of samples required to form a dense region.</p>
      <p class="normal">DBSCAN starts by randomly selecting a point and expanding its neighborhood to find all reachable points within ε distance. If the number of reachable points exceeds <code class="inlineCode">min_samples</code>, the point is labeled as a core point and a new cluster is formed. The process is repeated recursively for all core points and their neighborhoods until all points are assigned to a cluster or labeled as noise.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-194">Discovering underlying topics in newsgroups</h1>
    <p class="normal">A <strong class="keyWord">topic model</strong> is a type of statistical model for discovering the probability distributions of words linked to the topic. The topic in topic modeling does not exactly match the dictionary definition but corresponds to a nebulous statistical concept, which is an abstraction that occurs in a collection of documents.</p>
    <p class="normal">When we read a document, we expect certain words appearing in the title or the body of the text to capture the semantic context of the document. An article about Python programming might have words such as <em class="italic">class</em> and <em class="italic">function</em>, while a story about snakes might have words such as <em class="italic">eggs</em> and <em class="italic">afraid</em>. Documents usually have multiple topics; for instance, this section is about three things: topic modeling, non-negative matrix factorization, and latent Dirichlet allocation, which we will discuss shortly. We can therefore define an additive model for topics by assigning different weights to topics.</p>
    <p class="normal"><strong class="keyWord">Topic modeling</strong> is widely <a id="_idIndexMarker837"/>used for mining hidden semantic structures in given text data. There are two popular topic modeling algorithms—<strong class="keyWord">non-negative matrix factorization</strong> (<strong class="keyWord">NMF</strong>) and <strong class="keyWord">latent Dirichlet allocation</strong> (<strong class="keyWord">LDA</strong>). We will go through both of these in the next two sections.</p>
    <h2 class="heading-2" id="_idParaDest-195">Topic modeling using NMF</h2>
    <p class="normal"><strong class="keyWord">Non-negative matrix factorization</strong> (<strong class="keyWord">NMF</strong>) is a dimensionality reduction technique used for feature <a id="_idIndexMarker838"/>extraction and data representation. It factorizes <a id="_idIndexMarker839"/>a non-negative input matrix, <strong class="keyWord">V</strong>, into a product of two smaller matrices, <strong class="keyWord">W</strong> and <strong class="keyWord">H</strong>, in such a way that these three matrices <a id="_idIndexMarker840"/>have no negative values. These two lower-dimensional matrices represent features and their associated coefficients. In the context of NLP, these three matrices have the following meanings:</p>
    <ul>
      <li class="bulletList">The input matrix <strong class="keyWord">V</strong> is the term count or tf-idf matrix of size <em class="italic">n</em> * <em class="italic">m</em>, where <em class="italic">n</em> is the number of documents or samples, and <em class="italic">m</em> is the number of terms.</li>
      <li class="bulletList">The first decomposition output matrix <strong class="keyWord">W</strong> is the feature matrix of size <em class="italic">t</em> * <em class="italic">m</em>, where <em class="italic">t</em> is the number of topics specified. Each row of <strong class="keyWord">W</strong> represents a topic with each element in the row representing the rank of a term in the topic.</li>
      <li class="bulletList">The second decomposition output matrix <strong class="keyWord">H</strong> is the coefficient matrix of size <em class="italic">n</em> * <em class="italic">t</em>. Each row of <strong class="keyWord">H</strong> represents a document, with each element in the row representing the weight of a topic within the document.</li>
    </ul>
    <p class="normal">How to derive the computation of <strong class="keyWord">W</strong> and <strong class="keyWord">H</strong> is beyond the scope of this book. However, you can refer to the following example to get a better sense of how NMF works:</p>
    <figure class="mediaobject"><img alt="A picture containing text, diagram, screenshot, line  Description automatically generated" src="../Images/B21047_08_12.png"/></figure>
    <p class="packt_figref">Figure 8.12: Example of matrix W and matrix H derived from an input matrix V</p>
    <div class="note">
      <p class="normal">If you are interested in reading more about NMF, feel free to check out the original paper <em class="italic">Generalized Nonnegative Matrix Approximations with Bregman Divergences</em>, by Inderjit S. Dhillon and Suvrit Sra, in NIPS 2005.</p>
    </div>
    <p class="normal">Let’s now <a id="_idIndexMarker841"/>apply NMF to our newsgroups data. Scikit-learn has <a id="_idIndexMarker842"/>a nice module for decomposition that includes NMF:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.decomposition </span><span class="hljs-con-keyword">import</span><span class="language-python"> NMF</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">t = </span><span class="hljs-con-number">20</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nmf = NMF(n_components=t, random_state=</span><span class="hljs-con-number">42</span><span class="language-python">)</span>
</code></pre>
    <p class="normal">We specify 20 topics (<code class="inlineCode">n_components</code>) as an example. Important parameters of the model are included in the following table:</p>
    <table class="table-container" id="table002-3">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Constructor parameter</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Default value</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Example values</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Description</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">n_components</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">5</code>, <code class="inlineCode">10</code>, <code class="inlineCode">20</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Number of components—in the context of topic modeling, this corresponds to the number of topics. If <code class="inlineCode">None</code>, it becomes the number of input features.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">max_iter</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">200</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">100</code>, <code class="inlineCode">200</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Maximum number of iterations.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">tol</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">1e-4</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">1e-5</code>, <code class="inlineCode">1e-8</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Tolerance to declare convergence.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 8.2: Parameters of the NMF class</p>
    <p class="normal">We used <a id="_idIndexMarker843"/>the term matrix as input to the NMF model, but you <a id="_idIndexMarker844"/>could also use the <code class="inlineCode">tf-idf</code> one instead. Now, fit the NMF model, <code class="inlineCode">nmf</code>, on the term matrix, <code class="inlineCode">data_cv</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">nmf.fit(data_cv)</span>
</code></pre>
    <p class="normal">We can obtain the resulting topic feature rank <strong class="keyWord">W</strong> after the model is trained:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(nmf.components_)</span>
[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  0.00000000e+00 1.82524532e-04]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  7.77697392e-04 3.85995474e-03]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  0.00000000e+00 0.00000000e+00]
 ...
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.71332203e-02
  0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  0.00000000e+00 4.31048632e-05]
 [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00
  0.00000000e+00 0.00000000e+00]]
</code></pre>
    <p class="normal">For each topic, we display the top 10 terms based on their ranks:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">terms_cv = count_vector.get_feature_names_out()</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> topic_idx, topic </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">enumerate</span><span class="language-python">(nmf.components_):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">"Topic {}:"</span><span class="language-python"> .</span><span class="hljs-con-built_in">format</span><span class="language-python">(topic_idx))</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">" "</span><span class="language-python">.join([terms_cv[i] </span><span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span><span class="language-python"> topic.argsort()[-</span><span class="hljs-con-number">10</span><span class="language-python">:]]))</span>
Topic 0:
available quality program free color version gif file image jpeg
Topic 1:
ha article make know doe say like just people think
Topic 2:
include available analysis user software ha processing data tool image
Topic 3:
atmosphere kilometer surface ha earth wa planet moon spacecraft solar
Topic 4:
communication technology venture service market ha commercial space satellite launch
Topic 5:
verse wa jesus father mormon shall unto mcconkie lord god
Topic 6:
format message server object image mail file ray send graphic
Topic 7:
christian people doe atheism believe religion belief religious god atheist
Topic 8:
file graphic grass program ha package ftp available image data
Topic 9:
speed material unified star larson book universe theory physicist physical
Topic 10:
planetary station program group astronaut center mission shuttle nasa space
Topic 11:
infrared high astronomical center acronym observatory satellite national telescope space
Topic 12:
used occurs true form ha ad premise conclusion argument fallacy
Topic 13:
gospel people day psalm prophecy christian ha matthew wa jesus
Topic 14:
doe word hanging say greek matthew mr act wa juda
Topic 15:
siggraph graphic file information format isbn data image ftp available
Topic 16:
venera mar lunar surface space venus soviet mission wa probe
Topic 17:
april book like year time people new did article wa
Topic 18:
site retrieve ftp software data information client database gopher search
Topic 19:
use look xv color make program correction bit gamma image
</code></pre>
    <p class="normal">There <a id="_idIndexMarker845"/>are a number <a id="_idIndexMarker846"/>of interesting topics, for instance:</p>
    <ul>
      <li class="bulletList">Computer graphics-related topics, such as <code class="inlineCode">0</code>, <code class="inlineCode">2</code>, <code class="inlineCode">6</code>, and <code class="inlineCode">8</code></li>
      <li class="bulletList">Space-related ones, such as <code class="inlineCode">3</code>, <code class="inlineCode">4</code>, and <code class="inlineCode">9</code></li>
      <li class="bulletList">Religion-related ones, such as <code class="inlineCode">5</code>, <code class="inlineCode">7</code>, and <code class="inlineCode">13</code></li>
    </ul>
    <p class="normal">Some topics, such as <code class="inlineCode">1</code> and <code class="inlineCode">12</code>, are hard to interpret. This is totally fine since topic modeling is a kind of free-form learning.</p>
    <h2 class="heading-2" id="_idParaDest-196">Topic modeling using LDA</h2>
    <p class="normal">Let’s <a id="_idIndexMarker847"/>explore another popular <a id="_idIndexMarker848"/>topic modeling algorithm, <strong class="keyWord">Latent Dirichlet Allocation</strong> (<strong class="keyWord">LDA</strong>). LDA is a generative probabilistic graphical model that explains each input document <a id="_idIndexMarker849"/>by means of a mixture of topics with certain probabilities. It assumes that each document is a mixture of multiple topics, and each topic is characterized by a specific word probability distribution. The algorithm iteratively assigns words in documents to topics and updates the topic distributions based on the observed word co-occurrences. Again, <strong class="keyWord">topic</strong> in topic modeling means a collection of words with a certain connection. In other words, LDA basically deals with two probability values, <em class="italic">P</em>(<em class="italic">term</em> V <em class="italic">topic</em>) and <em class="italic">P</em>(<em class="italic">topic</em> V <em class="italic">document</em>). This can be difficult to understand at the beginning. So, let’s start from the bottom, the end result of an LDA model.</p>
    <p class="normal">Let’s take a look at the following set of documents:</p>
    <pre class="programlisting code"><code class="hljs-code">Document <span class="hljs-number">1</span>: This restaurant <span class="hljs-keyword">is</span> famous <span class="hljs-keyword">for</span> fish <span class="hljs-keyword">and</span> chips.
Document <span class="hljs-number">2</span>: I had fish <span class="hljs-keyword">and</span> rice <span class="hljs-keyword">for</span> lunch.
Document <span class="hljs-number">3</span>: My sister bought me a cute kitten.
Document <span class="hljs-number">4</span>: Some research shows eating too much rice <span class="hljs-keyword">is</span> bad.
Document <span class="hljs-number">5</span>: I always forget to feed fish to my cat.
</code></pre>
    <p class="normal">Now, let’s say we want two topics. The topics derived from these documents may appear as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">Topic <span class="hljs-number">1</span>: <span class="hljs-number">30</span>% fish, <span class="hljs-number">20</span>% chip, <span class="hljs-number">30</span>% rice, <span class="hljs-number">10</span>% lunch, <span class="hljs-number">10</span>% restaurant (which we can interpret Topic <span class="hljs-number">1</span> to be food related)
Topic <span class="hljs-number">2</span>: <span class="hljs-number">40</span>% cute, <span class="hljs-number">40</span>% cat, <span class="hljs-number">10</span>% fish, <span class="hljs-number">10</span>% feed (which we can interpret Topic <span class="hljs-number">1</span> to be about pet)
</code></pre>
    <p class="normal">Therefore, we find how each document is represented by these two topics:</p>
    <pre class="programlisting code"><code class="hljs-code">Document <span class="hljs-number">1</span>: <span class="hljs-number">85</span>% Topic <span class="hljs-number">1</span>, <span class="hljs-number">15</span>% Topic <span class="hljs-number">2</span>
Document <span class="hljs-number">2</span>: <span class="hljs-number">88</span>% Topic <span class="hljs-number">1</span>, <span class="hljs-number">12</span>% Topic <span class="hljs-number">2</span>
Document <span class="hljs-number">3</span>: <span class="hljs-number">100</span>% Topic <span class="hljs-number">2</span>
Document <span class="hljs-number">4</span>: <span class="hljs-number">100</span>% Topic <span class="hljs-number">1</span>
Document <span class="hljs-number">5</span>: <span class="hljs-number">33</span>% Topic <span class="hljs-number">1</span>, <span class="hljs-number">67</span>% Topic <span class="hljs-number">2</span>
</code></pre>
    <p class="normal">After <a id="_idIndexMarker850"/>seeing a toy example, we <a id="_idIndexMarker851"/>come back to its learning procedure:</p>
    <ol>
      <li class="numberedList" value="1">Specify the number of topics, <em class="italic">T</em>. Now we have topics 1, 2, …, and <em class="italic">T</em>.</li>
      <li class="numberedList">For each document, randomly assign one of the topics to each term in the document.</li>
      <li class="numberedList">For each document, calculate <em class="italic">P</em>(<em class="italic">topic</em> = <em class="italic">t</em> V <em class="italic">document</em>), which is the proportion of terms in the document that are assigned to the topic <em class="italic">t</em>.</li>
      <li class="numberedList">For each topic, calculate <em class="italic">P</em>(<em class="italic">term</em> = <em class="italic">w</em> V <em class="italic">topic</em>), which is the proportion of term <em class="italic">w</em> among all terms that are assigned to the topic.</li>
      <li class="numberedList">For each term <em class="italic">w</em>, reassign its topic based on the latest probabilities <em class="italic">P</em>(<em class="italic">topic</em> = <em class="italic">t</em> V <em class="italic">document</em>) and <em class="italic">P</em>(<em class="italic">term</em> = <em class="italic">w</em> V <em class="italic">topic</em> = <em class="italic">t</em>).</li>
      <li class="numberedList">Repeat <em class="italic">steps 3</em> to <em class="italic">5</em> under the latest topic distributions for each iteration. The training stops if the model converges or reaches the maximum number of iterations.</li>
    </ol>
    <p class="normal">LDA is trained in a generative manner, where it tries to abstract from the documents a set of hidden topics that are likely to generate a certain collection of words.</p>
    <p class="normal">With all this in mind, let’s see LDA in action. The LDA model is also included in scikit-learn:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">from</span><span class="language-python"> sklearn.decomposition </span><span class="hljs-con-keyword">import</span><span class="language-python"> LatentDirichletAllocation</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">t = </span><span class="hljs-con-number">20</span>
<span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">lda = LatentDirichletAllocation(n_components=t,</span>
                      learning_method='batch',random_state=42)
</code></pre>
    <p class="normal">Again, we specify 20 topics (<code class="inlineCode">n_components</code>). The key parameters of the model are included in the following table:</p>
    <table class="table-container" id="table003-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Constructor parameter</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Default value</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Example values</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Description</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">n_components</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">10</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">5, 10, 20</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Number of components—in the context of topic modeling, this corresponds to the number of topics.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">learning_method</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">"batch"</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">"online", "batch"</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">In <code class="inlineCode">batch</code> mode, all training data is used for each update. In <code class="inlineCode">online</code> mode, a mini-batch of training data is used for each update. In general, if the data size is large, the <code class="inlineCode">online</code> mode is faster.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">max_iter</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">10</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">10, 20</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Maximum number of iterations.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">randome_state</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">None</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">0, 42</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">Seed used by the random number generator.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 8.3: Parameters of the LatentDirichletAllocation class</p>
    <p class="normal">For the input <a id="_idIndexMarker852"/>data to LDA, remember that LDA only takes in term <a id="_idIndexMarker853"/>counts as it is a probabilistic graphical model. This is unlike NMF, which can work with both the term count matrix and the tf-idf matrix as long as they are non-negative data. Again, we use the term matrix defined previously as input to the LDA model. Now, we fit the LDA model on the term matrix, <code class="inlineCode">data_cv</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="language-python">lda.fit(data_cv)</span>
</code></pre>
    <p class="normal">We can obtain the resulting topic term rank after the model is trained:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-built_in">print</span><span class="language-python">(lda.components_)</span>
[[0.05     2.05    2.05    ...   0.05      0.05    0.05 ]
 [0.05     0.05    0.05    ...   0.05      0.05    0.05 ]
 [0.05     0.05    0.05    ...   4.0336285 0.05    0.05 ]
 ...
 [0.05     0.05    0.05    ...   0.05      0.05    0.05 ]
 [0.05     0.05    0.05    ...   0.05      0.05    0.05 ]
 [0.05     0.05    0.05    ...   0.05      0.05    3.05 ]]
</code></pre>
    <p class="normal">Similarly, for each topic, we display the top 10 terms based on their ranks as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> <span class="hljs-con-keyword">for</span><span class="language-python"> topic_idx, topic </span><span class="hljs-con-keyword">in</span><span class="language-python"> </span><span class="hljs-con-built_in">enumerate</span><span class="language-python">(lda.components_):</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">"Topic {}:"</span><span class="language-python"> .</span><span class="hljs-con-built_in">format</span><span class="language-python">(topic_idx))</span>
<span class="hljs-con-meta">...</span> <span class="language-python">        </span><span class="hljs-con-built_in">print</span><span class="language-python">(</span><span class="hljs-con-string">" "</span><span class="language-python">.join([terms_cv[i] </span><span class="hljs-con-keyword">for</span><span class="language-python"> i </span><span class="hljs-con-keyword">in</span>
                                   topic.argsort()[-10:]]))
Topic 0:
atheist doe ha believe say jesus people christian wa god
Topic 1:
moment just adobe want know ha wa hacker article radius
Topic 2:
center point ha wa available research computer data graphic hst
Topic 3:
objective argument just thing doe people wa think say article
Topic 4:
time like brian ha good life want know just wa
Topic 5:
computer graphic think know need university just article wa like
Topic 6:
free program color doe use version gif jpeg file image
Topic 7:
gamma ray did know university ha just like article wa
Topic 8:
tool ha processing using data software color program bit image
Topic 9:
apr men know ha think woman just university article wa
Topic 10:
jpl propulsion mission april mar jet command data spacecraft wa
Topic 11:
russian like ha university redesign point option article space station
Topic 12:
ha van book star material physicist universe physical theory wa
Topic 13:
bank doe book law wa article rushdie muslim islam islamic
Topic 14:
think gopher routine point polygon book university article know wa
Topic 15:
ha rocket new lunar mission satellite shuttle nasa launch space
Topic 16:
want right article ha make like just think people wa
Topic 17:
just light space henry wa like zoology sky article toronto
Topic 18:
comet venus solar moon orbit planet earth probe ha wa
Topic 19:
site format image mail program available ftp send file graphic
</code></pre>
    <p class="normal">There are a number of interesting topics that we just mined, for instance:</p>
    <ul>
      <li class="bulletList">Computer graphics-related topics, such as <code class="inlineCode">2</code>, <code class="inlineCode">5</code>, <code class="inlineCode">6</code>, <code class="inlineCode">8</code>, and <code class="inlineCode">19</code></li>
      <li class="bulletList">Space-related ones, such as <code class="inlineCode">10</code>, <code class="inlineCode">11</code>, <code class="inlineCode">12</code>, and <code class="inlineCode">15</code></li>
      <li class="bulletList">Religion-related ones, such as <code class="inlineCode">0</code> and <code class="inlineCode">13</code></li>
    </ul>
    <p class="normal">There are <a id="_idIndexMarker854"/>also topics involving noise, for example, <code class="inlineCode">9</code> and <code class="inlineCode">16</code>, which <a id="_idIndexMarker855"/>may require some imagination to interpret. Once more, this observation is entirely expected, given that LDA or topic modeling, as mentioned before, falls under the category of free-form learning.</p>
    <h1 class="heading-1" id="_idParaDest-197">Summary</h1>
    <p class="normal">The project in this chapter was about finding hidden similarities underneath newsgroups data, be it semantic groups, themes, or word clouds. We started with what unsupervised learning does and the typical types of unsupervised learning algorithms. We then introduced unsupervised learning clustering and studied a popular clustering algorithm, k-means, in detail. We also explored using ChatGPT to describe the topics of individual clusters based on their keywords.</p>
    <p class="normal">We also talked about tf-idf as a more efficient feature extraction tool for text data. After that, we performed k-means clustering on the newsgroups data and obtained four meaningful clusters. After examining the key terms in each resulting cluster, we went straight to extracting representative terms among original documents using topic modeling techniques. Two powerful topic modeling approaches, NMF and LDA, were discussed and implemented. Finally, we had some fun interpreting the topics we obtained from both methods.</p>
    <p class="normal">So far, we have covered all the main categories of unsupervised learning, including dimensionality reduction, clustering, and topic modeling, which is also dimensionality reduction in a way.</p>
    <p class="normal">In the next chapter, we will talk about <strong class="keyWord">Support Vector Machines</strong> (<strong class="keyWord">SVMs</strong>) for face recognition. SVM is a popular choice for a wide range of classification and regression tasks, especially when dealing with complex decision boundaries. We will also cover another dimensionality reduction technique called principal component analysis.</p>
    <h1 class="heading-1" id="_idParaDest-198">Exercises</h1>
    <ol>
      <li class="numberedList" value="1">Ask ChatGPT to describe other clusters we generated through k-means clustering. You may experiment with various prompts and discover intriguing information within these clusters.</li>
      <li class="numberedList">Perform k-means clustering on newsgroups data using different values of <em class="italic">k</em>, or use the elbow method to find the optimal one. See if you get better grouping results.</li>
      <li class="numberedList">Try different numbers of topics in either NMF or LDA and see which one produces more meaningful topics in the end. This should be a fun exercise.</li>
      <li class="numberedList">Can you experiment with NMF or LDA on the entire 20 groups of newsgroups data? Are the resulting topics full of noise or gems?</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-199">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/yuxi"><span class="url">https://packt.link/yuxi</span></a></p>
    <p class="normal"><img alt="" role="presentation" src="../Images/QR_Code187846872178698968.png"/></p>
  </div>
</body></html>
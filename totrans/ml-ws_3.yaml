- en: 3\. Supervised Learning – Key Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about key concepts for solving a supervised
    learning data problem. Starting from splitting the dataset to effectively create
    unbiased models that perform well on unseen data, you will learn how to measure
    the performance of the model in order to analyze it and take the necessary actions
    to improve it. By the end of this chapter, you will have a firm understanding
    of how to split a dataset, measure a model's performance, and perform error analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapter, we saw how to solve data problems using unsupervised
    learning algorithms and applied the concepts that we learned about to a real-life
    dataset. We also learned how to compare the performance of various algorithms
    and studied two different metrics for performance evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the main steps for working on a supervised
    machine learning problem. First, this chapter explains the different sets in which
    data needs to be split for training, validating, and testing your model. Next,
    the most common evaluation metrics will be explained. It is important to highlight
    that, among all the metrics available, only one should be selected as the evaluation
    metric of the study, and its selection should be made by considering the purpose
    of the study. Finally, we will learn how to perform error analysis, with the purpose
    of understanding what measures to take to improve the results of a model.
  prefs: []
  type: TYPE_NORMAL
- en: The previous concepts apply to both classification and regression tasks, where
    the former refers to problems where the output corresponds to a finite number
    of labels, while the latter deals with a continuous output number. For instance,
    a model that's created to determine whether a person will attend a meeting falls
    within the classification tasks group. On the other hand, a model that predicts
    the price of a product is solving a regression task.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Differing from unsupervised learning algorithms, supervised learning algorithms
    are characterized by their ability to find relationships between a set of features
    and a target value (be it discrete or continuous). Supervised learning can solve
    two types of tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: The objective of these tasks is to approximate a function
    that maps a set of features to a discrete set of outcomes. These outcomes are
    commonly known as class labels or categories. Each observation in the dataset
    should have a class label associated with it to be able to train a model that
    is capable of predicting such an outcome for future data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of a classification task is one that uses demographical data to determine
    someone's marital status.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Regression**: Although in regression tasks a function is also created to
    map a relationship between some inputs and some targets, in regression tasks,
    the outcome is continuous. This means that the outcome is a real value that can
    be an integer or a float.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of a regression task is using the different characteristics of a
    product to predict its price.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Although many algorithms can be adapted to solve both of these tasks, it is
    important to highlight that there are some algorithms that don't, which is why
    it is important to know the task that we want to perform in order to choose the
    algorithm accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore several topics that are crucial for performing any supervised
    learning task.
  prefs: []
  type: TYPE_NORMAL
- en: Model Validation and Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the information now available online, it is easy for almost anybody
    to start working on a machine learning project. However, choosing the right algorithm
    for your data is a challenge when there are many options available. Due to this,
    the decision to use one algorithm over another is achieved through trial and error,
    where different alternatives are tested.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the decision process to arrive at a good model covers not only the
    selection of the algorithm but also the tuning of its hyperparameters. To do this,
    a conventional approach is to divide the data into three parts (training, validation,
    and testing sets), which will be explained further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data Partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data partitioning** is a process involving dividing a dataset into three
    subsets so that each set can be used for a different purpose. This way, the development
    of a model is not affected by the introduction of bias. The following is an explanation
    of each subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: As the name suggests, this is the portion of the dataset
    that''s used for training the model. It consists of the input data (the observations)
    paired with an outcome (the label class). This set can be used to train as many
    models as desired, using different algorithms. However, performance evaluation
    is not done on this set because, since this set was used to train the model, the
    measure would be biased.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set**: Also known as the dev set, this set is used to perform
    an unbiased evaluation of each model while fine-tuning the hyperparameters. Performance
    evaluation is frequently done on this set of data to test different configurations
    of the hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the model does not learn from this data (it learns from the training
    set data), it is indirectly affected by the data in this set due to its participation
    in the process of deciding the changes to the hyperparameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After running different configurations of hyperparameters based on the performance
    of the model on the validation set, a winning model is selected for each algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Testing set**: This is used to perform the final evaluation of the model''s
    performance (after training and validation) on unseen data. This helps measure
    the performance of the model with real-life data for future predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The testing set is also used to compare competing models. Considering that the
    training set was used to train different models and the validation set was used
    to fine-tune the hyperparameters of each model to select a winning configuration,
    the purpose of the testing set is to perform an unbiased comparison of the final models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following diagram shows the process of selecting the ideal model and using
    the sets mentioned previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Dataset partition purposes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Dataset partition purposes'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sections `A`–`D` shown in the preceding diagram are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Section `A` refers to the process of training the model for the desired algorithms
    using the data contained in the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section `B` represents the fine-tuning process of the hyperparameters of each
    model. The selection of the best configuration of hyperparameters is based on
    the performance of the model on the validation set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Section `C` shows the process of selecting the final model by comparing the
    final configuration of each algorithm based on its performance on the testing
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, section `D` represents the selected model that will be applied to real-life
    data for prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Initially, machine learning problems were solved by only partitioning data
    into two sets: a training and a testing set. This approach consisted of using
    the training set to train the model, which is the same as the approach with three
    sets. However, the testing set was used for fine-tuning the hyperparameters as
    well as for determining the ultimate performance of the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Although this approach can also work, models that are created using this approach
    do not always perform equally well on unseen real-life data. This is mainly because,
    as mentioned previously, the use of the sets to fine-tune the hyperparameters
    indirectly introduces bias into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, there is one way to achieve a less biased model while dividing
    the dataset into two sets, which is called a **cross-validation split**. We will
    explore this in the *Cross-Validation* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Split Ratio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the purposes of the various sets are clear, it is important to clarify
    the split ratio in which data needs to be divided. Although there is no exact
    science for calculating the split ratio, there are a couple of things to consider
    when doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Size of the dataset**: Previously, when data was not easily available, datasets
    contained between 100 and 100,000 instances, and the conventionally accepted split
    ratio was 60/20/20% for the training, validation, and testing sets, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With software and hardware improving every day, researchers can put together
    datasets that contain over a million instances. This capacity to gather huge amounts
    of data allows the split ratio to be 98/1/1%, respectively. This is mainly because
    the larger the dataset, the more data can be used for training a model, without
    compromising the amount of data left for the validation and testing sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**The algorithm**: It is important to consider that some algorithms may require
    higher amounts of data to train a model, as is the case with neural networks.
    In this case, as with the preceding approaches, you should always opt for a larger
    training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, some algorithms do not require the validation and testing
    sets to be split equally. For instance, a model with fewer hyperparameters can
    be easily tuned, which allows the validation set to be smaller than the testing
    set. However, if a model has many hyperparameters, you will need to have a larger
    validation set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Nevertheless, even though the preceding measures serve as a guide for splitting
    the dataset, it is always important to consider the distribution of your dataset
    and the purpose of the study. For instance, a model that is going to be used to
    predict an outcome on data with a different distribution than the one used to
    train the model, the real-life data, even if limited, must at least be a part
    of the testing set to make sure that the model will work for the desired purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram displays the proportional partition of the dataset into
    three subsets. It is important to highlight that the training set must be larger
    than the other two, as it is the one to be used for training the model. Additionally,
    it is possible to observe that both the training and validation sets have an effect
    on the model, while the testing set is mainly used to validate the actual performance
    of the model with unseen data. Considering this, the training and validation sets
    must come from the same distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Visualization of the split ratio'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.2: Visualization of the split ratio'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.01: Performing a Data Partition on a Sample Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will be performing a data partition on the `wine` dataset
    using the split ratio method. The partition in this exercise will be done using
    the three-splits approach. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For the exercises and activities within this chapter, you will need to have
    Python 3.7, NumPy, Jupyter, Pandas, and scikit-learn installed on your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise. Import the required elements,
    as well as the `load_wine` function from scikit-learn''s `datasets` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first line imports the function that will be used to load the dataset from
    scikit-learn. Next, `pandas` library is imported. Finally, the `train_test_split`
    function is imported, which will be in charge of partitioning the dataset. The
    function partitions the data into two subsets (a train and a test set). As the
    objective of this exercise is to partition data into three subsets, the function
    will be used twice to achieve the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the `wine` toy dataset and store it in a variable named `data`. Use the
    following code snippet to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `load_wine` function loads the toy dataset provided by scikit-learn.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To check the characteristics of the dataset, visit the following link: [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output from the preceding function is a dictionary-like object, which separates
    the features (callable as data) from the target (callable as target) into two
    attributes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert each attribute (data and target) into a Pandas DataFrame to facilitate
    data manipulation. Print the shape of both DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output from the `print` function should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the values in the first parenthesis represent the shape of DataFrame `X`
    (known as the features matrix), while the values in the second parenthesis refer
    to the shape of DataFrame `Y` (known as the target matrix).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform your first split of the data using the `train_test_split` function.
    Use the following code snippet to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The inputs of the `train_test_split` function are the two matrices `(X,Y)` and
    the size of the test set, as a value between 0 and 1, which represents the proportion.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By printing the shape of all four matrices, as per the preceding code snippet,
    it is possible to confirm that the size of the test subset (both `X` and `Y`)
    is 20% of the total size of the original dataset (150 * 0.2 = 35.6) rounded to
    an integer, while the size of the train set is the remaining 80%:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create a validation set (dev set), we will use the `train_test_split` function
    to divide the train sets we obtained in the previous step. However, to obtain
    a dev set that''s the same shape as the test set, it is necessary to calculate
    the proportion of the size of the test set over the size of the train set before
    creating a validation set. This value will be used as the `test_size` for the
    next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `36` is the size of the test set we created in the previous step, while
    `142` is the size of the train set that will be further split. The result from
    this operation is around `0.25`, which can be verified using the `print` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `train_test_split` function to divide the train set into two subsets
    (train and dev sets). Use the result from the operation in the previous step as
    the `test_size`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the `print` function is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2AtXAWS](https://packt.live/2AtXAWS).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YECtsG](https://packt.live/2YECtsG).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have successfully split the dataset into three subsets to develop efficient
    machine learning projects. Feel free to test different split ratios.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the split ratio to partition data is not fixed and should be
    decided by taking into account the amount of data available, the type of algorithm
    to be used, and the distribution of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Cross-validation** is also a procedure that''s used to partition data by
    resampling the data that''s used to train and validate the model. It consists
    of a parameter, *K,* that represents the number of groups that the dataset will
    be divided into.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to this, the procedure is also referred to as K-fold cross-validation,
    where *K* is usually replaced by a number of your choice. For instance, a model
    that''s created using a 10-fold cross-validation procedure signifies a model where
    data is divided into 10 subgroups. The procedure of cross-validation is illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Cross-validation procedure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.3: Cross-validation procedure'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram displays the general procedure that''s followed during
    cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is shuffled randomly, considering that the cross-validation process is repeated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data is split into *K* subgroups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The validation/testing set is selected as one of the subgroups that were created.
    The rest of the subgroups become the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model is trained on the training set, as usual. The model is evaluated using
    the validation/testing set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result from that iteration is saved. The parameters are tuned based on the
    results, and the process starts again by reshuffling the data. This process is
    repeated K number of times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: According to the preceding steps, the dataset is divided into *K* sets and the
    model is trained *K* times. Each time, one set is selected as the validation set
    and the remaining sets are used for the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation can be done using a three-split approach or a two-split one.
    For the former, the dataset is initially divided into training and testing sets,
    after which the training set is divided using cross-validation to create different
    configurations of training and validation sets. The latter approach, on the other
    hand, uses cross-validation on the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The popularity of cross-validation is due to its capacity to build "unbiased"
    models as it allows us to measure the performance of the algorithm on different
    segments of the dataset, which also provides us with an idea of its performance
    on unseen data. It is also popular because it allows you to build highly effective
    models out of a small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There is no exact science to choosing the value for *K*, but it is important
    to consider that lower values for *K* tend to decrease variance and increase bias,
    while higher *K* values result in the opposite behavior. Also, the lower *K* is,
    the less expensive the processes, which results in faster running times.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The concepts of variance and bias will be explained in the *Bias, Variance,
    and Data Mismatch* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.02: Using Cross-Validation to Partition the Train Set into a Training
    and a Validation Set'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will be performing a data partition on the `wine` dataset
    using the cross-validation method. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise and import all the required elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last line in the preceding code imports the `KFold` class from scikit-learn,
    which will be used to partition the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the `wine` dataset as per the previous exercise and create the Pandas
    DataFrames containing the features and target matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and testing sets using the `train_test_split`
    function, which you learned about in the previous exercise, using a `test_size`
    of 0.10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `KFold` class with a 10-fold configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Feel free to experiment with the values of `K` to see how the output shapes
    of this exercise vary.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apply the `split` method to the data in `X`. This method will output the index
    of the instances to be used as training and validation sets. This method creates
    10 different split configurations. Save the output in a variable named `splits`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that it is not necessary to run the `split` method on the data in `Y`,
    as the method only saves the index numbers, which will be the same for `X` and
    `Y`. The actual splitting is handled next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a `for` loop that will go through the different split configurations.
    In the loop body, create the variables that will hold the data for the training
    and validation sets. Use the following code snippet to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `for` loop goes through `K` number of configurations. In the body of the
    loop, the data is split using the index numbers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By printing the shape of all the subsets, as per the preceding snippet, the
    output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The code to train and evaluate the model should be written inside the loop body,
    given that the objective of the cross-validation procedure is to train and validate
    the model using the different split configurations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have successfully performed a cross-validation split on a sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2N0lPi0](https://packt.live/2N0lPi0).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Y290tK](https://packt.live/2Y290tK).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, cross-validation is a procedure that's used to shuffle and split
    the data into training and validation sets so that the process of training and
    validating is done each time on a different set of data, thus achieving a model
    with low bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.01: Data Partitioning on a Handwritten Digit Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Your company specializes in recognizing handwritten characters. It wants to
    improve the recognition of digits, which is why they have gathered a dataset of
    1,797 handwritten digits from 0 to 9\. The images have already been converted
    into their numeric representation, and so they have provided you with the dataset
    to split it into training/validation/testing sets. You can choose to either perform
    conventional splitting or cross-validation. Follow these steps to complete this
    activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import all the required elements to split a dataset, as well as the `load_digits`
    function from scikit-learn to load the `digits` dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `digits` dataset and create Pandas DataFrames containing the features
    and target matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the conventional split approach, using a split ratio of 60/20/20%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the same DataFrames, perform a 10-fold cross-validation split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 228\. Feel free to try different
    parameters to arrive at different results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model evaluation is indispensable for creating effective models that not only
    perform well on the data that was used to train the model but also on unseen data.
    The task of evaluating the model is especially easy when dealing with supervised
    learning problems, where there is a ground truth that can be compared against
    the prediction of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the accuracy percentage of the model is crucial for its application
    to unseen data that does not have a label class to compare to. For example, a
    model with an accuracy of 98% may allow the user to assume that the odds of having
    an accurate prediction are high, and hence the model should be trusted.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation of performance, as mentioned previously, should be done on the
    validation set (dev set) to fine-tune the model, and on the test set to determine
    the expected performance of the selected model on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics for Classification Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classification task refers to a model where the class label is a discrete
    value, as mentioned previously. Considering this, the most common measure to evaluate
    the performance of such tasks is calculating the accuracy of the model, which
    involves comparing the actual prediction to the real value. Even though this may
    be an appropriate metric in many cases, there are several others to consider as
    well before choosing one.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will take a look at the different performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **confusion matrix** is a table that contains the performance of the model,
    and is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The columns represent the instances that belong to a predicted class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rows refer to the instances that actually belong to that class (ground truth).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The configuration that confusion matrices present allows the user to quickly
    spot the areas in which the model is having greater difficulty. Consider the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: A confusion matrix of a classifier that predicts whether a woman
    is pregnant'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: A confusion matrix of a classifier that predicts whether a woman
    is pregnant'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following can be observed from the preceding table:'
  prefs: []
  type: TYPE_NORMAL
- en: By summing up the values in the first row, it is possible to know that there
    are 600 observations of pregnant women. However, from those 600 observations,
    the model predicted 556 as pregnant, and 44 as non-pregnant. Hence, the model's
    ability to predict that a woman is pregnant has a correctness level of 92.6%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding the second row, there are also 600 observations of non-pregnant women.
    Out of those 600, the model predicted that 123 of them were pregnant, and 477
    were non-pregnant. The model successfully predicted non-pregnant women 79.5% of
    the time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these statements, it is possible to conclude that the model performs
    at its worst when classifying observations that are not pregnant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that the rows in a confusion matrix refer to the occurrence or
    non-occurrence of an event, and the columns refer to the model''s predictions,
    the values in the confusion matrix can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** (**TP**): Refers to the instances that the model correctly
    classified the event as positive—for example, the instances correctly classified
    as pregnant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives** (**FP**): Refers to the instances that the model incorrectly
    classified the event as positive—for example, the non-pregnant instances that
    were incorrectly classified as pregnant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives** (**TN**): Represents the instances that the model correctly
    classified the event as negative—for example, the instances correctly classified
    as non-pregnant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negatives** (**FN**): Refers to the instances that the model incorrectly
    classified the event as negative—for example, the pregnant instances that were
    incorrectly predicted as non-pregnant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The values in the confusion matrix can be demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: A table showing confusion matrix values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5: A table showing confusion matrix values'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Accuracy**, as explained previously, measures the model''s ability to correctly
    classify all instances. Although this is considered to be one of the simplest
    ways of measuring performance, it may not always be a useful metric when the objective
    of the study is to minimize/maximize the occurrence of one class independently
    of its performance on other classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy level of the confusion matrix from *Figure 3.4* is measured as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: An equation showing the calculation for accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.6: An equation showing the calculation for accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *m* is the total number of instances.
  prefs: []
  type: TYPE_NORMAL
- en: The `86%` accuracy refers to the overall performance of the model in classifying
    both class labels.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This metric measures the model''s ability to correctly classify positive labels
    (the label that represents the occurrence of the event) by comparing it with the
    total number of instances predicted as positive. This is represented by the ratio
    between the *true positives* and the sum of the *true positives* and *false positives*,
    as shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: An equation showing the calculation for precision'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: An equation showing the calculation for precision'
  prefs: []
  type: TYPE_NORMAL
- en: The precision metric is only applicable to binary classification tasks, where
    there are only two class labels (for instance, true or false). It can also be
    applied to multiclass tasks considering that the classes are converted into two
    (for instance, predicting whether a handwritten number is a 6 or any other number),
    where one of the classes refers to the instances that have a condition while the
    other refers to those that do not.
  prefs: []
  type: TYPE_NORMAL
- en: For the example in *Figure 3.4*, the precision of the model is equal to 81.8%.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The recall metric measures the number of correctly predicted positive labels
    against all positive labels. This is represented by the ratio between *true positives*
    and the sum of *true positives* and *false negatives*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: An equation showing the calculation for recall'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.8: An equation showing the calculation for recall'
  prefs: []
  type: TYPE_NORMAL
- en: Again, this measure should be applied to two class labels. The value of recall
    for the example in *Figure 3.4* is 92.6%, which, when compared to the other two
    metrics, represents the highest performance of the model. The decision to choose
    one metric or the other will depend on the purpose of the study, which will be
    explained in more detail later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.03: Calculating Different Evaluation Metrics on a Classification
    Task'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will be using the breast cancer toy dataset to calculate
    the evaluation metrics using the scikit-learn library. Follow these steps to complete
    this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise and import all the required elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The fourth line imports the `tree` module from scikit-learn, which will be used
    to train a decision tree model on the training data in this exercise. The lines
    of code below that will import the different evaluation metrics that will be calculated
    during this exercise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The breast cancer toy dataset contains the final diagnosis (malignant or benign)
    of the analysis of masses found in the breasts of 569 women. Load the dataset
    and create features and target Pandas DataFrames, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the dataset using the conventional split approach:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the dataset is divided into two subsets (train and test sets) because
    the purpose of this exercise is to learn how to calculate the evaluation metrics
    using the scikit-learn package.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `random_state` parameter is used to set a seed that will ensure the same
    results every time you run the code. This guarantees that you will get the same
    results as the ones reflected in this exercise. Different numbers can be used
    as the seed; however, use the same number as suggested in the exercises and activities
    of this chapter to get the same results as the ones shown here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, instantiate the `DecisionTreeClassifier` class from scikit-learn''s
    `tree` module. Next, train a decision tree on the train set. Finally, use the
    model to predict the class label on the test set. Use the following code to do
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, the model is instantiated using a `random_state` to set a seed. Then,
    the `fit` method is used to train the model using the data from the train sets
    (both `X` and `Y`). Finally, the `predict` method is used to trigger the predictions
    on the data in the test set (only `X`). The data from `Y_test` will be used to
    compare the predictions with the ground truth.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The steps for training a supervised learning model will be explained further
    in *Chapter 4*, *Supervised Learning Algorithms: Predicting Annual Income* and
    *Chapter 5*, *Artificial Neural Networks: Predicting Annual Income*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use scikit-learn to construct a confusion matrix, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is as follows, where the ground truth is measured against the prediction:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the accuracy, precision, and recall of the model by comparing `Y_test`
    and `Y_pred`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are displayed as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Given that the positive labels are those where the mass is malignant, it can
    be concluded that the instances that the model predicts as malignant have a high
    probability (96.6%) of being malignant, but for the instances predicted as benign,
    the model has a 17.15% (100%–82.85%) probability of being wrong.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Yw0hiu](https://packt.live/2Yw0hiu).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3e4rRtE](https://packt.live/3e4rRtE).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have successfully calculated evaluation metrics on a classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an Evaluation Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several metrics that can be used to measure the performance of a model
    on classification tasks, and selecting the right one is key to building a model
    that performs exceptionally well for the purpose of the study.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, the importance of understanding the purpose of the study was mentioned
    as a useful insight to determine the pre-processing techniques that need to be
    performed on the dataset. Moreover, the purpose of the study is also useful to
    determine the ideal metric for measuring the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Why is the purpose of the study important for selecting the evaluation metric?
    Because by understanding the main goal of the study, it is possible to decide
    whether it is important to focus our attention on the overall performance of the
    model or only on one of the class labels.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a model that has been created to recognize when birds are present
    in a picture does not need to perform well in recognizing which other animals
    are present in the picture as long as it does not classify them as birds. This
    means that the model needs to focus on improving the performance of correctly
    classifying birds only.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, for a model that has been created to recognize handwritten
    characters, where no one character is more important than another, the ideal metric
    would be the one that measures the overall accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: What would happen if more than one metric was selected? It would become difficult
    to arrive at the best performance of the model, considering that measuring two
    metrics simultaneously can result in needing different approaches to improve results.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics for Regression Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering that regression tasks are those where the final output is continuous,
    without a fixed number of output labels, the comparison between the ground truth
    and the prediction is based on the proximity of the values rather than on them
    having exactly the same values. For instance, when predicting house prices, a
    model that predicts a value of USD 299,846 for a house valued at USD 300,000 can
    be considered to be a good model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two metrics most commonly used for evaluating the accuracy of continuous
    variables are the **Mean Absolute Error** (**MAE**) and the **Root Mean Squared
    Error** (**RMSE**), which are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean Absolute Error**: This metric measures the average absolute difference
    between a prediction and the ground truth, without taking into account the direction
    of the error. The formula to calculate the MAE is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.9: An equation showing the calculation of MAE'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: An equation showing the calculation of MAE'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *m* refers to the total number of instances, *y* is the ground truth,
    and *ŷ* is the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Root Mean Squared Error**: This is a quadratic metric that also measures
    the average magnitude of error between the ground truth and the prediction. As
    its name suggests, the RMSE is the square root of the average of the squared differences,
    as shown in the following formula:![Figure 3.10: An equation showing the calculation
    of RMSE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B15781_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: An equation showing the calculation of RMSE'
  prefs: []
  type: TYPE_NORMAL
- en: Both these metrics express the average error, in a range from 0 to infinity,
    where the lower the value, the better the performance of the model. The main difference
    between these two metrics is that the MAE assigns the same weight of importance
    to all errors, while the RMSE squares the error, assigning higher weights to larger
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Considering this, the RMSE metric is especially useful in cases where larger
    errors should be penalized, meaning that outliers are taken into account in the
    measurement of performance. For instance, the RMSE metric can be used when a value
    that is off by 4 is more than twice as bad as being off by 2\. The MAE, on the
    other hand, is used when a value that is off by 4 is just twice as bad as a value
    off by 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.04: Calculating Evaluation Metrics on a Regression Task'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will be calculating evaluation metrics on a model that
    was trained using linear regression. We will use the `boston` toy dataset for
    this purpose. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise and import all the required
    elements, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The fourth line imports the `linear_model` module from scikit-learn, which will
    be used to train a linear regression model on the training dataset. The lines
    of code that follow import two performance metrics that will be evaluated in this exercise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For this exercise, the `boston` toy dataset will be used. This dataset contains
    data about 506 house prices in Boston. Use the following code to load and split
    the dataset, the same as we did for the previous exercises:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train a linear regression model on the train set. Then, use the model to predict
    the class label on the test set, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As a general explanation, the `LinearRegression` class from scikit-learn's `linear_model`
    module is instantiated first. Then, the `fit` method is used to train the model
    using the data from the train sets (both `X` and `Y`). Finally, the `predict`
    method is used to trigger the predictions on the data in the test set (only `X`).
    The data from `Y_test` will be used to compare the predictions to the ground truth.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate both the MAE and RMSE metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are displayed as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The scikit-learn library allows you to directly calculate the MSE. To calculate
    the RMSE, the square root of the value obtained from the `mean_squared_error()`
    function is calculated. By using the square root, we ensure that the values from
    MAE and RMSE are comparable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the results, it is possible to conclude that the model performs well on
    the test set, considering that both values are close to zero. Nevertheless, this
    also means that the performance can still be improved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YxVXiU](https://packt.live/2YxVXiU).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2N0Elqy](https://packt.live/2N0Elqy).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have now successfully calculated evaluation metrics on a regression task
    that aimed to calculate the prices of houses based on their characteristics. In
    the next activity, we will calculate the performance of a classification model
    that was created to recognize handwritten characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.02: Evaluating the Performance of the Model Trained on a Handwritten
    Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You continue to work on creating a model to recognize handwritten digits. The
    team has built a model and they want you to evaluate the performance of the model.
    In this activity, you will calculate different performance evaluation metrics
    on a trained model. Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import all the required elements to load and split a dataset in order to train
    a model and evaluate the performance of the classification tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `digits` toy dataset from scikit-learn and create Pandas DataFrames
    containing the features and target matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and testing sets. Use 20% as the size of the testing
    set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a decision tree on the train set. Then, use the model to predict the class
    label on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To train the decision tree, revisit *Exercise 3.04*, *Calculating Different
    Evaluation Metrics on a Classification Task*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use scikit-learn to construct a confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the accuracy of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the precision and recall. Considering that both the precision and
    recall can only be calculated on binary classification problems, we'll assume
    that we are only interested in classifying instances as the number `6` or `any
    other number`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To be able to calculate the precision and recall, use the following code to
    convert `Y_test` and `Y_pred` into a one-hot vector. A one-hot vector consists
    of a vector that only contains zeros and ones. For this activity, the 0 represents
    the *number 6*, while the 1 represents `any other number`. This converts the class
    labels (`Y_test` and `Y_pred`) into binary data, meaning that there are only two
    possible outcomes instead of 10 different ones.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, calculate the precision and recall using the new variables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should obtain the following values as the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 230.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Error Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building an average model, as explained so far, is surprisingly easy through
    the use of the scikit-learn library. The key aspects of building an exceptional
    model come from the analysis and decision-making on the part of the researcher.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen so far, some of the most important tasks are choosing and pre-processing
    the dataset, determining the purpose of the study, and selecting the appropriate
    evaluation metric. After handling all of this and taking into account that a model
    needs to be fine-tuned in order to reach the highest standards, most data scientists
    recommend training a simple model, regardless of the hyperparameters, to get the
    study started.
  prefs: []
  type: TYPE_NORMAL
- en: '**Error analysis** is then introduced as a very useful methodology to turn
    an average model into an exceptional one. As the name suggests, it consists of
    analyzing the errors among the different subsets of the dataset in order to target
    the condition that is affecting the model at a greater scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Bias, Variance, and Data Mismatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the different conditions that may affect a machine learning model,
    it is important to understand what the **Bayes error** is. The Bayes error, also
    known as the **irreducible error**, is the lowest possible error rate that can
    be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Before the improvements that were made in technology and artificial intelligence,
    the Bayes error was considered to be the lowest possible error achievable by humans
    (**human error**). For instance, for a process that most humans achieve with an
    error rate of 0.1, but top experts achieve with an error rate of 0.05, the Bayes
    error would be 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: However, the Bayes error has now been redefined as being the lowest possible
    error that machines can achieve, which is unknown considering that, as humans,
    we can only understand as far as human error goes. Due to this, when using the
    Bayes error to analyze errors, it is not possible to know the lowest limit once
    the model is below the human error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is useful for analyzing the error rates among the different
    sets of data and determining the condition that is affecting the model in a greater
    proportion. The purpose of this diagram is to find the errors that differ to a
    greater extent from each other so that the model can be diagnosed and improved
    accordingly. It is important to highlight that the value of the error for each
    set is calculated by subtracting the evaluation metrics (for instance, the accuracy)
    of that set from 100%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: Error analysis methodology'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.11: Error analysis methodology'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the preceding diagram, the process to perform error analysis is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The performance evaluation is calculated on all sets of data. This measure is
    used to calculate the error for each set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Starting from the bottom to the top, the difference is calculated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dev set error (12%) is subtracted from the testing set error (12%). The
    resulting value (0%) is saved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The train-dev error (9%) is subtracted from the dev set error (12%). The resulting
    value (3%) is saved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The training set error (8%) is subtracted from the train-dev error (9%). The
    resulting value (1%) is saved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Bayes error (2%) is subtracted from the training set error (8%). The resulting
    value (6%) is saved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The bigger difference determines the condition that is most seriously affecting
    the model. In this case, the bigger difference occurs between the Bayes error
    and the training set error, which, as shown in the preceding diagram, determines
    that the model is suffering from *high bias*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The train/dev set is a combination of data in the training and the validation
    (dev) sets. It is usually of the same shape as the dev set and contains the same
    amount of data from both sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'An explanation of each of the conditions is as follows, along with some techniques
    to avoid/fix them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High Bias**: Also known as underfitting, this occurs when the model is not
    learning from the training set, which translates into the model performing poorly
    for all three sets (training, validation, and testing sets), as well as for unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underfitting is the easiest condition to detect and it usually requires changing
    to a different algorithm that may be a better fit for the data available. With
    regard to neural networks, it can typically be fixed by constructing a bigger
    network or by training for longer periods of time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**High Variance**: Also known as overfitting, this condition refers to the
    model''s inability to perform well on data that''s different than that of the
    training set. This basically means that the model has overfitted the training
    data by learning the details and outliers of the data, without making any generalizations.
    A model suffering from overfitting will not perform well on the dev or test sets,
    or on unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overfitting can be fixed by tuning the different hyperparameters of the algorithm,
    often with the objective of simplifying the algorithm's approximation of the data.
    For instance, for decision trees, this can be addressed by pruning the tree to
    delete some of the details that were learned from the training data. In neural
    networks, on the other hand, this can be addressed by adding regularization techniques
    that seek to reduce some of the neuron's influence on the overall result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Additionally, adding more data to the training set can also help the model avoid
    high variance, that is, increasing the dataset that's used for training the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Data mismatch**: This occurs when the training and validation sets do not
    follow the same distribution. This affects the model as although it generalizes
    based on the training data. This generalization does not describe the data that
    was found in the validation set. For instance, a model that''s created to describe
    landscape photographs may suffer from a data mismatch if it is trained using high-definition
    images, while the actual images that will be used once the model has been built
    are unprofessional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logically, the best way to avoid data mismatch is to make sure that the sets
    follow the same distribution. For example, you can do this by shuffling together
    the images from both sources (professional and unprofessional images) and then
    dividing them into the different sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Nevertheless, in cases where there is not enough data that follows the same
    distribution of unseen data (data that will be used in the future), it is highly
    recommended to create the dev and test sets entirely out of that data and add
    the remaining data to the large training set. From the preceding example, the
    unprofessional images should be used to create the dev and test sets, adding the
    remaining ones to the training set, along with the professional images. This helps
    to train a model with a set that contains enough images to make a generalization,
    but it uses data with the same distribution as the unseen data to fine-tune the
    model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, if the data from all sets comes from the same distribution, this condition
    actually refers to a problem of high variance and should be handled as such.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Overfitting to the dev set**: Lastly, similar to the variance condition,
    this occurs when the model is not generalizing but instead is fitting the dev
    set too well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should be addressed using the same approaches that were explained for high variance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next exercise, we will calculate the error rate of the model on the different
    sets of data, which can be used to perform error analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.05: Calculating the Error Rate on Different Sets of Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will calculate error rates for a model that has been trained
    using a decision tree. We will use the breast cancer dataset for this purpose.
    Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter Notebook to implement this exercise and import all the required
    elements to load and split the dataset. These will be used to train a model and
    evaluate its recall:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For this exercise, the `breast cancer` dataset will be used. Use the following
    code to load the dataset and create the Pandas DataFrames containing the features
    and target matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the dataset into training, validation, and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting shapes are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a train/dev set that combines data from both the training and validation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, a random seed is set to ensure the reproducibility of the results. Next,
    the NumPy `random.randint()` function is used to select random indices from the
    `X_train` set. To do that, 28 random integers are generated in a range between
    0 and the total length of `X_train`. The same process is used to generate the
    random indices of the dev set. Finally, a new variable is created to store the
    selected values of `X_train` and `X_dev`, as well as a variable to store the corresponding
    values from `Y_train` and `Y_dev`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The variables that have been created contain 25 instances/labels from the train
    set and 25 instances/labels from the dev set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The resulting shapes of the sets are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train a decision tree on the train set, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `predict` method to generate the predictions for all of your sets (train,
    train/dev, dev, and test). Next, considering that the objective of the study is
    to maximize the model''s ability to predict all malignant cases, calculate the
    recall scores for all predictions. Store all of the scores in a variable named
    `scores`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The error rates for all of the sets of data are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the preceding values, the following table containing the error rates can
    be created:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Error rates for all sets of data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15781_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.12: Error rates for all sets of data'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the Bayes error was assumed as `0`, considering that the classification
    between a malignant and a benign mass is done by taking a biopsy of the mass.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding table, it can be concluded that the model performs exceptionally
    well for the purpose of the study, considering that all error rates are close
    to 0, which is the lowest possible error.
  prefs: []
  type: TYPE_NORMAL
- en: The highest difference in error rates is found between the train/dev set and
    the dev set, which refers to data mismatch. However, taking into account that
    all the datasets come from the same distribution, this condition is considered
    a high variance issue, where adding more data to the training set should help
    reduce the error rate.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3e4Toer](https://packt.live/3e4Toer).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2UJzDkW](https://packt.live/2UJzDkW).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: You have successfully calculated the error rate of all the subsets of data.
    In the next activity, we will perform an error analysis to define the steps to
    be taken to improve the performance of a model that was created to recognize handwritten
    digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.03: Performing Error Analysis on a Model Trained to Recognize Handwritten
    Digits'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the different metrics that you have provided to your team to measure
    the performance of the model, they have selected accuracy as the ideal metric.
    Considering this, your team has asked you to perform an error analysis to determine
    how the model could be improved. In this activity, you will perform an error analysis
    by comparing the error rate of the different sets in terms of the accuracy of
    the model. Follow these steps to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required elements to load and split a dataset. We will do this to
    train the model and measure its accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `digits` toy dataset from scikit-learn and create Pandas DataFrames
    containing the features and target matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training, validation, and testing sets. Use 0.1 as the size
    of the test set, and an equivalent number to build a validation set of the same
    shape.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a train/dev set for both the features and target values that contains
    90 instances/labels of the train set and 90 instances/labels of the dev set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a decision tree on that training set data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the error rate for all sets of data in terms of the accuracy of the
    model and determine which condition is affecting the performance of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By completing this activity, you should obtain the following error rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Expected error rates'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15781_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Expected error rates'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 233.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter explained the different tasks that can be solved through supervised
    learning algorithms: classification and regression. Although both of these tasks''
    goal is to approximate a function that maps a set of features to an output, classification
    tasks have a discrete number of outputs, while regression tasks can have infinite
    continuous values as outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When developing machine learning models to solve supervised learning problems,
    one of the main goals is for the model to be capable of generalizing so that it
    will be applicable to future unseen data, instead of just learning a set of instances
    very well but performing poorly on new data. Accordingly, a methodology for validation
    and testing was explained in this chapter, which involved splitting the data into
    three sets: a training set, a dev set, and a test set. This approach eliminates
    the risk of bias.'
  prefs: []
  type: TYPE_NORMAL
- en: After this, we covered how to evaluate the performance of a model for both classification
    and regression problems. Finally, we covered how to analyze the performance of
    a model and perform error analysis on each of the sets to detect the condition
    affecting the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on applying different algorithms to a real-life
    dataset, with the underlying objective of applying the steps we learned about
    here to choose the best performing algorithm for the case study.
  prefs: []
  type: TYPE_NORMAL

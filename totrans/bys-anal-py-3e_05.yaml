- en: Chapter 6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modeling with Bambi
  prefs: []
  type: TYPE_NORMAL
- en: A good tool improves the way you work. A great tool improves the way you think.
    – Jeff Duntemann
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In *Chapter [4](CH04.xhtml#x1-760004)*, we described the basic ingredients of
    linear regression models and how to generalize them to better fit our needs. In
    this chapter, we are going to keep learning about linear models, but this time,
    we are going to work with Bambi [[Capretto et al.](Bibliography.xhtml#XCapretto_2022), [2022](Bibliography.xhtml#XCapretto_2022)],
    a high-level Bayesian model-building interface written on top of PyMC. Bambi is
    designed to make it extremely easy to fit linear models, including hierarchical
    ones. We will see that Bambi’s domain is more comprehensive than just linear models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Bambi to build and fit models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing results with Bambi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial regression and splines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributional models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable selection with Kulprit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.1 One syntax to rule them all
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyMC has a very simple and expressive syntax that allows us to build arbitrary
    models. That’s usually a blessing, but it can be a burden too. Bambi instead focuses
    on regression models, and this restriction leads to a more focused syntax and
    features, as we will see.
  prefs: []
  type: TYPE_NORMAL
- en: Bambi uses a Wilkinson-formula syntax similar to the one used by many R packages
    like nlme, lme4, and brms. Let’s assume `data` is a pandas DataFrame like the
    one shown in *Table [6.1](#x1-121002r1)*.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | *y* | *x* | *z* | *g* |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | -0.633494 | -0.196436 | -0.355148 | Group A |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2.32684 | 0.0163941 | -1.22847 | Group B |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.999604 | 0.107602 | -0.391528 | Group C |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | -0.119111 | 0.804268 | 0.967253 | Group A |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2.07504 | 0.991417 | 0.590832 | Group B |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | -0.412135 | 0.691132 | -2.13044 | Group C |'
  prefs: []
  type: TYPE_TB
- en: '**Table 6.1**: A dummy pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this data, we want to build a linear model that predicts `y` from `x`.
    Using PyMC, we would do something like the model in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.1**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The formula syntax used by Bambi allows us to define an equivalent model in
    a much more compact way:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.2**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'On the left side of the tilde (∼), we have the dependent variable, and on the
    right side, the independent variable(s). With this syntax, we are just specifying
    the mean (*μ* in the PyMC’s model `lm`). By default, Bambi assumes the likelihood
    is Gaussian; you can change this with the `family` argument. The formula syntax
    does not specify priors distribution, just how the dependent and independent variables
    are related. Bambi will automatically define (very) weakly informative priors
    for us. We can get more information by printing a Bambi model. If you print `a_model`,
    you should get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line shows the formula we used to define the model, and the second
    line is the likelihood. The third line is the link function. Then we have the
    number of observations used to fit the model, and the next is telling us we are
    linearly modeling the parameter `mu` of the Gaussian. The latter part of the output
    shows the model structure: the common-level effects, in this case, the intercept
    (`Intercept`) and the slope (`x`), and the auxiliary parameters, i.e., all the
    parameters not linearly modeled, in this case, the standard deviation of the Gaussian.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can override the default priors by passing a dictionary to the `priors`
    argument to `bmb.Model`. For instance, if we want to define a custom prior for
    the coefficient of the variable `x` and also for the auxiliary parameter `sigma`,
    we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.3**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, we will get the following model specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to omit the intercept from your model, you can do it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 6.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Or even like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 6.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Print the model `no_intercept_model`, and you will see that the intercept is
    not there anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we want to include more variables? We can do it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.6**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also include group-level effects (hierarchies); for example, if we want
    to use the variable `g` to partially pool the estimates of `x`, we can do it like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.7**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can see a visual representation of this model in *Figure [6.1](#x1-121081r1)*.
    Notice the variables `1|g_offset` and `x|g_offset`. By default, Bambi fits a noncentered
    hierarchical model; you can change this with the argument `noncentered`.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file169.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.1**: A visual representation of `model_h`'
  prefs: []
  type: TYPE_NORMAL
- en: The formula syntax is very simple, but it is also very powerful. We have just
    scratched the surface of what we can do with it. Instead of describing the syntax
    all at once, we are going to show it by example. If you want to go deeper, you
    can check Formulae documentation [https://bambinos.github.io/formulae/](https://bambinos.github.io/formulae/).
    formulae is the Python package in charge of parsing Wilkinson’s formulas for Bambi.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 The bikes model, Bambi’s version
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first model we are going to use to illustrate how to use Bambi is the bikes
    model from *Chapter [4](CH04.xhtml#x1-760004)*. We can load the data with:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.8**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build and fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.9**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure [6.2](#x1-122009r2)* shows a visual representation of the model. If
    you want to visually inspect the priors, you can use `model.plot_priors()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file170.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.2**: A visual representation of the bikes model, computed with the
    command `model.graph()`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now plot the posterior mean and the posterior predictive distribution
    (predictions). Omitting some details needed to make the plots look nice, the code
    to do this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.10**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`plot_predictions` is a function from Bambi’s submodule `interpret`. This function
    helps to analyze regression models by plotting conditional adjusted predictions,
    visualizing how a parameter of the (conditional) response distribution varies
    as a function of (some) interpolated explanatory variables. We can see the result
    of this code in *Figure [6.3](#x1-122018r3)*. The left panel shows the posterior
    mean and the 94% HDI, while the right panel shows the posterior predictive distribution
    (the predicted distribution of the rented bikes). Notice that the uncertainty
    for the predictions is much larger than the uncertainty for the mean (`pps=False`).
    This is because the posterior predictive distribution accounts for the uncertainty
    in the model parameters and the uncertainty in the data, whereas the posterior
    distribution of the mean only accounts for the uncertainty in the intercept and
    slope parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file171.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.3**: Posterior mean and posterior predictive distribution for the
    bikes model'
  prefs: []
  type: TYPE_NORMAL
- en: 'The utility of `plot_cap` becomes more evident when we have more than one explanatory
    variable. For example, let’s fit a model that uses both temperature and humidity
    to predict the number of rented bikes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.11**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure [6.4](#x1-122029r4)*, we can see five panels, each one showing the
    change of the number of rented bikes with the temperature at different values
    of `humidity`. As you can see, the number of rented bikes increases with temperature,
    but the slope is larger when humidity is low.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file172.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.4**: Posterior mean for the bikes model with temperature and humidity'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Polynomial regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One way to fit curves using a linear regression model is by building a polynomial,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![μ = 𝛽0 + 𝛽1x + 𝛽2x2 + 𝛽3x3 + 𝛽4x4...𝛽mxm ](img/file173.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We call *m* the degree of the polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: There are two important things to notice. First, polynomial regression is still
    linear regression; the linearity refers to the coefficients (the *β*s), not the
    variables (the *x*s). The second thing to note is that we are creating new variables
    out of thin air. The only observed variable is `x`, the rest are just powers of
    `x`. Creating new variables from observed ones is a perfectly valid ”trick” when
    doing regression; sometimes the transformation can be motivated or justified by
    theory (like taking the square root of the length of babies), but sometimes it
    is just a way to fit a curve. The intuition with polynomials is that for a given
    value of `x`, the higher the degree of the polynomial, the more flexible the curve
    can be. A polynomial of degree 1 is a line, a polynomial of degree 2 is a curve
    that can go up or down, a polynomial of degree 3 is a curve that can go up and
    then down (or the other way around), and so on. Notice I said ”can” because if
    we have a polynomial of degree 3, like *β*[0] + *β*[1]*x* + *β*[2]*x*² + *β*[3]*x*³,
    but the coefficients *β*[2] and *β*[3] are 0 (or practically 0), then the curve
    will be a line.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to define a polynomial regression with Bambi. We can write
    the *raw* polynomials:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.12**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the identity function `I()` to make it clear that we want to elevate
    *x* to some power. We need this because the `**` operator has a special meaning
    for Bambi. If we use this syntax, we are telling Bambi to model the mean of *y*
    as *α* + *β*[0]*x* + *β*[0]*x*² + *β*[0]*x*³ + *β*[0]*x*⁴.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.13**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will also generate a polynomial of degree 4, but the polynomial terms will
    be orthogonal to each other, meaning the correlation between the terms is reduced.
    Without going into the mathematical details, this has at least two important consequences
    with respect to the *standard* polynomial. First, the estimation can be numerically
    more stable, and second, the interpretation of the coefficients is different.
    In `standard` polynomial regression, the coefficients can be difficult to interpret,
    as changing the value of one coefficient affects the entire polynomial. In contrast,
    orthogonal polynomials allow you to interpret the effect of each term more clearly,
    as they are independent of each other. While the interpretation of the coefficients
    is different, other results remain the same. For instance, you should get the
    same predictions with both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build an orthogonal polynomial of degree 4 to model the bike data. For
    this example, we are going to use the `hour` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.14**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure [6.5](#x1-123014r5)* shows the posterior mean and the posterior predictive
    distribution. On the first row, you will see a polynomial of degree 1, which is
    equivalent to a linear model. On the second row, you will see a polynomial of
    degree 4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file174.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.5**: Posterior mean and posterior predictive distribution for the
    bikes model with temperature and humidity'
  prefs: []
  type: TYPE_NORMAL
- en: One problem with polynomials is that they act *globally*. When we apply a polynomial
    of degree *m*, we are saying that the relationship between the independent and
    dependent variables is of degree *m* for the entire dataset. This can be problematic
    when different regions of our data need different levels of flexibility. This
    could lead, for example, to curves that are too flexible. As the degree increases,
    the fit becomes more sensitive to the removal of points, or equivalently to the
    addition of future data. In other words, as the degree increases, the model becomes
    more prone to overfitting. Bayesian polynomial regression usually suffers less
    of this ”excess” of flexibility because we usually don’t use flat priors, and
    we do not compute a single set of coefficients, but the entire posterior distribution.
    Still, we can do better.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Splines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A general way to write very flexible models is to apply functions *B*[*m*]
    to *X*[*m*] and then multiply them by coefficients *β*[*m*]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![μ = 𝛽0 + 𝛽1B1 (X1) + 𝛽2B2(X2 )+ ⋅⋅⋅+ 𝛽mBm (Xm ) ](img/file175.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are free to pick *B*[*m*] as we wish; for instance, we can pick polynomials.
    But we can also pick other functions. A popular choice is to use B-splines; we
    are not going to discuss their definition, but we can think of them as a way to
    create smooth curves in such a way that we get flexibility, as with polynomials,
    but less prone to overfitting. We achieve this by using piecewise polynomials,
    that is, polynomials that are restricted to affect only a portion of the data.
    *Figure [6.6](#x1-124002r6)* shows three examples of piecewise polynomials of
    increasing degrees. The dotted vertical lines show the ”knots,” which are the
    points used to restrict the regions, the dashed gray line represents the function
    we want to approximate, and the black lines are the piecewise polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file176.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.6**: Piecewise polynomials of increasing degrees'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure [6.7](#x1-124004r7)* shows examples of splines of degree 1 and 3; the
    dots at the bottom represent the knots, and the dashed lines are the B-splines.
    At the top, we have all the B-splines with equal weight; we use grayscale to highlight
    that we have many B-splines. On the bottom panel, each B-spline is weighted differently
    (we multiply them by *β*[*m*] coefficients); if we sum the weighted B-splines,
    we get the black line as a result. This black line is what we usually call ”the
    spline.” We can use Bayesian statistics to find the proper weights for the B-splines.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file177.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.7**: B-splines of degree 1 (piecewise linear) or 3 (cubic spline)
    and the resulting splines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use B-splines with Bambi by using the `bs` function. For example, let’s
    fit a spline of degree 3 to the bikes data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.15**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure [6.8](#x1-124013r8)* shows that the number of rental bikes is at the
    lowest number late at night. There is then an increase, probably as people wake
    up and go to work or school, or do other activities. We have a first peak at around
    hour 8, then a slight decline, followed by the second peak at around hour 18,
    probably because people commute back home, after which there is a steady decline.
    Notice that the curve is not very smooth; this is not because of the spline but
    because of the data. We have measurements at discrete times (every hour).'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file178.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.8**: Posterior mean for the spline model'
  prefs: []
  type: TYPE_NORMAL
- en: When working with splines, one important decision we must make is determining
    the number and placement of knots. This can be a somewhat daunting task since
    the optimal number of knots and their spacing are not immediately apparent. A
    useful suggestion for determining the knot locations is to consider placing them
    based on quantiles rather than uniformly – something like `knots = np.quantile(bikes.hour,
    np.linspace(0, 1, num_knots))`. By doing so, we would position more knots in areas
    where we have a greater amount of data, while placing fewer knots in areas with
    less data. This results in a more adaptable approximation that effectively captures
    the variability in regions with a higher density of data points. Additionally,
    we may want to fit splines with varying numbers of knots and positions and then
    evaluate the results, using tools such as LOO, as we saw in *Chapter [5](CH05.xhtml#x1-950005)*.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Distributional models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw earlier that we can use linear models for parameters other than the mean
    (or location parameter). For example, we can use a linear model for the mean and
    a linear model for the standard deviation of a Gaussian distribution. These models
    are usually called distributional models. The syntax for distributional models
    is very similar; we just need to add a line for the auxiliary parameters we want
    to model. For instance, *σ* for a Gaussian, or *α* for a NegativeBinomial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now reproduce an example from *Chapter [4](CH04.xhtml#x1-760004)*, the
    babies example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.16**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure [6.9](#x1-125011r9)* shows the posterior distribution values of sigma
    for `model_dis` (varying sigma) and for a model with constant sigma. We can see
    that when sigma is allowed to vary, we obtain values below and above the estimate
    for a constant sigma, meaning that we are both under- and over-estimating this
    parameter when we don’t allow it to change.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file179.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.9**: Constant and varying sigma for the babies data'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure [6.10](#x1-125012r10)* shows the posterior fit for `model_dis`. Notice
    that the model can capture the increase in variability as the babies grow. This
    figure is very similar to *Figure [4.13](CH04.xhtml#x1-88022r13)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file180.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.10**: Posterior fit for `model_dis`'
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with PyMC, we saw that sampling from the posterior predictive
    distribution, at not observed values, requires us to define the ”Xs” as `Mutable
    data` and then update the variable before computing the posterior predictive distribution.
    With Bambi, this is not necessary. We can use the `predict` method to predict
    new values by passing the new values to the `data` argument. For example, let’s
    predict the length of a baby at 0.5 months (15 days):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.17**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 6.6 Categorical predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A categorical variable represents distinct groups or categories that can take
    on a limited set of values from those categories. These values are typically labels
    or names that don’t possess numerical significance on their own. Some examples
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Political affiliation: conservative, liberal, or progressive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sex: female or male.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Customer satisfaction level: very unsatisfied, unsatisfied, neutral, satisfied,
    or very satisfied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression models can easily accommodate categorical variables; we just
    need to encode the categories as numbers. There are a few options to do so. Bambi
    can easily handle the details for us. The devil is in the interpretation of the
    results, as we will explore in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.1 Categorical penguins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the current example, we are going to use the palmerpenguins dataset, [Horst
    et al.](Bibliography.xhtml#Xpenguins) [[2020](Bibliography.xhtml#Xpenguins)],
    which contains 344 observations of 8 variables. For the moment, we are interested
    in modeling the mass of the penguins as a function of the length of their bills.
    It is expected that the mass of the penguins increases as the bill length increases.
    The novelty of this example is that we are going to consider the categorical variable,
    `species`. In this dataset, we have 3 categories or levels for the species variable,
    namely, Adelie, Chinstrap, and Gentoo. *Figure [6.11](#x1-127002r11)* shows a
    scatter plot for the variables we want to model.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file181.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.11**: Bill length vs mass for 3 species of penguins'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load the data and fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.18**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Notice that there is no special syntax to define Bambi’s model for categorical
    variables. Bambi can detect and handle them automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure [6.12](#x1-127011r12)* show a forest plot for `model_p`. Notice something
    unexpected? There are no posterior values for Adelie. This is no mistake. By default,
    Bambi encodes categorical variables with N levels (3 species) as N-1 dummy variables
    (2 species). Thus the coefficients species-Chinstrap and species-Gentoo are modeled
    as deflections from the baseline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![mass = 𝛽0 + 𝛽1bill length ](img/file182.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To make this more clear, let’s check a couple of plots. We can read *Figure
    [6.12](#x1-127011r12)* as saying that the body mass of Chinstrap is, on average,
    -0.89 relative to Adelie’s body mass. The same goes for Gentoo, but this time,
    we have to add 0.66 to the mean of the baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file183.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.12**: Forest plot from `model_p`'
  prefs: []
  type: TYPE_NORMAL
- en: You can check that these two statements are true by looking at *Figure [6.13](#x1-127012r13)*.
    See how the three lines are essentially parallel to each other with Adelie in
    the middle, Chinstrap below (-0.89), and Gentoo above (0.58).
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file184.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.13**: Mean in-sample predictions from `model_p`'
  prefs: []
  type: TYPE_NORMAL
- en: 6.6.2 Relation to hierarchical models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter [3](CH03.xhtml#x1-670003)*, we discussed and contrasted pooled and
    hierarchical (or partially pooled) models. There we showed that it is often the
    case that we take advantage of the structure or hierarchies in data. Following
    the logic of that chapter, you could argue that Adelie, Gentoo, and Chinstrap,
    while being different species, are all penguins. So modeling their body masses
    hierarchically may be a good idea. And you would be right to think so. So what
    is the difference between such a model and the one we used in this section?
  prefs: []
  type: TYPE_NORMAL
- en: 'The distinguishing factor lies in the subtleties of the slope and intercept
    components. In the case of the latter, the slope remains the same across all three
    penguin species, while the intercepts can vary: `Intercept + 0` for Adelie, `Intercept
    + species[Chinstrap]` for Chinstrap, and `Intercept + species[Gentoo]` for Gentoo.
    Thus, this model highlights the distinct intercepts while keeping the slope uniform.'
  prefs: []
  type: TYPE_NORMAL
- en: If instead we had built the hierarchical model `body_mass ~(bill_length|species)`,
    we would have been asking for a partially pooled slope and intercept. And if instead
    we had modeled `body_mass ~(0 + bill_length | species)`, we would have been asking
    for a partially pooled slope and a common intercept.
  prefs: []
  type: TYPE_NORMAL
- en: Besides these particular models, when thinking about using a predictor as a
    grouping variable or as a categorical predictor, it is usually useful to ask if
    the variable includes all possible categories (like all days of the week, all
    species, and so on) or only a subgroup (some schools, or a few musical genres).
    If we have all possible categories, then we may prefer to model it as a categorical
    predictor, otherwise, as a grouping variable.
  prefs: []
  type: TYPE_NORMAL
- en: As we already discussed, we often create more than one model before deciding
    which one we like the most. The *best* model is the one that aligns with the goals
    of your analysis, provides meaningful insights, and accurately represents the
    underlying patterns in your data. It’s often a good idea to explore multiple models,
    compare their performance using appropriate criteria (such as those discussed
    in *Chapter [5](CH05.xhtml#x1-950005)*), and consider the practical implications
    of each model for your research or decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Interactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An interaction effect, or statistical interaction, happens when the effect
    of an independent variable on the response changes depending on the value of another
    independent variable. An interaction can occur between two or more variables.
    Some examples are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Education level and income impact**: Higher education may have a stronger
    positive effect on income for one gender compared to the other, resulting in an
    interaction between education and gender.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medication efficacy and age**: A drug that works better for older individuals
    than younger ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exercise and diet effects on weight loss**: It could be that the diet’s effect
    on weight loss is small for people who do little or no exercise and large for
    people who do moderate exercise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature and humidity for crop growth**: Some crops could thrive in hot
    and humid conditions, while others might perform better in cooler and less humid
    environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have an interaction when the combined effect of two or more variables acting
    together is not equal to the sum of their individual effects. So we cannot model
    an interaction if we have a model like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![μ = 𝛼 + 𝛽0X0 + 𝛽1X1 ](img/file185.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The most common way to model an interaction effect is by multiplying two (or
    more) variables. Take, for example, a model like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ main terms ◜----◞◟----◝ μ = 𝛼 + 𝛽0X0 + 𝛽1X1 + 𝛽◟2X0◝X◜1◞ interaction term
    ](img/file186.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is common when modeling interaction effects to also include the main effect/terms.
  prefs: []
  type: TYPE_NORMAL
- en: New predictors
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying two variables can be seen as a trick, similar to the one we use
    for polynomial regression (or any transformation of a given variable). Instead
    of multiplying a predictor with itself, we multiply two different predictors and
    obtain a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining an interaction between two variables is easy for a PyMC model; we
    just need to multiply the two predictors together and also add a coefficient.
    For a Bambi model, it is even easier; we use the `:` operator. To make the difference
    crystal clear, let’s look at an example of a model with and without interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.19**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We now use Bambi’s `plot_prediction` to compare how different values of `bill_length`
    affect `body_mass` as a function of `bill_depth` generate. *Figure [6.14](#x1-129015r14)*
    shows the result. We have the mean regression fit for `bill_depth` evaluated at
    5 fixed values of `bill_length`. On the left, we have the result for `model_noint`
    (no interactions), and on the right, for `model_int` (with interactions). We can
    see that when we don’t have interactions, the fitted lines for `bill_depth` are
    parallel at different levels of `bill_length`. Instead, when we have interactions,
    the lines are no longer parallel, precisely because the effect of changing `bill_depth`
    on how much `body_mass` changes is no longer constant but modulated by the values
    of `bill_length`.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file187.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.14**: Mean in-sample predictions from `model_noint` (left) and `model_int`
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: If you generate a figure like *Figure [6.14](#x1-129015r14)*, but instead of
    fixing `bill_length`, you decide to fix `bill_depth`, you will observe a similar
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In the GitHub repository for this book ( [https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)),
    you are going to find the file `interactions.ipynb`. This script generates a figure
    in 3D, which I hope will help you build intuition about what we are doing when
    adding interactions. If you run it, you will see that when there are no interactions,
    we are fitting a 2D plane, a flat surface like a sheet of paper. But when adding
    interactions, you are fitting a curved surface. Compare the result of `interactions.ipynb`
    with *Figure [6.14](#x1-129015r14)*.
  prefs: []
  type: TYPE_NORMAL
- en: We have just seen visually that interpreting linear models with interactions
    is not as easy as interpreting linear models without them. Let’s see this mathematically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume we have a model with 2 variables, *X*[0] and *X*[1], and an interaction
    between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![μ = 𝛼 + 𝛽0X0 + 𝛽1X1 + 𝛽2X0X1 ](img/file188.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite this model as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![μ = 𝛼+ (◟𝛽0-+◝𝛽◜2X1-)◞X0 + 𝛽1X1 slope of X0 ](img/file189.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Or even like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![μ = 𝛼+ 𝛽0X0 + (◟𝛽1 +◝𝛽◜2X0-)◞X1 slope of X1 ](img/file190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From this expression, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The interaction term can be understood as a linear model inside a linear model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The interaction is symmetric; we can think of it as the slope of *X*[0] as a
    function of *X*[1] and at the same time as the slope of *X*[1] as a function of
    *X*[0]. This can also be seen from the interactive figure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We know from before that the *β*[0] coefficient can be interpreted as the amount
    of change of *μ* per unit change of *X*[0] (that is why we call it the slope).
    If we add an interaction term, then this is only true at *X*[1] = 0\. Try using
    the interactive figure to see this by yourself. Mathematically, this is true because
    when *X*[1] = 0, then *β*[2]*X*[1] = 0, and thus the slope of *X*[0] reduces to
    *β*[0]*X*[0]. By symmetry, the same reasoning can be applied to *β*[1].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.8 Interpreting models with Bambi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have been using `bmb.interpret_plot_predictions` a lot in this chapter. But
    that’s not the only tool that Bambi offers us to help us understand models. One
    of them is `bmb.interpret_plot_comparisons`. This tool helps us answer the question,
    ”What is the expected predictive difference when we compare two values of a given
    variable while keeping all the rest at constant values?”.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use `model_int` from the previous section, so we don’t need to fit a
    new model. We use the following code block to generate *Figure [6.15](#x1-130007r15)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.20**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure [6.15](#x1-130007r15)* shows that when comparing a hypothetical penguin
    with `bill_depth` of 1.8 against one with `bill_depth` of 1.4, the expected difference
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: Approx 0.8 kg for a bill length of 3.5 cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -0.6 kg for a bill length of 4.5 cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approx -2 kg for a bill length of 5.5 cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![PIC](img/file191.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.15**: Contrast of `bill_depth` from 1.8 to 1.4 cm for 3 fixed values
    of `bill_length`'
  prefs: []
  type: TYPE_NORMAL
- en: If you want the information in tabular form, use the function `bmb.interpret.comparisons`
    and you will get a DataFrame instead of a plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful function is `bmb.interpret_plot_slopes`, which can be used to
    compute the ”instant rate of change” or slope at a given value. We use the following
    code block to generate *Figure [6.16](#x1-130014r16)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.21**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure [6.16](#x1-130014r16)* shows that the slopes at a `bill_depth` of 1.8
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: ≈ 2 kg/cm for a bill length of 3.5 cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -1.4 kg/cm for a bill length of 4.5 cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ≈ -5 kg/cm for a bill length of 5.5 cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![PIC](img/file192.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.16**: Slopes of `bill_depth` at 1.8 cm for 3 fixed values of `bill_length`'
  prefs: []
  type: TYPE_NORMAL
- en: If you want the information in tabular form, use the function `bmb.interpret.slopes`
    and you will get a DataFrame instead of a plot.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have just scratched the surface of what we can do with the
    tools in the `bmb.interpret` module. This module is a very useful feature of Bambi,
    especially for models with interactions and/or models with link functions other
    than the identity function. I highly recommend you read the Bambi documentation
    for more examples and details not covered here.
  prefs: []
  type: TYPE_NORMAL
- en: 6.9 Variable selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variable selection refers to the process of identifying the most relevant variables
    in a model from a larger set of potential predictors. We perform variable selection
    under the assumption that only a subset of variables have a considerable impact
    on the outcome of interest, while others contribute little or no additional value.
  prefs: []
  type: TYPE_NORMAL
- en: Arguably the ”most Bayesian thing to do” when building a model is to include
    all the variables that we may think of in a single model and then use the posterior
    from that model to make predictions or gain an understanding of the relationships
    of the variables. This is the ”most Bayesian” approach because we are using as
    much data as possible and incorporating in the posterior the uncertainty about
    the importance of the variables. However, being *more Bayesian than Bayes* is
    not always the best idea. We already saw in *Chapter [5](CH05.xhtml#x1-950005)*
    that Bayes factors can be problematic, even when they are a direct consequence
    of Bayes’ theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performing variable selection is a good idea when:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to reduce the measurement cost. For instance, in medicine, we may have
    the money and resources to run a pilot study and measure 30 variables for 200
    patients. But we cannot do the same for thousands. Or we may be able to place
    a lot of sensors in an open field to better model crop gains, but we cannot extend
    that to the size of a country. Reducing costs is not always about money or time;
    when working with humans or other animals, reducing pain and discomfort is important
    too. For example, we may want to predict the risk of a patient having a heart
    attack. We can do this by measuring a lot of variables, but we can also do it
    by measuring just a few variables that are less invasive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to reduce the computational cost. This is not a problem for small and
    simple models, but when we have a lot of variables, a lot of data, or both, the
    computational cost can be prohibitive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We seek a better understanding of significant correlation structures. That
    is, we are interested in understanding which variables provide better predictions.
    It is important to state that we are not talking about causality. While statistical
    models, in particular, GLMS, can be used to infer causality, doing so requires
    extra steps and assumptions. In this book, we do not discuss how to perform causal
    inference. For a very gentle introduction to causal inference, please see this
    video: [https://www.youtube.com/watch?v=gV6wzTk3o1U](https://www.youtube.com/watch?v=gV6wzTk3o1U).
    If you are more serious, you can check out the online book Causal Inference: The
    Mixtape by Scott Cunningham [[Cunningham](Bibliography.xhtml#Xcunningham2021causal), [2021](Bibliography.xhtml#Xcunningham2021causal)]
    [https://mixtape.scunning.com/](https://mixtape.scunning.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we desire a model that is more resilient to changes in the data-generating
    distribution, we can see variable selection as a method to make the model more
    robust against unrepresentative data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.9.1 Projection predictive inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many methods to perform variable selection. In this section, we will
    focus on one of them called projection predictive inference [[Piironen et al.](Bibliography.xhtml#XPiironen2020), [2020](Bibliography.xhtml#XPiironen2020), [McLatchie
    et al.](Bibliography.xhtml#Xmclatchie2023), [2023](Bibliography.xhtml#Xmclatchie2023)].
    The main reason we are focusing on this single method is that it has shown very
    good performance across a broad range of fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main steps of projective prediction inference are:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate a reference model, i.e., a model with all the variables you think can
    be relevant and/or you were able to measure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a set of submodels, i.e., models that only include some subset of the
    variables in the reference model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project the reference model’s posterior distribution into the submodels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick the smallest model that makes predictions close enough to the reference
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When doing projection predictive inference, we only need to perform Bayesian
    inference once, just for the reference model. For the submodels, the posteriors
    are projected. Without going into the technical details, the projection consists
    of finding the parameters for the submodels in such a way that the predictions
    of the submodels are as close as possible to the predictions of the reference
    model. The projection can be done in a computationally efficient way so the cost
    of estimating a posterior is orders of magnitude cheaper than with MCMC methods.
    This is relevant because the total number of possible submodels explodes as we
    increase the number of variables in the reference model. Consider that we need
    to evaluate all possible combinations, without repeating variables. For instance,
    say we have four variables (*A*, *B*, *C*, and *D*) and we need to evaluate 7
    models, namely, *A*, *B*, *C*, *AB*, *BC*, *AC*, and the reference model *ABC*.
    Seven does not sound like a lot, but by the time we reach 8 variables, we will
    need to evaluate 92 different models. See, we double the number of variables,
    and the number of models increases more than 10 times!
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there are ways to reduce the total number of submodels to explore.
    For instance, we could use some cheap method to filter out the most promising
    variables and only do projection predictive inference on those. Another alternative
    is known as forward search; that is, we first fit as many models as the variables
    we have. We then select one model/variable, the one generating the closest predictions
    to the reference model. We then generate all the submodels with 2 variables that
    included the variable selected in the previous step and so on. If we do this forward
    procedure for a reference model with 8 variables instead of 92 different models,
    we will need to evaluate just 36.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect that is relevant to consider when doing projection predictive
    inference is that we only provided priors for the reference model. The submodels
    don’t have explicit priors; they just inherit, somehow, the priors of the reference
    model through the projection procedure.
  prefs: []
  type: TYPE_NORMAL
- en: One reason projective prediction works in practice is thanks to the use of a
    reference model. By fitting the submodels to the in-sample predictions made by
    the reference model, instead of the observed data, we are filtering out the noise
    in the data. This helps separate the more relevant variables from the less relevant
    ones. Another factor is the use of cross-validation in selecting the submodels,
    as discussed in *Chapter [5](CH05.xhtml#x1-950005)*.
  prefs: []
  type: TYPE_NORMAL
- en: 6.9.2 Projection predictive with Kulprit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kulprit is a Python package for projection predictive inference. It works with
    Bambi, as we can pass a reference model built with it and Kulprit will do all
    the hard work for us. To illustrate how to use Kulprit, we are going to use the
    body fat dataset [[Penrose et al.](Bibliography.xhtml#Xpenrose1985), [1985](Bibliography.xhtml#Xpenrose1985)].
    This dataset has measurements from 251 individuals, including their age, weight,
    height, the circumference of the abdomen, etc. Our purpose is to predict the percentage
    of body fat (as estimated by the `siri` variable). Since obtaining accurate measurements
    of body fat is expensive and potentially annoying for patients, we want to reduce
    the measurements while keeping a good predictive accuracy for `siri`. The original
    dataset included 13 variables; to keep this example really simple, I have preselected
    6.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is to define and fit a Bambi model, as usual.
    We have to be sure that we include the argument `idata_kwargs=’log_likelihood’:True`.
    Internally, Kulprit computes the ELPD, and as we discussed in *Chapter [5](CH05.xhtml#x1-950005)*,
    we need the log likelihood in the InferenceData object to be able to estimate
    the ELPD:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.22**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we are ready to use Kulprit. First, we need to call the `ProjectionPredictive`
    class and pass the Bambi model and the idata resulting from the fit of that model.
    Then we ask Kulprit to perform a search; by default, it will do a forward search:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code 6.23:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After the search has finished, we can ask Kulprit to compare the submodels in
    terms of the ELPD. The submodels will show ordered from lowest ELPD to highest,
    as in *Figure [6.17](#x1-133012r17)*. On the x-axis, we have the submodel size,
    i.e., number of variables; we start at zero because we include the intercept-only
    model. The dashed gray line corresponds to the ELPD for the reference model.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file193.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.17**: Comparison of the submodels obtained with Kulprit. Generated
    with `ppi.plot_compare`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see then that a submodel of size 3 is practically equivalent to the
    reference model. But what variables are exactly included in this and the other
    submodels? If we print the `ppi` object, after performing a search, we will get
    an ordered list of the formulas for the submodels matching the order in the plot
    obtained with the command `ppi.plot_compare`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.24**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Then we can see that the model of size 3 is the one including the variables
    `abdomen`, `wrist`, and `height`. This result tells us that if we want to choose
    a model with fewer variables than the reference model but with similar predictive
    accuracy, then this is a good choice. Depending on the context, other submodels
    may also be a good idea. For instance, we may argue that the difference between
    the submodel of sizes 2 and 3 is rather small. Thus, we may be willing to sacrifice
    some accuracy in favor of an even smaller model. For this example, measuring the
    height of patients may not be that problematic, but for other scenarios, adding
    a third variable could be expensive, annoying, dangerous, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to interpret *Figure [6.17](#x1-133012r17)* is by noticing how close
    the ELPDs are for models with size 3 or larger. It may be the case that if we
    repeat the analysis with a slightly different dataset, or even the same dataset
    but with more posterior samples, we could get a slightly different order. Thus,
    if we have many models of size 3 with potentially the same practical predictive
    accuracy, we could justify the selection of the third variable by external factors
    such as how easy or cheap it is to measure, or which one will be less painful
    for patients, etc. In summary, as with other statistical tools, results should
    not be taken blindly but in context; you should have the final word and the tools
    should help you inform your decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'OK, let’s say that we are indeed interested in the submodel of size 3 computed
    by Kulprit; we can get it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code 6.25**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: From the `submodel` object, we can then retrieve some useful information like
    Bambi’s model `submodel.model` or the InferenceData object `submodel.idata`.
  prefs: []
  type: TYPE_NORMAL
- en: One word of caution about interpreting these two objects—`submodel.model` is
    a Bambi model generated from a formula. Thus, its priors will be those automatically
    computed by Bambi. But, the posterior that Kulprit computes, which is stored in
    `submodel.idata.posterior`, does not come directly from this model. Instead, it
    is computed using projection predictive inference (not MCMC) with priors that
    are implicitly inherited during the projection step (not explicit priors). *Figure
    [6.18](#x1-133019r18)* shows such a projected posterior.
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file194.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.18**: Projected posterior for submodel of size 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we trust projected posteriors? Under very general conditions this should
    be a valid posterior so we can trust it. It should be enough to give you an approximate
    idea of the values of the parameters and, of course, it is enough for variable
    selection. The lack of explicit priors could make the interpretation of the model
    more difficult, but if you only care about predictions, that should not be an
    issue. Of course, you can always use Bambi (or PyMC) to explicitly compute the
    full posterior as usual and specify the priors yourself if needed. *Figure [6.19](#x1-133020r19)*
    shows a forest plot for the posterior of the submodel as computed with Bambi (True)
    and approximated with Kulprit (Projected). Notice that there are two possible
    sources of differences here: the intrinsic differences between MCMC and projection
    predictive methods and the different priors for both models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file195.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6.19**: Comparison of the posterior of the submodel (`siri ~abdomen
    + wrist + height`) as computed by Kulprit and the reference model as computed
    by Bambi; variables not shared by both models have been omitted'
  prefs: []
  type: TYPE_NORMAL
- en: Kulprit is a very new library that will keep evolving, and users can expect
    numerous enhancements and refinements shortly. If Kulprit interests you, you can
    help with its development by reporting issues, suggesting ideas, improving the
    documentation, or working on its codebase at [https://github.com/bambinos/kulprit](https://github.com/bambinos/kulprit).
  prefs: []
  type: TYPE_NORMAL
- en: 6.10 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to use Bambi to fit Bayesian models as an
    alternative to the pure PyMC model. We start with the simplest case, a model with
    a single predictor, and then move to more complex models, including polynomials,
    splines, distributional models, models with categorical predictors, and interactions.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of Bambi is that it is very easy to use; it is very similar
    to R’s `formula` syntax. And internally, Bambi defines weakly informative priors
    and handles details that can be cumbersome for complex models. The main disadvantage
    is that it is not as flexible as PyMC. The range of models that Bambi can handle
    is a small subset of those from PyMC. Still, this subset contains many of the
    most commonly used statistical models in both industry and academia. The strength
    of Bambi is not just easy model building, but easier model interpretation. Across
    the chapter, we have seen how to use Bambi’s `interpret` module to gain a better
    understanding of the models we fit. Finally, we have seen how to use Kulprit to
    perform projection predictive inference and perform variable selection. Projection
    predictive inference offers a promising approach to variable selection, and Kulprit
    is a promising Pythonic way of doing it.
  prefs: []
  type: TYPE_NORMAL
- en: 6.11 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Read the Bambi documentation ( [https://bambinos.github.io/bambi/](https://bambinos.github.io/bambi/))
    and learn how to specify custom priors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply what you learned in the previous point and specify a HalfNormal prior
    for the slope of `model_t`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a model like `model_poly4`, but using `raw` polynomials, compare the
    coefficients and the mean fit of both models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain in your own words what a distributional model is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand `model_spline` to a distributional model. Use another spline to model
    the *α* parameter of the NegativeBinomial family.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a model named `model_p2` for the `body_mass` with the predictors `bill_length`,
    `bill_depth`, `flipper_length`, and `species`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use LOO to compare the model in the previous point and `model_p.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the functions in the `interpret` module to interpret `model_p2`. Use both
    plots and tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our community Discord space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  prefs: []
  type: TYPE_NORMAL
- en: '![PIC](img/file1.png)'
  prefs: []
  type: TYPE_IMG

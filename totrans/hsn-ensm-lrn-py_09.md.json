["```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom copy import deepcopy\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport numpy as np\nbc = load_breast_cancer()\ntrain_size = 400\ntrain_x, train_y = bc.data[:train_size], bc.target[:train_size]\ntest_x, test_y = bc.data[train_size:], bc.target[train_size:]\nnp.random.seed(123456)\n```", "```py\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 3\nbase_classifier = DecisionTreeClassifier(max_depth=1)\n# Create the initial weights\ndata_weights = np.zeros(train_size) + 1/train_size\n# Create a list of indices for the train set\nindices = [x for x in range(train_size)]\nbase_learners = []\nlearners_errors = np.zeros(ensemble_size)\nlearners_weights = np.zeros(ensemble_size)\n```", "```py\n\n# Create each base learner\nfor i in range(ensemble_size):\n    weak_learner = deepcopy(base_classifier)\n    # Choose the samples by sampling with replacement.\n    # Each instance's probability is dictated by its weight.\n    data_indices = np.random.choice(indices, train_size, p=data_weights)\n    sample_x, sample_y = train_x[data_indices], train_y[data_indices]\n```", "```py\n    # Fit the weak learner and evaluate it\n    weak_learner.fit(sample_x, sample_y)\n    predictions = weak_learner.predict(train_x)\n    errors = predictions != train_y\n    corrects = predictions == train_y\n```", "```py\n\n    # Calculate the weighted errors\n    weighted_errors = data_weights*errors\n    # The base learner's error is the average of the weighted errors\n    learner_error = np.mean(weighted_errors)\n    learners_errors[i] = learner_error\n```", "```py\n    # The learner's weight\n    learner_weight = np.log((1-learner_error)/learner_error)/2\n    learners_weights[i] = learner_weight\n    # Update the data weights\n    data_weights[errors] = np.exp(data_weights[errors] * learner_weight)\n    data_weights[corrects] = np.exp(-data_weights[corrects] * learner_weight)\n    data_weights = data_weights/sum(data_weights)\n    # Save the learner\n    base_learners.append(weak_learner)\n```", "```py\n# --- SECTION 3 ---\n# Evaluate the ensemble\nensemble_predictions = []\nfor learner, weight in zip(base_learners, learners_weights):\n    # Calculate the weighted predictions\n    prediction = learner.predict(test_x)\n    ensemble_predictions.append(prediction*weight)\n    # The final prediction is the weighted mean of the individual predictions\n    ensemble_predictions = np.mean(ensemble_predictions, axis=0) >= 0.5\n    ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# --- SECTION 4 ---\n# Print the accuracy\nprint('Boosting: %.2f' % ensemble_acc)\n```", "```py\n\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom copy import deepcopy\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\nimport numpy as np\ndiabetes = load_diabetes()\ntrain_size = 400\ntrain_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]\ntest_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]\nnp.random.seed(123456)\n```", "```py\n\n# --- SECTION 2 ---\n# Create the ensemble\n# Define the ensemble's size, learning rate and decision tree depth\nensemble_size = 50\nlearning_rate = 0.1\nbase_classifier = DecisionTreeRegressor(max_depth=3)\n# Create placeholders for the base learners and each step's prediction\nbase_learners = []\n# Note that the initial prediction is the target variable's mean\nprevious_predictions = np.zeros(len(train_y)) + np.mean(train_y)\n```", "```py\n# Create the base learners\nfor _ in range(ensemble_size):\n    # Start by calculating the pseudo-residuals\n    errors = train_y - previous_predictions\n    # Make a deep copy of the base classifier and train it on the\n    # pseudo-residuals\n    learner = deepcopy(base_classifier)\n    learner.fit(train_x, errors)\n    predictions = learner.predict(train_x) \n\n```", "```py\n\n    # Multiply the predictions with the learning rate and add the results\n    # to the previous prediction\n    previous_predictions = previous_predictions + learning_rate*predictions\n    # Save the base learner\n    base_learners.append(learner)\n```", "```py\n\n# --- SECTION 3 ---\n# Evaluate the ensemble\n# Start with the train set's mean\nprevious_predictions = np.zeros(len(test_y)) + np.mean(train_y)\n# For each base learner predict the pseudo-residuals for the test set and\n# add them to the previous prediction, \n# after multiplying with the learning rate\nfor learner in base_learners:\n    predictions = learner.predict(test_x)\n    previous_predictions = previous_predictions + learning_rate*predictions\n\n# --- SECTION 4 ---\n# Print the metrics\nr2 = metrics.r2_score(test_y, previous_predictions)\nmse = metrics.mean_squared_error(test_y, previous_predictions)\nprint('Gradient Boosting:')\nprint('R-squared: %.2f' % r2)\nprint('MSE: %.2f' % mse)\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import metrics\n\ndigits = load_digits()\ntrain_size = 1500\ntrain_x, train_y = digits.data[:train_size], digits.target[:train_size]\ntest_x, test_y = digits.data[train_size:], digits.target[train_size:]\nnp.random.seed(123456)\n\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 200\nensemble = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n                              algorithm=\"SAMME\",\n                              n_estimators=ensemble_size)\n\n# --- SECTION 3 ---\n# Train the ensemble\nensemble.fit(train_x, train_y)\n\n# --- SECTION 4 ---\n# Evaluate the ensemble\nensemble_predictions = ensemble.predict(test_x)\nensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# --- SECTION 5 ---\n# Print the accuracy\nprint('Boosting: %.2f' % ensemble_acc)\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom copy import deepcopy\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\n\nimport numpy as np\n\ndiabetes = load_diabetes()\n\ntrain_size = 400\ntrain_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]\ntest_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]\n\nnp.random.seed(123456)\n\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 1000\nensemble = AdaBoostRegressor(n_estimators=ensemble_size)\n\n# --- SECTION 3 ---\n# Evaluate the ensemble\nensemble.fit(train_x, train_y)\npredictions = ensemble.predict(test_x)\n\n# --- SECTION 4 ---\n# Print the metrics\nr2 = metrics.r2_score(test_y, predictions)\nmse = metrics.mean_squared_error(test_y, predictions)\n\nprint('Gradient Boosting:')\nprint('R-squared: %.2f' % r2)\nprint('MSE: %.2f' % mse)\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import metrics\nimport numpy as np\ndiabetes = load_diabetes()\ntrain_size = 400\ntrain_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]\ntest_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]\nnp.random.seed(123456)\n\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 200\nlearning_rate = 0.1\nensemble = GradientBoostingRegressor(n_estimators=ensemble_size,\n learning_rate=learning_rate)\n\n# --- SECTION 3 ---\n# Evaluate the ensemble\nensemble.fit(train_x, train_y)\npredictions = ensemble.predict(test_x)\n\n# --- SECTION 4 ---\n# Print the metrics\nr2 = metrics.r2_score(test_y, predictions)\nmse = metrics.mean_squared_error(test_y, predictions)\nprint('Gradient Boosting:')\nprint('R-squared: %.2f' % r2)\nprint('MSE: %.2f' % mse)\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\n\ndigits = load_digits()\n\ntrain_size = 1500\ntrain_x, train_y = digits.data[:train_size], digits.target[:train_size]\ntest_x, test_y = digits.data[train_size:], digits.target[train_size:]\n\nnp.random.seed(123456)\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 200\nlearning_rate = 0.1\nensemble = GradientBoostingClassifier(n_estimators=ensemble_size,\n learning_rate=learning_rate)\n\n# --- SECTION 3 ---\n# Train the ensemble\nensemble.fit(train_x, train_y)\n\n# --- SECTION 4 ---\n# Evaluate the ensemble\nensemble_predictions = ensemble.predict(test_x)\n\nensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# --- SECTION 5 ---\n# Print the accuracy\nprint('Boosting: %.2f' % ensemble_acc)\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom sklearn.datasets import load_diabetes\nfrom xgboost import XGBRegressor\nfrom sklearn import metrics\nimport numpy as np\ndiabetes = load_diabetes()\ntrain_size = 400\ntrain_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]\ntest_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]\nnp.random.seed(123456)\n\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 200\nensemble = XGBRegressor(n_estimators=ensemble_size, n_jobs=4,\n                        max_depth=1, learning_rate=0.1,\n objective ='reg:squarederror')\n```", "```py\n\n# --- SECTION 3 ---\n# Evaluate the ensemble\nensemble.fit(train_x, train_y)\npredictions = ensemble.predict(test_x)\n\n# --- SECTION 4 ---\n# Print the metrics\nr2 = metrics.r2_score(test_y, predictions)\nmse = metrics.mean_squared_error(test_y, predictions)\nprint('Gradient Boosting:')\nprint('R-squared: %.2f' % r2)\nprint('MSE: %.2f' % mse)\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom sklearn.datasets import load_digits\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\nimport numpy as np\ndigits = load_digits()\ntrain_size = 1500\ntrain_x, train_y = digits.data[:train_size], digits.target[:train_size]\ntest_x, test_y = digits.data[train_size:], digits.target[train_size:]\nnp.random.seed(123456)\n\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 100\nensemble = XGBClassifier(n_estimators=ensemble_size, n_jobs=4)\n\n# --- SECTION 3 ---\n# Train the ensemble\nensemble.fit(train_x, train_y)\n\n# --- SECTION 4 ---\n# Evaluate the ensemble\nensemble_predictions = ensemble.predict(test_x)\nensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# --- SECTION 5 ---\n# Print the accuracy\nprint('Boosting: %.2f' % ensemble_acc)\n```"]
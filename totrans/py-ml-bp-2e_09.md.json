["```py\n<category> \n<pattern>WHAT IS UP</pattern> \n<template>The sky, duh. Pfft. Humans...</template> \n</category> \n```", "```py\n<category> \n<pattern>* FOR ME<pattern> \n<template>I'm a bot. I don't <star/>. Ever.</template> \n</category> \n```", "```py\n<category> \n<pattern>I LIKE TURTLES</pattern> \n<template>I feel like this whole <set name=\"topic\">turtle</set> thing could be a problem. What do you like about them? </template> \n</category> \n```", "```py\n<topic name=\"turtles\"> \n\n<category> \n<pattern>* SHELL IS *</pattern> \n<template>I dislike turtles primarily because of their shells. What other creepy things do you like about turtles? </template> \n</category> \n\n<category> \n<pattern>* HIDE *</pattern> \n<template>I wish, like a turtle, that I could hide from this conversation. </template> \n</category> \n\n</topic> \n```", "```py\nimport pandas as pd \nimport re \npd.set_option('display.max_colwidth',200) \n```", "```py\ndf = pd.read_csv('nscb.csv') \ndf.head() \n```", "```py\nconvo = df.iloc[:,0] \nconvo \n```", "```py\nclist = [] \ndef qa_pairs(x): \n    cpairs = re.findall(\": (.*?)(?:$|\\n)\", x) \n    clist.extend(list(zip(cpairs, cpairs[1:]))) \n\nconvo.map(qa_pairs); \nconvo_frame = pd.Series(dict(clist)).to_frame().reset_index() \nconvo_frame.columns = ['q', 'a'] \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.metrics.pairwise import cosine_similarity \n\nvectorizer = TfidfVectorizer(ngram_range=(1,3)) \nvec = vectorizer.fit_transform(convo_frame['q']) \n```", "```py\nmy_q = vectorizer.transform(['Hi. My name is Alex.']) \n\ncs = cosine_similarity(my_q, vec) \n\nrs = pd.Series(cs[0]).sort_values(ascending=False) \ntop5 = rs.iloc[0:5] \ntop5 \n```", "```py\nconvo_frame.iloc[top5.index]['q'] \n```", "```py\nrsi = rs.index[0] \nrsi \n\nconvo_frame.iloc[rsi]['a'] \n```", "```py\ndef get_response(q): \n    my_q = vectorizer.transform([q]) \n    cs = cosine_similarity(my_q, vec) \n    rs = pd.Series(cs[0]).sort_values(ascending=False) \n    rsi = rs.index[0] \n    return convo_frame.iloc[rsi]['a'] \n\nget_response('Yes, I am clearly more clever than you will ever be!') \n```", "```py\nget_response('You are a stupid machine. Why must I prove anything to    \n              you?') \n```", "```py\nget_response('Did you eat tacos?') \n```", "```py\nget_response('With beans on top?') \n```", "```py\nget_response('What else do you like to do?') \n```", "```py\nget_response('What do you like about it?') \n```", "```py\nget_response('Me, random?') \n```", "```py\nget_response('I think you mean you\\'re') \n```", "```py\nfrom keras.models import Model \nfrom keras.layers import Input, LSTM, Dense \nimport numpy as np \n```", "```py\nbatch_size = 64  # Batch size for training. \nepochs = 100  # Number of epochs to train for. \nlatent_dim = 256  # Latent dimensionality of the encoding space. \nnum_samples = 1000  # Number of samples to train on. \n```", "```py\ninput_texts = [] \ntarget_texts = [] \ninput_characters = set() \ntarget_characters = set() \n```", "```py\nconvo_frame['q len'] = convo_frame['q'].astype('str').apply(lambda  \n                       x: len(x)) \nconvo_frame['a len'] = convo_frame['a'].astype('str').apply(lambda \n                       x: len(x)) \nconvo_frame = convo_frame[(convo_frame['q len'] < 50)&\n                          (convo_frame['a len'] < 50)] \n```", "```py\ninput_texts = list(convo_frame['q'].astype('str')) \ntarget_texts = list(convo_frame['a'].map(lambda x: '\\t' + x + \n                    '\\n').astype('str')) \n```", "```py\ninput_texts \n```", "```py\ntarget_texts \n```", "```py\ninput_characters \n```", "```py\ntarget_characters \n```", "```py\ninput_characters = sorted(list(input_characters)) \ntarget_characters = sorted(list(target_characters)) \nnum_encoder_tokens = len(input_characters) \nnum_decoder_tokens = len(target_characters) \nmax_encoder_seq_length = max([len(txt) for txt in input_texts]) \nmax_decoder_seq_length = max([len(txt) for txt in target_texts]) \n\nprint('Number of samples:', len(input_texts)) \nprint('Number of unique input tokens:', num_encoder_tokens) \nprint('Number of unique output tokens:', num_decoder_tokens) \nprint('Max sequence length for inputs:', max_encoder_seq_length) \nprint('Max sequence length for outputs:', max_decoder_seq_length) \n```", "```py\ninput_token_index = dict( \n    [(char, i) for i, char in enumerate(input_characters)]) \ntarget_token_index = dict( \n    [(char, i) for i, char in enumerate(target_characters)]) \n\nencoder_input_data = np.zeros( \n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), \n    dtype='float32') \ndecoder_input_data = np.zeros( \n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), \n    dtype='float32') \ndecoder_target_data = np.zeros( \n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), \n    dtype='float32') \n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)): \n    for t, char in enumerate(input_text): \n        encoder_input_data[i, t, input_token_index[char]] = 1\\. \n    for t, char in enumerate(target_text): \n        # decoder_target_data is ahead of decoder_input_data by one \n        # timestep \n        decoder_input_data[i, t, target_token_index[char]] = 1\\. \n        if t > 0: \n            # decoder_target_data will be ahead by one timestep \n            # and will not include the start character. \n            decoder_target_data[i, t - 1, target_token_index[char]] = \n                                1\\. \n```", "```py\nDecoder_input_data \n```", "```py\n# Define an input sequence and process it. \nencoder_inputs = Input(shape=(None, num_encoder_tokens)) \nencoder = LSTM(latent_dim, return_state=True) \nencoder_outputs, state_h, state_c = encoder(encoder_inputs) \n# We discard `encoder_outputs` and only keep the states. \nencoder_states = [state_h, state_c] \n\n# Set up the decoder, using `encoder_states` as initial state. \ndecoder_inputs = Input(shape=(None, num_decoder_tokens)) \n\n# We set up our decoder to return full output sequences, \n# and to return internal states as well. We don't use the \n# return states in the training model, but we will use them in  \n# inference. \ndecoder_lstm = LSTM(latent_dim, return_sequences=True,  \n               return_state=True) \ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs, \n                                     initial_state=encoder_states) \ndecoder_dense = Dense(num_decoder_tokens, activation='softmax') \ndecoder_outputs = decoder_dense(decoder_outputs) \n```", "```py\n# Define the model that will turn \n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data` \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n\n# Run training \nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy') \nmodel.fit([encoder_input_data, decoder_input_data], \n           decoder_target_data, \n           batch_size=batch_size, \n           epochs=epochs, \n           validation_split=0.2) \n# Save model \nmodel.save('s2s.h5') \n```", "```py\n# Next: inference mode (sampling). \n# Here's the drill: \n# 1) encode input and retrieve initial decoder state \n# 2) run one step of decoder with this initial state \n# and a \"start of sequence\" token as target. \n# Output will be the next target token \n# 3) Repeat with the current target token and current states \n\n# Define sampling models \nencoder_model = Model(encoder_inputs, encoder_states) \n\ndecoder_state_input_h = Input(shape=(latent_dim,)) \ndecoder_state_input_c = Input(shape=(latent_dim,)) \ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c] \ndecoder_outputs, state_h, state_c = decoder_lstm( \n    decoder_inputs, initial_state=decoder_states_inputs) \ndecoder_states = [state_h, state_c] \ndecoder_outputs = decoder_dense(decoder_outputs) \ndecoder_model = Model( \n    [decoder_inputs] + decoder_states_inputs, \n    [decoder_outputs] + decoder_states) \n\n# Reverse-lookup token index to decode sequences back to \n# something readable. \nreverse_input_char_index = dict( \n    (i, char) for char, i in input_token_index.items()) \nreverse_target_char_index = dict( \n    (i, char) for char, i in target_token_index.items()) \n\ndef decode_sequence(input_seq): \n    # Encode the input as state vectors. \n    states_value = encoder_model.predict(input_seq) \n\n    # Generate empty target sequence of length 1\\. \n    target_seq = np.zeros((1, 1, num_decoder_tokens)) \n    # Populate the first character of target sequence with the start character. \n    target_seq[0, 0, target_token_index['\\t']] = 1\\. \n\n    # Sampling loop for a batch of sequences \n    # (to simplify, here we assume a batch of size 1). \n    stop_condition = False \n    decoded_sentence = '' \n    while not stop_condition: \n        output_tokens, h, c = decoder_model.predict( \n            [target_seq] + states_value) \n\n        # Sample a token \n        sampled_token_index = np.argmax(output_tokens[0, -1, :]) \n        sampled_char = reverse_target_char_index[sampled_token_index] \n        decoded_sentence += sampled_char \n\n        # Exit condition: either hit max length \n        # or find stop character. \n        if (sampled_char == '\\n' or \n           len(decoded_sentence) > max_decoder_seq_length): \n            stop_condition = True \n\n        # Update the target sequence (of length 1). \n        target_seq = np.zeros((1, 1, num_decoder_tokens)) \n        target_seq[0, 0, sampled_token_index] = 1\\. \n\n        # Update states \n        states_value = [h, c] \n\n    return decoded_sentence \n\nfor seq_index in range(100): \n    # Take one sequence (part of the training set) \n    # for trying out decoding. \n    input_seq = encoder_input_data[seq_index: seq_index + 1] \n    decoded_sentence = decode_sequence(input_seq) \n    print('-') \n    print('Input sentence:', input_texts[seq_index]) \n    print('Decoded sentence:', decoded_sentence) \n```", "```py\nget_response(\"This is the end, Cleverbot. Say goodbye.\") \n```"]
<html><head></head><body>
		<div id="_idContainer119" class="Content">
			<h1 id="_idParaDest-80"><em class="italics"><a id="_idTextAnchor087"/>Chapter 3</em></h1>
		</div>
		<div id="_idContainer120" class="Content">
			<h1 id="_idParaDest-81"><a id="_idTextAnchor088"/>Regression Analysis</h1>
		</div>
		<div id="_idContainer121" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Describe regression models and explain the difference between regression and classification problems</li>
				<li class="bullets">Explain the concept of gradient descent, how it is used in linear regression problems, and how it can be applied to other model architectures</li>
				<li class="bullets">Use linear regression to construct a linear model for data in an <em class="italics">x-y</em> plane</li>
				<li class="bullets">Evaluate the performance of linear models and use the evaluation to choose the best model</li>
				<li class="bullets">Use feature engineering to create dummy variables for constructing more complicated linear models</li>
				<li class="bullets">Construct time series regression models using autoregression</li>
			</ul>
			<p>This chapter covers regression problems and analysis, introducing us to linear regression as well as multiple linear regression, gradient descent, and autoregression.</p>
		</div>
		<div id="_idContainer217" class="Content">
			<h2 id="_idParaDest-82"><a id="_idTextAnchor089"/>Introduction</h2>
			<p>In the first two chapters, we were introduced to the concept of supervised machine learning in Python and the essential techniques required for loading, cleaning, exploring, and visualizing raw data sources. We discussed the criticality of the correlations between the specified inputs and desired output for the given problem, as well as how the initial data preparation process can sometimes take a lot of the time spent on the entire project.</p>
			<p>In this chapter, we will delve into the model building process and will construct our first supervised machine learning solution using linear regression. So, let's get started.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor090"/>Regression and Classification Problems</h2>
			<p>We discussed two distinct methods, supervised learning and unsupervised learning, in <em class="italics">Chapter 1</em>, <em class="italics">Python Machine Learning Toolkit</em>. Supervised learning problems aim to map input information to a known output value or label, but there are two further subcategories to consider. Both supervised and unsupervised learning problems can be further divided into regression or classification problems. Regression problems, which are the subject of this chapter, aim to predict or model continuous values, for example, predicting the temperature tomorrow in degrees Celsius or determining the location of a face within an image. In contrast, classification problems, rather than returning a continuous value, predict membership of one of a specified number of classes or categories. The example supervised learning problem in <em class="italics">Chapter 1</em>, <em class="italics">Python Machine Learning Toolkit</em>, where we wanted to determine or predict whether a wig was from the 1960s or 1980s, is a good example of a supervised classification problem. There, we attempted to predict whether a wig was from one of two distinct groups or classes; class 1 being the 1960s and class 2 being the 1980s. Other classification problems include predicting whether a passenger of the Titanic survived or perished or the classic MNIST problem (<a href="B13323_03_ePub_Final_NT.xhtml#_idTextAnchor116">http://yann.lecun.com/exdb/mnist/</a>). MNIST is a database of 70,000 labeled images of handwritten digits 0 through 9. The task in classifying examples from MNIST is to take one of the 70,000 input images and predict or classify which digit 0-9 is written in the image; so, the model must predict the membership of the image in one of 10 different classes.</p>
			<h3 id="_idParaDest-84"><a id="_idTextAnchor091"/>Data, Models, Training, and Evaluation</h3>
			<p>Before we begin our deep dive into regression problems, we will first look at the four major stages involved in creating any machine learning model, supervised regression or otherwise. These stages are as follows:</p>
			<ol>
				<li>Data preparation</li>
				<li>Model architecture specification</li>
				<li>The design and execution of the training process</li>
				<li>The evaluation of the trained model</li>
			</ol>
			<p>It is advised that you ensure you are completely confident in your understanding of this pipeline and of what is described within this section, as each of these stages is critical in achieving high or even reasonable system performance. We will consider each of these stages in the context of the wig classification problem from <em class="italics">Chapter 1</em>, <em class="italics">Python Machine Learning Toolkit</em>.</p>
			<p><strong class="bold">Data Preparation</strong></p>
			<p>The first stage of the pipeline, data preparation, was the focus of a significant component of <em class="italics">Chapter 1</em>, <em class="italics">Python Machine Learning Toolkit</em>, and thus will not be the subject of further analysis in this section. It is important, however, that the criticality of the data specification, collection, and cleaning/tidying process is well understood. We cannot expect to produce a high-performing system if the input data is sub-optimal. One common phrase that you should always remember with regard to data quality is <em class="italics">junk in, junk out</em>. If you put junk data in, you are going to get a junk result out. In our wig example, we are looking for a sample size at least in the order of hundreds, ideally thousands, that have been correctly labeled as either from the 1960s or 1980s. We do not want samples that have been incorrectly labeled or aren't even from either era.</p>
			<p><strong class="bold">Model Architecture</strong></p>
			<p>The second stage, model architecture specification, will be described in more detail in this chapter. This stage defines the type of model that is to be used, as well as the types and values of the parameters that comprise the model itself. The model is essentially a mathematical equation that is used to define the relationship between the input data and the desired result. As with any equation, the model is composed of variables and constants combined by a set of processes, for example, addition, subtraction, or convolution. The nature and values of the model parameters will vary depending upon the type of model selected and the level of complexity at which the model is able to describe the relationship being observed. Simpler models will contain fewer parameters with greater constraints on their values, while more complex models have a greater number of possibly varying parameters. In this chapter, we will be employing a linear model, which is one of the simpler models available, compared with some others, such as convolutional neural network models, which may have more than one million parameters that need to be optimized for a good result. This simplicity should not be mistaken for a lack of power, or a lack of ability to describe relationships within data, but simply that fewer parameters are available for tuning (that is, changing the values to optimize performance).</p>
			<p><strong class="bold">Model Training</strong></p>
			<p>The third stage of the system pipeline is the design and execution of the training process, that is, the mechanism by which the values for the parameters of the model are determined. In a supervised learning problem, we can think about the training process as being analogous to being a student within a classroom. In a typical classroom environment, the teacher already has the answers to a given problem and is attempting to show the students how to solve the problem given some set of inputs. In such a scenario, the student is the model, and the parameters are all within the student's brain and are the means by which the student correctly answers the problem.</p>
			<p>The training process is the method the teacher uses to train the student to correctly answer the problem; this method can be tweaked and changed in response to the student's ability to learn and understand the content. Once a model architecture has been defined (that is, the student in the class), it is the training process that is used to provide the guidance and constraints required to approach an optimal solution. Just as some students perform better in different learning environments, so do models. Thus, there is an additional set of parameters known as <strong class="keyword">hyperparameters</strong>, which, while not being used within the model itself to make predictions given some set of input data, are defined, used, and tuned in an attempt to optimize the performance of the model against a specified cost (or error) function (for example, root mean squared error). We will also discuss hyperparameters in more detail in this chapter, but for the time being, it is simplest to think about hyperparameters as the environment in which the actual parameters of the model are determined.</p>
			<p><strong class="bold">Model Evaluation</strong></p>
			<p>The final stage of the pipeline is the evaluation of the model, which yields the final performance metric. This is the mechanism through which we know whether the model is worth publishing, better than a previous version, or has been effectively translated across programming languages or development environments. We will cover some of these metrics in more detail in <em class="italics">Chapter 6</em>, <em class="italics">Model Evaluation</em>, and as such this will not be discussed in detail at this stage. Just keep in mind that whichever validation technique is selected, it needs to be capable of consistently reporting and independently measuring the performance of the model against the dataset. Again, using our wig dataset as the example, the evaluation stage would look at how many correct predictions the model achieved, given wig images and the known eras as the labels.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor092"/>Linear Regression</h2>
			<p>We will start our investigation into regression problems with the selection of a linear model. Linear models, while being a great first choice due to their intuitive nature, are also very powerful in their predictive power, assuming datasets contain some degree of linear or polynomial relationship between the input features and values. The intuitive nature of linear models often arises from the ability to view data as plotted on a graph and observe a trending pattern in the data with, say, the output (the <em class="italics">y</em> axis value for the data) trending positively or negatively with the input (<em class="italics">x</em> axis value). While often not presented as such, the fundamental components of linear regression models are also often learned during high school mathematics classes. You may recall that the equation of a straight line, or linear model, is defined as follows:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/C12622_03_01.jpg" alt="Figure 3.1: Equation of a straight line"/>
				</div>
			</div>
			<h6><a id="_idTextAnchor093"/>Figure 3.1: Equation of a straight line</h6>
			<p>Here, <em class="italics">x</em> is the input value and <em class="italics">y</em> is the corresponding output or predicted value. The parameters of the model are the gradient or slope of the line (change in <em class="italics">y</em> values divided by change in <em class="italics">x</em>) defined by <em class="italics">m</em> as well as the <em class="italics">y</em>-intercept value <em class="italics">b</em>, which indicates where the line crosses the <em class="italics">y</em> axis. With such a model, we can provide values for the <em class="italics">m</em> and <em class="italics">b</em> parameters to construct a linear model. For example, <em class="italics">y = 2x + 1</em>, has a slope of 2, indicating the changes in <em class="italics">y</em> values are at a rate of twice that of <em class="italics">x</em>; the line crosses the <em class="italics">y</em> intercept at 1:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/C12622_03_02.jpg" alt="Figure 3.2: Parameters of a straight line"/>
				</div>
			</div>
			<h6>Figure 3.2: Parameters of a straight line</h6>
			<p>So, we have an understanding of the parameters that are required to define a straight line, but this isn't really doing anything particularly interesting. We just dictated the parameters of the model to construct a line. What we want to do is take a dataset and construct a model that best describes a dataset. As mentioned before, this dataset needs to have something that approximates a linear relationship between the input features and output values. For this purpose, we have created a synthetic dataset of recorded air temperatures from the years 1841 to 2010, which is available in the accompanying code bundle of this book or on GitHub at <a href="B13323_03_ePub_Final_NT.xhtml#_idTextAnchor115">https://github.com/TrainingByPackt/Supervised-Learning-with-Python</a>. This dataset is composed of values designed to demonstrate the subject matter of this chapter and should not be mistaken for data collected from a scientific study.</p>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor094"/>Exercise 28: Plotting Data with a Moving Average</h3>
			<p>As we discussed in <em class="italics">Chapter 1</em>, <em class="italics">Python Machine Learning Toolkit</em>, a thorough understanding of the dataset being used is critical if a high-performing model is to be built. So, with this in mind, let's use this exercise to load, plot, and interrogate the data source:</p>
			<ol>
				<li value="1">Import the <strong class="inline">numpy</strong>, <strong class="inline">pandas</strong>, and <strong class="inline">matplotlib</strong> packages with alternative handles:<p class="snippet">Import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p></li>
				<li>Use the pandas <strong class="inline">read_csv</strong> function to load the CSV file containing the <strong class="inline">synth_temp.csv</strong> dataset, and then display the first five lines of data:<p class="snippet">df = pd.read_csv('synth_temp.csv')</p><p class="snippet">df.head()</p><p>The output will be as follows:</p><div id="_idContainer124" class="IMG---Figure"><img src="image/C12622_03_03.jpg" alt="Figure 3.3: The first five rows"/></div><h6>Figure 3.3: The first five rows</h6></li>
				<li>Since we are only interested in the data from 1901 to 2010, remove all rows prior to 1901:<p class="snippet">df = df.loc[df.Year &gt; 1901]</p><p class="snippet">df.head()</p><p>The output will be:</p><div id="_idContainer125" class="IMG---Figure"><img src="image/C12622_03_04.jpg" alt="Figure 3.4: The first five rows after removing all rows prior to 1901"/></div><h6>Figure 3.4: The first five rows after removing all rows prior to 1901</h6></li>
				<li>The original dataset contains multiple temperature measurements per year, with more measurements for the later years (12 for 2010) and less for the earlier years (6 for 1841); however, we are interested in a list of yearly average temperatures. Group the data by year and use the <strong class="inline">agg</strong> method of the DataFrame to create the yearly averages:<p class="snippet">df_group_year = df.groupby('Year').agg(np.mean)</p><p class="snippet">df_group_year.head()</p><p>The output will be:</p><div id="_idContainer126" class="IMG---Figure"><img src="image/C12622_03_05.jpg" alt="Figure 3.5: Yearly average data"/></div><h6>Figure 3.5: Yearly average data</h6></li>
				<li>Given that the data is quite noisy, a moving average filter would provide a useful indicator of the overall trend. A moving average filter simply computes the average over the last <em class="italics">N</em> values and assigns this average to the <em class="italics">(N+1)</em><em class="italics">th</em> sample. Compute the values for a moving average signal for the temperature measurements using a window of 10 years:<p class="snippet">window = 10</p><p class="snippet">rolling = df_group_year.AverageTemperature.rolling(window).mean()</p><p class="snippet">rolling.head(n=20)</p><p>We will get the following output:</p><div id="_idContainer127" class="IMG---Figure"><img src="image/C12622_03_06.jpg" alt="Figure 3.6: Values for a moving average signal"/></div><h6>Figure 3.6: Values for a moving average signal</h6><p>Notice that the first 9 samples are <strong class="inline">NaN</strong>, which is because of the size of the moving average filter window. The window size is 10, thus 9 (10-1) samples are required to generate the first average, and thus the first 9 samples are <strong class="inline">NaN</strong>.</p></li>
				<li>Finally, plot the measurements by year along with the moving average signal:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_group_year.index, df_group_year.AverageTemperature, label='Raw Data', c='k');</p><p class="snippet">ax.plot(df_group_year.index, rolling, c='k', linestyle='--', label=f'{window} year moving average');</p><p class="snippet">ax.set_title('Mean Air Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Year')</p><p class="snippet">ax.set_ylabel('Temperature (degC)')</p><p class="snippet">ax.set_xticks(range(df_group_year.index.min(), df_group_year.index.max(), 10))</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/C12622_03_07.jpg" alt="Figure 3.7: Mean annual air temperature"/>
				</div>
			</div>
			<h6>Figure 3.7: Mean annual air temperature</h6>
			<p><em class="italics">Figure 3.7</em> is the expected output of this exercise and is a plot of the mean land temperature measurements for each year with a 10-year moving average trending. By simply looking at this plot, we can immediately make a couple of interesting observations. The first observation that we can make is that the temperature remained relatively consistent from the year 1901 to about 1960, after which there is an increasing trend until the data ends in 2010. Secondly, there is a reasonable amount of scatter or noise in the measurements.</p>
			<h3 id="_idParaDest-87"><a id="_idTextAnchor095"/>Activity 5: Plotting Data with a Moving Average</h3>
			<p>For this activity, we have acquired a dataset of weather information from Austin, Texas (<strong class="inline">austin_weather.csv</strong>), available in the accompanying source code, and will be looking at the changes in average daily temperature. We will plot a moving average filter for this dataset.</p>
			<p>Before we begin, we will need to import a few libraries, which can be done as follows:</p>
			<p class="snippet">import numpy as np</p>
			<p class="snippet">import pandas as pd</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Load the dataset into a pandas DataFrame from the CSV file.</li>
				<li>We only need the <strong class="inline">Date</strong> and <strong class="inline">TempAvgF</strong> columns; remove all others from the dataset.</li>
				<li>Initially, we will only be interested in the first year's data, so we need to extract that information only. Create a column in the DataFrame for the year value and extract the year value as an integer from the strings in the <strong class="inline">Date</strong> column and assign these values to the <strong class="inline">Year</strong> column.<p>Note that temperatures are recorded daily.</p></li>
				<li>Repeat this process to extract the month values and store the values as integers in a <strong class="inline">Month</strong> column.</li>
				<li>Copy the first year's worth of data to a DataFrame.</li>
				<li>Compute a 20-day moving average filter.</li>
				<li>Plot the raw data and moving average signal, with the <em class="italics">x</em> axis being the day number in the year.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 325.</p></li>
			</ol>
			<h3 id="_idParaDest-88"><a id="_idTextAnchor096"/>Least Squares Method</h3>
			<p>The field of machine learning and artificial intelligence evolved essentially as a specialized branch of statistics, and as such it is important to reflect on these origins from time to time to have a thorough understanding of how models are able to be used as predictive tools. It is also interesting to see the points where machine learning grew out of statistics and compare the more modern methods available today. Linear regression models are a great example of this as they can be used to demonstrate more classical solving techniques such as the least squares method, as well as more modern methods, such as gradient descent, which we will also cover in this chapter. Linear models also have the additional advantage of containing mathematical concepts commonly learned in high school, such as the equation of a straight line, providing a useful platform for describing the methods used to fit data.</p>
			<p>The traditional method of solving linear models, which is executed by toolkits such as scikit-learn, SciPy, Minitab, and Excel, is the <strong class="keyword">least squares method</strong>, and this is the first method we will cover. Referring to our standard equation for a straight line (<em class="italics">Figure 3.1</em>), <em class="italics">m</em> is the slope or gradient of the line and <em class="italics">c</em> is the <em class="italics">y</em> axis offset. These values can be directly calculated in the least squares method by first determining the average <em class="italics">x</em> and <em class="italics">y</em> values, which will be denoted as <img src="image/C12622_Formula_03_01.png" alt=""/> and <img src="image/C12622_Formula_03_02.png" alt=""/> respectively. With the mean values calculated, we can then calculate the gradient, <em class="italics">m</em>, by multiplying the differences in <em class="italics">x</em> values from the mean with the differences in <em class="italics">y</em> values from the mean and dividing by the squared differences in <em class="italics">x</em> from the mean. The offset can then be calculated by solving for <em class="italics">b</em> using the newly calculated <em class="italics">m</em> and <img src="image/C12622_Formula_03_01.png" alt=""/> and <img src="image/C12622_Formula_03_02.png" alt=""/>. This is represented mathematically as follows:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/C12622_03_08.jpg" alt=""/>
				</div>
			</div>
			<h6>Figure 3.8: Least squares method</h6>
			<p>We can consider this in more practical terms, recalling that the gradient is simply the change in the vertical (or <em class="italics">y</em>) values divided by the horizontal (or <em class="italics">x</em>) values. In the context of mean annual air temperature over time, we can see that we are taking the sum of the differences in the individual temperature values from the mean value multiplied by the individual differences in the time values from the mean. By dividing the result by the sum of the squared differences in time from the mean, the trending gradient is completed, providing part of the temperature over time model.</p>
			<p>Now, we don't need to worry about computing these values by hand, though it wouldn't be that hard to do. But specialized libraries such as SciPy and scikit-learn can be used do to the work for us as well as worrying about some of the details such as computational efficiency. For the purposes of this section, we will use scikit-learn as our library of choice as it provides a great introduction to the scikit-learn interface. </p>
			<p>One implementation detail to note is that the scikit-learn linear regression model is actually a wrapper around the SciPy ordinary least squares function and provides some additional convenience methods:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/C12622_03_09.jpg" alt="Figure 3.9: scikit-learn’s implementation of linear regression"/>
				</div>
			</div>
			<h6>Figure 3.9: scikit-learn's implementation of linear regression</h6>
			<h3 id="_idParaDest-89"><a id="_idTextAnchor097"/>The scikit-learn Model API</h3>
			<p>The scikit-learn API uses a reasonably simple code pattern irrespective of the type of model being constructed. Put simply, the model must first be defined with all appropriate hyperparameters that are relevant to the training or fitting process. In defining the model, a model object is returned, which is then used during the second stage of model construction, which is training or fitting. Calling the <strong class="inline">fit</strong> method on the model object with the appropriate training data will then train the model with the defined hyperparameters. We will now use this pattern to construct our first linear regression model.</p>
			<h3 id="_idParaDest-90"><a id="_idTextAnchor098"/>Exercise 29: Fitting a Linear Model Using the Least Squares Method</h3>
			<p>In this exercise, we will construct our first linear regression model using the least squares method.</p>
			<ol>
				<li value="1">We will use the scikit-learn <strong class="inline">LinearRegression</strong> model for this exercise, so import the class from the <strong class="inline">linear_regression</strong> module of scikit-learn:<p class="snippet">from sklearn.linear_model import LinearRegression</p></li>
				<li>Construct a linear regression model using the default values; that is, compute a value for the <em class="italics">y</em> intercept and do not normalize the input data:<p class="snippet">model = LinearRegression()</p><p class="snippet">model</p><div id="_idContainer135" class="IMG---Figure"><img src="image/C12622_03_10.jpg" alt="Figure 3.10: Linear regression model"/></div><h6>Figure 3.10: Linear regression model</h6></li>
				<li>Now we are ready to fit or train the model to the data. We will provide the year values as the input and the mean yearly temperature as the output. Note that the <strong class="inline">fit</strong> method of scikit-learn models expects 2D arrays to be provided as the <strong class="inline">X</strong> and <strong class="inline">Y</strong> value. As such, the year or index values need to be reshaped to suit the method. Get the values of the index as a NumPy array using the <strong class="inline">.values</strong> method and reshape the values to <strong class="inline">((-1, 1))</strong> which is an <em class="italics">N x 1</em> array. The value <strong class="inline">-1</strong> in a NumPy shape definition represents that its value is inferred from the current shape of the array and the target shape:<p class="snippet">model.fit(df_group_year.index.values.reshape((-1, 1)), gf_group_year.AverageTemperature)</p><p>The output will be as follows:</p><div id="_idContainer136" class="IMG---Figure"><img src="image/C12622_03_11.jpg" alt="Figure 3.11: Output of the fit method"/></div><h6>Figure 3.11: Output of the fit method</h6></li>
				<li>Get the parameters for the model by printing the values for <strong class="inline">model.coef_</strong> (which is the value for <em class="italics">m</em>) and <strong class="inline">model.intercept_</strong> (which is the value for the <em class="italics">y</em> intercept):<p class="snippet">print(f'm = {model.coef_[0]}')</p><p class="snippet">print(f'c = {model.intercept_}')</p><p class="snippet">print('\nModel Definition')</p><p class="snippet">print(f'y = {model.coef_[0]:0.4}x + {model.intercept_:0.4f}')</p><p>The output will be:</p><div id="_idContainer137" class="IMG---Figure"><img src="image/C12622_03_12.jpg" alt="Figure 3.12: Output of model co-efficient and model intercept"/></div><h6>Figure 3.12: Output of model co-efficient and model intercept</h6></li>
				<li>Now that we have our generated model, we can predict some values to construct our trendline. So, let's use the first, last, and average year value as the input to predict the local temperature. Construct a NumPy array with these values and call the array <strong class="inline">trend_x</strong>. Once you are done, pass the values for <strong class="inline">trend_x</strong> to the <strong class="inline">predict</strong> method of the model to get the predicted values:<p class="snippet">trend_x = np.array([</p><p class="snippet">    df_group_year.index.values.min(),</p><p class="snippet">    df_group_year.index.values.mean(),</p><p class="snippet">    df_group_year.index.values.max()</p><p class="snippet">])</p><p class="snippet">trend_y = model.predict(trend_x.reshape((-1, 1)))</p><p class="snippet">trend_y</p><p>The output will be as follows:</p><div id="_idContainer138" class="IMG---Figure"><img src="image/C12622_03_13.jpg" alt="Figure 3.13: Array showing min, mean, and max"/></div><h6>Figure 3.13: Array showing min, mean, and max</h6></li>
				<li>Now plot the trendline produced by the model, with the model parameters over the previous plot with the raw data:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_group_year.index, df_group_year.AverageTemperature, label='Raw Data', c='k');</p><p class="snippet">ax.plot(df_group_year.index, rolling, c='k', linestyle='--', label=f'{window} year moving average');</p><p class="snippet">ax.plot(trend_x, trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Mean Air Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Year')</p><p class="snippet">ax.set_ylabel('Temperature (degC)')</p><p class="snippet">ax.set_xticks(range(df_group_year.index.min(), df_group_year.index.max(), 10))</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/C12622_03_14.jpg" alt="Figure 3.14: Linear regression – a first simple linear model"/>
				</div>
			</div>
			<h6>Figure 3.14: Linear regression – a first simple linear model</h6>
			<p>Now that we have the model, we need to evaluate its performance to see how well it fits the data and to compare against other models we may like to generate. We will cover this topic in much more detail in <em class="italics">Chapter 6</em>, <em class="italics">Model Evaluation</em>, where we'll look at validation and cross validation, but for the moment we will compute the <strong class="keyword">R-squared</strong> value for the model against the dataset. R-squared, which is commonly reported in statistical-based modeling, is a ratio of the sum of squares between the predicted and actual values and the actual value from its own mean. A perfect fit will have an r2 of 1, and the score decreases to 0 as the performance degrades.</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/C12622_03_15.jpg" alt="Figure 3.15: R-squared score"/>
				</div>
			</div>
			<h6>Figure 3.15: R-squared score</h6>
			<p>We can compute the R2 value using the <strong class="inline">score</strong> method:</p>
			<p class="snippet"># Note the year values need to be provided as an N x 1 array</p>
			<p class="snippet">r2 = model.score(df_group_year.index.values.reshape((-1, 1)), df_group_year.AverageTemperature)</p>
			<p class="snippet">print(f'r2 score = {r2:0.4f}')</p>
			<p>We'll get an output like this:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/C12622_03_16.jpg" alt=""/>
				</div>
			</div>
			<h6>Figure 3.16: R-squared score for the model against the dataset</h6>
			<p>So, looking at the trendline in <em class="italics">Figure 3.14</em>, we can see that the linear model is OK. It definitely performs better in the linear region of the moving average post 1960, but could use some improvement for the data earlier than 1970. Is there something we can do to manage this? It seems something that two separate linear models could perform better than one. The data prior to 1960 could form one model and the post-1960 data another? We could do that and just split the data and create two separate models, evaluate them separately, and put them together in a piece-wise fashion. But we could also include similar features in our existing model through the use of dummy variables.</p>
			<h4>Note</h4>
			<p class="callout">Before continuing, it is important to note that when reporting the performance of machine learning models, the data used to train the model is <em class="italics">not</em> to be used to evaluate the model, as it will give an optimistic view of the model's performance. We will cover the concept of validation, which includes evaluating and reporting model performance, in <em class="italics">Chapter 6</em>, <em class="italics">Model Evaluation</em>. For the purpose of this chapter, however, we will use the training data to check the model's performance; just remember that once you have completed <em class="italics">Chapter 6</em>, <em class="italics">Model Evaluation</em>, you will know better.</p>
			<h3 id="_idParaDest-91"><a id="_idTextAnchor099"/>Activity 6: Linear Regression Using the Least Squares Method</h3>
			<p>For this activity, we will use the Austin, Texas weather dataset that we used in the previous activity. We will plot a linear regression model using the least squares method for the dataset.</p>
			<p>Before we begin, we will need to import a few libraries and load data from a previous activity, which can be done as follows:</p>
			<p class="snippet">import numpy as np</p>
			<p class="snippet">import pandas as pd</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p class="snippet">from sklearn.linear_model import LinearRegression</p>
			<p class="snippet"># Loading the data from activity 5</p>
			<p class="snippet">df = pd.read_csv('activity2_measurements.csv')</p>
			<p class="snippet">df_first_year = df[:365]</p>
			<p class="snippet">rolling = pd.read_csv('activity2_rolling.csv')</p>
			<p class="snippet">window = 20</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Visualize the measurements.</li>
				<li>Visualize the rolling average values.</li>
				<li>Create a linear regression model using the default parameters, that is, calculate a <em class="italics">y</em> intercept for the model and do not normalize the data.</li>
				<li>Now fit the model, where the input data is the day number for the year (1 to 365) and the output is the average temperature. To make later calculations easier, insert a column (<strong class="inline">DayOfYear</strong>) that corresponds with the day of the year for that measurement.</li>
				<li>Fit the model with the <strong class="inline">DayOfYear</strong> values as the input and <strong class="inline">df_first_year.TempAvgF</strong> as the output.</li>
				<li>Print the parameters of the model.</li>
				<li>Let's check the trendline provided by the model. Plot this simply using the first, middle, and last values (days in years) in the linear equation.</li>
				<li>Plot the values with the trendline.</li>
				<li>Evaluate the performance of the model.</li>
				<li>Let's check how well the model fits the data. Calculate the r2 score to find out.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 329.</p></li>
			</ol>
			<h3 id="_idParaDest-92"><a id="_idTextAnchor100"/>Linear Regression with Dummy Variables</h3>
			<p>Dummy variables are categorical variables that we can introduce into a model, using information provided within the existing dataset. The design and selection of these variables is considered a component of feature engineering, and depending upon the choice of variables, the results may vary. We made the observation earlier that the moving average begins to continually increase from approximately 1960 and that the initial plateau ends at approximately 1945. We will introduce two dummy variables, <strong class="inline">Gt_1960</strong> and <strong class="inline">Gt_1945</strong>; these variables will indicate whether the time period for the measurements is greater than the year 1960 and greater than the year 1945. Dummy variables are typically assigned the values 0 or 1 to indicate the lack of or presence of the assigned category for each row. In our example, given the magnitude of the <strong class="inline">Year</strong> values, we will need to increase the positive value of the dummy variables as, with a value of 1, they will have little to no effect given that the values for <strong class="inline">Year</strong> are in the thousands. Throughout the following exercise, we will demonstrate that regression models can be composed of both discrete and continuous values and that, depending on an appropriate choice of dummy variables, performance can be improved.</p>
			<h3 id="_idParaDest-93"><a id="_idTextAnchor101"/>Exercise 30: Introducing Dummy Variables</h3>
			<p>In this exercise, we will introduce two dummy variables into our linear regression model:</p>
			<ol>
				<li value="1">For convenience, assign the index values of the <strong class="inline">df_group_year</strong> DataFrame to the <strong class="inline">Year</strong> column:<p class="snippet">df_group_year['Year'] = df_group_year.index</p></li>
				<li>Create a dummy variable with a column labeled <strong class="inline">Gt_1960</strong>, where the value is <strong class="inline">0</strong> if the year is less than 1960 and <strong class="inline">10</strong> if greater:<p class="snippet">df_group_year['Gt_1960'] = [0 if year &lt; 1960 else 10 for year in df_group_year.Year] # Dummy Variable - greater than 1960</p><p class="snippet">df_group_year.head(n=2)</p><p>The output will be as follows:</p><div id="_idContainer142" class="IMG---Figure"><img src="image/C12622_03_17.jpg" alt="Figure 3.17: Added column Gt_1960"/></div><h6>Figure 3.17: Added column Gt_1960</h6></li>
				<li>Create a dummy variable with a column labeled <strong class="inline">Gt_1945</strong>, where the value is <strong class="inline">0</strong> if the year is less than 1945 and <strong class="inline">10</strong> if greater:<p class="snippet">df_group_year['Gt_1945'] = [0 if year &lt; 1945 else 10 for year in df_group_year.Year]# Dummy Variable - greater than 1945</p><p class="snippet">df_group_year.head(n=2)</p><p>The output will be:</p><div id="_idContainer143" class="IMG---Figure"><img src="image/C12622_03_18.jpg" alt="Figure 3.18: Added column Gt_1945"/></div><h6>Figure 3.18: Added column Gt_1945</h6></li>
				<li>Call the <strong class="inline">tail()</strong> method to look at the last two rows of the <strong class="inline">df_group_year</strong> DataFrame to confirm that the post 1960 and 1945 labels have been correctly assigned:<p class="snippet">df_group_year.tail(n=2)</p><p>The output will be:</p><div id="_idContainer144" class="IMG---Figure"><img src="image/C12622_03_19.jpg" alt="Figure 3.19: Last two values"/></div><h6>Figure 3.19: Last two values</h6></li>
				<li>Fit the linear model with the additional dummy variables by passing the <strong class="inline">Year</strong>, <strong class="inline">Gt_1960</strong>, and <strong class="inline">Gt_1945</strong> columns as inputs to the model, with <strong class="inline">AverageTemperature</strong> again as the output:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">model.fit(df_group_year[['Year', 'Gt_1960', 'Gt_1945']], df_group_year.AverageTemperature)</p><p>The output will be:</p><div id="_idContainer145" class="IMG---Figure"><img src="image/C12622_03_20.jpg" alt="Figure 3.20: Linear model fitted on data"/></div><h6>Figure 3.20: Linear model fitted on data</h6></li>
				<li>Check the R-squared score for the new model against the training data to see whether we made an improvement:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">r2 = model.score(df_group_year[['Year', 'Gt_1960', 'Gt_1945']], df_group_year.AverageTemperature)</p><p class="snippet">print(f'r2 score = {r2:0.4f}')</p><p>The output will be as follows:</p><div id="_idContainer146" class="IMG---Figure"><img src="image/C12622_03_21.jpg" alt="Figure 3.21: R-squared score for the model"/></div><h6>Figure 3.21: R-squared score for the model</h6></li>
				<li>We have made an improvement! This is a reasonable step in accuracy given that the first model's performance was 0.8618. We will plot another trendline, but we will need more values than before to accommodate the additional complexity of the dummy variables. Use <strong class="inline">linspace</strong> to create 20 linearly spaced values between 1902 and 2013:<p class="snippet"># Use linspace to get a range of values, in 20 year increments</p><p class="snippet">x = np.linspace(df_group_year['Year'].min(), df_group_year['Year'].max(), 20)</p><p class="snippet">x</p><p>We'll get this output:</p><div id="_idContainer147" class="IMG---Figure"><img src="image/C12622_03_22.jpg" alt="Figure 3.22: Array of 20 years created using linspace"/></div><h6>Figure 3.22: Array of 20 years created using linspace</h6></li>
				<li>Create an array of zeros in the shape <em class="italics">20 x 3</em> and fill the first column of values with <strong class="inline">x</strong>, the second column with the dummy variable value for greater than 1960, and the third column with the dummy variable value for greater than 1945:<p class="snippet">trend_x = np.zeros((20, 3))</p><p class="snippet">trend_x[:,0] = x # Assign to the first column</p><p class="snippet">trend_x[:,1] = [10 if _x &gt; 1960 else 0 for _x in x] # Assign to the second column</p><p class="snippet">trend_x[:,2] = [10 if _x &gt; 1945 else 0 for _x in x] # Assign to the third column </p><p class="snippet">trend_x</p><p>The output will be:</p><div id="_idContainer148" class="IMG---Figure"><img src="image/C12622_03_23.jpg" alt="Figure 3.23: Finding trend_x"/></div><h6>Figure 3.23: Finding trend_x</h6></li>
				<li>Now get the <em class="italics">y</em> values for the trendline by making predictions for <strong class="inline">trend_x</strong>:<p class="snippet">trend_y = model.predict(trend_x)</p><p class="snippet">trend_y</p><p>The output will be as follows:</p><div id="_idContainer149" class="IMG---Figure"><img src="image/C12622_03_24.jpg" alt="Figure 3.24: Finding trend_y"/></div><h6>Figure 3.24: Finding trend_y</h6></li>
				<li>Plot the trendline:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_group_year.index, df_group_year.AverageTemperature, label='Raw Data', c='k');</p><p class="snippet">ax.plot(df_group_year.index, rolling, c='k', linestyle='--', label=f'{window} year moving average');</p><p class="snippet">ax.plot(trend_x[:,0], trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Mean Air Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Year')</p><p class="snippet">ax.set_ylabel('Temperature (degC)')</p><p class="snippet">ax.set_xticks(range(df_group_year.index.min(), df_group_year.index.max(), 10))</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/C12622_03_25.jpg" alt="Figure 3.25: Predictions using dummy variables"/>
				</div>
			</div>
			<h6>Figure 3.25: Predictions using dummy variables</h6>
			<p>Incorporating dummy variables made quite an improvement to the model, but looking at the trendline, it doesn't seem like a reasonable path for natural phenomena such as temperature to take and could be suffering from overfitting. We will cover overfitting in more detail in <em class="italics">Chapter 5</em>, <em class="italics">Ensemble Modeling</em>; however, let's use linear regression to fit a model with a smoother prediction curve, such as a parabola.</p>
			<h3 id="_idParaDest-94"><a id="_idTextAnchor102"/>Activity 7: Dummy Variables</h3>
			<p>For this activity, we will use the Austin, Texas weather dataset that we used in the previous activity. In this activity, we will use dummy variables to enhance our linear regression model for this dataset.</p>
			<p>Before we begin, we will need to import a few libraries and load data from a previous activity, which can be done as follows:</p>
			<p class="snippet">import numpy as np</p>
			<p class="snippet">import pandas as pd</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p class="snippet">from sklearn.linear_model import LinearRegression</p>
			<p class="snippet"># Loading the data from activity 5</p>
			<p class="snippet">df = pd.read_csv('activity2_measurements.csv')</p>
			<p class="snippet">df_first_year = pd.read_csv('activity_first_year.csv')</p>
			<p class="snippet">rolling = pd.read_csv('activity2_rolling.csv')</p>
			<p class="snippet">window = 20</p>
			<p class="snippet"># Trendline values</p>
			<p class="snippet">trend_x = np.array([</p>
			<p class="snippet">    1,</p>
			<p class="snippet">    182.5,</p>
			<p class="snippet">    365</p>
			<p class="snippet">])</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Plot the raw data (<strong class="inline">df</strong>) and moving average (<strong class="inline">rolling</strong>).</li>
				<li>Looking at the result of the previous step, there seems to be an inflection point around day 250. Create a dummy variable to introduce this feature into the linear model.</li>
				<li>Check the first and last samples to confirm that the dummy variable is correct.</li>
				<li>Use a least squares linear regression model and fit the model to the <strong class="inline">DayOfYear</strong> values and the dummy variable to predict <strong class="inline">TempAvgF</strong>.</li>
				<li>Compute the R2 score.</li>
				<li>Using the <strong class="inline">DayOfYear</strong> values, create a set of predictions using the model to construct a trendline.</li>
				<li>Plot the trendline against the data and moving average.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 334.</p></li>
			</ol>
			<h3 id="_idParaDest-95"><a id="_idTextAnchor103"/>Parabolic Model with Linear Regression</h3>
			<p>Linear regression models are not simply constrained to straight-line linear models. We can fit some more complicated models using the exact same techniques. We have mentioned that there seems to be some parabolic characteristics to the data, so let's try fitting a parabolic model. As a reminder, the equation for a parabola is:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/C12622_03_26.jpg" alt=""/>
				</div>
			</div>
			<h6>Figure 3.26: Equation for a parabola</h6>
			<p>The addition of this squared term transforms the model from a straight line to one that has a parabolic (or arc like) trajectory.</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/C12622_03_27.jpg" alt="Figure 3.27: Parabolic curve"/>
				</div>
			</div>
			<h6>Figure 3.27: Parabolic curve</h6>
			<h3 id="_idParaDest-96"><a id="_idTextAnchor104"/>Exercise 31: Parabolic Models with Linear Regression</h3>
			<p>In order to fit a parabolic model using linear regression, we just need to manipulate our inputs a little. In this exercise, we'll see how to do it:</p>
			<ol>
				<li value="1">The first thing we need to do is provide a squared term for year values. For convenience, create a copy of the index and store it in a <strong class="inline">Year</strong> column. Now square the <strong class="inline">Year</strong> column to provide parabolic features and assign the result to the <strong class="inline">Year2</strong> column:<p class="snippet">df_group_year['Year'] = df_group_year.index</p><p class="snippet">df_group_year['Year2'] = df_group_year.index ** 2</p><p class="snippet">df_group_year.head()</p><p>We'll get this:</p><div id="_idContainer153" class="IMG---Figure"><img src="image/C12622_03_28.jpg" alt="Figure 3.28: First five rows"/></div><h6>Figure 3.28: First five rows</h6></li>
				<li>Fit the data to the model. This time, we will need to provide two sets of values as the inputs to the model, <strong class="inline">Year</strong> and <strong class="inline">Year2</strong>, which is equivalent to passing <em class="italics">x</em> and <em class="italics">x</em><em class="italics">2</em> to the parabolic equation. As we are providing two columns of data, we do not need to reshape the input data as it will be provided as an <em class="italics">N x 2</em> array by default. The target <em class="italics">y</em> value remains the same:<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">model.fit(df_group_year[['Year2', 'Year']], df_group_year.AverageTemperature)</p><p>The output will be as follows:</p><div id="_idContainer154" class="IMG---Figure"><img src="image/C12622_03_29.jpg" alt="Figure 3.29: Model fitted"/></div><h6>Figure 3.29: Model fitted</h6></li>
				<li>Print the parameters of the model by looking at the coefficients and the intercept; there will now be two coefficients to print:<p class="snippet">print(f'a = {model.coef_[0]}')</p><p class="snippet">print(f'm = {model.coef_[1]}')</p><p class="snippet">print(f'c = {model.intercept_}')</p><p class="snippet">print('\nModel Definition')</p><p class="snippet">print(f'y = {model.coef_[0]:0.4}x^2 + {model.coef_[1]:0.4}x + {model.intercept_:0.4f}')</p><p>The output will be:</p><div id="_idContainer155" class="IMG---Figure"><img src="image/C12622_03_30.jpg" alt="Figure 3.30: Model coefficients and intercept"/></div><h6>Figure 3.30: Model coefficients and intercept</h6></li>
				<li>Evaluate the performance of the model using the <strong class="inline">score</strong> method. Has the performance improved?<p class="snippet"># Note the year values need to be provided as an N x 1 array</p><p class="snippet">r2 = model.score(df_group_year[['Year2', 'Year']], df_group_year.AverageTemperature)</p><p class="snippet">print(f'r2 score = {r2:0.4f}')</p><p>We'll get the following output:</p><div id="_idContainer156" class="IMG---Figure"><img src="image/C12622_03_31.jpg" alt="Figure 3.31: R-squared score"/></div><h6>Figure 3.31: R-squared score</h6></li>
				<li>Yes, the model has improved slightly on the dummy variable method, but let's look at the trendline to see whether it is a more reasonable fit. Plot the trendline as we did before. Again, to effectively plot the parabolic arc of the trendline, we will need more predicted values. Use <strong class="inline">linspace</strong> to create 20 linearly spaced values between 1902 and 2013:<p class="snippet"># Use linspace to get a range of values, in 20 yr increments</p><p class="snippet">x = np.linspace(df_group_year['Year'].min(), df_group_year['Year'].max(), 20)</p><p class="snippet">x</p><p>We'll get this:</p><div id="_idContainer157" class="IMG---Figure"><img src="image/C12622_03_32.jpg" alt="Figure 3.32: Finding 20 increments using linspace"/></div><h6>Figure 3.32: Finding 20 increments using linspace</h6></li>
				<li>Now the model we trained takes two columns of year data as an input: the first column containing squared yearly values and the second just the year value itself. To provide the data to the model, create a NumPy array (<strong class="inline">trend_x</strong>) of zeros with 20 rows and 2 columns. Square the values for <strong class="inline">x</strong> and assign to the first column of <strong class="inline">trend_x</strong>, and simply assign <strong class="inline">x</strong> to the second column of <strong class="inline">trend_x</strong>:<p class="snippet">trend_x = np.zeros((20, 2))</p><p class="snippet">trend_x[:,0] = x ** 2 # Assign to the first column</p><p class="snippet">trend_x[:,1] = x # Assign to the second column </p><p class="snippet">trend_x</p><p>The output will be:</p><div id="_idContainer158" class="IMG---Figure"><img src="image/C12622_03_33.jpg" alt="Figure 3.33: Trends for the x variable"/></div><h6>Figure 3.33: Trends for the x variable</h6></li>
				<li>Now get the <em class="italics">y</em> values for the trendline by making predictions for <strong class="inline">trend_x</strong>:<p class="snippet">trend_y = model.predict(trend_x)</p><p class="snippet">trend_y</p><p>We'll get this:</p><div id="_idContainer159" class="IMG---Figure"><img src="image/C12622_03_34.jpg" alt="Figure 3.34: Trends for the y variable"/></div><h6>Figure 3.34: Trends for the y variable</h6></li>
				<li>Plot the trendline as per the straight-line model. Remember that the <em class="italics">x</em> axis values for <strong class="inline">trend_y</strong> are the years; that is, the second column of <strong class="inline">trend_x</strong>, and not the years squared:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_group_year.index, df_group_year.AverageTemperature, label='Raw Data', c='k');</p><p class="snippet">ax.plot(df_group_year.index, rolling, c='k', linestyle='--', label=f'{window} year moving average');</p><p class="snippet">ax.plot(trend_x[:,1], trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Mean Air Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Year')</p><p class="snippet">ax.set_ylabel('Temperature (degC)')</p><p class="snippet">ax.set_xticks(range(df_group_year.index.min(), df_group_year.index.max(), 10))</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/C12622_03_35.jpg" alt="Figure 3.35: Linear regression with a parabolic model"/>
				</div>
			</div>
			<h6>Figure 3.35: Linear regression with a parabolic model</h6>
			<p>Referring to <em class="italics">Figure 3.35</em>, we can see the performance benefit in using the parabolic model, with the trendline almost following the 10-year moving average. This is a reasonably good fit given the amount of noise in the yearly average raw data. In such a case, it should not be expected that the model will fit the data perfectly. If our model was to perfectly fit the observed examples, there would be a very strong case for overfitting the data, leading to poor predictive power with unseen examples.</p>
			<h3 id="_idParaDest-97"><a id="_idTextAnchor105"/>Activity 8: Other Model Types with Linear Regression</h3>
			<p>We have tried a standard linear model as well as a dummy variable. In this activity, we will experiment with a few different functions to try and get a better fit for the data. For each different function, try to make sure you print the function parameters, R2 value, and plot the trendline against the original and moving average data.</p>
			<p>Try a few different functions, experiment with the data, and see how good your predictions can get. In this activity, we will use the sine function.</p>
			<p>Before we begin, we will need to import a few libraries and load data from a previous activity, which can be done as follows:</p>
			<p class="snippet">import numpy as np</p>
			<p class="snippet">import pandas as pd</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p class="snippet">from sklearn.linear_model import LinearRegression</p>
			<p class="snippet"># Loading the data from activity 5</p>
			<p class="snippet">df = pd.read_csv('activity2_measurements.csv')</p>
			<p class="snippet">df_first_year = pd.read_csv('activity_first_year.csv')</p>
			<p class="snippet">rolling = pd.read_csv('activity2_rolling.csv')</p>
			<p class="snippet">window = 20</p>
			<p class="snippet"># Trendline values</p>
			<p class="snippet">trend_x = np.array([</p>
			<p class="snippet">    1,</p>
			<p class="snippet">    182.5,</p>
			<p class="snippet">    365</p>
			<p class="snippet">])</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Use a sine curve function as the basis of the model.</li>
				<li>Print the parameters of the model.</li>
				<li>Compute the r2 value to measure the performance.</li>
				<li>Construct the trendline values.</li>
				<li>Plot the trendline with the raw data and the moving average.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 338.</p></li>
			</ol>
			<h3 id="_idParaDest-98"><a id="_idTextAnchor106"/>Generic Model Training</h3>
			<p>The least squares method of constructing a linear regression model is a useful and accurate method of training, assuming that the dimensionality of the dataset is low and that the system memory is sufficiently large to be able to manage the dataset and, in the case of the scikit-learn implementation, the matrix division operation. In recent times, large datasets have become more readily available, with universities, governments, and even some companies releasing large datasets for free online; as such, it may be relatively easy to exceed system memory when using the least squares method of regression modeling. In this situation, we will need to employ a different method of training the algorithm, such as gradient descent, which is not <em class="italics">as</em> susceptible to high dimensionality, allows large datasets to be trained, and avoids the use of memory intensive matrix operations. Before we look at gradient descent in a little more detail, we will revisit the process of training a model in a more general form, as most training methods, including gradient descent, adhere to this generic process (<em class="italics">Figure 3.36</em>). The training process involves the repeated exposure of the model and its parameters to a set of example training data and passing the predicted values issued by the model to a specified cost or error function. </p>
			<p>The cost function is used to determine how close the model is to its target values and a measure of progress throughout the training process. The final piece of the process is the definition of the training hyperparameters, which, as discussed at the start of this chapter, are the means by which the process of updating the model is regulated:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/C12622_03_36.jpg" alt="Figure 3.36: Generic training process"/>
				</div>
			</div>
			<h6>Figure 3.36: Generic training process</h6>
			<h3 id="_idParaDest-99"><a id="_idTextAnchor107"/>Gradient Descent</h3>
			<p>The process of gradient descent can be summarized as a means of updating the parameters of the model proportionally and in response to an error within the system, as defined by the cost function. There are a number of cost functions that can be selected, depending on the type of model being fitted or the problem being solved. We will select the simple but effective mean squared error cost function, but first we will rewrite our model equation in notation consistent with that generally used within machine learning literature. Using the equation of a straight line as our model:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/C12622_03_37.jpg" alt="Figure 3.37: Equation of a straight line"/>
				</div>
			</div>
			<h6>Figure 3.37: Equation of a straight line</h6>
			<p>It can be rewritten as:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/C12622_03_38.jpg" alt="Figure 3.38: Shortened linear model"/>
				</div>
			</div>
			<h6>Figure 3.38: Shortened linear model</h6>
			<p>Where <img src="image/C12622_Formula_03_03.png" alt=""/> is the prediction made by the model and, by convention, <img src="image/C12622_Formula_03_04.png" alt=""/> is used to represent the intercept term. With the new model notation, we can define the mean squared error function as follows:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/C12622_03_39.jpg" alt="Figure 3.39: Mean squared error"/>
				</div>
			</div>
			<h6>Figure 3.39: Mean squared error</h6>
			<p>Where <em class="italics">y</em><em class="italics">t</em> is the corresponding ground truth value and <em class="italics">N</em> is the number of training samples.</p>
			<p>With these two functions defined, we can now look at the gradient descent algorithm in greater detail:</p>
			<ol>
				<li value="1">Gradient descent starts by taking an initial, random guess at the values for all <img src="image/C12622_Formula_03_05.png" alt=""/>.</li>
				<li>A prediction for each of the samples in the training set is made using the random values for <img src="image/C12622_Formula_03_05.png" alt=""/>.</li>
				<li>The error for those parameters <img src="image/C12622_Formula_03_06.png" alt=""/> is then computed.</li>
				<li>The values for <img src="image/C12622_Formula_03_05.png" alt=""/> are then modified, making a small adjustment proportional to the error, in an attempt to minimize the error. More formally, the update process takes the current value for <img src="image/C12622_Formula_03_07.png" alt=""/> and subtracts the component of <img src="image/C12622_Formula_03_06.png" alt=""/> attributed to <img src="image/C12622_Formula_03_07.png" alt=""/> times the small adjustment <img src="image/C12622_Formula_03_08.png" alt=""/>, otherwise known as the learning rate.</li>
			</ol>
			<p>Without delving too deeply into the mathematical details, the equation to update the parameters or weights (<img src="image/C12622_Formula_03_09.png" alt=""/>) can be written as follows:</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/C12622_03_40.jpg" alt="Figure 3.40: Gradient descent update step"/>
				</div>
			</div>
			<h6>Figure 3.40: Gradient descent update step</h6>
			<p>Let's discuss this equation:</p>
			<ul>
				<li>The<strong class="inline">:=</strong> operator denotes variable reassignment or update as a computer programming concept.</li>
				<li>This training process will continue until convergence; that is, until the changes to the weights are so small that there is essentially no change to the parameters, or until we intervene and stop the process, as is the case in cross-validation.</li>
				<li>The value assigned to the learning rate is critical for the training process as it defines how large the changes to the weights are and subsequently how big the steps to take down the error curve are. If the value is too small, the training process may take far too long or may get stuck in areas of local minima of the error curve and not find an optimal global value. Alternatively, if the steps are too large, the training process can become unstable, as they pass over the local and global minima.</li>
			</ul>
			<p>This process is visualized in the following graph:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/C12622_03_41.jpg" alt="Figure 3.41: Gradient descent process"/>
				</div>
			</div>
			<h6>Figure 3.41: Gradient descent process</h6>
			<h4>Note</h4>
			<p class="callout">As a general hint for setting the learning rate, start larger, say around 0.1, and if a solution cannot be found, that is, the error is a <strong class="inline">NaN</strong> (not a number), reduce by a factor of 10. Keep going until the training is continuing and the error is continually reducing. Once you are happy with the model and are almost done, reduce the learning rate by a small amount and let the training run for longer.</p>
			<p>While this process may sound complicated, it isn't anywhere near as scary as it looks. Gradient descent can be summarized by making a one-time only guess at the values for the weights, calculating the error in the guess, making small adjustments to the weights, and continually repeating the process until the error converges at a minimum value. To reinforce our understanding, let's look at a more concrete example. We will use gradient descent to train the original linear regression model we constructed in <em class="italics">Exercise 29: Fitting a Linear Model Using the Least Squares Method</em>, replacing the least squares method with gradient descent.</p>
			<h3 id="_idParaDest-100"><a id="_idTextAnchor108"/>Exercise 32: Linear Regression with Gradient Descent</h3>
			<p>Before we can start the gradient descent process, we need to spend a little bit of time setting up the model. In our Jupyter notebook, perform these steps:</p>
			<ol>
				<li value="1">Write a function to define our linear model. This is where the advantage of using the shortened form of the linear model (<em class="italics">Figure 3.38</em>) comes in handy. We can use linear algebra multiplication between the weights (theta) and the input values, <em class="italics">x</em>, which is the equivalent of <img src="image/C12622_Formula_03_10.png" alt=""/>:<p class="snippet">def h_x(weights, x):</p><p class="snippet">    return np.dot(weights, x).flatten()</p></li>
				<li>In order to use this linear algebra multiplication technique, we must modify the input data by inserting a row of ones to represent the bias term. Create an array of ones with a shape of two columns (one for the gradient term of the weights and one for the bias term). Insert the normalized <strong class="inline">Year</strong> values into the first row of the newly created array.<p>To use the input data in a gradient descent process, we must also normalize all of the values to be between 0 and 1. This is a critical aspect of the process, as if one variable has values in the order of, say 1,000, and the second in the order of 10, then the first variable will be 100 times more influential in the training process and could lead to the inability to train the model. By ensuring that all variables are scaled between 0 and 1, they will have equal influence during training. Scale the input by dividing the values for <strong class="inline">Year</strong> by the maximum value:</p><p class="snippet">x = np.ones((2, len(df_group_year)))</p><p class="snippet">x[0,:] = df_group_year.Year</p><p class="snippet">x[1,:] = 1</p><p class="snippet">x /= x.max()</p><p class="snippet">x[:,:5]</p><p>You'll get this output:</p><div id="_idContainer179" class="IMG---Figure"><img src="image/C12622_03_42.jpg" alt="Figure 3.42: Modified data"/></div><h6>Figure 3.42: Modified data</h6></li>
				<li>As we have learned, we need to take an initial guess at the values for the weights. We need to define two weight values, one for the gradient and one for the <em class="italics">y</em> intercept. To ensure that the same first random number is initialized each time, seed the NumPy random number generator. Seeding the random number generator ensures that each time the script is run, the same set of random numbers are produced. This ensures the consistency of the same model in multiple runs and provides the opportunity to check the performance of the model against possible changes:<p class="snippet">np.random.seed(255) # Ensure the same starting random values</p></li>
				<li>Initialize the weights with a normally distributed random number with a mean of 0 and standard deviation of 0.1. We want the initialized weights to be random, but still close to zero to give them a chance to find a good solution. In order to execute the matrix multiplication operation in <strong class="inline">h_x</strong>, reshape the random numbers to one row and two columns (one for the gradient and one for the <em class="italics">y</em> intercept):<p class="snippet">Theta = np.random.randn(2).reshape((1, 2)) * 0.1</p><p class="snippet">Theta</p><p>We'll get the following output:</p><div id="_idContainer180" class="IMG---Figure"><img src="image/C12622_03_43.jpg" alt="Figure 3.43: Theta value"/></div><h6>Figure 3.43: Theta value</h6></li>
				<li>Define the ground truth values as the average yearly temperatures:<p class="snippet">y_true = df_group_year.AverageTemperature.values</p></li>
				<li>Define the cost function (mean squared error) as a Python function:<p class="snippet">def J_theta(pred, true):</p><p class="snippet">    return np.mean((pred - true) ** 2) # mean squared error</p></li>
				<li>Define the learning rate as discussed earlier. This is a very important parameter and it must be set appropriately. As mentioned earlier, set it too small and the model may take a very long time to find a minimum; set it too large and it may not reach it at all. Define the learning rate as <strong class="inline">1e-6</strong>:<p class="snippet">gamma = 1e-6</p></li>
				<li>Define a function that implements a step of gradient descent (<em class="italics">Figure 3.40</em>). The function will take the predicted and true values as well as the values for <em class="italics">x</em> and <em class="italics">gamma</em>, and return the value to be added to the weights (theta):<p class="snippet">def update(pred, true, x, gamma):</p><p class="snippet">    return gamma * np.sum((true - pred) * x, axis=1)</p></li>
				<li>Define the maximum number of epochs (or iterations) we want the training process to run for. Each epoch predicts the values of <em class="italics">y</em> (the normalized annual mean land temperature) given <em class="italics">x</em> and updates the weights in accordance with the error in the predictions:<p class="snippet">max_epochs = 100000</p></li>
				<li>Make an initial prediction and calculate the error or cost in that prediction using the defined <strong class="inline">h_x</strong> and <strong class="inline">J_theta</strong> functions:<p class="snippet">y_pred = h_x(Theta, x)</p><p class="snippet">print(f'Initial cost J(Theta) = {J_theta(y_pred, y_true): 0.3f}')</p><p>The output will be as follows:</p><div id="_idContainer181" class="IMG---Figure"><img src="image/C12622_03_44.jpg" alt="Figure 3.44: Initial cost of J theta"/></div><h6>Figure 3.44: Initial cost of J theta</h6></li>
				<li>Complete the first update step by hand. Use the newly predicted values to call the <strong class="inline">update</strong> function, make another call to <strong class="inline">h_x</strong> to get the predicted values, and get the new error:<p class="snippet">Theta += update(y_pred, y_true, x, gamma)</p><p class="snippet">y_pred = h_x(Theta, x)</p><p class="snippet">print(f'Initial cost J(Theta) = {J_theta(y_pred, y_true): 0.3f}')</p><p>We'll get the following output:</p><div id="_idContainer182" class="IMG---Figure"><img src="image/C12622_03_45.jpg" alt="Figure 3.45: Updated cost of J theta"/></div><h6>Figure 3.45: Updated cost of J theta</h6></li>
				<li>Notice the small reduction in the error; as such, many epochs of training will be required. Put the <strong class="inline">predict</strong> and <strong class="inline">update</strong> function calls in a <strong class="inline">for</strong> loop for <strong class="inline">max_epochs</strong> and print the corresponding error at each tenth epoch:<p class="snippet">error_hist = []</p><p class="snippet">epoch_hist = []</p><p class="snippet">for epoch in range(max_epochs):</p><p class="snippet">    Theta += update(y_pred, y_true, x, gamma)</p><p class="snippet">    y_pred = h_x(Theta, x)  </p><p class="snippet">    </p><p class="snippet">    if (epoch % 10) == 0:</p><p class="snippet">        _err = J_theta(y_pred, y_true)</p><p class="snippet">        error_hist.append(_err)</p><p class="snippet">        epoch_hist.append(epoch)</p><p class="snippet">        print(f'epoch:{epoch:4d} J(Theta) = {_err: 9.3f}')</p><p>The output will be as follows:</p><div id="_idContainer183" class="IMG---Figure"><img src="image/C12622_03_46.jpg" alt="Figure 3.46: Ten epochs"/></div><h6>Figure 3.46: Ten epochs</h6></li>
				<li>Visualize the training history by plotting <strong class="inline">epoch_hist</strong> versus <strong class="inline">error_hist</strong>:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(epoch_hist, error_hist);</p><p class="snippet">plt.title('Training History');</p><p class="snippet">plt.xlabel('epoch');</p><p class="snippet">plt.ylabel('Error');</p><p>The output will be:</p><div id="_idContainer184" class="IMG---Figure"><img src="image/C12622_03_47.jpg" alt="Figure 3.47: Training history curve: a very important tool"/></div><h6>Figure 3.47: Training history curve: a very important tool</h6><p>Notice that the error reaches an asymptote at 30,000 epochs, and thus <strong class="inline">max_epochs</strong> could be reduced.</p></li>
				<li>Use the <strong class="inline">r2_score</strong> function from <strong class="inline">sklearn.metrics</strong> to compute the R-squared score for the model trained using gradient descent:<p class="snippet">from sklearn.metrics import r2_score</p><p class="snippet">r2_score(y_true, y_pred)</p><p>We'll get the following output:</p><div id="_idContainer185" class="IMG---Figure"><img src="image/C12622_03_48.jpg" alt="Figure 3.48: R-squared score"/></div><h6>Figure 3.48: R-squared score</h6></li>
				<li>To plot the trendline for the new model, again create 20 linearly spaced year values between 1901 and 2013:<p class="snippet"># Use linspace to get a range of values, in 20 yr increments</p><p class="snippet">x = np.linspace(df_group_year['Year'].min(), df_group_year['Year'].max(), 20)</p><p class="snippet">x</p><p>The output will be as follows:</p><div id="_idContainer186" class="IMG---Figure"><img src="image/C12622_03_49.jpg" alt="Figure 3.49: Values using linspace"/></div><h6>Figure 3.49: Values using linspace</h6></li>
				<li>In order to use this data with our model, we must first normalize the maximum value to scale between 0 and 1 and insert a row of ones. Execute this step in a similar way to when the data was prepared for training in <em class="italics">Step 2</em>.<p class="snippet">trend_x = np.ones((2, len(x)))</p><p class="snippet">trend_x[0,:] = x</p><p class="snippet">trend_x[1,:] = 1</p><p class="snippet">trend_x /= trend_x.max()</p><p class="snippet">trend_x</p><p>The output will be as follows:</p><div id="_idContainer187" class="IMG---Figure"><img src="image/C12622_03_50.jpg" alt="Figure 3.50: Trends in x"/></div><h6>Figure 3.50: Trends in x</h6></li>
				<li>Call the <strong class="inline">h_x</strong> model function with the weights saved from the training process to get predicted <em class="italics">y</em> values for the trendline:<p class="snippet">trend_y = h_x(Theta, trend_x)</p><p class="snippet">trend_y</p><div id="_idContainer188" class="IMG---Figure"><img src="image/C12622_03_51.jpg" alt="Figure 3.51: Trends in y"/></div><h6>Figure 3.51: Trends in y</h6></li>
				<li>Plot the trendline with the data:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_group_year.index, df_group_year.AverageTemperature, label='Raw Data', c='k');</p><p class="snippet">ax.plot(df_group_year.index, rolling, c='k', linestyle='--', label=f'{window} year moving average');</p><p class="snippet">ax.plot(x, trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Mean Air Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Year')</p><p class="snippet">ax.set_ylabel('Temperature (degC)')</p><p class="snippet">ax.set_xticks(range(df_group_year.index.min(), df_group_year.index.max(), 10))</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/C12622_03_52.jpg" alt="Figure 3.52: Mean air temperature measurements using gradient descent"/>
				</div>
			</div>
			<h6>Figure 3.52: Mean air temperature measurements using gradient descent</h6>
			<p>Congratulations! You have just trained your first model with gradient descent. This is an important step as this simple tool can be used to construct more complicated models such as logistic regression and neural network models. We must first, however, note one important observation: the r-squared value produced by the gradient descent model is not as high as the least squares model.</p>
			<p>In the first step of gradient descent, we guess some plausible values for the weights, and then make small adjustments to the weights in an attempt to reduce the error and stop training only when the error stops reducing. Gradient descent finds its power in two specific applications:</p>
			<ul>
				<li>Solving more complicated models for which a mathematically optimal solution has yet to be or cannot be found</li>
				<li>Providing a means of training with datasets or parameters that are so large that physical hardware restrictions, such as available memory, prevent the use of other methods such as least squares</li>
			</ul>
			<p>So, if the dataset is not excessively large and can be solved optimally, we should definitely use the more precise method. That being said, there are many more options available to modify the gradient descent process, including different types of gradient descent algorithms and more advanced uses of learning rate and the way the data is supplied during training. These modifications fall outside the scope of this book, as an entire book could be written on the gradient descent process and methods for improving performance.</p>
			<h3 id="_idParaDest-101"><a id="_idTextAnchor109"/>Exercise 33: Optimizing Gradient Descent</h3>
			<p>In the previous exercise, we implemented gradient descent directly; however, we would not typically use this implementation. The scikit-learn method of gradient descent contains a number of optimizations and can be used in only a few lines of code:</p>
			<ol>
				<li value="1">Import the <strong class="inline">SGDRegressor</strong> class and construct a model using the same parameters as used in the previous exercise:<p class="snippet">from sklearn.linear_model import SGDRegressor</p><p class="snippet">model = SGDRegressor(</p><p class="snippet">    max_iter=100000,</p><p class="snippet">    learning_rate='constant',</p><p class="snippet">    eta0=1e-6,</p><p class="snippet">    random_state=255,</p><p class="snippet">    tol=1e-6,</p><p class="snippet">    penalty='none',</p><p class="snippet">)</p></li>
				<li>Use the year values, divided by the maximum year value, as an input and fit with the <strong class="inline">AverageTemperature</strong> values as the ground truth:<p class="snippet">x = df_group_year.Year / df_group_year.Year.max()</p><p class="snippet">y_true = df_group_year.AverageTemperature.values.ravel()</p><p class="snippet">model.fit(x.values.reshape((-1, 1)), y_true)</p></li>
				<li>Predict the values using the trained model and determine the r-squared value:<p class="snippet">y_pred = model.predict(x.values.reshape((-1, 1)))</p><p class="snippet">r2_score(y_true, y_pred)</p></li>
				<li>Plot the trendline as determined by the model in addition to the raw data and the moving average:<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">ax = fig.add_axes([1, 1, 1, 1]);</p><p class="snippet"># Temp measurements</p><p class="snippet">ax.scatter(df_group_year.index, df_group_year.AverageTemperature, label='Raw Data', c='k');</p><p class="snippet">ax.plot(df_group_year.index, rolling, c='k', linestyle='--', label=f'{window} year moving average');</p><p class="snippet">ax.plot(x, trend_y, c='k', label='Model: Predicted trendline')</p><p class="snippet">ax.set_title('Mean Air Temperature Measurements')</p><p class="snippet">ax.set_xlabel('Year')</p><p class="snippet">ax.set_ylabel('Temperature (degC)')</p><p class="snippet">ax.set_xticks(range(df_group_year.index.min(), df_group_year.index.max(), 10))</p><p class="snippet">ax.legend();</p><p>The output will be as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/C12622_03_53.jpg" alt="Figure 3.53: Optimized gradient descent predicted trendline"/>
				</div>
			</div>
			<h6>Figure 3.53: Optimized gradient descent predicted trendline</h6>
			<p>Compare this graph to the one constructed using the manual implementation of gradient descent. Notice the similarities: this provides us with confidence that both implementations of gradient descent are correct.</p>
			<h3 id="_idParaDest-102"><a id="_idTextAnchor110"/>Activity 9: Gradient Descent</h3>
			<p>In this activity, we will implement the same model as <em class="italics">Activity 6</em>, <em class="italics">Linear Regression Using the Least Squares Method</em>; however, we will use the gradient descent process.</p>
			<p>Before we begin, we will need to import a few libraries and load data from a previous activity, which can be done as follows:</p>
			<p class="snippet">import numpy as np</p>
			<p class="snippet">import pandas as pd</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p class="snippet">from sklearn.linear_model import SGDRegressor</p>
			<p class="snippet"># Loading the data from activity 5</p>
			<p class="snippet">df = pd.read_csv('activity2_measurements.csv')</p>
			<p class="snippet">df_first_year = pd.read_csv('activity_first_year.csv')</p>
			<p class="snippet">rolling = pd.read_csv('activity2_rolling.csv')</p>
			<p class="snippet">window = 20</p>
			<p class="snippet"># Trendline values</p>
			<p class="snippet">trend_x = np.array([</p>
			<p class="snippet">    1,</p>
			<p class="snippet">    182.5,</p>
			<p class="snippet">    365</p>
			<p class="snippet">])</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Create a generic gradient descent model and normalize the day of year values to be between 0 and 1.</li>
				<li>Fit the model.</li>
				<li>Print the details of the model.</li>
				<li>Prepare the <em class="italics">x</em> <strong class="inline">(trend_x</strong>) trendline values by dividing by the maximum. Predict <strong class="inline">y_trend_values</strong> using the gradient descent model.</li>
				<li>Plot the data and the moving average with the trendline.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 341.</p></li>
			</ol>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor111"/>Multiple Linear Regression</h2>
			<p>We have already covered regular linear regression, as well as linear regression with polynomial terms, and considered training them with both the least squares method and gradient descent. This section of the chapter will consider an additional type of linear regression: multiple linear regression, where more than one type of variable (or feature) is used to construct the model. To examine multiple linear regression, we will use a modified version of the Boston Housing Dataset, available from <a href="B13323_03_ePub_Final_NT.xhtml#_idTextAnchor114">https://archive.ics.uci.edu/ml/index.php</a>. The modified dataset can be found in the accompanying source code or on GitHub at <a href="B13323_03_ePub_Final_NT.xhtml#_idTextAnchor115">https://github.com/TrainingByPackt/Supervised-Learning-with-Python</a> and has been reformatted for simplified use. This dataset contains a list of different attributes for property in the Boston area, including the crime rate per capita by town, the percentage of the population with a lower socio-economic status, as well as the average number of rooms per dwelling, and the median value of owner-occupied homes in the area.</p>
			<h3 id="_idParaDest-104"><a id="_idTextAnchor112"/>Exercise 34: Multiple Linear Regression</h3>
			<p>We will use the Boston Housing Dataset to construct a multiple linear model that predicts the median value of owner-occupied homes given the percentage of the population with a lower socio-economic status and the average number of rooms per dwelling:</p>
			<ol>
				<li value="1">Import the required dependencies:<p class="snippet">import numpy as np</p><p class="snippet">import pandas as pd</p><p class="snippet">import matplotlib.pyplot as plt</p><p class="snippet">from sklearn.linear_model import LinearRegression</p></li>
				<li>Read in the housing database:<p class="snippet">df = pd.read_csv('housing_data.csv')</p><p class="snippet">df.head()</p><p>The <strong class="inline">head()</strong> function will return the following output:</p><div id="_idContainer191" class="IMG---Figure"><img src="image/C12622_03_54.jpg" alt="Figure 3.54: First five rows"/></div><h6>Figure 3.54: First five rows</h6></li>
				<li>Plot both columns: average number of rooms (<strong class="inline">RM</strong>) and the percentage of the population of a lower socio-economic status (<strong class="inline">PTRATIO</strong>):<p class="snippet">fig = plt.figure(figsize=(10, 7))</p><p class="snippet">fig.suptitle('Parameters vs Median Value')</p><p class="snippet">ax1 = fig.add_subplot(121)</p><p class="snippet">ax1.scatter(df.LSTAT, df.MEDV, marker='*', c='k');</p><p class="snippet">ax1.set_xlabel('% lower status of the population')</p><p class="snippet">ax1.set_ylabel('Median Value in $1000s')</p><p class="snippet">ax2 = fig.add_subplot(122, sharey=ax1)</p><p class="snippet">ax2.scatter(df.RM, df.MEDV, marker='*', c='k');</p><p class="snippet">ax2.get_yaxis().set_visible(False)</p><p class="snippet">ax2.set_xlabel('average number of rooms per dwelling');</p><p>The output will be as follows:</p><div id="_idContainer192" class="IMG---Figure"><img src="image/C12622_03_55.jpg" alt="Figure 3.55: Parameters versus the median value"/></div><h6>Figure 3.55: Parameters versus the median value</h6></li>
				<li>Construct a linear regression model for the percentage of lower socio-economic status (<strong class="inline">LSTAT</strong>) versus the median property value (<strong class="inline">MEDV</strong>), and compute the performance of the model in terms of the R-squared value:<p class="snippet">model = LinearRegression()</p><p class="snippet">model.fit(df.LSTAT.values.reshape((-1, 1)), df.MEDV.values.reshape((-1, 1)))</p><p class="snippet">model.score(df.LSTAT.values.reshape((-1, 1)), df.MEDV.values.reshape((-1, 1)))</p><p>We'll get the following output:</p><div id="_idContainer193" class="IMG---Figure"><img src="image/C12622_03_56.jpg" alt="Figure 3.56: Model score using LSTAT"/></div><h6>Figure 3.56: Model score using LSTAT</h6></li>
				<li>Compute the prediction performance of the linear model trained using the average number of rooms to predict the property value:<p class="snippet">model.fit(df.RM.values.reshape((-1, 1)), df.MEDV.values.reshape((-1, 1)))</p><p class="snippet">model.score(df.RM.values.reshape((-1, 1)), df.MEDV.values.reshape((-1, 1)))</p><p>The output will be as follows:</p><div id="_idContainer194" class="IMG---Figure"><img src="image/C12622_03_57.jpg" alt="Figure 3.57: Model score using RM"/></div><h6>Figure 3.57: Model score using RM</h6></li>
				<li>Create a multiple linear regression model using both the <strong class="inline">LSTAT</strong> and <strong class="inline">RM</strong> values as input to predict the median property value:<p class="snippet">model.fit(df[['LSTAT', 'RM']], df.MEDV.values.reshape((-1, 1)))</p><p class="snippet">model.score(df[['LSTAT', 'RM']], df.MEDV.values.reshape((-1, 1)))</p><p>The output will be:</p></li>
			</ol>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/C12622_03_58.jpg" alt="Figure 3.58: Model score using LSTAT and RM"/>
				</div>
			</div>
			<h6>Figure 3.58: Model score using LSTAT and RM</h6>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor113"/>Autoregression Models</h2>
			<p>Autoregression models are part of a more classical statistical modeling technique that is used on time series data (that is, any dataset that changes with time) and extends upon the linear regression techniques covered in this chapter. Autoregression models are commonly used in the economics and finance industry as they are particularly powerful in time series datasets with a sizeable number of measurements. To reflect this, we will change our dataset to the S&amp;P daily closing prices from 1986 to 2018, which is available in the accompanying source code.</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/C12622_03_59.jpg" alt="Figure 3.59: S&amp;P 500 Daily Closing Price"/>
				</div>
			</div>
			<h6>Figure 3.59: S&amp;P 500 Daily Closing Price</h6>
			<p>The main principle behind autoregression models is that, given enough previous observations, a reasonable prediction for the future can be made; that is, we are essentially constructing a model using the dataset as a regression against itself, hence <strong class="keyword">autoregression</strong>. This relationship can be modeled mathematically as a linear equation:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/C12622_03_60.jpg" alt=""/>
				</div>
			</div>
			<h6>Figure 3.60: First-order autoregression model</h6>
			<p>Where <img src="image/C12622_Formula_03_11.png" alt=""/> is the predicted value for time, <em class="italics">t</em>, <img src="image/C12622_Formula_03_12.png" alt=""/> is the first weight of the model, <img src="image/C12622_Formula_03_13.png" alt=""/> is the second weight with <img src="image/C12622_Formula_03_14.png" alt=""/> as the previous value in the dataset, and <img src="image/C12622_Formula_03_15.png" alt=""/> is an error term.</p>
			<p>The equation in <em class="italics">Figure 3.60</em> represents a model using only the previous value in the dataset to make a prediction. This is a first-order autoregression model and can be extended to include more previous samples.</p>
			<p>The equation in <em class="italics">Figure 3.61</em> provides an example of a second-order model, including the previous two values.</p>
			<p>Similarly, a <em class="italics">k</em><em class="italics">th</em> order autoregression model contains values with corresponding parameters between <img src="image/C12622_Formula_03_16.png" alt=""/>, adding more context about the previous observations about the model. Again, referring to the equation in <em class="italics">Figure 3.61</em> and the <em class="italics">k</em><em class="italics">th</em> order autoregression, the recursive properties of the autoregression model can also be observed. Each prediction uses the previous value(s) in its summation, and thus, if we take the previously predicted values, they themselves use the predictions of the previous value, hence the recursion.</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/C12622_03_61.jpg" alt="Figure 3.61: Second and kth order autoregression model"/>
				</div>
			</div>
			<h6>Figure 3.61: Second and kth order autoregression model</h6>
			<h3 id="_idParaDest-106"><a id="_idTextAnchor114"/>Exercise 35: Creating an Autoregression Model</h3>
			<p>We will use the S&amp;P 500 model to create an autoregression model:</p>
			<ol>
				<li value="1">Load the S&amp;P 500 dataset, extract the year represented as two digits in the column date, and create a new column, <strong class="inline">Year</strong>, with the year represented in the four-digit format (for example, 02-Jan-86 will become 1986 and 31-Dec-04 will become 2004):<p class="snippet">df = pd.read_csv('spx.csv')</p><p class="snippet">yr = []</p><p class="snippet">for x in df.date:</p><p class="snippet">    x = int(x[-2:])</p><p class="snippet">    if x &lt; 10:</p><p class="snippet">        x = f'200{x}'</p><p class="snippet">    elif x &lt; 20:</p><p class="snippet">        x = f'20{x}'</p><p class="snippet">    else:</p><p class="snippet">        x = f'19{x}'  </p><p class="snippet">    yr.append(x)</p><p class="snippet">df['Year'] = yr</p><p class="snippet">df.head()</p><p>We'll get the following output:</p><div id="_idContainer205" class="IMG---Figure"><img src="image/C12622_03_62.jpg" alt="Figure 3.62: First five rows"/></div><h6>Figure 3.62: First five rows</h6></li>
				<li>Plot the raw dataset with years along the <em class="italics">x</em> axis in multiples of five:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(df.close.values);</p><p class="snippet">yrs = [yr for yr in df.Year.unique() if (int(yr[-2:]) % 5 == 0)]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('S&amp;P 500 Daily Closing Price');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Price ($)');</p><p>The output will be as follows:</p><div id="_idContainer206" class="IMG---Figure"><img src="image/C12622_03_63.jpg" alt="Figure 3.63: Plot of closing price through the years"/></div><h6>Figure 3.63: Plot of closing price through the years</h6></li>
				<li>Before we can construct an autoregression model, we must first check to see whether the model is able to be used as a regression against itself. To do that, we can once again use the pandas library to check for correlations between the dataset and a copy of the dataset that is shifted by a defined number of samples, known as <strong class="bold">lag</strong>. For a more concrete understanding of this regression, print out the first 10 values of the closing prices. Then, using the pandas <strong class="inline">shift</strong> method, introduce a sample lag of <strong class="inline">3</strong> into the first 10 values of the closing price and look at the result:<p class="snippet">df.close[:10].values</p><p class="snippet">df.close[:10].shift(3).values</p><p>We'll get this output:</p><div id="_idContainer207" class="IMG---Figure"><img src="image/C12622_03_64.jpg" alt="Figure 3.64: Values with a lag of three"/></div><h6>Figure 3.64: Values with a lag of three</h6><p>Notice the introduction of three NaN values into the array and that the last three values have dropped off the array. This is the effect of shifting, essentially sliding the dataset forward in time by the period defined by the lag.</p></li>
				<li>Shift the dataset by a lag of 100 and plot the result:<p class="snippet">plt.figure(figsize=(15, 7))</p><p class="snippet">plt.plot(df.close.values, label='Original Dataset', c='k', linestyle='-');</p><p class="snippet">plt.plot(df.close.shift(100), c='k', linestyle=':', label='Lag 100');</p><p class="snippet">yrs = [yr for yr in df.Year.unique() if (int(yr[-2:]) % 5 == 0)]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('S&amp;P 500 Daily Closing Price');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Price ($)');</p><p class="snippet">plt.legend();</p><p>The output will be as follows:</p><div id="_idContainer208" class="IMG---Figure"><img src="image/C12622_03_65.jpg" alt="Figure 3.65: Plot of closing price over the year"/></div><h6>Figure 3.65: Plot of closing price over the year</h6></li>
				<li>Now that we have an understanding of the time shift, we will confirm that the data can be correlated against itself. To do this, use the pandas <strong class="inline">autocorrelation_plot</strong> method to check for randomness within the data:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">pd.plotting.autocorrelation_plot(df.close);</p><p>The output will be as follows:</p><div id="_idContainer209" class="IMG---Figure"><img src="image/C12622_03_66.jpg" alt="Figure 3.66: Relation of autocorrelation versus the lag"/></div><h6>Figure 3.66: Relation of autocorrelation versus the lag</h6><p>All of the information required to determine whether autoregression is possible is defined within this plot. We can see on the <em class="italics">x</em> axis, the values for <strong class="bold">Lag</strong> range from 0 to 8,000 samples, and the values for <strong class="bold">Autocorrelation</strong> vary from approximately -0.4 to 1. There are five other additional lines of interest; however, at this scale on the <em class="italics">y</em> axis, it is difficult to see them.</p></li>
				<li>Set the <em class="italics">y</em> axis limits to be between -0.1 and 0.1:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">ax = pd.plotting.autocorrelation_plot(df.close);</p><p class="snippet">ax.set_ylim([-0.1, 0.1]);</p><p> The output will be as follows:</p><div id="_idContainer210" class="IMG---Figure"><img src="image/C12622_03_67.jpg" alt="Figure 3.67: Plot of autocorrelation versus lag"/></div><h6>Figure 3.67: Plot of autocorrelation versus lag</h6><p>We can see in the enhanced view that there are two gray dashed lines, which represent the 99% confidence band that the series is non-random. The solid gray line represents the 95% confidence band. Once the autocorrelation plot approaches zero within these bands, the time series with the specified lag becomes sufficiently random that autoregression models would not be appropriate.</p></li>
				<li>To further solidify our understanding, create a plot of the closing prices versus the closing prices with a lag of 100 samples. According to our autocorrelation plot, there is a high correlation between these sets. What does that look like?<p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">ax = pd.plotting.lag_plot(df.close, lag=100);</p><p>The output will be:</p><div id="_idContainer211" class="IMG---Figure"><img src="image/C12622_03_68.jpg" alt="Figure 3.68: Autocorrelation plot"/></div><h6>Figure 3.68: Autocorrelation plot</h6></li>
				<li>Create a plot of closing prices versus closing prices with a lag of 4,000 samples. Again, looking at the autocorrelation plot at a lag of 4,000, the autocorrelation value is approximately 0, indicating that there is no real correlation between the two and it is mostly random:<p class="snippet">plt.figure(figsize=(10,7))</p><p class="snippet">ax = pd.plotting.lag_plot(df.close, lag=4000);</p><p>The output will be:</p><div id="_idContainer212" class="IMG---Figure"><img src="image/C12622_03_69.jpg" alt="Figure 3.69: Plot of closing prices versus closing prices with a lag of 4,000 samples"/></div><h6>Figure 3.69: Plot of closing prices versus closing prices with a lag of 4,000 samples</h6></li>
				<li>Now we are ready to create our model. To do this, however, we will need another Python package, the <strong class="inline">statsmodel</strong> package (<a href="B13323_03_ePub_Final_NT.xhtml#_idTextAnchor113">http://www.statsmodels.org</a>), which is similar to scikit-learn but is dedicated to creating models and executing tests using the more classical statistical techniques. Install the <strong class="inline">statsmodel</strong> package. You can do this either using <strong class="inline">conda install</strong> or <strong class="inline">pip</strong>. For the Anaconda installation, the <strong class="inline">conda install</strong> method is preferred:<p class="snippet">#!pip install statsmodels</p><p class="snippet"> !conda install -c conda-forge statsmodels</p></li>
				<li>Import the autoregression class (<strong class="inline">AR</strong>) from <strong class="inline">statsmodel</strong> and construct the model using the closing price data:<p class="snippet">from statsmodels.tsa.ar_model import AR</p><p class="snippet">model = AR(df.close)</p></li>
				<li>Fit the model using the <strong class="inline">fit</strong> method and print out the lag that was selected for use and the coefficients of the model:<p class="snippet">model_fit = model.fit()</p><p class="snippet">print('Lag: %s' % model_fit.k_ar)</p><p class="snippet">print('Coefficients: %s' % model_fit.params)</p><p>The output will be:</p><div id="_idContainer213" class="IMG---Figure"><img src="image/C12622_03_70.jpg" alt="Figure 3.70: Lag co-efficients"/></div><h6>Figure 3.70: Lag coefficients</h6><p>Note that there are 36 coefficients for each of the weights and one constant; only an extract is shown for simplicity. All coefficients can be found in the <strong class="inline">Ex7-AutoRegressors.ipynb</strong> Jupyter notebook in the accompanying source code.</p></li>
				<li>Use the model to create a set of predictions starting at sample 36 (the lag) and finishing at 500 samples after the dataset has ended:<p class="snippet">predictions = model_fit.predict(start=36, end=len(df) + 500)</p><p class="snippet">predictions[:10].values</p><p>We'll get the following output:</p><div id="_idContainer214" class="IMG---Figure"><img src="image/C12622_03_71.jpg" alt=""/></div><h6>Figure 3.71: Prediction values</h6></li>
				<li>Plot the predictions' values over the top of the original dataset:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(predictions, c='g', linestyle=':', label='Predictions');</p><p class="snippet">plt.plot(df.close.values, label='Original Dataset');</p><p class="snippet">yrs = [yr for yr in df.Year.unique() if (int(yr[-2:]) % 5 == 0)]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('S&amp;P 500 Daily Closing Price');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Price ($)');</p><p class="snippet">plt.legend();</p><p>This will give the following output:</p><div id="_idContainer215" class="IMG---Figure"><img src="image/C12622_03_72.jpg" alt="Figure 3.72: Plot of price through the year"/></div><h6>Figure 3.72: Plot of price through the year</h6><p>Note that the predictions do an excellent job of following the dataset, and that after the dataset has ended, the predictions are relatively linear. Given that the model is constructed from the previous samples, it makes sense that it becomes less certain once the dataset has finished, particularly as there are no repetitive patterns in the data.</p></li>
				<li>The fit seems really close – what does the difference between the predictions and original dataset look like? Enhance the model to observe the differences:<p class="snippet">plt.figure(figsize=(10, 7))</p><p class="snippet">plt.plot(predictions, c='g', linestyle=':', label='Predictions');</p><p class="snippet">plt.plot(df.close.values, label='Original Dataset');</p><p class="snippet">yrs = [yr for yr in df.Year.unique() if (int(yr[-2:]) % 5 == 0)]</p><p class="snippet">plt.xticks(np.arange(0, len(df), len(df) // len(yrs)), yrs);</p><p class="snippet">plt.title('S&amp;P 500 Daily Closing Price');</p><p class="snippet">plt.xlabel('Year');</p><p class="snippet">plt.ylabel('Price ($)');</p><p class="snippet">plt.xlim([2000, 2500])</p><p class="snippet">plt.ylim([420, 500])</p><p class="snippet">plt.legend();</p><p>This provides the following plot:</p></li>
			</ol>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/C12622_03_73.jpg" alt="Figure 3.73: Predictions on the original dataset values"/>
				</div>
			</div>
			<h6>Figure 3.73: Predictions on the original dataset values</h6>
			<p>From this exercise using an autoregressor, we can see that there is significant predictive power in using these models when there is missing data from the set or when we are attempting to predict between measurement intervals. The autoregressor model shown for the S&amp;P 500 dataset was able to effectively provide predictions within the range of observed samples. However, outside of this range, when predicting future values for which no measurements have been taken, the predictive power may be somewhat limited.</p>
			<h3 id="_idParaDest-107"><a id="_idTextAnchor115"/>Activity 10: Autoregressors</h3>
			<p>In this activity, we will now use autoregressors to model the Austin weather dataset and predict future values:</p>
			<p>Before we begin, we will need to import a few libraries and load data from a previous activity, which can be done as follows:</p>
			<p class="snippet">import numpy as np</p>
			<p class="snippet">import pandas as pd</p>
			<p class="snippet">import matplotlib.pyplot as plt</p>
			<p class="snippet">from statsmodels.tsa.ar_model import AR</p>
			<p class="snippet"># Loading the data from activity 5</p>
			<p class="snippet">df = pd.read_csv('activity2_measurements.csv')</p>
			<p>The steps to be performed are as follows:</p>
			<ol>
				<li value="1">Plot the complete set of average temperature values (<strong class="inline">df.TempAvgF</strong>) with years on the <em class="italics">x</em> axis.</li>
				<li>Create a 20-day lag and plot the lagged data on the original dataset.</li>
				<li>Construct an autocorrelation plot to see whether the average temperature can be used with an autoregressor.</li>
				<li>Choose an acceptable lag and an unacceptable lag and construct lag plots using these values.</li>
				<li>Create an autoregressor model, note the selected lag, calculate the r2 value, and plot the autoregressor model with the original plot. The model is to project past the available data by 1,000 samples.</li>
				<li>Fit the model to the data.</li>
				<li>Create a set of predictions for 1,000 days after the last sample.</li>
				<li>Plot the predictions, as well as the original dataset.</li>
				<li>Enhance the view to look for differences by showing the 100th to 200th sample.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 344.</p></li>
			</ol>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor116"/>Summary</h2>
			<p>In t<a id="_idTextAnchor117"/>his chapter, we took our first big leap into constructing machine learning models and making predictions with labeled datasets. We began our analysis by looking at a variety of different ways to construct linear models, starting with the precise least squares method, which is very good when modeling small amounts of data that can be processed using the available computer memory. The performance of our vanilla linear model was improved using dummy variables, which we created from categorical variables, adding additional features and context to the model. We then used linear regression analysis with a parabolic model to further improve performance, fitting a more natural curve to the dataset. We also implemented the gradient descent algorithm, which we noticed, while not as precise as the least squares method was for our limited dataset, was most powerful when the dataset cannot be processed on the resources available on the system.</p>
			<p>Finally, we investigated the use of autoregression models, which predict future values based on the experience of previous data in the set. Using autoregressors, we were able to accurately model the closing price of the S&amp;P 500 over the years 1986 – 2018.</p>
			<p>Now that we have experience with supervised regression problems, we will turn our attention to classification problems in the next chapter.</p>
		</div>
	</body></html>
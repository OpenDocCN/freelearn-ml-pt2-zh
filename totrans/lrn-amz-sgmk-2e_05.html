<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer090">
			<h1 id="_idParaDest-51"><a id="_idTextAnchor049"/>Chapter 3: AutoML with Amazon SageMaker Autopilot</h1>
			<p>In the previous chapter, you learned how Amazon SageMaker helps you build and prepare datasets. In a typical machine learning project, the next step would be to start experimenting with algorithms in order to find an early fit and get a sense of the predictive power you could expect from the model.</p>
			<p>Whether you work with traditional machine learning or deep learning, three options are available when it comes to selecting an algorithm:</p>
			<ul>
				<li>Write your own, or customize an existing one. This only makes sense if you have strong skills in statistics and computer science, if you're quite sure that you can do better than well-tuned, off-the-shelf algorithms, and if you're given enough time to work on the project. Let's face it, these conditions are rarely met.</li>
				<li>Use a built-in algorithm implemented in one of your favorite libraries, such as <strong class="bold">linear regression</strong> or <strong class="bold">XGBoost</strong>. For deep learning problems, this includes pre-trained models available in <strong class="bold">TensorFlow</strong>, <strong class="bold">PyTorch</strong>, and so on. This option saves you the trouble of writing machine learning code. Instead, it lets you focus on feature engineering and model optimization. </li>
				<li>Use <strong class="bold">AutoML</strong>, a rising technique that lets you automatically build, train, and optimize machine learning models.</li>
			</ul>
			<p>In this chapter, you will learn about <strong class="bold">Amazon SageMaker Autopilot</strong>, an AutoML capability part of Amazon SageMaker with built-in model explainability. We'll see how to use it in Amazon SageMaker Studio without writing a single line of code, and also how to use it with the Amazon SageMaker SDK:</p>
			<ul>
				<li>Discovering Amazon SageMaker Autopilot</li>
				<li>Using Amazon SageMaker Autopilot in SageMaker Studio</li>
				<li>Using Amazon SageMaker Autopilot with the SageMaker SDK</li>
				<li>Diving deep on Amazon SageMaker Autopilot</li>
			</ul>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor050"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you haven't got one already, please point your browser at <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create it. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).</p>
			<p>You will need a working Python 3.x environment. Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory, but is strongly encouraged as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>Code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor051"/>Discovering Amazon SageMaker Autopilot</h1>
			<p>Added to Amazon SageMaker in late 2019, <strong class="bold">Amazon SageMaker Autopilot</strong> is an AutoML capability that takes care of all the machine <a id="_idIndexMarker180"/>learning steps for you. You only need to upload a columnar dataset to an Amazon S3 bucket and define the column you <a id="_idIndexMarker181"/>want the model to learn (the <strong class="bold">target attribute</strong>). Then, you simply launch an Autopilot job, with either a few clicks in the SageMaker Studio GUI or a couple of lines of code with the SageMaker SDK.</p>
			<p>The simplicity of SageMaker Autopilot doesn't come at the expense of transparency and control. You can see how your models are built, and you can keep experimenting to refine results. In that respect, SageMaker Autopilot should appeal to new and seasoned practitioners alike.</p>
			<p>In this section, you'll learn <a id="_idIndexMarker182"/>about the different steps of a SageMaker Autopilot job and how they contribute to delivering high-quality models:</p>
			<ul>
				<li>Analyzing data</li>
				<li>Feature engineering</li>
				<li>Model tuning</li>
			</ul>
			<p>Let's start by seeing how SageMaker Autopilot analyzes data.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor052"/>Analyzing data</h2>
			<p>This step is responsible for understanding what <a id="_idIndexMarker183"/>type of machine <a id="_idIndexMarker184"/>learning <a id="_idIndexMarker185"/>problem we're trying <a id="_idIndexMarker186"/>to <a id="_idIndexMarker187"/>solve. SageMaker Autopilot currently supports <strong class="bold">linear regression</strong>, <strong class="bold">binary classification</strong>, and <strong class="bold">multi-class classification</strong>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">A frequent question is ”how much data is needed to build such models?” This is a surprisingly difficult question. The answer—if there is one—depends on many factors, such as the number of features and their quality. As a basic rule of thumb, some practitioners recommend having 10-100 times more samples than features. In any case, I'd advise you to collect no fewer than hundreds of samples (for each class, if you're building a classification model). Thousands or tens of thousands are better, especially if you have more features. For statistical machine learning, there is rarely a need for millions of samples, so start with what you have, analyze the results, and iterate before going on a data collection rampage!</p>
			<p>By analyzing the distribution of the target attribute, SageMaker Autopilot can easily figure out which one is the right one. For instance, if the target attribute has only two values (say, "yes" and "no"), you're likely trying to build a binary classification model.</p>
			<p>Then, SageMaker Autopilot computes statistics on the dataset and individual columns: the number of unique values, the mean, median, and so on. Machine learning practitioners very often do this in order to get an initial <a id="_idIndexMarker188"/>feel for the data, and it's nice to see it automated. In addition, SageMaker Autopilot generates <a id="_idIndexMarker189"/>a Jupyter notebook, the <strong class="bold">data exploration notebook</strong>, to present these statistics in a user-friendly way.</p>
			<p>Once SageMaker Autopilot <a id="_idIndexMarker190"/>has analyzed the dataset, it builds <strong class="bold">candidate pipelines</strong> that will be used to train candidate models. A pipeline is a combination of the following:</p>
			<ul>
				<li>A data processing job, in charge of feature <a id="_idIndexMarker191"/>engineering. As you can guess, this job runs on <strong class="bold">Amazon SageMaker Processing</strong>, which we studied in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a><em class="italic">,</em> <em class="italic">Handling Data Preparation Techniques</em>.</li>
				<li>A training job, running on the processed dataset. Algorithms include the built-in Linear Learner in SageMaker, XGBoost, and multi-layer perceptrons.</li>
			</ul>
			<p>Next, let's see how Autopilot can be used in feature engineering.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor053"/>Feature engineering</h2>
			<p>This step is responsible for pre-processing <a id="_idIndexMarker192"/>the input dataset <a id="_idIndexMarker193"/>according to the pipelines defined during data analysis. </p>
			<p>Candidate pipelines are fully documented in another autogenerated notebook – the <strong class="bold">candidate generation notebook</strong>. This notebook isn't just descriptive: you can actually run its cells, and manually reproduce the steps performed by SageMaker Autopilot. This level of transparency and control is extremely <a id="_idIndexMarker194"/>important as it lets you understand exactly how the model was built. Thus, you're able to verify that it performs the way it should, and you're able to explain it to your stakeholders. Also, you can use the notebook as a starting point for additional optimization and tweaking if you're so inclined.</p>
			<p>Lastly, let's take a look at model tuning in Autopilot.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor054"/>Model tuning</h2>
			<p>This step is <a id="_idIndexMarker195"/>responsible for training and <a id="_idIndexMarker196"/>tuning models according to the pipelines defined during data <a id="_idIndexMarker197"/>analysis. For each pipeline, SageMaker Autopilot <a id="_idIndexMarker198"/>will launch an <strong class="bold">automatic model tuning</strong> job (we'll cover this topic in detail in a later chapter). In a nutshell, each tuning job will use <strong class="bold">hyperparameter optimization</strong> to train a large <a id="_idIndexMarker199"/>number of increasingly accurate models on the processed dataset. As usual, all of this happens on managed infrastructure.</p>
			<p>Once the model <a id="_idIndexMarker200"/>tuning is complete, you can view the model information and metrics in Amazon SageMaker Studio, build visualizations, and so on. You can do the same programmatically with the <strong class="bold">Amazon SageMaker Experiments</strong> SDK.</p>
			<p>Finally, you can deploy your model of choice just like any other SageMaker model using either the SageMaker Studio GUI or the SageMaker SDK.</p>
			<p>Now that we understand the different steps of an Autopilot job, let's run a job in SageMaker Studio.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor055"/>Using Amazon SageMaker Autopilot in SageMaker Studio</h1>
			<p>We will build a <a id="_idIndexMarker201"/>model using <a id="_idIndexMarker202"/>only SageMaker Studio. We won't write a line of machine learning code, so get ready for zero-code AI.</p>
			<p>In this section, you'll learn how to do the following:</p>
			<ul>
				<li>Launch a SageMaker Autopilot job in SageMaker Studio.</li>
				<li>Monitor the different steps of the job.</li>
				<li>Visualize models and compare their properties.</li>
			</ul>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor056"/>Launching a job</h2>
			<p>First, we need <a id="_idIndexMarker203"/>a dataset. We'll reuse the direct marketing dataset used in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>. This dataset describes a binary classification problem: will a customer accept a marketing offer, yes or no? It contains a little more than 41,000 labeled customer samples. Let's dive in:</p>
			<ol>
				<li>Let's open SageMaker Studio. Create a new Python 3 notebook using the <strong class="bold">Data Science</strong> kernel, as shown in the following screenshot:<div id="_idContainer065" class="IMG---Figure"><img src="Images/B17705_03_001.jpg" alt="Figure 3.1 – Creating a notebook&#13;&#10;" width="605" height="283"/></div><p class="figure-caption">Figure 3.1 – Creating a notebook</p></li>
				<li>Now, let's download and extract the dataset as follows:<p class="source-code">%%sh</p><p class="source-code">apt-get -y install unzip</p><p class="source-code">wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</p><p class="source-code">unzip -o bank-additional.zip</p></li>
				<li>In <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>, we ran a feature engineering script with Amazon SageMaker Processing. We will do no such thing here: we simply upload the dataset as is to S3, into the <strong class="bold">default bucket</strong> created by SageMaker:<p class="source-code">import sagemaker</p><p class="source-code">prefix = 'sagemaker/DEMO-autopilot/input'</p><p class="source-code">sess   = sagemaker.Session()</p><p class="source-code">uri = sess.upload_data(path=”./bank-additional/bank-additional-full.csv”, key_prefix=prefix)</p><p class="source-code">print(uri)</p><p>The dataset will be available in S3 at the following location:</p><p class="source-code"><strong class="bold">s3://sagemaker-us-east-2-123456789012/sagemaker/DEMO-autopilot/input/bank-additional-full.csv</strong></p></li>
				<li>Now, we click on the <strong class="bold">Components and registries</strong> icon in the left-hand vertical <a id="_idIndexMarker204"/>icon bar, as can be seen in the following screenshot. This opens the <strong class="bold">Experiments</strong> tab, and we click on the <strong class="bold">Create Autopilot Experiment</strong> button to create a new Autopilot job.<div id="_idContainer066" class="IMG---Figure"><img src="Images/B17705_03_002.jpg" alt="Figure 3.2 – Viewing experiments&#13;&#10;" width="788" height="817"/></div><p class="figure-caption">Figure 3.2 – Viewing experiments</p></li>
				<li>The next screen is where we configure the job. Let's enter <strong class="source-inline">my-first-autopilot-job</strong> as the experiment name.</li>
				<li>We set the location of the input dataset using the path returned in <em class="italic">step 3</em>. As can <a id="_idIndexMarker205"/>be seen in the following screenshot, we can either browse S3 buckets or enter the S3 location directly:<p class="figure-caption"> </p><div id="_idContainer067" class="IMG---Figure"><img src="Images/B17705_03_003.jpg" alt="Figure 3.3 – Defining the input location&#13;&#10;" width="836" height="296"/></div><p class="figure-caption">Figure 3.3 – Defining the input location</p></li>
				<li>The next step is to define the name of the <strong class="bold">target attribute</strong>, as shown in the following screenshot. The column storing the "yes" or "no" label is called "y".<div id="_idContainer068" class="IMG---Figure"><img src="Images/B17705_03_004.jpg" alt="Figure 3.4 – Defining the target attribute&#13;&#10;" width="571" height="164"/></div><p class="figure-caption">Figure 3.4 – Defining the target attribute</p></li>
				<li>As shown in the following screenshot, we set the S3 output location where job artifacts will be copied to. I use <strong class="source-inline">s3://sagemaker-us-east-2-123456789012/sagemaker/DEMO-autopilot/output/</strong> here, and you should, of course, update it with your own region and account number.<p class="figure-caption"> </p><div id="_idContainer069" class="IMG---Figure"><img src="Images/B17705_03_005.jpg" alt="Figure 3.5 – Defining the output location&#13;&#10;" width="535" height="212"/></div><p class="figure-caption">Figure 3.5 – Defining the output location</p></li>
				<li>We set the type of job we want to train, as shown in the following screenshot. Here, we <a id="_idIndexMarker206"/>select <strong class="bold">Auto</strong> in order to let SageMaker Autopilot figure out the problem type. Alternatively, we could select <strong class="bold">Binary classification</strong>, and pick our metric: <strong class="bold">Accuracy</strong>, <strong class="bold">AUC</strong>, or <strong class="bold">F1</strong> (the default setting).<p class="figure-caption"> </p><div id="_idContainer070" class="IMG---Figure"><img src="Images/B17705_03_006.jpg" alt="Figure 3.6 – Setting the problem type&#13;&#10;" width="737" height="137"/></div><p class="figure-caption">Figure 3.6 – Setting the problem type</p></li>
				<li>Finally, we decide whether we want to run a full job, or simply generate notebooks. We'll go with the former, as shown in the following screenshot. The latter would be a good option if we wanted to train and tweak the parameters manually. We also decide not to deploy the best model automatically for now.<div id="_idContainer071" class="IMG---Figure"><img src="Images/B17705_03_007.jpg" alt="Figure 3.7 – Running a complete experiment&#13;&#10;" width="515" height="198"/></div><p class="figure-caption">Figure 3.7 – Running a complete experiment</p></li>
				<li>Optionally, in the <strong class="bold">Advanced Settings</strong> section, we would change the IAM role, set an encryption key for job artifacts, define the VPC where we'd like to launch job instances, and so on. Let's keep default values here.</li>
				<li>The job <a id="_idIndexMarker207"/>setup is complete: all it took was this one screen. Then, we click on <strong class="bold">Create Experiment</strong>, and off it goes!</li>
			</ol>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor057"/>Monitoring a job</h2>
			<p>Once the job is launched, it goes through the three steps that we already discussed, which <a id="_idIndexMarker208"/>should take around 5 hours to complete. The new experiment is listed in the <strong class="bold">Experiments</strong> tab, and we can right-click <strong class="bold">Describe AutoML Job</strong> to describe its current status. This opens the following screen, where we can see the progress of the job:</p>
			<ol>
				<li value="1">As expected, the job starts by analyzing data, as highlighted in the following screenshot:<div id="_idContainer072" class="IMG---Figure"><img src="Images/B17705_03_008.jpg" alt="Figure 3.8 – Viewing job progress&#13;&#10;" width="1195" height="321"/></div><p class="figure-caption">Figure 3.8 – Viewing job progress</p></li>
				<li>About 10 minutes later, data analysis is complete, and the job moves on to feature engineering, where the input dataset will be transformed according to the steps <a id="_idIndexMarker209"/>defined in the candidate pipelines. As <a id="_idIndexMarker210"/>shown in the following screenshot, we can also see new two buttons in the top-right corner, pointing at the <strong class="bold">candidate generation</strong> and <strong class="bold">data exploration</strong> notebooks: don't worry, we'll take a deeper look at both later in the chapter.<div id="_idContainer073" class="IMG---Figure"><img src="Images/B17705_03_009.jpg" alt="Figure 3.9 – Viewing job progress&#13;&#10;" width="1084" height="359"/></div><p class="figure-caption">Figure 3.9 – Viewing job progress</p></li>
				<li>Once feature engineering is complete, the job then moves on to model tuning, where candidate models are trained and tuned. As can be seen in the following screenshot, the first training jobs quickly show up in the <strong class="bold">Trials</strong> tab. A "trial" is the <a id="_idIndexMarker211"/>name SageMaker uses for a collection of related jobs, such as processing jobs, batch transform jobs, and training jobs. We can see the <strong class="bold">Objective</strong>, that is to say, the metric that the job tried to optimize (in this case, it's the F1 score). We can sort jobs based on this metric, and the best tuning job so far is highlighted with a star.<p class="figure-caption"> </p><div id="_idContainer074" class="IMG---Figure"><img src="Images/B17705_03_010.jpg" alt="Figure 3.10 – Viewing tuning jobs&#13;&#10;" width="848" height="602"/></div><p class="figure-caption">Figure 3.10 – Viewing tuning jobs</p></li>
				<li>Once the AutoPilot job is complete, your screen should look similar to the following screenshot. Here, the top model has reached an F1 score of 0.8031.<div id="_idContainer075" class="IMG---Figure"><img src="Images/B17705_03_011.jpg" alt="Figure 3.11 – Viewing results&#13;&#10;" width="722" height="308"/></div><p class="figure-caption">Figure 3.11 – Viewing results</p></li>
				<li>If we select <a id="_idIndexMarker212"/>the best job and right-click <strong class="bold">Open in model details</strong>, we can see a model explainability graph showing us the most important features, as can be seen in the following <a id="_idIndexMarker213"/>screenshot. This graph is based on global <strong class="bold">SHapley Additive exPlanations </strong>(<strong class="bold">SHAP</strong>) (<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>) values computed automatically by AutoPilot.<div id="_idContainer076" class="IMG---Figure"><img src="Images/B17705_03_012.jpg" alt="Figure 3.12 – Viewing the most important features&#13;&#10;" width="1086" height="815"/></div><p class="figure-caption">Figure 3.12 – Viewing the most important features</p></li>
				<li>In the <strong class="bold">Artifacts</strong> tab, we can also see a list of training artifacts and parameters involved in <a id="_idIndexMarker214"/>building the model: input data, training and validation splits, transformed datasets, feature engineering code, the algorithm (XGBoost in my case), and more.</li>
			</ol>
			<p>At this point, we could simply deploy the best job, but instead, let's compare the top 10 ones using the visualization tools built into SageMaker Studio.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor058"/>Comparing jobs</h2>
			<p>A single SageMaker <a id="_idIndexMarker215"/>Autopilot job trains 250 jobs by default. Over time, you may end up with tens of thousands of jobs, and you may wish to compare their properties. Let's see how:</p>
			<ol>
				<li value="1">Going to the <strong class="bold">Experiments</strong> tab on the left, we locate our job and right-click <strong class="bold">Open in trial component list</strong>, as can be seen in the following screenshot:<div id="_idContainer077" class="IMG---Figure"><img src="Images/B17705_03_013.jpg" alt="Figure 3.13 – Opening the list of trials&#13;&#10;" width="1166" height="675"/></div><p class="figure-caption">Figure 3.13 – Opening the list of trials</p></li>
				<li>This opens <strong class="bold">Trial Component List</strong>, as shown in the following screenshot. <p>We <a id="_idIndexMarker216"/>open the <strong class="bold">Table Properties</strong> panel on the right by clicking on the icon representing a cog, and we untick everything except <strong class="bold">Experiment name</strong>, <strong class="bold">Trial component name</strong>, and <strong class="bold">ObjectiveMetric</strong>. In the main panel, we sort jobs by descending objective metrics by clicking on the arrow. We hold down the <em class="italic">Shift</em> key and click the top 10 jobs to select them, as shown in the following screenshot: </p><div id="_idContainer078" class="IMG---Figure"><img src="Images/B17705_03_014.jpg" alt="Figure 3.14 – Comparing jobs&#13;&#10;" width="1250" height="516"/></div><p class="figure-caption">Figure 3.14 – Comparing jobs</p></li>
				<li>Then, we <a id="_idIndexMarker217"/>click on the <strong class="bold">Add chart</strong> button. This opens a new view that can be seen in the following screenshot. Click inside the chart box at the bottom to open the <strong class="bold">Chart properties</strong> panel on the right.<div id="_idContainer079" class="IMG---Figure"><img src="Images/B17705_03_015.jpg" alt="Figure 3.15 – Building a chart&#13;&#10;" width="1250" height="737"/></div><p class="figure-caption">Figure 3.15 – Building a chart</p><p>As our training jobs are very short (about a minute), there won't be enough data for <strong class="bold">Time series</strong> charts, so let's select <strong class="bold">Summary statistics</strong> instead. We're going to build a <strong class="bold">scatter plot</strong>, putting the eta and lambda hyperparameters <a id="_idIndexMarker218"/>in perspective, as shown in the following screenshot. We also color data points with our trial names.</p><div id="_idContainer080" class="IMG---Figure"><img src="Images/B17705_03_016.jpg" alt="Figure 3.16 – Creating a chart&#13;&#10;" width="371" height="690"/></div><p class="figure-caption">Figure 3.16 – Creating a chart</p></li>
				<li>Zooming in on the following chart, we can quickly visualize our jobs and their respective parameters. We could build additional charts showing the impact of certain <a id="_idIndexMarker219"/>hyperparameters on accuracy. This would help us shortlist a few models for further testing. Maybe we would end up considering several of them for ensemble prediction.</li>
			</ol>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="Images/B17705_03_017.jpg" alt="Figure 3.17 – Plotting hyperparameters&#13;&#10;" width="925" height="638"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Plotting hyperparameters</p>
			<p>The next step is to <a id="_idIndexMarker220"/>deploy a model and start testing it. </p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor059"/>Deploying and invoking a model</h2>
			<p>SageMaker Studio makes it extremely easy to deploy a model. Let's see how:</p>
			<ol>
				<li value="1">Going back to the <strong class="bold">Experiments</strong> tab, we right-click the name of our experiment and select <strong class="bold">Describe AutoML Job</strong>. This opens the list of training jobs. Making sure that <a id="_idIndexMarker221"/>they're sorted by <a id="_idIndexMarker222"/>descending objective, we select the best one (it's highlighted with a star), as shown in the screenshot that follows, and then we click on the <strong class="bold">Deploy model</strong> button:<p class="figure-caption"> </p><div id="_idContainer082" class="IMG---Figure"><img src="Images/B17705_03_018.jpg" alt="Figure 3.18 – Deploying a model&#13;&#10;" width="1177" height="507"/></div><p class="figure-caption">Figure 3.18 – Deploying a model</p></li>
				<li>Under <strong class="bold">REALTIME DEPLOYMENT SETTINGS</strong>, let's give the endpoint a name (<strong class="source-inline">my-first-autopilot-endpoint</strong>), leave all other settings as is, and click on <strong class="bold">Deploy model</strong>. As shown in the following screenshot, the model will be deployed on a real-time HTTPS endpoint backed by an <strong class="source-inline">ml.m5.xlarge</strong> instance: <div id="_idContainer083" class="IMG---Figure"><img src="Images/B17705_03_019.jpg" alt="Figure 3.19 – Deploying a model&#13;&#10;" width="365" height="238"/></div><p class="figure-caption">Figure 3.19 – Deploying a model</p></li>
				<li>Heading to the <strong class="bold">Endpoints</strong> section in the left-hand vertical panel, we can see the endpoint <a id="_idIndexMarker223"/>being created. As shown <a id="_idIndexMarker224"/>in the following screenshot, it will initially be in the <strong class="bold">Creating</strong> state. After a few minutes, it's <strong class="bold">In service</strong>:<div id="_idContainer084" class="IMG---Figure"><img src="Images/B17705_03_020.jpg" alt="Figure 3.20 – Creating an endpoint&#13;&#10;" width="321" height="241"/></div><p class="figure-caption">Figure 3.20 – Creating an endpoint</p></li>
				<li>Moving to a Jupyter notebook (we can reuse the one we wrote to download the dataset), we define the name of the endpoint, and a sample to predict. Here, I'm using the first line of the dataset:<p class="source-code">ep_name = 'my-first-autopilot-endpoint'</p><p class="source-code">sample = '56,housemaid,married,basic.4y,no,no,no,telephone,may,mon,261,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191.0'</p></li>
				<li>We create a <strong class="source-inline">boto3</strong> client for the SageMaker runtime. This runtime contains a single API, <strong class="source-inline">invoke_endpoint</strong> (<a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html</a>). This makes it efficient to embed in client applications that just need to invoke models:<p class="source-code">import boto3</p><p class="source-code">sm_rt = boto3.Session().client('runtime.sagemaker')</p></li>
				<li>We send <a id="_idIndexMarker225"/>the sample to the endpoint, also passing the input and output content types:<p class="source-code">response = sm_rt.invoke_endpoint(EndpointName=ep_name,</p><p class="source-code">                              ContentType='text/csv',</p><p class="source-code">                              Accept='text/csv',</p><p class="source-code">                              Body=sample)</p></li>
				<li>We decode <a id="_idIndexMarker226"/>the prediction and print it – this customer is not likely to accept the offer:<p class="source-code">response = response['Body'].read().decode(”utf-8”)</p><p class="source-code">print(response)</p><p>This sample is predicted as a "no":</p><p class="source-code"><strong class="bold">no</strong></p></li>
				<li>When we're done testing the endpoint, we should delete it to avoid unnecessary charges. We can do this with the <strong class="source-inline">delete_endpoint</strong> API in <strong class="source-inline">boto3</strong> (<a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.delete_endpoint">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.delete_endpoint</a>):<p class="source-code">sm = boto3.Session().client('sagemaker')</p><p class="source-code">sm.delete_endpoint(EndpointName=ep_name)</p></li>
			</ol>
			<p>Congratulations! You've successfully built, trained, and deployed your first machine learning model on Amazon SageMaker. That was pretty simple, wasn't it? The only code we wrote was to download the dataset and to predict with our model.</p>
			<p>Using <strong class="bold">SageMaker Studio</strong> is a great way to quickly experiment with a new dataset, and also to let fewer technical users <a id="_idIndexMarker227"/>build models on their own. Advanced users can also <a id="_idIndexMarker228"/>add their own custom images <a id="_idIndexMarker229"/>to SageMaker Studio, and they'll <a id="_idIndexMarker230"/>find more details at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html">https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html</a>.</p>
			<p>Now, let's see how we can use SageMaker Autopilot programmatically <a id="_idIndexMarker231"/>with the <strong class="bold">SageMaker SDK</strong>.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor060"/>Using the SageMaker Autopilot SDK</h1>
			<p>The Amazon SageMaker SDK <a id="_idIndexMarker232"/>includes a simple API for SageMaker Autopilot. You can <a id="_idIndexMarker233"/>find its documentation at <a href="https://sagemaker.readthedocs.io/en/stable/automl.html">https://sagemaker.readthedocs.io/en/stable/automl.html</a>. </p>
			<p>In this section, you'll learn how to use this API to train a model on the same dataset as in the previous section.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor061"/>Launching a job</h2>
			<p>The SageMaker SDK <a id="_idIndexMarker234"/>makes it extremely easy to launch an Autopilot job – just upload your data in S3, and call a single API! Let's see how:</p>
			<ol>
				<li value="1">First, we import the SageMaker SDK:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p></li>
				<li>Then, we download the dataset:<p class="source-code">%%sh</p><p class="source-code">wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip</p><p class="source-code">unzip -o bank-additional.zip</p></li>
				<li>Next, we upload the dataset to S3:<p class="source-code">bucket = sess.default_bucket()                     </p><p class="source-code">prefix = 'sagemaker/DEMO-automl-dm'</p><p class="source-code">s3_input_data = sess.upload_data(path=”./bank-additional/bank-additional-full.csv”, key_prefix=prefix+'input')</p></li>
				<li>We then configure the AutoML job, which only takes one line of code. We define the <strong class="bold">target attribute</strong> (remember, that column is named "y"), and where to store training artifacts. Optionally, we can also set a maximum runtime for the job, a maximum runtime per job, or reduce the number of candidate models that will be tuned. Please note that restricting the job's duration too much is likely to impact its accuracy. For development purposes, this isn't a problem, so let's cap our job at one hour, or 250 tuning jobs (whichever limit it hits first):<p class="source-code">from sagemaker.automl.automl import AutoML</p><p class="source-code">auto_ml_job = AutoML(</p><p class="source-code">    role = sagemaker.get_execution_role(),                                          </p><p class="source-code">    sagemaker_session = sess,                             </p><p class="source-code">    target_attribute_name = 'y',                             </p><p class="source-code">    output_path = </p><p class="source-code">        's3://{}/{}/output'.format(bucket,prefix),</p><p class="source-code">    max_runtime_per_training_job_in_seconds = 600,</p><p class="source-code">    max_candidates = 250,</p><p class="source-code">    total_job_runtime_in_seconds = 3600</p><p class="source-code">)</p></li>
				<li>Next, we launch the Autopilot job, passing it the location of the training set. We turn <a id="_idIndexMarker235"/>logs off (who wants to read hundreds of tuning logs?), and we set the call to non-blocking, as we'd like to query the job status in the next cells:<p class="source-code">auto_ml_job.fit(inputs=s3_input_data, logs=False, wait=False)</p></li>
			</ol>
			<p>The job starts right away. Now let's see how we can monitor its status.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor062"/>Monitoring a job</h2>
			<p>While the <a id="_idIndexMarker236"/>job is running, we can use the <strong class="source-inline">describe_auto_ml_job()</strong> API to monitor its progress:</p>
			<ol>
				<li value="1">For example, the following code will check the job's status every 60 seconds until the data analysis step completes:<p class="source-code">from time import sleep</p><p class="source-code">job = auto_ml_job.describe_auto_ml_job()</p><p class="source-code">job_status = job['AutoMLJobStatus']</p><p class="source-code">job_sec_status = job['AutoMLJobSecondaryStatus']</p><p class="source-code">if job_status not in ('Stopped', 'Failed'):</p><p class="source-code">    while job_status in ('InProgress') and job_sec_status in ('AnalyzingData'):</p><p class="source-code">        sleep(60)</p><p class="source-code">        job = auto_ml_job.describe_auto_ml_job()</p><p class="source-code">        job_status = job['AutoMLJobStatus']</p><p class="source-code">        job_sec_status =   </p><p class="source-code">             job['AutoMLJobSecondaryStatus']</p><p class="source-code">        print (job_status, job_sec_status)</p></li>
				<li>Once <a id="_idIndexMarker237"/>the data analysis is complete, the two autogenerated notebooks are available. We can find their location using the same API:<p class="source-code">job = auto_ml_job.describe_auto_ml_job()</p><p class="source-code">job_candidate_notebook = job['AutoMLJobArtifacts']['CandidateDefinitionNotebookLocation']</p><p class="source-code">job_data_notebook = job['AutoMLJobArtifacts']['DataExplorationNotebookLocation']</p><p class="source-code">print(job_candidate_notebook)</p><p class="source-code">print(job_data_notebook)</p><p>This prints out the S3 paths for the two notebooks:</p><p class="source-code"><strong class="bold">s3://sagemaker-us-east-2-123456789012/sagemaker/DEMO-automl-dm/output/automl-2020-04-24-14-21-16-938/sagemaker-automl-candidates/pr-1-a99cb56acb5945d695c0e74afe8ffe3ddaebafa94f394655ac973432d1/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb</strong></p><p class="source-code"><strong class="bold">s3://sagemaker-us-east-2-123456789012/sagemaker/DEMO-automl-dm/output/automl-2020-04-24-14-21-16-938/sagemaker-automl-candidates/pr-1-a99cb56acb5945d695c0e74afe8ffe3ddaebafa94f394655ac973432d1/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb</strong></p></li>
				<li>Using the AWS CLI, we can copy the two notebooks locally. We'll take a look at them later in this chapter:<p class="source-code">%%sh -s $job_candidate_notebook $job_data_notebook</p><p class="source-code">aws s3 cp $1 .</p><p class="source-code">aws s3 cp $2 .</p></li>
				<li>While <a id="_idIndexMarker238"/>the feature engineering runs, we can wait for completion using the same code snippet as the preceding, looping while <strong class="source-inline">job_sec_status</strong> is equal to <strong class="source-inline">FeatureEngineering</strong>.</li>
				<li>Once model tuning is complete, we can very easily find the best candidate:<p class="source-code">job_best_candidate = auto_ml_job.best_candidate()</p><p class="source-code">print(job_best_candidate['CandidateName'])</p><p class="source-code">print(job_best_candidate['FinalAutoMLJobObjectiveMetric'])</p><p>This prints out the name of the best tuning job, along with its validation accuracy:</p><p class="source-code"><strong class="bold">tuning-job-1-57d7f377bfe54b40b1-030-c4f27053</strong></p><p class="source-code"><strong class="bold">{'MetricName': 'validation:accuracy', 'Value': 0.9197599935531616}</strong></p></li>
			</ol>
			<p>Then, we can deploy and test the model using the SageMaker SDK. We've covered a lot of ground already, so let's save that for future chapters, where we'll revisit this example.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor063"/>Cleaning up</h2>
			<p>SageMaker Autopilot creates many underlying <a id="_idIndexMarker239"/>artifacts, such as dataset splits, pre-processing scripts, pre-processed datasets, and models. If you'd like to clean up completely, the following code snippet will do that. Of course, you could also use the AWS CLI:</p>
			<p class="source-code">import boto3</p>
			<p class="source-code">job_outputs_prefix = '{}/output/{}'.format(prefix, job['AutoMLJobName'])</p>
			<p class="source-code">s3_bucket = boto3.resource('s3').Bucket(bucket)</p>
			<p class="source-code">s3_bucket.objects.filter(Prefix=job_outputs_prefix).delete()</p>
			<p>Now that we know how to train models using both the SageMaker Studio GUI and the SageMaker SDK, let's take a look under the hood. Engineers like to understand how things really work, right?</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor064"/>Diving deep on SageMaker Autopilot</h1>
			<p>In this section, we're going to learn in detail how SageMaker Autopilot processes data and trains <a id="_idIndexMarker240"/>models. If this feels too advanced for now, you're welcome to skip this material. You can always revisit it later once you've gained more experience with the service.</p>
			<p>First, let's look at the artifacts that SageMaker Autopilot produces.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor065"/>The job artifacts</h2>
			<p>Listing our <a id="_idIndexMarker241"/>S3 bucket confirms the existence of many different artifacts:</p>
			<p class="source-code">$ aws s3 ls s3://sagemaker-us-east-2-123456789012/sagemaker/DEMO-autopilot/output/my-first-autopilot-job/</p>
			<p>We can see many new prefixes. Let's figure out what's what:</p>
			<p class="source-code"><strong class="bold">PRE data-processor-models/</strong></p>
			<p class="source-code"><strong class="bold">PRE documentation/</strong></p>
			<p class="source-code"><strong class="bold">PRE preprocessed-data/</strong></p>
			<p class="source-code"><strong class="bold">PRE sagemaker-automl-candidates/</strong></p>
			<p class="source-code"><strong class="bold">PRE transformed-data/</strong></p>
			<p class="source-code"><strong class="bold">PRE tuning/</strong></p>
			<p class="source-code"><strong class="bold">PRE validations/</strong></p>
			<ul>
				<li>The <strong class="source-inline">preprocessed-data/tuning_data</strong> prefix contains the training and validation splits generated from the input dataset. Each split is broken down further into small CSV chunks.</li>
				<li>The <strong class="source-inline">sagemaker-automl-candidates</strong> prefix contains 10 data pre-processing scripts (<strong class="source-inline">dpp[0-9].py</strong>), one for each pipeline. It also contains the code to train them (<strong class="source-inline">trainer.py</strong>) on the input dataset, and the code to process <a id="_idIndexMarker242"/>the input dataset with each one of the 10 resulting models (<strong class="source-inline">sagemaker_serve.py</strong>). Last but not least, it contains the autogenerated notebooks.</li>
				<li>The <strong class="source-inline">data-processor-models</strong> prefix contains the 10 data processing models trained by the <strong class="source-inline">dpp</strong> scripts.</li>
				<li>The <strong class="source-inline">transformed-data</strong> prefix contains the 10 processed versions of the training and validation splits.</li>
				<li>The <strong class="source-inline">tuning</strong> prefix contains the actual models trained during the <strong class="bold">Model Tuning</strong> step.</li>
				<li>The <strong class="source-inline">documentation</strong> prefix contains the explainability report.</li>
			</ul>
			<p>The following diagram summarizes the relationship between these artifacts:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="Images/B17705_03_021.jpg" alt="Figure 3.21 – Summing up the Autopilot process&#13;&#10;" width="1353" height="787"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21 – Summing up the Autopilot process</p>
			<p>In the next <a id="_idIndexMarker243"/>sections, we'll take a look at the two <strong class="bold">autogenerated notebooks</strong>, which <a id="_idIndexMarker244"/>are one of the most important features in SageMaker Autopilot.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor066"/>The data exploration notebook</h2>
			<p>This notebook <a id="_idIndexMarker245"/>is available in Amazon S3 once the data analysis step is complete. </p>
			<p>The first section, seen in the following screenshot, simply displays a sample of the dataset:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="Images/B17705_03_022.jpg" alt="Figure 3.22 – Viewing dataset statistics&#13;&#10;" width="1387" height="773"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22 – Viewing dataset statistics</p>
			<p>Shown in the following screenshot, the second section focuses on column analysis: percentages <a id="_idIndexMarker246"/>of missing values, counts of unique values, and descriptive statistics. For instance, it appears that the <strong class="source-inline">pdays</strong> field has both a maximum value and a median of 999, which looks suspicious. As explained in the previous chapter, 999 is indeed a placeholder value, meaning that a customer has never been contacted before.</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="Images/B17705_03_023.jpg" alt="Figure 3.23 – Viewing dataset statistics&#13;&#10;" width="1420" height="742"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.23 – Viewing dataset statistics</p>
			<p>As you can see, this notebook saves us the trouble of computing these statistics ourselves, and <a id="_idIndexMarker247"/>we can use them to quickly check that the dataset is what we expect. </p>
			<p>Now, let's look at the second notebook. As you will see, it's extremely insightful!</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor067"/>The candidate generation notebook</h2>
			<p>This notebook contains the definition <a id="_idIndexMarker248"/>of the 10 candidate pipelines, and how they're trained. This is a <strong class="bold">runnable notebook</strong>, and <a id="_idIndexMarker249"/>advanced practitioners can use it to replay the AutoML process, and keep refining <a id="_idIndexMarker250"/>their experiment. Please note that this is totally optional! It's perfectly OK to deploy the top model directly and start testing it.</p>
			<p>Having said that, let's run one of the pipelines manually:</p>
			<ol>
				<li value="1">We open the notebook and save a read-write copy by clicking on the <strong class="bold">Import notebook</strong> link in the top-right corner. </li>
				<li>Then, we run the cells in the <strong class="bold">SageMaker Setup</strong> section to import all the required <a id="_idIndexMarker251"/>artifacts and parameters.</li>
				<li>Moving to the <strong class="bold">Candidate Pipelines</strong> section, we create a runner object that will launch jobs for selected candidate pipelines:<p class="source-code">from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate</p><p class="source-code">automl_interactive_runner = AutoMLInteractiveRunner(AUTOML_LOCAL_RUN_CONFIG)</p></li>
				<li>Then, we add the first pipeline (<strong class="source-inline">dpp0</strong>). The notebook tells us: "<em class="italic">This data transformation strategy first transforms 'numeric' features using </em><strong class="source-inline">RobustImputer</strong><em class="italic"> (converts missing values to nan) and 'categorical' features using </em><strong class="source-inline">ThresholdOneHotEncoder</strong><em class="italic">. It merges all the generated features and applies </em><strong class="source-inline">RobustStandardScaler</strong><em class="italic">. The transformed data will be used to tune an XGBoost model</em>". We just need to run the following cell to add it:<p class="source-code">automl_interactive_runner.select_candidate(</p><p class="source-code">    {”data_transformer”: {</p><p class="source-code">        ”name”: ”dpp0”,</p><p class="source-code">        …</p><p class="source-code">    }</p><p class="source-code">)</p><p>If you're curious about the implementation of <strong class="source-inline">RobustImputer</strong> or <strong class="source-inline">ThresholdOneHotEncoder</strong>, hyperlinks take you to the appropriate source file in the <strong class="source-inline">sagemaker_sklearn_extension</strong> <a id="_idIndexMarker252"/>module (https://github.com/aws/sagemaker-scikit-learn-extension/). </p><p>This way, you can understand exactly how data has been processed. As these objects <a id="_idIndexMarker253"/>are based on scikit-learn objects, they should quickly look very familiar. For instance, we can see that <strong class="source-inline">RobustImputer</strong> is built on top of <strong class="source-inline">sklearn.impute.SimpleImputer</strong>, with added functionality. Likewise, <strong class="source-inline">ThresholdOneHotEncoder</strong> is an extension of <strong class="source-inline">sklearn.preprocessing.OneHotEncoder</strong>. </p></li>
				<li>Taking a <a id="_idIndexMarker254"/>quick look at other pipelines, we see different processing strategies and algorithms. You should see the <strong class="bold">Linear Learner</strong> algorithm used in <a id="_idIndexMarker255"/>some pipelines. It's one of the <strong class="bold">built-in algorithms</strong> in SageMaker, and <a id="_idIndexMarker256"/>we'll cover it in the next chapter. You should also see the <strong class="bold">mlp</strong> algorithm, which is based on neural networks.</li>
				<li>Scrolling down, we get to the <strong class="bold">Selected Candidates</strong> section, where we can indeed confirm that we have only selected the first pipeline:<p class="source-code">automl_interactive_runner.display_candidates()</p><p>This is visible in the result here:</p><div id="_idContainer088" class="IMG---Figure"><img src="Images/01.jpg" alt="Figure 3.24 – The results table&#13;&#10;" width="1176" height="130"/></div><p class="figure-caption">Figure 3.24 – The results table</p><p>This also tells us that data will be processed by the <strong class="source-inline">dpp0.py</strong> script and that the model will be trained using the XGBoost algorithm.</p></li>
				<li>Clicking on the <strong class="bold">dpp0</strong> hyperlink opens the script. As expected, we see that it builds a scikit-learn transformer pipeline (not to be confused with the SageMaker pipeline composed of pre-processing and training jobs). Missing values are imputed <a id="_idIndexMarker257"/>in the numerical features, and the categorical features are one-hot encoded. Then, all features are scaled and the labels are encoded:<p class="source-code">numeric_processors = Pipeline(</p><p class="source-code">  steps=[('robustimputer',</p><p class="source-code">         RobustImputer(strategy='constant',fill_values=nan))]</p><p class="source-code">)</p><p class="source-code">categorical_processors = Pipeline(</p><p class="source-code">  steps=[('thresholdonehotencoder', </p><p class="source-code">         ThresholdOneHotEncoder(threshold=301))]</p><p class="source-code">)</p><p class="source-code">column_transformer = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ('numeric_processing', numeric_processors, numeric),</p><p class="source-code">    ('categorical_processing', categorical_processors,   </p><p class="source-code">     categorical)]</p><p class="source-code">)</p><p class="source-code">return Pipeline(steps=[</p><p class="source-code">  ('column_transformer', column_transformer),   </p><p class="source-code">  ('robuststandardscaler', RobustStandardScaler())]</p><p class="source-code">)</p></li>
				<li>Back in the notebook, we launch this script in the <strong class="bold">Run Data Transformation Steps</strong> section:<p class="source-code">automl_interactive_runner.fit_data_transformers(parallel_jobs=7)</p></li>
				<li>This <a id="_idIndexMarker258"/>creates two sequential SageMaker jobs and their artifacts are stored in a new prefix created for the notebook run:<p class="source-code"><strong class="bold">$ aws s3 ls s3://sagemaker-us-east-2-123456789012/sagemaker/DEMO-autopilot/output/my-first-autopilot-job/my-first-a-notebook-run-24-13-17-22/</strong></p><p>The first job trains the <strong class="source-inline">dpp0</strong> transformers on the input dataset.</p><p>The second <a id="_idIndexMarker259"/>job processes the input dataset with the resulting model. For the record, this job uses the SageMaker <strong class="bold">Batch Transform</strong> feature, which will be covered in a later chapter.</p></li>
				<li>Going back to SageMaker Studio, let's find out more about these two jobs. Starting from the <strong class="bold">SageMaker components and registries</strong> icon on the left, we select <strong class="bold">Unassigned trial components</strong>, and we see our two jobs there: <strong class="source-inline">my-first-a-notebook-run-24-13-17-22-dpp0-train-24-13-38-38-aws-training-job</strong> and <strong class="source-inline">my-first-a-notebook-run-24-13-17-22-dpp0-transform-24-13-38-38-aws-transform-job</strong>. </li>
				<li>Double-clicking a job name opens the <strong class="bold">Open in trial details</strong> window, as shown in the following screenshot. It tells us everything there is to know about the job: the parameters, location of artifacts, and more:</li>
			</ol>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="Images/B17705_03_025.jpg" alt="Figure 3.25 – Describing a trial&#13;&#10;" width="1175" height="567"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25 – Describing a trial</p>
			<p>Once data processing is <a id="_idIndexMarker260"/>complete, the notebook <a id="_idIndexMarker261"/>proceeds with <strong class="bold">automatic model tuning</strong> and <strong class="bold">model deployment</strong>. We haven't yet discussed these topics, so let's stop there for now. I encourage <a id="_idIndexMarker262"/>you to go through the rest of the notebook once you're comfortable with them.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor068"/>Summary</h1>
			<p>As you can see, Amazon SageMaker Autopilot makes it easy to build, train, and optimize machine learning models for beginners and advanced users alike.</p>
			<p>In this chapter, you learned about the different steps of an Autopilot job, and what they mean from a machine learning perspective. You also learned how to use both the SageMaker Studio GUI and the SageMaker SDK to build a classification model with minimal coding. Then, we dived deep into the autogenerated notebooks, which give you full control and transparency over the modeling processing. In particular, you learned how to run the candidate generation notebook manually to replay all the steps involved. </p>
			<p>In the next chapter, you will learn how to use the built-in algorithms in Amazon SageMaker to train models for a variety of machine learning problems.</p>
		</div>
	</div></body></html>
["```py\nShall I take an umbrella with me?\n|--- Chance of Rainy <= 0.6\n|    |--- UV Index <= 7.0\n|    |    |--- class: False\n|    |--- UV Index >  7.0\n|    |    |--- class: True\n|--- Chance of Rainy >  0.6\n|    |--- class: True\n```", "```py\nfrom sklearn import datasets\nimport pandas as pd\niris = datasets.load_iris()\n```", "```py\ndir(iris)\n```", "```py\nprint(iris.DESCR)\n```", "```py\n.. _iris_dataset:\n Iris plants dataset\n --------------------\nData Set Characteristics:\n :Number of Instances: 150 (50 in each of three classes)\n  :Number of Attributes: 4 numeric, predictive attributes and the class\n   :Attribute Information:        \n- sepal length in cm\n- sepal width in cm\n- petal length in cm\n- petal width in cm\n- class:\n    - Iris-Setosa\n    - Iris-Versicolor\n    - Iris-Virginica\n:Summary Statistics:\n    ============== ==== ==== ======= ===== ====================\n                Min  Max   Mean    SD     Class   Correlation\n    ============== ==== ==== ======= ===== ====================\nsepal length:   4.3  7.9   5.84   0.83    0.7826\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n:Missing Attribute Values: None\n:Class Distribution: 33.3% for each of 3 classes.\n\n:Creator: R.A. Fisher\n```", "```py\narray([[5.1, 3.5, 1.4, 0.2], [4.9, 3\\. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5\\. , 3.6, 1.4, 0.2], [5.4, 3.9, 1.7, 0.4], [4.6, 3.4, 1.4, 0.3], [5\\. , 3.4, 1.5, 0.2]])\n```", "```py\ndf = pd.DataFrame(\n    iris.data,\n    columns=iris.feature_names\n)\n\ndf['target'] = pd.Series(\n iris.target\n)\n```", "```py\ndf['target_names'] = df['target'].apply(lambda y: iris.target_names[y])\n```", "```py\n# print(df.sample(n=6))\ndf.sample(n=6)\n```", "```py\ndf.sample(n=6, random_state=42) \n```", "```py\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.3)\n```", "```py\nx_train = df_train[iris.feature_names]\nx_test = df_test[iris.feature_names]\n\ny_train = df_train['target']\ny_test = df_test['target']\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\n\n# It is common to call the classifier instance clf\nclf = DecisionTreeClassifier()\n```", "```py\nclf.fit(x_train, y_train)\n```", "```py\n# If y_test is our truth, then let's call our predictions y_test_pred\ny_test_pred = clf.predict(x_test)\n```", "```py\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_test_pred)\n```", "```py\npd.DataFrame(\n  {\n    'feature_names': iris.feature_names,\n    'feature_importances': clf.feature_importances_\n  }\n).sort_values(\n  'feature_importances', ascending=False\n).set_index('feature_names')\n```", "```py\nfrom sklearn.tree import export_text\nprint(\n  export_text(clf, feature_names=iris.feature_names, spacing=3, decimals=1)\n) \n```", "```py\n|--- petal width (cm) <= 0.8\n| |--- class: 0\n|--- petal width (cm) > 0.8\n| |--- petal width (cm) <= 1.8\n| | |--- petal length (cm) <= 5.3\n| | | |--- sepal length (cm) <= 5.0\n| | | | |--- class: 2\n| | | |--- sepal length (cm) > 5.0\n| | | | |--- class: 1\n| | |--- petal length (cm) > 5.3\n| | | |--- class: 2\n| |--- petal width (cm) > 1.8\n| | |--- class: 2\n```", "```py\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# A list to store the score from each iteration\naccuracy_scores = []\n```", "```py\nfor _ in range(100):\n\n    # At each iteration we freshly split our data\n    df_train, df_test = train_test_split(df, test_size=0.3) \n    x_train = df_train[iris.feature_names]\n    x_test = df_test[iris.feature_names]\n\n    y_train = df_train['target']\n    y_test = df_test['target']\n\n    # We then create a new classifier\n    clf = DecisionTreeClassifier()\n\n    # And use it for training and prediction\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n\n    # Finally, we append the score to our list\n    accuracy_scores.append(round(accuracy_score(y_test, y_pred), 3))\n\n# Better convert accuracy_scores from a list into a series\n# Pandas series provides statistical methods to use later\naccuracy_scores = pd.Series(accuracy_scores)\n```", "```py\naccuracy_scores.plot(\n    title='Distribution of classifier accuracy',\n    kind='box',\n)\n\nprint(\n    'Average Score: {:.3} [5th percentile: {:.3} & 95th percentile: {:.3}]'.format(\n        accuracy_scores.mean(),\n        accuracy_scores.quantile(.05),\n        accuracy_scores.quantile(.95),\n    )\n)\n```", "```py\nimport pandas as pd\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracy_scores = []\n\n# Create a shuffle split instance\nrs = ShuffleSplit(n_splits=100, test_size=0.3)\n\n# We now get 100 pairs of indices \nfor train_index, test_index in rs.split(df):\n\n x_train = df.loc[train_index, iris.feature_names]\n x_test = df.loc[test_index, iris.feature_names]\n\n y_train = df.loc[train_index, 'target']\n y_test = df.loc[test_index, 'target']\n\n clf = DecisionTreeClassifier()\n\n clf.fit(x_train, y_train)\n y_pred = clf.predict(x_test)\n\n accuracy_scores.append(round(accuracy_score(y_test, y_pred), 3))\n\naccuracy_scores = pd.Series(accuracy_scores)\n```", "```py\nimport pandas as pd\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_validate\n\nclf = DecisionTreeClassifier()\nrs = ShuffleSplit(n_splits=100, test_size=0.3)\n\nx = df[iris.feature_names]\ny = df['target']\n\ncv_results = cross_validate(\n    clf, x, y, cv=rs, scoring='accuracy'\n)\n\naccuracy_scores = pd.Series(cv_results['test_score'])\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.25)\n\nx_train = df_train[iris.feature_names]\nx_test = df_test[iris.feature_names]\n\ny_train = df_train['target']\ny_test = df_test['target']\n```", "```py\nimport pandas as pd\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_validate\n\nfor max_depth in [1, 2, 3, 4]:\n\n    # We initialize a new classifier each iteration with different max_depth\n    clf = DecisionTreeClassifier(max_depth=max_depth)\n    # We also initialize our shuffle splitter\n    rs = ShuffleSplit(n_splits=20, test_size=0.25)\n\n    cv_results = cross_validate(\n        clf, x_train, y_train, cv=rs, scoring='accuracy'\n    )\n    accuracy_scores = pd.Series(cv_results['test_score'])\n\nprint(\n        '@ max_depth = {}: accuracy_scores: {}~{}'.format(\n            max_depth, \n            accuracy_scores.quantile(.1).round(3), \n            accuracy_scores.quantile(.9).round(3)\n        )\n    )\n```", "```py\n@ max_depth = 1: accuracy_scores: 0.532~0.646\n@ max_depth = 2: accuracy_scores: 0.925~1.0\n@ max_depth = 3: accuracy_scores: 0.929~1.0\n@ max_depth = 4: accuracy_scores: 0.929~1.0\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_decision_boundary(clf, x, y):\n\n feature_names = x.columns\n x, y = x.values, y.values\n\n x_min, x_max = x[:,0].min(), x[:,0].max()\n y_min, y_max = x[:,1].min(), x[:,1].max()\n\n step = 0.02\n\n xx, yy = np.meshgrid(\n np.arange(x_min, x_max, step),\n np.arange(y_min, y_max, step)\n )\n Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n Z = Z.reshape(xx.shape)\n\n plt.figure(figsize=(12,8))\n plt.contourf(xx, yy, Z, cmap='Paired_r', alpha=0.25)\n plt.contour(xx, yy, Z, colors='k', linewidths=0.7)\n plt.scatter(x[:,0], x[:,1], c=y, edgecolors='k')\n plt.title(\"Tree's Decision Boundaries\")\n plt.xlabel(feature_names[0])\n plt.ylabel(feature_names[1])\n```", "```py\nx = df[['petal width (cm)', 'petal length (cm)']]\ny = df['target']\n\nclf = DecisionTreeClassifier(max_depth=3)\nclf.fit(x, y)\n\nplot_decision_boundary(clf, x, y)\n```", "```py\ndf['petal length x width (cm)'] = df['petal length (cm)'] * df['petal width (cm)']\ndf['sepal length x width (cm)'] = df['sepal length (cm)'] * df['sepal width (cm)']\n```", "```py\nfig, ax = plt.subplots(1, 1, figsize=(12, 6));\n\nh_label = 'petal length x width (cm)'\nv_label = 'sepal length x width (cm)'\n\nfor c in df['target'].value_counts().index.tolist():\n    df[df['target'] == c].plot(\n        title='Class distribution vs the newly derived features',\n        kind='scatter',\nx=h_label,\ny=v_label,\ncolor=['r', 'g', 'b'][c], # Each class different color\nmarker=f'${c}$', # Use class id as marker\ns=64,\n        alpha=0.5,\n        ax=ax,\n    )\n\nfig.show()\n```", "```py\nfeatures_orig = iris.feature_names\nfeatures_new = ['petal length x width (cm)', 'sepal length x width (cm)']\n\naccuracy_scores_orig = []\naccuracy_scores_new = []\n\nfor _ in range(500):\n\n    df_train, df_test = train_test_split(df, test_size=0.3)\n\nx_train_orig = df_train[features_orig]\nx_test_orig = df_test[features_orig]\n\nx_train_new = df_train[features_new]\nx_test_new = df_test[features_new]\n\n     y_train = df_train['target']\ny_test = df_test['target']\n\nclf_orig = DecisionTreeClassifier(max_depth=2)\nclf_new = DecisionTreeClassifier(max_depth=2)\n\n     clf_orig.fit(x_train_orig, y_train)\nclf_new.fit(x_train_new, y_train)\n\ny_pred_orig = clf_orig.predict(x_test_orig)\ny_pred_new = clf_new.predict(x_test_new)\n\naccuracy_scores_orig.append(round(accuracy_score(y_test, y_pred_orig), \n                                       3))\naccuracy_scores_new.append(round(accuracy_score(y_test, y_pred_new), \n                                      3))\n\naccuracy_scores_orig = pd.Series(accuracy_scores_orig)\naccuracy_scores_new = pd.Series(accuracy_scores_new)\n```", "```py\nfig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True);\n\naccuracy_scores_orig.plot(\n    title='Distribution of classifier accuracy [Original Features]',\n    kind='box',\ngrid=True,\nax=axs[0]\n)\n\naccuracy_scores_new.plot(\ntitle='Distribution of classifier accuracy [New Features]',\nkind='box',\ngrid=True,\nax=axs[1]\n)\n\nfig.show()\n```", "```py\n# It's customary to call numpy np\nimport numpy as np\n\n# We need 200 samples from each\nn = 200\n\n# From each population we get 200 male and 200 female samples\nheight_pop1_f = np.random.normal(loc=155, scale=4, size=n)\nheight_pop1_m = np.random.normal(loc=175, scale=5, size=n)\nheight_pop2_f = np.random.normal(loc=165, scale=15, size=n)\nheight_pop2_m = np.random.normal(loc=185, scale=12, size=n)\n```", "```py\n# We group all females together and all males together\nheight_f = np.concatenate([height_pop1_f, height_pop2_f])\nheight_m = np.concatenate([height_pop1_m, height_pop2_m])\n```", "```py\ndf_height = pd.DataFrame(\n    {\n        'Gender': [1 for i in range(height_f.size)] + \n                   [2 for i in range(height_m.size)],\n        'Height': np.concatenate((height_f, height_m))\n    }\n)\n```", "```py\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\n\ndf_height[df_height['Gender'] == 1]['Height'].plot(\n    label='Female', kind='hist', \n    bins=10, alpha=0.7, ax=ax\n)\ndf_height[df_height['Gender'] == 2]['Height'].plot(\n    label='Male', kind='hist', \n    bins=10, alpha=0.7, ax=ax\n)\n\nax.legend()\n\nfig.show()\n```", "```py\ndf_height.groupby('Gender')[['Height']].agg([np.mean, np.median]).round(1)\n```", "```py\ndf_train, df_test = train_test_split(df_height, test_size=0.3)\nx_train, x_test = df_train[['Gender']], df_test[['Gender']]\ny_train, y_test = df_train['Height'], df_test['Height']\n```", "```py\nfrom sklearn.tree import export_text\nfrom sklearn.tree import DecisionTreeRegressor\n\nfor criterion in ['mse', 'mae']:\n    rgrsr = DecisionTreeRegressor(criterion=criterion)\n    rgrsr.fit(x_train, y_train)\n\n    print(f'criterion={criterion}:\\n')\n    print(export_text(rgrsr, feature_names=['Gender'], spacing=3, decimals=1))\n```", "```py\ncriterion=mse:\n\n|--- Gender <= 1.5\n|    |--- value: [160.2]\n|--- Gender > 1.5\n|    |--- value: [180.8]\n\ncriterion=mae:\n\n|--- Gender <= 1.5\n|    |--- value: [157.5]\n|--- Gender > 1.5\n|    |--- value: [178.6]\n```", "```py\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ny_test_pred = rgrsr.predict(x_test)\nprint('MSE:', mean_squared_error(y_test, y_test_pred))\nprint('MAE:', mean_absolute_error(y_test, y_test_pred))\n```", "```py\nrgrsr = DecisionTreeRegressor(criterion='mse')\nsample_weight = y_train.apply(lambda h: 10 if h > 150 else 1)\nrgrsr.fit(x_train, y_train, sample_weight=sample_weight)\n```", "```py\nsample_weight = y_train.apply(lambda h: 10 if h <= 150 else 1)\n```", "```py\nEmphasis on \"below 150\":\n\n|--- Gender <= 1.5\n|    |--- value: [150.7]\n|--- Gender > 1.5\n|    |--- value: [179.2]\n\nEmphasis on \"above 150\":\n\n|--- Gender <= 1.5\n|    |--- value: [162.4]\n|--- Gender > 1.5\n|    |--- value: [180.2]\n\n```"]
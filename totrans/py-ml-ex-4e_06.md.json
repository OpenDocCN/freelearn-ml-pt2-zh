["```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> def sigmoid(z):\n        return 1.0 / (1 + np.exp(-z))\n>>> z = np.linspace(-8, 8, 1000)\n>>> y = sigmoid(z)\n>>> plt.plot(z, y)\n>>> plt.xlabel('z')\n>>> plt.ylabel('y(z)')\n>>> plt.title('logistic')\n>>> plt.grid()\n>>> plt.show() \n```", "```py\n>>> def tanh(z):\n        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n>>> z = np.linspace(-8, 8, 1000)\n>>> y = tanh(z)\n>>> plt.plot(z, y)\n>>> plt.xlabel('z')\n>>> plt.ylabel('y(z)')\n>>> plt.title('tanh')\n>>> plt.grid()\n>>> plt.show() \n```", "```py\n>>> relu(z):\n        return np.maximum(np.zeros_like(z), z)\n>>> z = np.linspace(-8, 8, 1000)\n>>> y = relu(z)\n>>> plt.plot(z, y)\n>>> plt.xlabel('z')\n>>> plt.ylabel('y(z)')\n>>> plt.title('relu')\n>>> plt.grid()\n>>> plt.show() \n```", "```py\n>>> def sigmoid_derivative(z):\n...     return sigmoid(z) * (1.0 - sigmoid(z)) \n```", "```py\n>>> def train(X, y, n_hidden, learning_rate, n_iter):\n...     m, n_input = X.shape\n...     W1 = np.random.randn(n_input, n_hidden)\n...     b1 = np.zeros((1, n_hidden))\n...     W2 = np.random.randn(n_hidden, 1)\n...     b2 = np.zeros((1, 1))\n...     for i in range(1, n_iter+1):\n...         Z2 = np.matmul(X, W1) + b1\n...         A2 = sigmoid(Z2)\n...         Z3 = np.matmul(A2, W2) + b2\n...         A3 = Z3\n...\n...         dZ3 = A3 - y\n...         dW2 = np.matmul(A2.T, dZ3)\n...         db2 = np.sum(dZ3, axis=0, keepdims=True)\n...\n...         dZ2 = np.matmul(dZ3, W2.T) * sigmoid_derivative(Z2)\n...         dW1 = np.matmul(X.T, dZ2)\n...         db1 = np.sum(dZ2, axis=0)\n...\n...         W2 = W2 - learning_rate * dW2 / m\n...         b2 = b2 - learning_rate * db2 / m\n...         W1 = W1 - learning_rate * dW1 / m\n...         b1 = b1 - learning_rate * db1 / m\n...\n...         if i % 100 == 0:\n...             cost = np.mean((y - A3) ** 2)\n...             print('Iteration %i, training loss: %f' %\n                                                  (i, cost))\n...     model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n...     return model \n```", "```py\n>>> from sklearn import datasets\n>>> housing = datasets.fetch_california_housing()\n>>> num_test = 10 # the last 10 samples as testing set\n>>> from sklearn import preprocessing\n>>> scaler = preprocessing.StandardScaler()\n>>> X_train = housing.data[:-num_test, :]\n>>> X_train = scaler.fit_transform(X_train)\n>>> y_train = housing.target[:-num_test].reshape(-1, 1)\n>>> X_test = housing.data[-num_test:, :]\n>>> X_test = scaler.transform(X_test)\n>>> y_test = housing.target[-num_test:] \n```", "```py\n>>> n_hidden = 20\n>>> learning_rate = 0.1\n>>> n_iter = 2000\n>>> model = train(X_train, y_train, n_hidden, learning_rate, n_iter)\nIteration 100, training loss: 0.557636\nIteration 200, training loss: 0.519375\nIteration 300, training loss: 0.501025\nIteration 400, training loss: 0.487536\nIteration 500, training loss: 0.476553\nIteration 600, training loss: 0.467207\nIteration 700, training loss: 0.459076\nIteration 800, training loss: 0.451934\nIteration 900, training loss: 0.445621\nIteration 1000, training loss: 0.440013\nIteration 1100, training loss: 0.435024\nIteration 1200, training loss: 0.430558\nIteration 1300, training loss: 0.426541\nIteration 1400, training loss: 0.422920\nIteration 1500, training loss: 0.419653\nIteration 1600, training loss: 0.416706\nIteration 1700, training loss: 0.414049\nIteration 1800, training loss: 0.411657\nIteration 1900, training loss: 0.409502\nIteration 2000, training loss: 0.407555 \n```", "```py\n>>> def predict(x, model):\n...     W1 = model['W1']\n...     b1 = model['b1']\n...     W2 = model['W2']\n...     b2 = model['b2']\n...     A2 = sigmoid(np.matmul(x, W1) + b1)\n...     A3 = np.matmul(A2, W2) + b2\n...     return A3 \n```", "```py\n>>> predictions = predict(X_test, model) \n```", "```py\n>>> print(predictions[:, 0])\n[1.11805681 1.1387508  1.06071523 0.81930286 1.21311999 0.6199933 0.92885765 0.81967297 0.90882797 0.87857088]\n>>> print(y_test)\n[1.12  1.072 1.156 0.983 1.168 0.781 0.771 0.923 0.847 0.894] \n```", "```py\n>>> from sklearn.neural_network import MLPRegressor\n>>> nn_scikit = MLPRegressor(hidden_layer_sizes=(16, 8),\n                         activation='relu',\n                         solver='adam',\n                         learning_rate_init=0.001,\n                         random_state=42,\n                         max_iter=2000) \n```", "```py\n>>> nn_scikit.fit(X_train, y_train.ravel())\n>>> predictions = nn_scikit.predict(X_test)\n>>> print(predictions)\n[1.19968791 1.2725324  1.30448323 0.88688675 1.18623612 0.72605956 0.87409406 0.85671201 0.93423154 0.94196305] \n```", "```py\n>>> from sklearn.metrics import mean_squared_error\n>>> print(mean_squared_error(y_test, predictions))\n0.010613171947751738 \n```", "```py\n    >>> import tensorflow as tf\n    >>> from tensorflow import keras\n    >>> tf.random.set_seed(42) \n    ```", "```py\n    >>> model = keras.Sequential([\n    ...     keras.layers.Dense(units=16, activation='relu'),\n    ...     keras.layers.Dense(units=8, activation='relu'),\n    ...     keras.layers.Dense(units=1)\n    ... ]) \n    ```", "```py\n    >>> model.compile(loss='mean_squared_error',\n    ...               optimizer=tf.keras.optimizers.Adam(0.01)) \n    ```", "```py\n    >>> model.fit(X_train, y_train, epochs=300)\n    Train on 496 samples\n    Epoch 1/300\n    645/645 [==============================] - 1s 1ms/step - loss: 0.6494\n    Epoch 2/300\n    645/645 [==============================] - 1s 1ms/step - loss: 0.3827\n    Epoch 3/300\n    645/645 [==============================] - 1s 1ms/step - loss: 0.3700\n    â€¦â€¦\n    â€¦â€¦\n    Epoch 298/300\n    645/645 [==============================] - 1s 1ms/step - loss: 0.2724\n    Epoch 299/300\n    645/645 [==============================] - 1s 1ms/step - loss: 0.2735\n    Epoch 300/300\n    645/645 [==============================] - 1s 1ms/step - loss: 0.2730\n    1/1 [==============================] - 0s 82ms/step \n    ```", "```py\n    >>> predictions = model.predict(X_test)[:, 0]\n    >>> print(predictions)\n    [1.2387774  1.2480505  1.229521   0.8988129  1.1932802  0.75052583 0.75052583 0.88086814 0.9921638  0.9107932 ]\n    >>> print(mean_squared_error(y_test, predictions))\n    0.008271122735361234 \n    ```", "```py\n    >>> import torch\n    >>> import torch.nn as nn\n    >>> torch.manual_seed(42) \n    ```", "```py\n    >>> model = nn.Sequential(nn.Linear(X_train.shape[1], 16),\n                          nn.ReLU(),\n                          nn.Linear(16, 8),\n                          nn.ReLU(),\n                          nn.Linear(8, 1)) \n    ```", "```py\n    >>> loss_function = nn.MSELoss()\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n    ```", "```py\n    >>> X_train_torch = torch.from_numpy(X_train.astype(np.float32))\n    >>> y_train_torch = torch.from_numpy(y_train.astype(np.float32)) \n    ```", "```py\n    >>> def train_step(model, X_train, y_train, loss_function, optimizer):\n            pred_train = model(X_train)\n            loss = loss_function(pred_train, y_train)\n\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            return loss.item() \n    ```", "```py\n    >>> for epoch in range(500):\n            loss = train_step(model, X_train_torch, y_train_torch,\n                              loss_function, optimizer)      \n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch} - loss: {loss}\")\n    Epoch 0 - loss: 4.908532619476318\n    Epoch 100 - loss: 0.5002815127372742\n    Epoch 200 - loss: 0.40820521116256714\n    Epoch 300 - loss: 0.3870624303817749\n    Epoch 400 - loss: 0.3720889091491699 \n    ```", "```py\n    >>> X_test_torch = torch.from_numpy(X_test.astype(np.float32))\n    >>> predictions = model(X_test_torch).detach().numpy()[:, 0]\n    >>> print(predictions)\n    [1.171479  1.130001  1.1055213 0.8627995 1.0910968 0.6725116 0.8869568 0.8009699 0.8529027 0.8760005]\n    >>> print(mean_squared_error(y_test, predictions))\n    0.006939044434639928 \n    ```", "```py\n>>> model_with_dropout = nn.Sequential(nn.Linear(X_train.shape[1], 16),\n                                   nn.ReLU(),\n                                   nn.Dropout(0.1),\n                                   nn.Linear(16, 8),\n                                   nn.ReLU(),\n                                   nn.Linear(8, 1)) \n```", "```py\n    >>> optimizer = torch.optim.Adam(model_with_dropout.parameters(), lr=0.01) \n    ```", "```py\n    >>> for epoch in range(1000):\n            loss = train_step(model_with_dropout, X_train_torch, y_train_torch,\n                              loss_function, optimizer)\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch} - loss: {loss}\")\n    Epoch 0 - loss: 4.921249866485596\n    Epoch 100 - loss: 0.5313398838043213\n    Epoch 200 - loss: 0.4458008408546448\n    Epoch 300 - loss: 0.4264270067214966\n    Epoch 400 - loss: 0.4085545539855957\n    Epoch 500 - loss: 0.3640516400337219\n    Epoch 600 - loss: 0.35677382349967957\n    Epoch 700 - loss: 0.35208994150161743\n    Epoch 800 - loss: 0.34980857372283936\n    Epoch 900 - loss: 0.3431631028652191 \n    ```", "```py\n    >>> model_with_dropout.eval()\n    >>> predictions = model_with_dropout (X_test_torch).detach().numpy()[:, 0]\n    >>> print(mean_squared_error(y_test, predictions))\n     0.005699420832357341 \n    ```", "```py\n    >>> model = nn.Sequential(nn.Linear(X_train.shape[1], 16),\n                          nn.ReLU(),\n                          nn.Linear(16, 8),\n                          nn.ReLU(),\n                          nn.Linear(8, 1))\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n    ```", "```py\n    >>> patience = 100\n    >>> epochs_no_improve = 0\n    >>> best_test_loss = float('inf') \n    ```", "```py\n    >>> import copy\n    >>> best_model = model\n    >>> for epoch in range(500):\n            loss = train_step(model, X_train_torch, y_train_torch,\n                              loss_function, optimizer)      \n            predictions = model(X_test_torch).detach().numpy()[:, 0]\n            test_loss = mean_squared_error(y_test, predictions)\n            if test_loss > best_test_loss:\n                epochs_no_improve += 1\n                if epochs_no_improve > patience:\n                    print(f\"Early stopped at epoch {epoch}\")\n                    break\n            else:\n                epochs_no_improve = 0\n                best_test_loss = test_loss\n                best_model = copy.deepcopy(model)\n    Early stopped at epoch 224 \n    ```", "```py\n    >>> predictions = best_model(X_test_torch).detach().numpy()[:, 0]\n    >>> print(mean_squared_error(y_test, predictions))\n    0.005459465255681108 \n    ```", "```py\n    >>> data_raw = pd.read_csv('19900101_20230630.csv', index_col='Date')\n    >>> data = generate_features(data_raw) \n    ```", "```py\n    >>> start_train = '1990-01-01'\n    >>> end_train = '2022-12-31'\n    >>> start_test = '2023-01-01'\n    >>> end_test = '2023-06-30'\n    >>> data_train = data.loc[start_train:end_train]\n    >>> X_train = data_train.drop('close', axis=1).values\n    >>> y_train = data_train['close'].values\n    >>> data_test = data.loc[start_test:end_test]\n    >>> X_test = data_test.drop('close', axis=1).values\n    >>> y_test = data_test['close'].values \n    ```", "```py\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> scaler = StandardScaler() \n    ```", "```py\n>>> X_scaled_train = scaler.fit_transform(X_train)\n>>> X_scaled_test = scaler.transform(X_test) \n```", "```py\n    >>> X_train_torch = torch.from_numpy(X_scaled_train.astype(np.float32))\n    >>> X_test_torch = torch.from_numpy(X_scaled_test.astype(np.float32))\n    >>> y_train = y_train.reshape(y_train.shape[0], 1)\n    >>> y_train_torch = torch.from_numpy(y_train.astype(np.float32)) \n    ```", "```py\n    >>> torch.manual_seed(42)\n    >>> model = nn.Sequential(nn.Linear(X_train.shape[1], 32),\n                              nn.ReLU(),\n                              nn.Linear(32, 1)) \n    ```", "```py\n    >>> loss_function = nn.MSELoss()\n    >>> optimizer = torch.optim.Adam(model.parameters(), lr=0.3) \n    ```", "```py\n    >>> for epoch in range(1000):\n            loss = train_step(model, X_train_torch, y_train_torch,\n                              loss_function, optimizer)\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch} - loss: {loss}\")\n    Epoch 0 - loss: 24823446.0\n    Epoch 100 - loss: 189974.171875\n    Epoch 200 - loss: 52102.01171875\n    Epoch 300 - loss: 17849.333984375\n    Epoch 400 - loss: 8928.6689453125\n    Epoch 500 - loss: 6497.75927734375\n    Epoch 600 - loss: 5670.634765625\n    Epoch 700 - loss: 5265.48828125\n    Epoch 800 - loss: 5017.7021484375\n    Epoch 900 - loss: 4834.28466796875 \n    ```", "```py\n    >>> predictions = model(X_test_torch).detach().numpy()[:, 0]\n    >>> from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n    >>> print(f'MSE: {mean_squared_error(y_test, predictions):.3f}')\n    MSE: 30051.643\n    >>> print(f'MAE: {mean_absolute_error(y_test, predictions):.3f}')\n    MAE: 137.096\n    >>> print(f'R^2: {r2_score(y_test, predictions):.3f}')\n    R^2: 0.954 \n    ```", "```py\n    >>> from torch.utils.tensorboard import SummaryWriter \n    ```", "```py\n    >>> hparams_config = {\n            \"hidden_size\": [16, 32],\n            \"epochs\": [1000, 3000],\n            \"lr\": [0.1, 0.3],\n        } \n    ```", "```py\n    >>> def train_validate_model(hidden_size, epochs, lr):\n            model = nn.Sequential(nn.Linear(X_train.shape[1], hidden_size),\n                                      nn.ReLU(),\n                                      nn.Linear(hidden_size, 1))\n            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n            # Create the TensorBoard writer\n            writer_path = f\"runs/{experiment_num}/{hidden_size}/{epochs}/{lr}\"\n            writer = SummaryWriter(log_dir=writer_path)\n            for epoch in range(epochs):\n                loss = train_step(model, X_train_torch, y_train_torch, loss_function, \n                                  optimizer)\n                predictions = model(X_test_torch).detach().numpy()[:, 0]\n                test_mse = mean_squared_error(y_test, predictions)\n                writer.add_scalar(\n                    tag=\"train loss\",\n                    scalar_value=loss,\n                    global_step=epoch,\n                )\n                writer.add_scalar(\n                    tag=\"test loss\",\n                    scalar_value=test_mse,\n                    global_step=epoch,\n                )\n            test_r2 = r2_score(y_test, predictions)\n            print(f'R^2: {test_r2:.3f}\\n')\n            # Add the hyperparameters and metrics to TensorBoard\n            writer.add_hparams(\n                {\n                    \"hidden_size\": hidden_size,\n                    \"epochs\": epochs,\n                    \"lr\": lr,\n                },\n                {\n                    \"test MSE\": test_mse,\n                    \"test R^2\": test_r2,\n                },\n            ) \n    ```", "```py\n    >>> torch.manual_seed(42)\n    >>> experiment_num = 0\n    >>> for hidden_size in hparams_config[\"hidden_size\"]:\n            for epochs in hparams_config[\"epochs\"]:\n                for lr in hparams_config[\"lr\"]:\n                    experiment_num += 1\n                    print(f\"Experiment {experiment_num}: hidden_size = {hidden_size}, \n                            epochs = {epochs}, lr = {lr}\")\n                    train_validate_model(hidden_size, epochs, lr) \n    ```", "```py\nExperiment 1: hidden_size = 16, epochs = 1000, lr = 0.1\nR^2: 0.771\nExperiment 2: hidden_size = 16, epochs = 1000, lr = 0.3\nR^2: 0.952\nExperiment 3: hidden_size = 16, epochs = 3000, lr = 0.1\nR^2: 0.969\nExperiment 4: hidden_size = 16, epochs = 3000, lr = 0.3\nR^2: 0.977\nExperiment 5: hidden_size = 32, epochs = 1000, lr = 0.1\nR^2: 0.877\nExperiment 6: hidden_size = 32, epochs = 1000, lr = 0.3\nR^2: 0.957\nExperiment 7: hidden_size = 32, epochs = 3000, lr = 0.1\nR^2: 0.970\nExperiment 8: hidden_size = 32, epochs = 3000, lr = 0.3\nR^2: 0.959 \n```", "```py\n    tensorboard --host 0.0.0.0 --logdir runs \n    ```", "```py\n    >>> hidden_size = 16\n    >>> epochs = 3000\n    >>> lr = 0.3\n    >>> best_model = nn.Sequential(nn.Linear(X_train.shape[1], hidden_size),\n                               nn.ReLU(),\n                               nn.Linear(hidden_size, 1))\n    >>> optimizer = torch.optim.Adam(best_model.parameters(), lr=lr)\n    >>> for epoch in range(epochs):\n        train_step(best_model, X_train_torch, y_train_torch, loss_function,\n                   optimizer\n    >>> predictions = best_model(X_test_torch).detach().numpy()[:, 0] \n    ```", "```py\n    >>> import matplotlib.pyplot as plt\n    >>> plt.rc('xtick', labelsize=10)\n    >>> plt.rc('ytick', labelsize=10)\n    >>> plt.plot(data_test.index, y_test, c='k')\n    >>> plt.plot(data_test.index, predictions, c='b')\n    >>> plt.xticks(range(0, 130, 10), rotation=60)\n    >>> plt.xlabel('Date' , fontsize=10)\n    >>> plt.ylabel('Close price' , fontsize=10)\n    >>> plt.legend(['Truth', 'Neural network'] , fontsize=10)\n    >>> plt.show() \n    ```"]
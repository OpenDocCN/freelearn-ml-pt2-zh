<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">High-Performance Machine Learning – NumPy</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>NumPy basics</li>
<li>Loading the iris dataset</li>
<li>Viewing the iris dataset</li>
<li>Viewing the iris dataset with pandas</li>
<li>Plotting with NumPy and matplotlib</li>
<li>A minimal machine learning recipe – SVM classification</li>
<li>Introducing cross-validation</li>
<li>Putting it all together</li>
<li>Machine learning overview – classification versus regression</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll learn how to make predictions with scikit-learn. Machine learning emphasizes on measuring the ability to predict, and with scikit-learn we will predict accurately and quickly.</p>
<p>We will examine the <kbd>iris</kbd> dataset, which consists of measurements of three types of Iris flowers: <em>Iris S</em><em>etosa</em>, <em>Iris V</em><em>ersicolor</em>, and <em>Iris Virginica</em>.</p>
<p>To measure the strength of the predictions, we will:</p>
<ul>
<li>Save some data for testing</li>
<li>Build a model using only training data</li>
<li>Measure the predictive power on the test set</li>
</ul>
<p>The prediction—one of three flower types is categorical. This type of problem is called a <strong>classification problem</strong>.</p>
<p>Informally, classification asks, <em>Is it an apple or an orange?</em> Contrast this with machine learning regression, which asks, <em>How many apples?</em> By the way, the answer can be <em>4.5 apples</em> for regression.</p>
<p>By the evolution of its design, scikit-learn addresses machine learning mainly via four categories:</p>
<ul>
<li>Classification:
<ul>
<li>Non-text classification, like the Iris flowers example</li>
<li>Text classification</li>
</ul>
</li>
<li>Regression</li>
<li>Clustering</li>
<li>Dimensionality reduction</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NumPy basics</h1>
                </header>
            
            <article>
                
<p>Data science deals in part with structured tables of data. The <kbd>scikit-learn</kbd> library requires input tables of two-dimensional NumPy arrays. In this section, you will learn about the <kbd>numpy</kbd> library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We will try a few operations on NumPy arrays. NumPy arrays have a single type for all of their elements and a predefined shape. Let us look first at their shape.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The shape and dimension of NumPy arrays</h1>
                </header>
            
            <article>
                
<ol>
<li>Start by importing NumPy:</li>
</ol>
<pre style="padding-left: 60px"><strong>import numpy as np</strong></pre>
<ol start="2">
<li>Produce a NumPy array of 10 digits, similar to Python's <kbd>range(10)</kbd> method:</li>
</ol>
<pre style="padding-left: 60px"><strong>np.arange(10)</strong><br/><strong>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</strong></pre>
<ol start="3">
<li>The array looks like a Python list with only one pair of brackets. This means it is of one dimension. Store the array and find out the shape:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1 = np.arange(10)</strong><br/><strong>array_1.shape</strong><br/><strong>(10L,)</strong></pre>
<ol start="4">
<li>The array has a data attribute, <kbd>shape</kbd>. The type of <kbd>array_1.shape</kbd> is a tuple <kbd>(10L,)</kbd>, which has length <kbd>1</kbd>, in this case. The number of dimensions is the same as the length of the tuple—a dimension of <kbd>1</kbd>, in this case:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1.ndim      #Find number of dimensions of array_1</strong><br/><strong>1</strong></pre>
<ol start="5">
<li>The array has 10 elements. Reshape the array by calling the <kbd>reshape</kbd> method:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1.reshape((5,2))</strong><br/><strong>array([[0, 1],</strong><br/><strong>       [2, 3],</strong><br/><strong>       [4, 5],</strong><br/><strong>       [6, 7],</strong><br/><strong>       [8, 9]])</strong></pre>
<ol start="6">
<li>This reshapes the array into 5 x 2 data object that resembles a list of lists (a three dimensional NumPy array looks like a list of lists of lists). You did not save the changes. Save the reshaped array as follows::</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1 = array_1.reshape((5,2))</strong></pre>
<ol start="7">
<li>Note that <kbd>array_1</kbd> is now two-dimensional. This is expected, as its shape has two numbers and it looks like a Python list of lists:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1.ndim</strong><br/><strong>2</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NumPy broadcasting</h1>
                </header>
            
            <article>
                
<ol start="8">
<li>Add <kbd>1</kbd> to every element of the array by broadcasting. Note that changes to the array are not saved:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1 + 1</strong><br/><strong>array([[ 1,  2],</strong><br/><strong>       [ 3,  4],</strong><br/><strong>       [ 5,  6],</strong><br/><strong>       [ 7,  8],</strong><br/><strong>       [ 9, 10]])</strong></pre>
<p style="padding-left: 60px">The term <strong>broadcasting</strong> refers to the smaller array being stretched or broadcast across the larger array. In the first example, the scalar <kbd>1</kbd> was stretched to a 5 x 2 shape and then added to <kbd>array_1</kbd>.</p>
<ol start="9">
<li>Create a new <kbd>array_2</kbd> array. Observe what occurs when you multiply the array by itself (this is not matrix multiplication; it is element-wise multiplication of arrays):</li>
</ol>
<pre style="padding-left: 60px"><strong>array_2 = np.arange(10)</strong><br/><strong>array_2 * array_2</strong><br/><strong>array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])</strong></pre>
<ol start="10">
<li>Every element has been squared. Here, element-wise multiplication has occurred. Here is a more complicated example:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_2 = array_2 ** 2  #Note that this is equivalent to array_2 * array_2</strong><br/><strong>array_2 = array_2.reshape((5,2))</strong><br/><strong>array_2</strong><br/><strong>array([[ 0,  1],</strong><br/><strong>       [ 4,  9],</strong><br/><strong>       [16, 25],</strong><br/><strong>       [36, 49],</strong><br/><strong>       [64, 81]])</strong></pre>
<ol start="11">
<li>Change <kbd>array_1</kbd> as well:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1 = array_1 + 1</strong><br/><strong>array_1</strong><br/><strong>array([[ 1,  2],</strong><br/><strong>       [ 3,  4],</strong><br/><strong>       [ 5,  6],</strong><br/><strong>       [ 7,  8],</strong><br/><strong>       [ 9, 10]])</strong></pre>
<ol start="12">
<li>Now add <kbd>array_1</kbd> and <kbd>array_2</kbd> element-wise by simply placing a plus sign between the arrays:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1 + array_2</strong><br/><strong>array([[ 1,  3],</strong><br/><strong>       [ 7, 13],</strong><br/><strong>       [21, 31],</strong><br/><strong>       [43, 57],</strong><br/><strong>       [73, 91]])</strong></pre>
<ol start="13">
<li>The formal broadcasting rules require that whenever you are comparing the shapes of both arrays from right to left, all the numbers have to either match or be one. The shapes <strong>5 X 2</strong> and <strong>5 X 2</strong> match for both entries from right to left. However, the shape <strong>5 X 2 X 1</strong> does not match <strong>5 X 2</strong>, as the second values from the right, <strong>2</strong> and <strong>5</strong> respectively, are mismatched:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="218" width="305" src="assets/862cc6cc-1c37-433c-870d-70f656a4da57.jpg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing NumPy arrays and dtypes</h1>
                </header>
            
            <article>
                
<p>There are several ways to initialize NumPy arrays besides <kbd>np.arange</kbd>:</p>
<ol start="14">
<li>Initialize an array of zeros with <kbd>np.zeros</kbd>. The <kbd>np.zeros((5,2))</kbd> command creates a 5 x 2 array of zeros:</li>
</ol>
<pre style="padding-left: 60px"><strong>np.zeros((5,2))</strong><br/><strong>array([[ 0.,  0.],</strong><br/><strong>       [ 0.,  0.],</strong><br/><strong>       [ 0.,  0.],</strong><br/><strong>       [ 0.,  0.],</strong><br/><strong>       [ 0.,  0.]])</strong></pre>
<ol start="15">
<li>Initialize an array of ones using <kbd>np.ones</kbd>. Introduce a <kbd>dtype</kbd> argument, set to <kbd>np.int</kbd>, to ensure that the ones are of NumPy integer type. Note that scikit-learn expects <kbd>np.float</kbd> arguments in arrays. The <kbd>dtype</kbd> refers to the type of every element in a NumPy array. It remains the same throughout the array. Every single element of the array below has a np.int integer type.</li>
</ol>
<pre style="padding-left: 60px"><strong>np.ones((5,2), dtype = np.int)</strong><br/><strong>array([[1, 1],</strong><br/><strong>       [1, 1],</strong><br/><strong>       [1, 1],</strong><br/><strong>       [1, 1],</strong><br/><strong>       [1, 1]])</strong></pre>
<ol start="16">
<li>Use <kbd>np.empty</kbd> to allocate memory for an array of a specific size and <kbd>dtype</kbd>, but no particular initialized values:</li>
</ol>
<pre style="padding-left: 60px"><strong>np.empty((5,2), dtype = np.float)</strong><br/><strong>array([[  3.14724935e-316,   3.14859499e-316],</strong><br/><strong>       [  3.14858945e-316,   3.14861159e-316],</strong><br/><strong>       [  3.14861435e-316,   3.14861712e-316],</strong><br/><strong>       [  3.14861989e-316,   3.14862265e-316],</strong><br/><strong>       [  3.14862542e-316,   3.14862819e-316]])</strong></pre>
<ol start="17">
<li>Use <kbd>np.zeros</kbd>, <kbd>np.ones</kbd>, and <kbd>np.empty</kbd> to allocate memory for NumPy arrays with different initial values.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Indexing</h1>
                </header>
            
            <article>
                
<ol start="18">
<li>Look up the values of the two-dimensional arrays with indexing:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1[0,0]   #Finds value in first row and first column.</strong><br/><strong>1</strong></pre>
<ol start="19">
<li>View the first row:</li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="CodeMirror cm-s-ipython">
<div class="CodeMirror-scroll">
<div class="CodeMirror-sizer">
<div>
<div class="CodeMirror-lines">
<div>
<div class="CodeMirror-code">
<pre style="padding-left: 60px" class=" CodeMirror-line"><strong><span class="cm-variable">array_1</span><span class=" CodeMirror-matchingbracket">[</span><span class="cm-number">0</span>,:<span class=" CodeMirror-matchingbracket">]<br/></span>array([1, 2])</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<ol start="20">
<li>Then view the first column:</li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="CodeMirror cm-s-ipython">
<div class="CodeMirror-scroll">
<div class="CodeMirror-sizer">
<div>
<div class="CodeMirror-lines">
<div>
<div class="CodeMirror-code">
<pre style="padding-left: 60px" class=" CodeMirror-line"><strong><span class="cm-variable">array_1</span><span class=" CodeMirror-matchingbracket">[</span>:,<span class="cm-number">0</span><span class=" CodeMirror-matchingbracket">]<br/></span>array([1, 3, 5, 7, 9])</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<ol start="21">
<li>View specific values along both axes. Also view the second to the fourth rows:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1[2:5, :]</strong><br/><strong>array([[ 5,  6],</strong><br/><strong>       [ 7,  8],</strong><br/><strong>       [ 9, 10]])</strong></pre>
<ol start="22">
<li>View the <span>second to the fourth</span> rows only along the first column:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1[2:5,0]</strong><br/><strong>array([5, 7, 9])</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boolean arrays</h1>
                </header>
            
            <article>
                
<p>Additionally, NumPy handles indexing with Boolean logic:</p>
<ol start="23">
<li>First produce a Boolean array:</li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="CodeMirror cm-s-ipython">
<div class="CodeMirror-scroll">
<div class="CodeMirror-sizer">
<div>
<div class="CodeMirror-lines">
<div>
<div class="CodeMirror-code">
<pre style="padding-left: 60px" class=" CodeMirror-line"><strong><span class="cm-variable">array_1</span> <span class="cm-operator">&gt;</span> <span class="cm-number">5<br/></span>array([[False, False],</strong><br/><strong>       [False, False],</strong><br/><strong>       [False,  True],</strong><br/><strong>       [ True,  True],</strong><br/><strong>       [ True,  True]], dtype=bool)</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<ol start="24">
<li>Place brackets around the Boolean array to filter by the Boolean array:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1[array_1 &gt; 5]</strong><br/><strong>array([ 6,  7,  8,  9, 10])</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Arithmetic operations</h1>
                </header>
            
            <article>
                
<ol start="25">
<li>Add all the elements of the array with the <kbd>sum</kbd> method. Go back to <kbd>array_1</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1</strong><br/><strong>array([[ 1,  2],</strong><br/><strong>       [ 3,  4],</strong><br/><strong>       [ 5,  6],</strong><br/><strong>       [ 7,  8],</strong><br/><strong>       [ 9, 10]])</strong><br/><strong>array_1.sum()</strong><br/><strong>55</strong></pre>
<ol start="26">
<li>Find all the sums by row:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1.sum(axis = 1)</strong><br/><strong>array([ 3,  7, 11, 15, 19])</strong></pre>
<ol start="27">
<li>Find all the sums by column:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1.sum(axis = 0)</strong><br/><strong>array([25, 30])</strong></pre>
<ol start="28">
<li>Find the mean of each column in a similar way. Note that the <kbd>dtype</kbd> of the array of averages is <kbd>np.float</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_1.mean(axis = 0)</strong><br/><strong>array([ 5.,  6.])</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NaN values</h1>
                </header>
            
            <article>
                
<ol start="29">
<li>Scikit-learn will not accept <kbd>np.nan</kbd> values. Take <kbd>array_3</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_3 = np.array([np.nan, 0, 1, 2, np.nan])</strong></pre>
<ol start="30">
<li>Find the NaN values with a special Boolean array created by the <kbd>np.isnan</kbd> function:</li>
</ol>
<pre style="padding-left: 60px"><strong>np.isnan(array_3)</strong><br/><strong>array([ True, False, False, False,  True], dtype=bool)</strong></pre>
<ol start="31">
<li>Filter the NaN values by negating the Boolean array with the symbol ~ and placing brackets around the expression:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_3[~np.isnan(array_3)]</strong><br/>&gt;<strong>array([ 0.,  1.,  2.])</strong></pre>
<ol start="32">
<li>Alternatively, set the NaN values to zero:</li>
</ol>
<pre style="padding-left: 60px"><strong>array_3[np.isnan(array_3)] = 0</strong><br/><strong>array_3</strong><br/><strong>array([ 0.,  0.,  1.,  2.,  0.])</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Data, in the present and minimal sense, is about 2D tables of numbers, which NumPy handles very well. Keep this in mind in case you forget the NumPy syntax specifics. Scikit-learn accepts only 2D NumPy arrays of real numbers with no missing <kbd>np.nan</kbd> values.</p>
<p>From experience, it tends to be best to change <kbd>np.nan</kbd> to some value instead of throwing away data. Personally, I like to keep track of Boolean masks and keep the data shape roughly the same, as this leads to fewer coding errors and more coding flexibility.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the iris dataset</h1>
                </header>
            
            <article>
                
<p>To perform machine learning with scikit-learn, we need some data to start with. We will load the <kbd>iris</kbd> dataset, one of the several datasets available in scikit-learn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>A scikit-learn program begins with several imports. Within Python, preferably in Jupyter Notebook, load the <kbd>numpy</kbd>, <kbd>pandas</kbd>, and <kbd>pyplot</kbd> libraries:</p>
<pre style="padding-left: 30px"><strong>import numpy as np    #Load the numpy library for fast array computations</strong><br/><strong>import pandas as pd   #Load the pandas data-analysis library</strong><br/><strong>import matplotlib.pyplot as plt   #Load the pyplot visualization library</strong></pre>
<p>If you are within a Jupyter Notebook, type the following to see a graphical output instantly:</p>
<pre><strong>%matplotlib inline</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>From the scikit-learn <kbd>datasets</kbd> module, access the <kbd>iris</kbd> dataset:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn import datasets</strong><br/><strong>iris = datasets.load_iris()</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Similarly, you could have imported the <kbd>diabetes</kbd> dataset as follows:</p>
<pre><strong>from sklearn import datasets  #Import datasets module from scikit-learn</strong><br/><strong>diabetes = datasets.load_diabetes()  </strong> </pre>
<p>There! You've loaded <kbd>diabetes</kbd> using the <kbd>load_diabetes()</kbd> function of the <kbd>datasets</kbd> module. To check which datasets are available, type:</p>
<pre><strong>datasets.load_*?</strong></pre>
<p>Once you try that, you might observe that there is a dataset named <kbd>datasets.load_digits</kbd>. To access it, type the <kbd>load_digits()</kbd> function, analogous to the other loading functions:</p>
<pre><strong>digits = datasets.load_digits()</strong></pre>
<p>To view information about the dataset, type <kbd>digits.DESCR</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Viewing the iris dataset</h1>
                </header>
            
            <article>
                
<p>Now that we've loaded the dataset, let's examine what is in it. The <kbd>iris</kbd> dataset pertains to a supervised classification problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>To access the observation variables, type:</li>
</ol>
<pre style="padding-left: 60px"><strong>iris.data</strong></pre>
<p style="padding-left: 60px">This outputs a NumPy array:</p>
<pre style="padding-left: 60px"><strong>array([[ 5.1,  3.5,  1.4,  0.2],</strong><br/><strong>       [ 4.9,  3. ,  1.4,  0.2],</strong><br/><strong>       [ 4.7,  3.2,  1.3,  0.2], </strong><br/><strong>#...rest of output suppressed because of length</strong></pre>
<ol start="2">
<li>Let's examine the NumPy array:</li>
</ol>
<pre style="padding-left: 60px"><strong>iris.data.shape</strong></pre>
<p style="padding-left: 60px">This returns:</p>
<pre style="padding-left: 60px"><strong>(150L, 4L)</strong></pre>
<p style="padding-left: 60px">This means that the data is 150 rows by 4 columns. Let's look at the first row:</p>
<pre style="padding-left: 60px"><strong>iris.data[0]</strong><br/><br/><strong>array([ 5.1,  3.5,  1.4,  0.2])</strong></pre>
<p style="padding-left: 60px">The NumPy array for the first row has four numbers.</p>
<ol start="3">
<li>To determine what they mean, type:</li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="CodeMirror cm-s-ipython">
<div class="CodeMirror-scroll">
<div class="CodeMirror-sizer">
<div>
<div class="CodeMirror-lines">
<div>
<div class="CodeMirror-code">
<pre style="padding-left: 90px" class=" CodeMirror-line"><strong><span class="cm-variable">iris</span>.<span class="cm-variable">feature_names<br/></span>['sepal length (cm)',</strong><br/><strong> 'sepal width (cm)',</strong><br/><strong> 'petal length (cm)',</strong><br/><strong> 'petal width (cm)']</strong></pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>The feature or column names name the data. They are strings, and in this case, they correspond to dimensions in different types of flowers. Putting it all together, we have 150 examples of flowers with four measurements per flower in centimeters. For example, the first flower has measurements of 5.1 cm for sepal length, 3.5 cm for sepal width, 1.4 cm for petal length, and 0.2 cm for petal width. Now, let's look at the output variable in a similar manner:</p>
<pre><strong>iris.target</strong></pre>
<p>This yields an array of outputs: <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd>. There are only three outputs. Type this:</p>
<pre><strong>iris.target.shape</strong></pre>
<p>You get a shape of:</p>
<pre><strong>(150L,)</strong></pre>
<p>This refers to an array of length 150 (150 x 1). Let's look at what the numbers refer to:</p>
<pre><strong>iris.target_names</strong><br/><br/><strong>array(['setosa', 'versicolor', 'virginica'], </strong><br/><strong>      dtype='|S10')</strong></pre>
<p>The output of the <kbd>iris.target_names</kbd> <span>variable</span><span> </span><span>gives the English names for the numbers in the</span> <kbd>iris.target</kbd> <span>variable. The number zero corresponds to the <kbd>setosa</kbd> flower, number one corresponds to the <kbd>versicolor</kbd> flower, and number two corresponds to the <kbd>virginica</kbd> flower. Look at the first row of <kbd>iris.target</kbd>:</span></p>
<pre><strong>iris.target[0]</strong></pre>
<p>This produces zero, and thus the first row of observations we examined before correspond to the <kbd>setosa</kbd> flower.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In machine learning, we often deal with data tables and two-dimensional arrays corresponding to examples. In the <kbd>iris</kbd> set, we have 150 observations of flowers of three types. With new observations, we would like to predict which type of flower those observations correspond to. The observations in this case are measurements in centimeters. It is important to look at the data pertaining to real objects. Quoting my high school physics teacher, <q>"<em>Do not forget the units!</em>"</q></p>
<p>The <kbd>iris</kbd> dataset is intended to be for a supervised machine learning task because it has a target array, which is the variable we desire to predict from the observation variables. Additionally, it is a classification problem, as there are three numbers we can predict from the observations, one for each type of flower. In a classification problem, we are trying to distinguish between categories. The simplest case is binary classification. The <kbd>iris</kbd> dataset, with three flower categories, is a multi-class classification problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>With the same data, we can rephrase the problem in many ways, or formulate new problems. What if we want to determine relationships between the observations? We can define the petal width as the target variable. We can rephrase the problem as a regression problem and try to predict the target variable as a real number, not just three categories. Fundamentally, it comes down to what we intend to predict. Here, we desire to predict a type of flower.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Viewing the iris dataset with Pandas</h1>
                </header>
            
            <article>
                
<p>In this recipe we will use the handy <kbd>pandas</kbd> data analysis library to view and visualize the <kbd>iris</kbd> dataset. It contains the notion o, a dataframe which might be familiar to you if you use the language R's dataframe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>You can view the <kbd>iris</kbd> dataset with Pandas, a library built on top of NumPy:</p>
<ol>
<li>Create a dataframe with the observation variables <kbd>iris.data</kbd>, and column names <kbd>columns</kbd>, as arguments:</li>
</ol>
<pre style="padding-left: 60px"><strong>import pandas as pd</strong><br/><strong>iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)</strong></pre>
<p style="padding-left: 60px">The dataframe is more user-friendly than the NumPy array.</p>
<ol start="2">
<li>Look at a quick histogram of the values in the dataframe for <kbd>sepal length</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>iris_df['sepal length (cm)'].hist(bins=30)</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img height="291" width="412" src="assets/bc06d071-510d-464e-8d01-1b6917d17b23.jpg"/></div>
<ol start="3">
<li>You can also color the histogram by the <kbd>target</kbd> variable:</li>
</ol>
<pre style="padding-left: 60px"><strong>for class_number in np.unique(iris.target):</strong><br/><strong>    plt.figure(1)</strong><br/><strong>    iris_df['sepal length (cm)'].iloc[np.where(iris.target == class_number)[0]].hist(bins=30)</strong></pre>
<ol start="4">
<li>Here, iterate through the target numbers for each flower and draw a color histogram for each. Consider this line:</li>
</ol>
<pre style="padding-left: 60px"><strong>np.where(iris.target== class_number)[0]</strong></pre>
<p style="padding-left: 60px">It finds the NumPy index location for each class of flower:</p>
<div class="CDPAlignCenter CDPAlign"><img height="301" width="412" src="assets/fc479efd-c396-4ea0-8faa-2d699d0f84c5.jpg"/></div>
<p>Observe that the histograms overlap. This encourages us to model the three histograms as three normal distributions. This is possible in a machine learning manner if we model the training data only as three normal distributions, not the whole set. Then we use the test set to test the three normal distribution models we just made up. Finally, we test the accuracy of our predictions on the test set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The dataframe data object is a 2D NumPy array with column names and row names. In data science, the fundamental data object looks like a 2D table, possibly because of SQL's long history. NumPy allows for 3D arrays, cubes, 4D arrays, and so on. These also come up often.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting with NumPy and matplotlib</h1>
                </header>
            
            <article>
                
<p>A simple way to make visualizations with NumPy is by using the library <kbd>matplotlib</kbd>. Let's make some visualizations quickly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Start by importing <kbd>numpy</kbd> and <kbd>matplotlib</kbd>. You can view visualizations within an IPython Notebook using the <kbd>%matplotlib inline</kbd> command:</p>
<pre><strong>import numpy as np</strong><br/><strong> import matplotlib.pyplot as plt</strong><br/><strong> %matplotlib inline</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>The main command in matplotlib, in pseudo code, is as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>plt.plot(numpy array, numpy array of same length)</strong></pre>
<ol start="2">
<li>Plot a straight line by placing two NumPy arrays of the same length:</li>
</ol>
<pre style="padding-left: 60px"><strong>plt.plot(np.arange(10), np.arange(10))</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/287e1a2e-a723-403f-912a-dc2ab10cfd2c.png"/></div>
<ol start="3">
<li>Plot an exponential:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>plt.plot(np.arange(10), np.exp(np.arange(10)))</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/df166a79-ba13-48c5-adfa-765233d0aec8.png"/></div>
<ol start="4">
<li>Place the two graphs side by side:</li>
</ol>
<pre style="padding-left: 60px"><strong>plt.figure()</strong><br/><strong>plt.subplot(121)</strong><br/><strong>plt.plot(np.arange(10), np.exp(np.arange(10)))</strong><br/><strong>plt.subplot(122)</strong><br/><strong>plt.scatter(np.arange(10), np.exp(np.arange(10)))</strong></pre>
<p style="padding-left: 60px">Or top to bottom:</p>
<pre style="padding-left: 60px"><strong>plt.figure()</strong><br/><strong>plt.subplot(211)</strong><br/><strong>plt.plot(np.arange(10), np.exp(np.arange(10)))</strong><br/><strong>plt.subplot(212)</strong><br/><strong>plt.scatter(np.arange(10), np.exp(np.arange(10)))</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/490c815b-e265-454c-967b-e7dc4e617bfd.png"/></div>
<p style="padding-left: 60px">The first two numbers in the subplot command refer to the grid size in the figure instantiated by <kbd>plt.figure()</kbd>. The grid size referred to in <kbd>plt.subplot(221)</kbd> is 2 x 2, the first two digits. The last digit refers to traversing the grid in reading order: left to right and then up to down.</p>
<ol start="5">
<li>Plot in a 2 x 2 grid traversing in reading order from one to four:</li>
</ol>
<pre style="padding-left: 60px"><strong>plt.figure()</strong><br/><strong>plt.subplot(221)</strong><br/><strong>plt.plot(np.arange(10), np.exp(np.arange(10)))</strong><br/><strong>plt.subplot(222)</strong><br/><strong>plt.scatter(np.arange(10), np.exp(np.arange(10)))</strong><br/><strong>plt.subplot(223)</strong><br/><strong>plt.scatter(np.arange(10), np.exp(np.arange(10)))</strong><br/><strong>plt.subplot(224)</strong><br/><strong>plt.scatter(np.arange(10), np.exp(np.arange(10)))</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b6546b85-8074-4d63-9b2f-66bdadc52305.png"/></div>
<ol start="6">
<li>Finally, with real data:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root"><strong>from sklearn.datasets import load_iris</strong><br/><br/><strong>iris = load_iris()</strong><br/><strong>data = iris.data</strong><br/><strong>target = iris.target</strong><br/><br/><strong># Resize the figure for better viewing</strong><br/><strong>plt.figure(figsize=(12,5))</strong><br/><br/><strong># First subplot</strong><br/><strong>plt.subplot(121)</strong><br/><br/><strong># Visualize the first two columns of data:</strong><br/><strong>plt.scatter(data[:,0], data[:,1], c=target)</strong><br/><br/><strong># Second subplot</strong><br/><strong>plt.subplot(122)</strong><br/><br/><strong># Visualize the last two columns of data:</strong><br/><strong>plt.scatter(data[:,2], data[:,3], c=target)</strong></pre>
<p>The <kbd>c</kbd> parameter takes an array of colors—in this case, the colors <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> in the <kbd>iris</kbd> target:</p>
<div class="CDPAlignCenter CDPAlign"><img height="224" width="514" src="assets/975ccab7-6564-4a71-82cd-daee92065546.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A minimal machine learning recipe – SVM classification</h1>
                </header>
            
            <article>
                
<p>Machine learning is all about making predictions. To make predictions, we will:</p>
<ul>
<li>State the problem to be solved</li>
<li>Choose a model to solve the problem</li>
<li>Train the model</li>
<li>Make predictions</li>
<li>Measure how well the model performed</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Back to the iris example, we now store the first two features (columns) of the observations as <kbd>X</kbd> and the target as <kbd>y</kbd>, a convention in the machine learning community:</p>
<pre><strong>X = iris.data[:, :2]  </strong><br/><strong>y = iris.target</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>First, we state the problem. We are trying to determine the flower-type category from a set of new observations. This is a classification task. The data available includes a target variable, which we have named <kbd>y</kbd>. This is a supervised classification problem.</li>
</ol>
<div class="packt_tip packt_infobox">The task of supervised learning involves predicting values of an output variable with a model that trains using input variables and an output variable.</div>
<ol start="2">
<li>Next, we choose a model to solve the supervised classification. For now, we will use a support vector classifier. Because of its simplicity and interpretability, it is a commonly used algorithm (<em>interpretable</em> means easy to read into and understand).</li>
<li>To measure the performance of prediction, we will split the dataset into training and test sets. The training set refers to data we will learn from. The test set is the data we hold out and pretend not to know as we would like to measure the performance of our learning procedure. So, import a function that will split the dataset:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import train_test_split</strong></pre>
<ol start="4">
<li>Apply the function to both the observation and target data:</li>
</ol>
<p class="text_cell_render rendered_html"/>
<pre style="padding-left: 60px"><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)</strong></pre>
<p style="padding-left: 60px">The test size is 0.25 or 25% of the whole dataset. A random state of one fixes the random seed of the function so that<span> </span><span>you get the same results</span><span> every time you call the function, which is important for now to reproduce the same results consistently.</span></p>
<ol start="5">
<li>Now load a regularly used estimator, a support vector machine:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.svm import SVC</strong></pre>
<ol start="6">
<li>You have imported a support vector classifier from the <kbd>svm</kbd> module. Now create an instance of a linear SVC:</li>
</ol>
<pre style="padding-left: 60px"><strong>clf = SVC(kernel='linear',random_state=1)</strong></pre>
<p>The random state is fixed to reproduce the same results with the same code later.</p>
<p>The supervised models in scikit-learn implement a <kbd>fit(X, y)</kbd> method, which trains the model and returns the trained model. <kbd>X</kbd> is a subset of the observations, and each element of <kbd>y</kbd> corresponds to the target of each observation in <kbd>X</kbd>. Here, we fit a model on the training data:</p>
<pre><strong>clf.fit(X_train, y_train)</strong></pre>
<p>Now, the <kbd>clf</kbd> <span>variable</span><span> </span><span>is the fitted, or trained, model.</span></p>
<p>The estimator also has a <kbd>predict(X)</kbd> method that returns predictions for several unlabeled observations, <kbd>X_test</kbd>, and returns the predicted values, <kbd>y_pred</kbd>. Note that the function does not return the estimator. It returns a set of predictions:</p>
<pre><strong>y_pred = clf.predict(X_test)</strong></pre>
<p>So far, you have done all but the last step. To examine the model performance, load a scorer from the metrics module:</p>
<pre><strong>from sklearn.metrics import accuracy_score</strong></pre>
<p>With the scorer, compare the predictions with the held-out test targets:</p>
<pre><strong>accuracy_score(y_test,y_pred)</strong><br/><br/><strong>0.76315789473684215</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Without knowing very much about the details of support vector machines, we have implemented a predictive model. To perform machine learning, we held out one-fourth of the data and examined how the SVC performed on that data. In the end, we obtained a number that measures accuracy, or how the model performed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>To summarize, we will do all the steps with a different algorithm, logistic regression:</p>
<ol>
<li>First, import <kbd>LogisticRegression</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.linear_model import LogisticRegression</strong></pre>
<ol start="2">
<li>Then write a program with the modeling steps:
<ol>
<li>Split the data into training and testing sets.</li>
<li>Fit the logistic regression model.</li>
<li>Predict using the test observations.</li>
<li>Measure the accuracy of the predictions with <kbd>y_test</kbd> versus <kbd>y_pred</kbd>:</li>
</ol>
</li>
</ol>
<pre><strong>import matplotlib.pyplot as plt</strong><br/><strong>from sklearn import datasets</strong><br/><br/><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>from sklearn.metrics import accuracy_score</strong><br/><br/><strong>X = iris.data[:, :2]   #load the iris data</strong><br/><strong>y = iris.target</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)</strong><br/><br/><strong>#train the model</strong><br/><strong>clf = LogisticRegression(random_state = 1)</strong><br/><strong>clf.fit(X_train, y_train)</strong><br/><br/><strong>#predict with Logistic Regression</strong><br/><strong>y_pred = clf.predict(X_test)</strong><br/><br/><strong>#examine the model accuracy</strong><br/><strong>accuracy_score(y_test,y_pred)</strong><br/><br/><strong>0.60526315789473684</strong></pre>
<p>This number is lower; yet we cannot make any conclusions comparing the two models, SVC and logistic regression classification. We cannot compare them, because we were not supposed to look at the test set for our model. If we made a choice between SVC and <span>logistic regression</span>, the choice would be part of our model as well, so the test set cannot be involved in the choice. Cross-validation, which we will look at next, is a way to choose between models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing cross-validation</h1>
                </header>
            
            <article>
                
<p>We are thankful for the <kbd>iris</kbd> dataset, but as you might recall, it has only 150 observations. To make the most out of the set, we will employ cross-validation. Additionally, in the last section, we wanted to compare the performance of two different classifiers, support vector classifier and logistic regression. Cross-validation will help us with this comparison issue as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Suppose we wanted to choose between the support vector classifier and the logistic regression classifier. We cannot measure their performance on the unavailable test set.</p>
<p>What if, instead, we:</p>
<ul>
<li>Forgot about the test set for now?</li>
<li>Split the training set into two parts, one to train on and one to test the training?</li>
</ul>
<p>Split the training set into two parts using the train_test_split function used in previous sections:</p>
<pre><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_train, y_train, test_size=0.25, random_state=1)</strong></pre>
<p><kbd>X_train_2</kbd> consists of 75% of the <kbd>X_train</kbd> data, while <kbd>X_test_2</kbd> is the remaining 25%. <kbd>y_train_2</kbd> is 75% of the target data, and matches the observations of <kbd>X_train_2</kbd>. <kbd>y_test_2</kbd> is 25% of the target data present in <kbd>y_train</kbd>.</p>
<p>As you might have expected, you have to use these new splits to choose between the two models: SVC and <span>logistic regression</span>. Do so by writing a predictive program.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Start with all the imports and load the <kbd>iris</kbd> dataset:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn import datasets</strong><br/><br/><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>from sklearn.metrics import accuracy_score</strong><br/><br/><strong>#load the classifying models</strong><br/><strong>from sklearn.linear_model import LogisticRegression</strong><br/><strong>from sklearn.svm import SVC</strong><br/><br/><strong>iris = datasets.load_iris()</strong><br/><strong>X = iris.data[:, :2]  #load the first two features of the iris data </strong><br/><strong>y = iris.target #load the target of the iris data</strong><br/><br/><strong>#split the whole set one time</strong><br/><strong>#Note random state is 7 now</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7)</strong><br/><br/><strong>#split the training set into parts</strong><br/><strong>X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_train, y_train, test_size=0.25, random_state=7)</strong></pre>
<ol start="2">
<li>Create an instance of an SVC classifier and fit it:</li>
</ol>
<pre style="padding-left: 60px"><strong>svc_clf = SVC(kernel = 'linear',random_state = 7)</strong><br/><strong>svc_clf.fit(X_train_2, y_train_2)</strong></pre>
<ol start="3">
<li>Do the same for logistic regression (both lines for <span>logistic regression</span> are compressed into one):</li>
</ol>
<pre style="padding-left: 60px"><strong>lr_clf = LogisticRegression(random_state = 7).fit(X_train_2, y_train_2)</strong></pre>
<ol start="4">
<li>Now predict and examine the SVC and <span>logistic regression'</span>s performance on <kbd>X_test_2</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>svc_pred = svc_clf.predict(X_test_2)</strong><br/><strong>lr_pred = lr_clf.predict(X_test_2)</strong><br/><br/><strong>print "Accuracy of SVC:",accuracy_score(y_test_2,svc_pred)</strong><br/><strong>print "Accuracy of LR:",accuracy_score(y_test_2,lr_pred)</strong><br/><br/><strong>Accuracy of SVC: 0.857142857143</strong><br/><strong>Accuracy of LR: 0.714285714286</strong></pre>
<ol start="5">
<li>The SVC performs better, but we have not yet seen the original test data. Choose SVC over <span>logistic regression</span> and try it on the original test set:</li>
</ol>
<pre style="padding-left: 60px"><strong>print "Accuracy of SVC on original Test Set: ",accuracy_score(y_test, svc_clf.predict(X_test))</strong><br/><br/><strong>Accuracy of SVC on original Test Set:  0.684210526316</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In comparing the SVC and logistic regression classifier, you might wonder (and be a little suspicious) about a lot of scores being very different. The final test on SVC scored lower than logistic regression. To help with this situation, we can do cross-validation in scikit-learn.</p>
<p>Cross-validation involves splitting the training set into parts, as we did before. To match the preceding example, we split the training set into four parts, or folds. We are going to design a cross-validation iteration by taking turns with one of the four folds for testing and the other three for training. It is the same split as done before four times over with the same set, thereby rotating, in a sense, the test set:</p>
<div class="CDPAlignCenter CDPAlign"><img height="131" width="195" src="assets/f1e7c4be-773e-49cb-9291-1b06a0b75050.jpg"/></div>
<p>With scikit-learn, this is relatively easy to accomplish:</p>
<ol>
<li>We start with an import:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import cross_val_score</strong></pre>
<ol start="2">
<li>Then we produce an accuracy score on four folds:</li>
</ol>
<pre style="padding-left: 60px"><strong>svc_scores = cross_val_score(svc_clf, X_train, y_train, cv=4)</strong><br/><strong>svc_scores<br/></strong><br/><strong>array([ 0.82758621,  0.85714286,  0.92857143,  0.77777778])</strong></pre>
<ol start="3">
<li>We can find the mean for average performance and standard deviation for a measure of spread of all scores relative to the mean:</li>
</ol>
<pre style="padding-left: 60px"><strong>print "Average SVC scores: ", svc_scores.mean()</strong><br/><strong>print "Standard Deviation of SVC scores: ", svc_scores.std()</strong><br/><br/><strong>Average SVC scores:  0.847769567597</strong><br/><strong>Standard Deviation of SVC scores:  0.0545962864696</strong></pre>
<ol start="4">
<li>Similarly, with the logistic regression instance, we compute four scores:</li>
</ol>
<pre style="padding-left: 60px"><strong>lr_scores = cross_val_score(lr_clf, X_train, y_train, cv=4)</strong><br/><strong>print "Average SVC scores: ", lr_scores.mean()</strong><br/><strong>print "Standard Deviation of SVC scores: ", lr_scores.std()</strong><br/><br/><strong>Average SVC scores:  0.748893906221</strong><br/><strong>Standard Deviation of SVC scores:  0.0485633168699</strong></pre>
<p>Now we have many scores, which confirms our selection of SVC over <span>logistic regression</span>. Thanks to cross-validation, we used the training multiple times and had four small test sets within it to score our model.</p>
<p>Note that our model is a bigger model that consists of:</p>
<ul>
<li>Training an SVM through cross-validation</li>
<li>Training a <span>logistic regression</span> through cross-validation</li>
<li>Choosing between SVM and <span>logistic regression</span></li>
</ul>
<div class="packt_tip">The choice at the end is part of the model.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Despite our hard work and the elegance of the scikit-learn syntax, the score on the test set at the very end remains suspicious. The reason for this is that the test and train split are not necessarily balanced; the train and test sets do not necessarily have similar proportions of all the classes.</p>
<p>This is easily remedied by using a stratified test-train split:</p>
<pre><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)</strong></pre>
<p>By selecting the target set as the stratified argument, the target classes are balanced. This brings the SVC scores closer together.</p>
<pre><strong>svc_scores = cross_val_score(svc_clf, X_train, y_train, cv=4)</strong><br/><strong>print "Average SVC scores: " , svc_scores.mean()</strong><br/><strong>print "Standard Deviation of SVC scores: ", svc_scores.std()</strong><br/><strong>print "Score on Final Test Set:", accuracy_score(y_test, svc_clf.predict(X_test))</strong><br/><br/><strong>Average SVC scores:  0.831547619048</strong><br/><strong>Standard Deviation of SVC scores:  0.0792488953372</strong><br/><strong>Score on Final Test Set: 0.789473684211</strong></pre>
<p>Additionally, note that in the preceding example, the cross-validation procedure produces stratified folds<span> </span><span>by default:</span></p>
<pre><strong>from sklearn.model_selection import cross_val_score</strong><br/><strong>svc_scores = cross_val_score(svc_clf, X_train, y_train, cv = 4)</strong></pre>
<p>The preceding code is equivalent to:</p>
<pre><strong>from sklearn.model_selection import cross_val_score, StratifiedKFold</strong><br/><strong>skf = StratifiedKFold(n_splits = 4)</strong><br/><strong>svc_scores = cross_val_score(svc_clf, X_train, y_train, cv = skf)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together</h1>
                </header>
            
            <article>
                
<p>Now, we are going to perform the same procedure as before, except that we will reset, regroup, and try a new algorithm: <strong>K-Nearest Neighbors</strong> (<strong>KNN</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Start by importing the model from <kbd>sklearn</kbd>, followed by a balanced split:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.neighbors import KNeighborsClassifier</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state = 0)<br/></strong></pre>
<div class="packt_tip"><span>The <kbd>random_state</kbd> parameter fixes the <kbd>random_seed</kbd> in the function <kbd>train_test_split</kbd>. In the preceding example, the <kbd>random_state</kbd> is set to zero and can be set to any integer.<br/></span></div>
<ol start="2">
<li>Construct two different KNN models by varying the <kbd>n_neighbors</kbd> parameter. Observe that the number of folds is now 10. Tenfold cross-validation is common in the machine learning community, particularly in data science competitions:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import cross_val_score</strong><br/><strong>knn_3_clf = KNeighborsClassifier(n_neighbors = 3)</strong><br/><strong>knn_5_clf = KNeighborsClassifier(n_neighbors = 5)</strong><br/><br/><strong>knn_3_scores = cross_val_score(knn_3_clf, X_train, y_train, cv=10)</strong><br/><strong>knn_5_scores = cross_val_score(knn_5_clf, X_train, y_train, cv=10)</strong></pre>
<ol start="3">
<li>Score and print out the scores for selection:</li>
</ol>
<pre style="padding-left: 60px"><strong>print "knn_3 mean scores: ", knn_3_scores.mean(), "knn_3 std: ",knn_3_scores.std()</strong><br/><strong>print "knn_5 mean scores: ", knn_5_scores.mean(), " knn_5 std: ",knn_5_scores.std()</strong><br/><br/><strong>knn_3 mean scores:  0.798333333333 knn_3 std:  0.0908142181722</strong><br/><strong>knn_5 mean scores:  0.806666666667 knn_5 std:  0.0559320575496</strong></pre>
<p>Both nearest neighbor types score similarly, yet the KNN with parameter <kbd>n_neighbors = 5</kbd> is a bit more stable. This is an example of <em>hyperparameter optimization</em> which we will examine closely throughout the book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>You could have just as easily run a simple loop to score the function more quickly:</p>
<pre><strong>all_scores = []</strong><br/><strong>for n_neighbors in range(3,9,1):</strong><br/><strong>    knn_clf = KNeighborsClassifier(n_neighbors = n_neighbors)</strong><br/><strong>    all_scores.append((n_neighbors, cross_val_score(knn_clf, X_train, y_train, cv=10).mean()))</strong><br/><strong>sorted(all_scores, key = lambda x:x[1], reverse = True) </strong> </pre>
<p>Its output suggests that <kbd>n_neighbors = 4</kbd> is a good choice:</p>
<pre><strong>[(4, 0.85111111111111115),</strong><br/><strong> (7, 0.82611111111111113),</strong><br/><strong> (6, 0.82333333333333347),</strong><br/><strong> (5, 0.80666666666666664),</strong><br/><strong> (3, 0.79833333333333334),</strong><br/><strong> (8, 0.79833333333333334)]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning overview – classification versus regression</h1>
                </header>
            
            <article>
                
<p>In this recipe we will examine how regression can be viewed as being very similar to classification. This is done by reconsidering the categorical labels of regression as real numbers. In this section we will also look at at several aspects of machine learning from a very broad perspective including the purpose of scikit-learn. scikit-learn allows us to find models that work well incredibly quickly. We do not have to work out all the details of the model, or optimize, until we found one that works well. Consequently, your company saves precious development time and computational resources thanks to scikit-learn giving us the ability to develop models relatively quickly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The purpose of scikit-learn</h1>
                </header>
            
            <article>
                
<p>As we have seen before, scikit-learn allowed us to find a model that works fairly quickly. We tried SVC, logistic regression, and a few KNN classifiers. Through cross-validation, we selected models that performed better than others. In industry, after trying SVMs and logistic regression, we might focus on SVMs and optimize them further. Thanks to scikit-learn, we saved a lot of time and resources, including mental energy. After optimizing the SVM at work on a realistic dataset, we might re-implement it for speed in Java or C and gather more data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised versus unsupervised</h1>
                </header>
            
            <article>
                
<p>Classification and regression are supervised, as we know the target variables for the observations. Clustering—creating regions in space for each category without being given any labels is unsupervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In classification, the target variable is one of several categories, and there must be more than one instance of every category. In regression, there can be only one instance of every target variable, as the only requirement is that the target is a real number.</p>
<p>In the case of logistic regression, we saw previously that the algorithm first performs a regression and estimates a real number for the target. Then the target class is estimated by using thresholds. In scikit-learn, there are <kbd>predict_proba</kbd> methods that yield probabilistic estimates, which relate regression-like real number estimates with classification classes in the style of logistic regression.</p>
<p>Any regression can be turned into classification by using thresholds. A binary classification can be viewed as a regression problem by using a regressor. The target variables produced will be real numbers, not the original class variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quick SVC – a classifier and regressor</h1>
                </header>
            
            <article>
                
<ol>
<li>Load <kbd>iris</kbd> from the <kbd>datasets</kbd> module:</li>
</ol>
<pre style="padding-left: 60px"><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><strong>from sklearn import datasets</strong><br/><br/><strong>iris = datasets.load_iris()</strong></pre>
<ol start="2">
<li>For simplicity, consider only targets <kbd>0</kbd> and <kbd>1</kbd>, corresponding to Setosa and Versicolor. Use the Boolean array <kbd>iris.target &lt; 2</kbd> to filter out target <kbd>2</kbd>. Place it within brackets to use it as a filter in defining the observation set <kbd>X</kbd> and the target set <kbd>y</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>X = iris.data[iris.target &lt; 2]</strong><br/><strong>y = iris.target[iris.target &lt; 2]</strong></pre>
<ol start="3">
<li>Now import <kbd>train_test_split</kbd> and apply it:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>from sklearn.metrics import accuracy_score</strong><br/><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state= 7)</strong></pre>
<ol start="4">
<li>Prepare and run an SVC by importing it and scoring it with cross-validation:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.svm import SVC</strong><br/><strong>from sklearn.model_selection import cross_val_score</strong><br/><br/><strong>svc_clf = SVC(kernel = 'linear').fit(X_train, y_train)</strong><br/><strong>svc_scores = cross_val_score(svc_clf, X_train, y_train, cv=4)</strong></pre>
<ol start="5">
<li>As done in previous sections, view the average of the scores:</li>
</ol>
<pre style="padding-left: 60px"><strong>svc_scores.mean()</strong><br/><br/><strong>0.94795321637426899</strong></pre>
<ol start="6">
<li>Perform the same with support vector regression by importing <kbd>SVR</kbd> from <kbd>sklearn.svm</kbd>, the same module that contains SVC:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.svm import SVR</strong></pre>
<ol start="7">
<li>Then write the necessary syntax to fit the model. It is almost identical to the syntax for SVC, just replacing some <kbd>c</kbd> keywords with <kbd>r</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>svr_clf = SVR(kernel = 'linear').fit(X_train, y_train)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making a scorer</h1>
                </header>
            
            <article>
                
<p>To make a scorer, you need:</p>
<ul>
<li>A scoring function that compares <kbd>y_test</kbd>, the ground truth, with <kbd>y_pred</kbd>, the predictions</li>
<li>To determine whether a high score is good or bad</li>
</ul>
<p><span>Before passing the SVR regressor to the cross-validation, make a scorer by supplying two elements:</span></p>
<ol start="8">
<li>In practice, begin by importing the <kbd>make_scorer</kbd> function:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.metrics import make_scorer</strong></pre>
<ol start="9">
<li>Use this sample scoring function:</li>
</ol>
<pre style="padding-left: 60px"><strong>#Only works for this iris example with targets 0 and 1</strong><br/><strong>def for_scorer(y_test, orig_y_pred):</strong><br/><strong>    y_pred = np.rint(orig_y_pred).astype(np.int)   #rounds prediction to the nearest integer</strong><br/><strong>    return accuracy_score(y_test, y_pred)</strong></pre>
<p style="padding-left: 60px">The <kbd>np.rint</kbd> function rounds off the prediction to the nearest integer, hopefully one of the targets, <kbd>0</kbd> or <kbd>1</kbd>. The <kbd>astype</kbd> method changes the type of the prediction to integer type, as the original target is in integer type and consistency is preferred with regard to types. After the rounding occurs, the scoring function uses the old <kbd>accuracy_score</kbd> function, which you are familiar with.</p>
<ol start="10">
<li>Now, determine whether a higher score is better. Higher accuracy is better, so for this situation, a higher score is better. In scikit code:</li>
</ol>
<pre style="padding-left: 60px"><strong>svr_to_class_scorer = make_scorer(for_scorer, greater_is_better=True)</strong> </pre>
<ol start="11">
<li>Finally, run the cross-validation with a new parameter, the scoring parameter:</li>
</ol>
<pre style="padding-left: 60px"><strong>svr_scores = cross_val_score(svr_clf, X_train, y_train, cv=4, scoring = svr_to_class_scorer)</strong></pre>
<ol start="12">
<li>Find the mean:</li>
</ol>
<pre style="padding-left: 60px"><strong>svr_scores.mean()</strong><br/><br/><strong>0.94663742690058483</strong></pre>
<p>The accuracy scores are similar for the SVR regressor-based classifier and the traditional SVC classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>You might ask, why did we take out class <kbd>2</kbd> out of the target set?</p>
<p>The reason is that, to use a regressor, our intent has to be to predict a real number. The categories had to have real number properties: that they are ordered (informally, if we have three ordered categories <em>x</em>, <em>y</em>, <em>z</em> and <em>x</em> &lt; <em>y</em> and <em>y</em> &lt; <em>z</em> then <em>x</em> &lt; <em>z</em>). By eliminating the third category, the <span>remaining</span><span> </span><span>flowers (Setosa and Versicolor) became ordered by a property we invented: Setosaness or Versicolorness.</span></p>
<p>The next time you encounter categories, you can consider whether they can be ordered. For example, if the dataset consists of shoe sizes, they can be ordered and a regressor can be applied, even though no one has a shoe size of 12.125.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear versus nonlinear</h1>
                </header>
            
            <article>
                
<p>Linear algorithms involve lines or hyperplanes. Hyperplanes are flat surfaces in any <em>n</em>-dimensional space. They tend to be easy to understand and explain, as they involve ratios (with an offset). Some functions that consistently and monotonically increase or decrease can be mapped to a linear function with a transformation. For example, exponential growth can be mapped to a line with the log transformation.</p>
<p>Nonlinear algorithms tend to be tougher to explain to colleagues and investors, yet ensembles of decision trees that are nonlinear tend to perform very well. KNN, which we examined earlier, is nonlinear. In some cases, functions not increasing or decreasing in a familiar manner are acceptable for the sake of accuracy.</p>
<p>Try a simple SVC with a polynomial kernel, as follows:</p>
<pre><strong>from sklearn.svm import SVC   #Usual import of SVC</strong><br/><strong>svc_poly_clf = SVC(kernel = 'poly', degree= 3).fit(X_train, y_train)  #Polynomial Kernel of Degree 3</strong></pre>
<p>The polynomial kernel of degree 3 looks like a cubic curve in two dimensions. It leads to a slightly better fit, but note that it can be harder to explain to others than a linear kernel with consistent behavior throughout all of the Euclidean space:</p>
<pre><strong>svc_poly_scores = cross_val_score(svc_clf, X_train, y_train, cv=4)</strong><br/><strong>svc_poly_scores.mean()</strong><br/><br/><strong>0.95906432748538006</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Black box versus not</h1>
                </header>
            
            <article>
                
<p>For the sake of efficiency, we did not examine the classification algorithms used very closely. When we compared SVC and logistic regression, we chose SVMs. At that point, both algorithms were black boxes, as we did not know any internal details. Once we decided to focus on SVMs, we could proceed to compute coefficients of the separating hyperplanes involved, optimize the hyperparameters of the SVM, use the SVM for big data, and do other processes. The SVMs have earned our time investment because of their superior performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interpretability</h1>
                </header>
            
            <article>
                
<p>Some machine learning algorithms are easier to understand than others. These are usually easier to explain to others as well. For example, linear regression is well known and easy to understand and explain to potential investors of your company. SVMs are more difficult to entirely understand.</p>
<p>My general advice: if SVMs are highly effective for a particular dataset, try to increase your personal interpretability of SVMs in the particular problem context. Also, consider merging algorithms somehow, using linear regression as an input to SVMs, for example. This way, you have the best of both worlds.</p>
<div class="packt_infobox">This is really context-specific, however. Linear SVMs are relatively simple to visualize and understand. Merging linear regression with SVM could complicate things. You can start by comparing them side by side.</div>
<p>However, if you cannot understand every detail of the math and practice of SVMs, be kind to yourself, as machine learning is focused more on prediction performance rather than traditional statistics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A pipeline</h1>
                </header>
            
            <article>
                
<p>In programming, a pipeline is a set of procedures connected in series, one after the other, where the output of one process is the input to the next:</p>
<div class="CDPAlignCenter CDPAlign"><img height="159" width="158" src="assets/dc22816a-cefb-4269-9c4d-04900308b55e.png"/></div>
<p>You can replace any procedure in the process with a different one, perhaps better in some way, without compromising the whole system. For the model in the middle step, you can use an SVC or logistic regression:</p>
<div class="CDPAlignCenter CDPAlign"><img height="161" width="161" src="assets/d2a52812-d38e-45ae-b047-3c96e13f0d62.png"/></div>
<p>One can also keep track of the classifier itself and build a flow diagram from the classifier. Here is a pipeline keeping track of the SVC classifier:</p>
<div class="CDPAlignCenter CDPAlign"><img height="167" width="156" src="assets/8267f221-917f-41ea-b55a-d9185d3fbb01.png"/></div>
<p>In the upcoming chapters, we will see how scikit-learn uses the intuitive notion of a pipeline. So far, we have used a simple one: train, predict, test.</p>


            </article>

            
        </section>
    </body></html>
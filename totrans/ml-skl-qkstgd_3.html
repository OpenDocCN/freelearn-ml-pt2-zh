<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Categories with Logistic Regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">The logistic regression algorithm is one of the most interpretable algorithms in the world of machine learning, and although the word "regression" implies predicting a numerical outcome, the logistic regression algorithm is, used to predict categories and solve classification machine learning problems. </p>
<p>In this chapter, you will learn about the following:</p>
<ul>
<li>How the logistic regression algorithm works mathematically</li>
<li>Implementing and evaluating your first logistic regression algorithm with scikit-learn</li>
<li>Fine-tuning the hyperparameters using <kbd>GridSearchCV</kbd></li>
<li>Scaling your data for a potential improvement in accuracy</li>
<li>Interpreting the results of the model</li>
</ul>
<p>Logistic regression has a wide range of applications, especially in the field of finance, where building interpretable machine learning models is key in convincing both investors and regulators alike that your model makes intuitive and logical sense. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You will be required to have <span class="fontstyle2">Python 3.6 or greater, </span><span class="fontstyle2">Pandas ≥ 0.23.4, </span><span class="fontstyle2">Scikit-learn ≥ 0.20.0, and </span><span class="fontstyle2">Matplotlib ≥ 3.0.0</span>  </span><span class="fontstyle0">installed on your system.</span></p>
<p class="mce-root">The code files of this chapter can be found on GitHub:<br/>
<a href="https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_03.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_03.ipynb</a></p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2DaTNgQ">http://bit.ly/2DaTNgQ</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding logistic regression mathematically </h1>
                </header>
            
            <article>
                
<p>As the name implies, logistic regression is fundamentally derived from the linear regression algorithm. The linear regression algorithm will be discussed in depth in the upcoming chapters. For now, let's consider a hypothetical case in which we want to predict the probability that a particular loan will default based on the loan's interest rate. Using linear regression, the following equation can be constructed:</p>
<p class="CDPAlignCenter CDPAlign"><kbd>Default = (Interest Rate × x) + c</kbd><em> </em></p>
<p>In the preceding equation, <em>c </em>is the intercept and <em>x </em>is a coefficient that will be the output from the logistic regression model. The intercept and the coefficient will have numeric values. For the purpose of this example, let's assume <em>c</em> is 5 and <em>x </em>is -0.2. The equation now becomes this:</p>
<p class="CDPAlignCenter CDPAlign"><kbd>Default = (Interest Rate × -0.2) + 5</kbd><em> </em></p>
<p>The equation can be represented in a two-dimensional plot using the following diagram: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/62295c65-377b-4206-a2a8-cc7052c49232.png" style=""/></div>
<p>Assuming that the interest rate is 10%, the value of default produced by the equation is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>Default = (10 × -0.2) + 5</em></p>
<p class="CDPAlignCenter CDPAlign"><em>Default = 3</em></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The logistic regression model now uses the <kbd>logit</kbd> function to transform this value of 3 into a probability between 0 and 1: </p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/8355ffed-7009-4f68-b1c7-896eba1d6313.png" style="width:4.67em;height:2.83em;"/></div>
<p>After evaluating the preceding equation, we get an answer of 0.95. In other words, using the logistic regression model that we just built mathematically, we obtained a probability of 95% that the loan would default if the interest rate was 10%. </p>
<p>After applying the <kbd>logit</kbd> function to the linear equation, the two-dimensional plot shown previously changes to the following diagram: </p>
<p class="CDPAlignCenter CDPAlign"/>
<p>In the preceding diagram, the following is happening:</p>
<ul>
<li>The function approaches 1 as the interest rate nears infinity along the <em>x</em>-axis.</li>
<li>The function approaches 0 as the interest rate nears 0 along the <em>x</em>-axis.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing logistic regression using scikit-learn</h1>
                </header>
            
            <article>
                
<p>In this section, you will learn how you can implement and quickly evaluate a logistic regression model for your dataset. We will be using the same dataset that we have already cleaned and prepared for the purpose of predicting whether a particular transaction was fraudulent. In the previous chapter, we saved this dataset as <kbd>fraud_detection.csv</kbd>. The first step is to load this dataset into your Jupyter Notebook. This can be done by using the following code: </p>
<pre>import pandas as pd<br/><br/># Reading in the dataset <br/><br/>df = pd.read_csv('fraud_prediction.csv')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splitting the data into training and test sets</h1>
                </header>
            
            <article>
                
<p>The first step to building any machine learning model with scikit-learn is to split the data into training and test sets. This can be done by using the following code: </p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>#Creating the features and target<br/><br/>features = df.drop('isFraud', axis = 1).values<br/>target = df['isFraud'].values<br/><br/>#Creating the training and testing data<br/><br/>X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = 42, stratify = target)</pre>
<p>The next step is to implement a base logistic regression classifier and evaluate its accuracy score. This can be done by using the following code: </p>
<pre>from sklearn import linear_model<br/><br/>#Initializing an logistic regression object<br/><br/>logistic_regression = linear_model.LogisticRegression()<br/><br/>#Fitting the model to the training and test sets<br/><br/>logistic_regression.fit(X_train, y_train)</pre>
<p>In the preceding code, the <kbd>linear_model</kbd><em> </em>package is imported from <kbd>sklearn</kbd> and is used to initialize the logistic regression algorithm by calling the <kbd>LogisticRegression()</kbd><em> <br/></em> method. This logistic regression algorithm is then fit into the training data. </p>
<p>In order to extract the accuracy score, we use the following code on the test data: </p>
<pre>#Accuracy score of the logistic regression model<br/><br/>logistic_regression.score(X_test, y_test)</pre>
<p>This model has produced an accuracy of 58.9% on the test data. This means that the base logistic regression model only performs slightly better than an algorithm that randomly guesses the output. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fine-tuning the hyperparameters</h1>
                </header>
            
            <article>
                
<p>From the output of the logistic regression model implemented in the preceding section, it is clear that the model performs slightly better than random guessing. Such a model fails to provide value to us. In order to optimize the model, we are going to optimize the hyperparameters of the logistic regression model by using the <kbd>GridSearchCV</kbd> algorithm that we used in the previous chapter. </p>
<p>The hyperparameter that is used by the logistic regression model is known as the inverse regularization strength. This is because we are implementing a type of linear regression known as <strong>l1</strong> regression. This type of linear regression will explained in detail in <a href="589b9373-c8dd-4243-aec8-9d6c4851f987.xhtml" target="_blank">Chapter 5</a>, <em>Predicting Numeric Outcomes with Linear Regression</em>. </p>
<p>In order to optimize the inverse regularization strength, or <strong>C</strong> as it is called in short, we use the following code: </p>
<pre>#Building the model with L1 penality <br/><br/>logistic_regression = linear_model.LogisticRegression(penalty='l1')<br/><br/>#Using GridSearchCV to search for the best parameter<br/><br/>grid = GridSearchCV(logistic_regression, {'C':[0.0001, 0.001, 0.01, 0.1, 10]})<br/>grid.fit(X_train, y_train)<br/><br/># Print out the best parameter<br/><br/>print("The most optimal inverse regularization strength is:", grid.best_params_)</pre>
<p>This produces an output as illustrated in the following screenshot: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2c9f6a51-729a-4cab-99c4-ba716af3bc17.png" style=""/></div>
<p>In the preceding code, we first initialize a logistic regression model with the penalty argument set to <strong>l1</strong>, indicating that we are using <strong>l1</strong> regression. We then initialize a grid with the possible values of inverse regularization strengths that go from 0.0001 to 10. </p>
<div class="packt_tip">The number of values that you initialize in a grid object for the hyperparameter of a model is arbitrary. However, the more values, the longer it takes for <kbd>GridSearchCV</kbd> to give you the optimal value of the hyperparameter, therby making the process computationally expensive. </div>
<p class="mce-root"/>
<p><span>The grid object with the possible values of the inverse regularization strengths are then fit into the training data and the optimal value is printed out, which in this case is 10. We can now build a new logistic regression model with this newly obtained optimal hyperparameter value by using the following code: </span></p>
<pre>#Initializing an logistic regression object<br/><br/>logistic_regression = linear_model.LogisticRegression(C = 10, penalty = 'l1')<br/><br/>#Fitting the model to the training and test sets<br/><br/>logistic_regression.fit(X_train, y_train)</pre>
<p>Evaluating the model on the test data by using the following code, we obtain an accuracy score of 99.6%! That's quite the improvement. </p>
<pre>#Accuracy score of the logistic regression model<br/><br/>logistic_regression.score(X_test, y_test)</pre>
<p>One way to check whether <kbd>GridSearchCV</kbd> is giving us accurate results is to plot the accuracy scores along the <em>y</em>-axis for different values of the inverse regularization strengths along the x-axis. This can be done by using the following code: </p>
<pre>train_errors = []<br/>test_errors = []<br/><br/>C_list = [0.0001, 0.001, 0.01, 0.1, 10, 100, 1000]<br/><br/># Evaluate the training and test classification errors for each value of C<br/><br/>for value in C_list:<br/> <br/> # Create LogisticRegression object and fit<br/> logistic_regression = linear_model.LogisticRegression(C= value, penalty = 'l1')<br/> logistic_regression.fit(X_train, y_train)<br/> <br/> # Evaluate error rates and append to lists<br/> train_errors.append(logistic_regression.score(X_train, y_train) )<br/> test_errors.append(logistic_regression.score(X_test, y_test))<br/> <br/># Plot results<br/>plt.semilogx(C_list, train_errors, C_list, test_errors)<br/>plt.legend(("train", "test"))<br/>plt.ylabel('Accuracy Score')<br/>plt.xlabel('C (Inverse regularization strength)')<br/>plt.show()</pre>
<p>This results in a plot as illustrated in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/822b690d-6fec-4165-9de9-1df7f8a6939b.png" style=""/></div>
<p>From the preceding plot, it is clear that an inverse regularization strength of 10 provides a high value of accuracy for both the training and testing sets. Such plots are also used to determine whether a particular value of the hyperparameter is overfitting the data by giving us a high accuracy score on the training set, but low accuracy scores on the test set. Conversely, they can also be used to check whether a model is undercutting the data by giving us low values of accuracy on the training set itself. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling the data</h1>
                </header>
            
            <article>
                
<p>Although the model has performed extremely well, scaling the data is still a useful step in building machine learning models with logistic regression, as it standardizes your data across the same range of values. In order to scale your data, we will use the same <kbd>StandardScaler()</kbd><em> </em>function that we used in the previous chapter. This is done by using the following code: </p>
<pre>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/><br/>#Setting up the scaling pipeline <br/><br/>pipeline_order = [('scaler', StandardScaler()), ('logistic_reg', linear_model.LogisticRegression(C = 10, penalty = 'l1'))]<br/><br/>pipeline = Pipeline(pipeline_order)<br/><br/>#Fitting the classfier to the scaled dataset <br/><br/>logistic_regression_scaled = pipeline.fit(X_train, y_train)<br/><br/>#Extracting the score <br/><br/>logistic_regression_scaled.score(X_test, y_test)</pre>
<p>The preceding code resulted in the improvement in the accuracy score of the model by 0.1%, which is good considering how the model had a very high accuracy score in the first place. The code is similar to the pipeline for scaling we built in the previous chapter for the k-NN algorithm, and there are no changes except for the fact that we have used a logistic regression model instead of the k-NN model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interpreting the logistic regression model</h1>
                </header>
            
            <article>
                
<p>One of the key benefits of the logistic regression algorithm is that it is highly interpretable. This means that the outcome of the model can be interpreted as a function of the input variables. This allows us to understand how each variable contributes to the eventual outcome of the model. </p>
<p>In the first section, we understood that the logistic regression model consists of coefficients for each variable and an intercept that can be used to explain how the model works. In order to extract the coefficients for each variable in the model, we use the following code: </p>
<pre>#Printing out the coefficients of each variable <br/><br/>print(logistic_regression.coef_)</pre>
<p>This results in an output as illustrated by the following screenshot: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7076338c-7f04-40e8-bb96-4965b1ce2713.png" style=""/></div>
<p>The coefficients are in the order in which the variables were in the dataset that was input into the model. In order to extract the intercept from the model, we use the following code: </p>
<pre>#Printing out the intercept of the model<br/><br/>print(logistic_regression.intercept_)</pre>
<p>This results in an output as shown in the following screenshot: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b59cf245-50ce-4030-9eb7-92a0fc145adc.png" style=""/></div>
<p>Now that we have the coefficients for each variable along with the intercept, we can construct an equation in the following form:</p>
<div class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/eab80713-884d-44a2-b5ab-9908a296763f.png" style="width:25.92em;height:1.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, you have learned how the logistic regression model works on a mathematical level. Although simplistic, the model proves to be formidable in terms of interpretability, which is highly beneficial in the financial industry. </p>
<p>You have also learned how to build and evaluate logistic regression algorithms using scikit-learn, and looked at hyperparameter optimization using the <kbd>GridSearchCV</kbd> algorithm. Additionally, you have learned to verify whether the results provided to you by the <kbd>GridSearchCV</kbd> algorithm are accurate by plotting the accuracy scores for different values of the hyperparameter. </p>
<p>Finally, you have scaled your data in order make it standardized and learned how to interpret your model on a mathematical level. </p>
<p class="mce-root"><span>In the next chapter, you will learn how to implement tree-based algorithms, such as decision trees, random forests, and gradient-boosted trees, using scikit-learn.</span></p>


            </article>

            
        </section>
    </body></html>
["```py\npip install gymnasium \n```", "```py\npip install gymnasium [toy-text] \n```", "```py\n>>> import gymnasium as gym\n>>> print(gym.envs.registry.keys())\ndict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'phys2d/CartPole-v0', 'phys2d/CartPole-v1', 'phys2d/Pendulum-v0', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'tabular/Blackjack-v0', 'tabular/CliffWalking-v0', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4', 'GymV21Environment-v0', 'GymV26Environment-v0']) \n```", "```py\n    >>> env = gym.make(\"FrozenLake-v1\" , render_mode=\"rgb_array\")\n    >>> n_state = env.observation_space.n\n    >>> print(n_state)\n    16\n    >>> n_action = env.action_space.n\n    >>> print(n_action)\n    4 \n    ```", "```py\n    >>> env.reset()\n    (0, {'prob': 1}) \n    ```", "```py\n    >>> import matplotlib.pyplot as plt\n    >>> plt.imshow(env.render()) \n    ```", "```py\npip install pyglet \n```", "```py\n    >>> new_state, reward, terminated, truncated, info = env.step(2)\n    >>> is_done = terminated or truncated\n    >>> env.render()\n    >>> print(new_state)\n    4\n    >>> print(reward)\n    0.0\n    >>> print(is_done)\n    False\n    >>> print(info)\n    {'prob': 0.3333333333333333} \n    ```", "```py\n>>> plt.imshow(env.render()) \n```", "```py\n    >>> def run_episode(env, policy):\n    ...     state, _= env.reset()\n    ...     total_reward = 0\n    ...     is_done = False\n    ...     while not is_done:\n    ...         action = policy[state].item()\n    ...         state, reward, terminated, truncated, info = env.step(action)\n    ...         is_done = terminated or truncated\n    ...         total_reward += reward\n    ...         if is_done:\n    ...             break\n    ...     return total_reward \n    ```", "```py\n    >>> import torch\n    >>> n_episode = 1000\n    >>> total_rewards = []\n    >>> for episode in range(n_episode):\n    ...     random_policy = torch.randint(high=n_action, size=(n_state,))\n    ...     total_reward = run_episode(env, random_policy)\n    ...     total_rewards.append(total_reward)\n    ...\n    >>> print(f'Average total reward under random policy:\n              {sum(total_rewards)/n_episode}')\n    Average total reward under random policy: 0.016 \n    ```", "```py\n    >>> print(env.env.P[6])\n    {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]} \n    ```", "```py\n    >>> gamma = 0.99\n    >>> threshold = 0.0001 \n    ```", "```py\n    >>> def value_iteration(env, gamma, threshold):\n    ...     \"\"\"\n    ...     Solve a given environment with value iteration algorithm\n    ...     @param env: Gymnasium environment\n    ...     @param gamma: discount factor\n    ...     @param threshold: the evaluation will stop once values for all states are less than the threshold\n    ...     @return: values of the optimal policy for the given environment\n    ...     \"\"\"\n    ...     n_state = env.observation_space.n\n    ...     n_action = env.action_space.n\n    ...     V = torch.zeros(n_state)\n    ...     while True:\n    ...         V_temp = torch.empty(n_state)\n    ...         for state in range(n_state):\n    ...             v_actions = torch.zeros(n_action)\n    ...             for action in range(n_action):\n    ...                 for trans_prob, new_state, reward, _ in \\\n                                           env.env.P[state][action]:\n    ...                     v_actions[action] += trans_prob * (\n                                         reward + gamma * V[new_state])\n    ...             V_temp[state] = torch.max(v_actions)\n    ...         max_delta = torch.max(torch.abs(V - V_temp))\n    ...         V = V_temp.clone()\n    ...         if max_delta <= threshold:\n    ...             break\n    ...     return V \n    ```", "```py\n    >>> V_optimal = value_iteration(env, gamma, threshold) \n    ```", "```py\n>>> print('Optimal values:\\n', V_optimal)\nOptimal values:\ntensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905, 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000]) \n```", "```py\n    >>> def extract_optimal_policy(env, V_optimal, gamma):\n    ...     \"\"\"\n    ...     Obtain the optimal policy based on the optimal values\n    ...     @param env: Gymnasium environment\n    ...     @param V_optimal: optimal values\n    ...     @param gamma: discount factor\n    ...     @return: optimal policy\n    ...     \"\"\"\n    ...     n_state = env.observation_space.n\n    ...     n_action = env.action_space.n\n    ...     optimal_policy = torch.zeros(n_state)\n    ...     for state in range(n_state):\n    ...         v_actions = torch.zeros(n_action)\n    ...         for action in range(n_action):\n    ...             for trans_prob, new_state, reward, _ in\n                                       env.env.P[state][action]:\n    ...                 v_actions[action] += trans_prob * (\n                               reward + gamma * V_optimal[new_state])\n    ...         optimal_policy[state] = torch.argmax(v_actions)\n    ...     return optimal_policy \n    ```", "```py\n    >>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma) \n    ```", "```py\n>>> print('Optimal policy:\\n', optimal_policy)\nOptimal policy:\ntensor([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.]) \n```", "```py\n    >>> def run_episode(env, policy):\n            state, _ = env.reset()\n            total_reward = 0\n            is_done = False\n            while not is_done:\n                action = policy[state].item()\n                state, reward, terminated, truncated, info = env.step(action)\n                is_done = terminated or truncated\n                total_reward += reward\n                if is_done:\n                    break\n            return total_reward\n    >>> n_episode = 1000\n    >>> total_rewards = []\n    >>> for episode in range(n_episode):\n    ...     total_reward = run_episode(env, optimal_policy)\n    ...     total_rewards.append(total_reward) \n    ```", "```py\n>>> print('Average total reward under the optimal policy:', sum(total_rewards) / n_episode)\nAverage total reward under the optimal policy: 0.738 \n```", "```py\n    >>> def policy_evaluation(env, policy, gamma, threshold):\n    ...  \"\"\"\n    ...     Perform policy evaluation\n    ...     @param env: Gymnasium environment\n    ...     @param policy: policy matrix containing actions and\n            their probability in each state\n    ...     @param gamma: discount factor\n    ...     @param threshold: the evaluation will stop once values\n            for all states are less than the threshold\n    ...     @return: values of the given policy\n    ...  \"\"\"\n    ...     n_state = policy.shape[0]\n    ...     V = torch.zeros(n_state)\n    ...     while True:\n    ...         V_temp = torch.zeros(n_state)\n    ...         for state in range(n_state):\n    ...             action = policy[state].item()\n    ...             for trans_prob, new_state, reward, _ in \\\n                                         env.env.P[state][action]:\n    ...                 V_temp[state] += trans_prob * \n                                         (reward + gamma * V[new_state])\n    ...         max_delta = torch.max(torch.absâ€“V - V_temp))\n    ...         V = V_temp.clone()\n    ...         if max_delta <= threshold:\n    ...             break\n    ...     return V \n    ```", "```py\n    >>> def policy_improvement(env, V, gamma):\n    ...  \"\"\"\"\"\"\n    ...     Obtain an improved policy based on the values\n    ...     @param env: Gymnasium environment\n    ...     @param V: policy values\n    ...     @param gamma: discount factor\n    ...     @return: the policy\n    ...  \"\"\"\"\"\"\n    ...     n_state = env.observation_space.n\n    ...     n_action = env.action_space.n\n    ...     policy = torch.zeros(n_state)\n    ...     for state in range(n_state):\n    ...         v_actions = torch.zeros(n_action)\n    ...         for action in range(n_action):\n    ...             for trans_prob, new_state, reward, _ in\n                                          env.env.P[state][action]:\n    ...                 v_actions[action] += trans_prob * (\n                                      reward + gamma * V[new_state])\n    ...         policy[state] = torch.argmax(v_actions)\n    ...     return policy \n    ```", "```py\n    >>> def policy_iteration(env, gamma, threshold):\n    ...  \"\"\"\n    ...     Solve a given environment with policy iteration algorithm\n    ...     @param env: Gymnasium environment\n    ...     @param gamma: discount factor\n    ...     @param threshold: the evaluation will stop once values for all states are less than the threshold\n    ...     @return: optimal values and the optimal policy for the given environment\n    ...  \"\"\"\n    ...     n_state = env.observation_space.n\n    ...     n_action = env.action_space.n\n    ...     policy = torch.randint(high=n_action,\n                                   size=(n_state,)).float()\n    ...     while True:\n    ...         V = policy_evaluation(env, policy, gamma, threshold)\n    ...         policy_improved = policy_improvement(env, V, gamma)\n    ...         if torch.equal(policy_improved, policy):\n    ...             return V, policy_improved\n    ...         policy = policy_improved \n    ```", "```py\n    >>> V_optimal, optimal_policy = policy_iteration(env, gamma, threshold) \n    ```", "```py\n    >>> print('Optimal values'\\n', V_optimal)\n    Optimal values:\n    tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905, 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])\n    >>> print('Optimal policy'\\n', optimal_policy)\n    Optimal policy:\n    tensor([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.]) \n    ```", "```py\n    >>> env = gym.make('Blackjack'v1') \n    ```", "```py\n    >>> env.reset(seed=0)\n    ((11, 10, False), {}) \n    ```", "```py\n    >>> env.step(1)\n    ((12, 10, False), 0.0, False, False, {}) \n    ```", "```py\n    >>> env.step(1)\n    ((13, 10, False), 0.0, False, False, {}) \n    ```", "```py\n    >>> env.step(0)\n    ((13, 10, False), -1.0, True, False, {}) \n    ```", "```py\n    >>> def run_episode(env, hold_score):\n    ...     state , _ = env.reset()\n    ...     rewards = []\n    ...     states = [state]\n    ...     while True:\n    ...         action = 1 if state[0] < hold_score else 0\n    ...         state, reward, terminated, truncated, info = env.step(action)\n    ...         is_done = terminated or truncated\n    ...         states.append(state)\n    ...         rewards.append(reward)\n    ...         if is_done:\n    ...             break\n    ...     return states, rewards \n    ```", "```py\n    >>> from collections import defaultdict\n    >>> def mc_prediction_first_visit(env, hold_score, gamma, n_episode):\n    ...     V = defaultdict(float)\n    ...     N = defaultdict(int)\n    ...     for episode in range(n_episode):\n    ...         states_t, rewards_t = run_episode(env, hold_score)\n    ...         return_t = 0\n    ...         G = {}\n    ...         for state_t, reward_t in zip(\n                               states_t[1::-1], rewards_t[::-1]):\n    ...             return_t = gamma * return_t + reward_t\n    ...             G[state_t] = return_t\n    ...         for state, return_t in G.items():\n    ...             if state[0] <= 21:\n    ...                 V[state] += return_t\n    ...                 N[state] += 1\n    ...     for state in V:\n    ...         V[state] = V[state] / N[state]\n    ...     return V \n    ```", "```py\n    >>> hold_score = 18\n    >>> gamma = 1\n    >>> n_episode = 500000 \n    ```", "```py\n    >>> value = mc_prediction_first_visit(env, hold_score, gamma, n_episode) \n    ```", "```py\n>>> print(value)\ndefaultdict(<class 'float'>, {(13, 10, False): -0.2743235693191795, (5, 10, False): -0.3903118040089087, (19, 7, True): 0.6293800539083558, (17, 7, True): -0.1297709923664122, (18, 7, False): 0.4188926663428849, (13, 7, False): -0.04472843450479233, (19, 10, False): -0.016647081864473168, (12, 10, False): -0.24741546832491254, (21, 10, True):\nâ€¦â€¦\nâ€¦â€¦\nâ€¦â€¦\n2, 2, True): 0.07981220657276995, (5, 5, False): -0.25877192982456143, (4, 9, False): -0.24497991967871485, (15, 5, True): -0.011363636363636364, (15, 2, True): -0.08379888268156424, (5, 3, False): -0.19078947368421054, (4, 3, False): -0.2987012987012987}) \n```", "```py\n>>> print('Number of states:', len(value))\nNumber of states: 280 \n```", "```py\n    >>> def run_episode(env, Q, n_action):\n    ...     state, _ = env.reset()\n    ...     rewards = []\n    ...     actions = []\n    ...     states = []\n    ...     action = torch.randint(0, n_action, [1]).item()\n    ...     while True:\n    ...         actions.append(action)\n    ...         states.append(state)\n    ...         state, reward, terminated, truncated, info = env.step(action)\n    ...         is_done = terminated or truncated\n    ...         rewards.append(reward)\n    ...         if is_done:\n    ...             break\n    ...         action = torch.argmax(Q[state]).item()\n    ...     return states, actions, rewards \n    ```", "```py\n    >>> def mc_control_on_policy(env, gamma, n_episode):\n    ...     G_sum = defaultdict(float)\n    ...     N = defaultdict(int)\n    ...     Q = defaultdict(lambda: torch.empty(env.action_space.n))\n    ...     for episode in range(n_episode):\n    ...         states_t, actions_t, rewards_t =\n                           run_episode(env,  Q,  env.action_space.n)\n    ...         return_t = 0\n    ...         G = {}\n    ...         for state_t, action_t, reward_t in zip(state_t[::-1], \n                                                       actions_t[::-1],\n                                                       rewards_t[::-1]):\n    ...             return_t = gamma * return_t + reward_t\n    ...             G[(state_t, action_t)] = return_t\n    ...         for state_action, return_t in G.items():\n    ...             state, action = state_action\n    ...             if state[0] <= 21:\n    ...                 G_sum[state_action] += return_t\n    ...                 N[state_action] += 1\n    ...                 Q[state][action] =\n                              G_sum[state_action] / N[state_action]\n    ...     policy = {}\n    ...     for state, actions in Q.items():\n    ...         policy[state] = torch.argmax(actions).item()\n    ...     return Q, policy \n    ```", "```py\n    >>> gamma = 1\n    >>> n_episode = 500000\n    >>> optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode) \n    ```", "```py\n>>> print(optimal_policy)\n{(16, 8, True): 1, (11, 2, False): 1, (15, 5, True): 1, (14, 9, False): 1, (11, 6, False): 1, (20, 3, False): 0, (9, 6, False):\n0, (12, 9, False): 0, (21, 2, True): 0, (16, 10, False): 1, (17, 5, False): 0, (13, 10, False): 1, (12, 10, False): 1, (14, 10, False): 0, (10, 2, False): 1, (20, 4, False): 0, (11, 4, False): 1, (16, 9, False): 0, (10, 8,\nâ€¦â€¦\nâ€¦â€¦\n1, (18, 6, True): 0, (12, 2, True): 1, (8, 3, False): 1, (13, 3, True): 0, (4, 7, False): 1, (18, 8, True): 0, (6, 5, False): 1, (17, 6, True): 0, (19, 9, True): 0, (4, 4, False): 0, (14, 5, True): 1, (12, 6, True): 0, (4, 9, False): 1, (13, 4, True): 1, (4, 8, False): 1, (14, 3, True): 1, (12, 4, True): 1, (4, 6, False): 0, (12, 5, True): 0, (4, 2, False): 1, (4, 3, False): 1, (5, 4, False): 1, (4, 1, False): 0} \n```", "```py\n    >>> def simulate_hold_episode(env, hold_score):\n    ...     state, _ = env.reset()\n    ...     while True:\n    ...         action = 1 if state[0] < hold_score else 0\n    ...         state, reward, terminated, truncated, info = env.step(action)\n    ...         is_done = terminated or truncated\n    ...         if is_done:\n    ...             return reward \n    ```", "```py\n    >>> def simulate_episode(env, policy):\n    ...     state, _ = env.reset()\n    ...     while True:\n    ...         action = policy[state]\n    ...         state, reward, terminated, truncated, info = env.step(action)\n    ...         is_done = terminated or truncated\n    ...         if is_done:\n    ...             return reward \n    ```", "```py\n    >>> n_episode = 100000\n    >>> hold_score = 18\n    >>> n_win_opt = 0\n    >>> n_win_hold = 0\n    >>> for _ in range(n_episode):\n    ...     reward = simulate_episode(env, optimal_policy)\n    ...     if reward == 1:\n    ...         n_win_opt += 1\n    ...     reward = simulate_hold_episode(env, hold_score)\n    ...     if reward == 1:\n    ...         n_win_hold += 1 \n    ```", "```py\n>>> print(f'Winning probability under the simple policy: {n_win_hold/n_episode}')\nWinning probability under the simple policy: 0.40256\n>>>print(f'Winning probability under the optimal policy: {n_win_opt/n_episode}')\nWinning probability under the optimal policy: 0.43148 \n```", "```py\n    >>> def epsilon_greedy_policy(n_action, epsilon, state, Q):\n    ...     probs = torch.ones(n_action) * epsilon / n_action\n    ...     best_action = torch.argmax(Q[state]).item()\n    ...     probs[best_action] += 1.0 - epsilon\n    ...     action = torch.multinomial(probs, 1).item()\n    ...     return action \n    ```", "```py\n    >>> epsilon = 1.0\n    >>> final_epsilon = 0.1 \n    ```", "```py\n    >>> def q_learning(env, gamma, n_episode, alpha, epsilon, final_epsilon):\n            n_action = env.action_space.n\n            Q = defaultdict(lambda: torch.zeros(n_action))\n            epsilon_decay = epsilon / (n_episode / 2)\n            for episode in range(n_episode):\n                state, _ = env.reset()\n                is_done = False\n                epsilon = max(final_epsilon, epsilon - epsilon_decay)\n                while not is_done:\n                    action = epsilon_greedy_policy(n_action, epsilon, state, Q)\n                    next_state, reward, terminated, truncated, info = env.step(action)\n                    is_done = terminated or truncated\n                    delta = reward + gamma * torch.max(\n                                               Q[next_state]) - Q[state][action]\n                    Q[state][action] += alpha * delta\n                    total_reward_episode[episode] += reward\n                    if is_done:\n                        break\n                    state = next_state\n            policy = {}\n            for state, actions in Q.items():\n                policy[state] = torch.argmax(actions).item()\n            return Q, policy \n    ```", "```py\n    >>> n_episode = 10000\n    >>> total_reward_episode = [0] * n_episode \n    ```", "```py\n    >>> gamma = 1\n    >>> alpha = 0.003\n    >>> optimal_Q, optimal_policy = q_learning(env, gamma, n_episode, alpha,\n                                               epsilon, final_epsilon) \n    ```", "```py\n    >>> rolling_avg_reward = [total_reward_episode[0]]\n    >>> for i, reward in enumerate(total_reward_episode[1:], 1):\n            rolling_avg_reward.append((rolling_avg_reward[-1]*i + reward)/(i+1))\n    >>> plt.plot(rolling_avg_reward)\n    >>> plt.title('Average reward over time')\n    >>> plt.xlabel('Episode')\n    >>> plt.ylabel('Average reward')\n    >>> plt.ylim([-1, 1])\n    >>> plt.show() \n    ```", "```py\n    >>> n_episode = 100000\n    >>> n_win_opt = 0\n    >>> for _ in range(n_episode):\n    ...     reward = simulate_episode(env, optimal_policy)\n    ...     if reward == 1:\n    ...         n_win_opt += 1\n    >>>print(f'Winning probability under the optimal policy: {n_win_opt/n_episode}')\n    Winning probability under the optimal policy: 0.42398 \n    ```"]
<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Finding Patterns &#x2013; Market Basket Analysis Using Association Rules"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Finding Patterns – Market Basket Analysis Using Association Rules</h1></div></div></div><p>Think back to the last time you made an impulse purchase. Maybe you were waiting in the grocery store checkout lane and bought a pack of chewing gum or a candy bar. Perhaps on a late-night trip for diapers and formula you picked up a caffeinated beverage or a six-pack of beer. You might have even bought this book on a whim on a bookseller's recommendation. These impulse buys are no coincidence, as retailers use sophisticated data analysis techniques to identify patterns that will drive retail behavior.</p><p>In years past, such recommendation systems were based on the subjective intuition of marketing professionals and inventory managers or buyers. More recently, as barcode scanners, computerized inventory systems, and online shopping trends have built a wealth of transactional data, machine learning has been increasingly applied to learn purchasing patterns. The <a id="id00" class="indexterm"/>practice is commonly known as <span class="strong"><strong>market basket analysis</strong></span> due to the fact that it has been so frequently applied to supermarket data.</p><p>Although the technique originated with shopping data, it is useful in other contexts as well. By the time you finish this chapter, you will be able to apply market basket analysis techniques to your own tasks, whatever they may be. Generally, the work involves:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Using simple performance measures to find associations in large databases</li><li class="listitem" style="list-style-type: disc">Understanding the peculiarities of transactional data</li><li class="listitem" style="list-style-type: disc">Knowing how to identify the useful and actionable patterns</li></ul></div><p>The results of a market basket analysis are actionable patterns. Thus, as we apply the technique, you are likely to identify applications to your work, even if you have no affiliation with a retail chain.</p><div class="section" title="Understanding association rules"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec01"/>Understanding association rules</h1></div></div></div><p>The building <a id="id01" class="indexterm"/>blocks of a market basket analysis are the items that may appear in any given transaction. Groups of one or more items are surrounded by brackets to <a id="id02" class="indexterm"/>indicate that they form a set, or more specifically, an <span class="strong"><strong>itemset</strong></span> that appears in the data with some regularity. Transactions are specified in terms of itemsets, such as the following transaction that might be found in a typical grocery store:</p><div class="mediaobject"><img src="graphics/B03905_08_01.jpg" alt="Understanding association rules"/></div><p>The result of a market basket analysis is a collection of <span class="strong"><strong>association rules</strong></span> that specify patterns found in the relationships among items he itemsets. Association rules are always composed from subsets of itemsets and are denoted by relating one itemset on the left-hand <a id="id03" class="indexterm"/>side (LHS) of the rule to another itemset on the right-hand side (RHS) of the rule. The LHS is the condition that needs to be met in order to trigger the rule, and the RHS is the expected result of meeting that condition. A rule identified from the example transaction might be expressed in the form:</p><div class="mediaobject"><img src="graphics/B03905_08_02.jpg" alt="Understanding association rules"/></div><p>In plain language, this association rule states that if peanut butter and jelly are purchased together, then bread is also likely to be purchased. In other words, "peanut butter and jelly imply bread."</p><p>Developed in the context of retail transaction databases, association rules are not used for prediction, but rather for unsupervised knowledge discovery in large databases. This is unlike the classification and numeric prediction algorithms presented in previous chapters. Even so, you will find that association rule learners are closely related to and share many features of the classification rule learners presented in <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span>.</p><p>Because association rule learners are unsupervised, there is no need for the algorithm to be trained; data does not need to be labeled ahead of time. The program is simply unleashed on a dataset in the hope that interesting associations are found. The downside, of course, is that there isn't an easy way to objectively measure the performance of a rule learner, aside from evaluating them for qualitative usefulness—typically, an eyeball test of some sort.</p><p>Although association rules are most often used for market basket analysis, they are helpful for finding patterns in many different types of data. Other potential applications include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Searching <a id="id04" class="indexterm"/>for interesting and frequently occurring patterns of DNA and protein sequences in cancer data</li><li class="listitem" style="list-style-type: disc">Finding patterns of purchases or medical claims that occur in combination with fraudulent credit card or insurance use</li><li class="listitem" style="list-style-type: disc">Identifying combinations of behavior that precede customers dropping their cellular phone service or upgrading their cable television package</li></ul></div><p>Association rule analysis is used to search for interesting connections among a very large number of elements. Human beings are capable of such insight quite intuitively, but it often takes expert-level knowledge or a great deal of experience to do what a rule learning algorithm can do in minutes or even seconds. Additionally, some datasets are simply too large and complex for a human being to find the needle in the haystack.</p><div class="section" title="The Apriori algorithm for association rule learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec01"/>The Apriori algorithm for association rule learning</h2></div></div></div><p>Just as it is <a id="id05" class="indexterm"/>challenging for humans, transactional data makes association rule mining a challenging task for machines as well. Transactional datasets are typically extremely large, both in terms of the number of transactions as well as the number of items or features that are monitored. The problem is that the number of potential itemsets grows exponentially with the number of features. Given <span class="emphasis"><em>k</em></span> items that can appear or not appear in a set, there are <span class="emphasis"><em>2^k</em></span> possible itemsets that could be potential rules. A retailer that sells only 100 different items could have on the order of <span class="emphasis"><em>2^100 = 1.27e+30</em></span> itemsets that an algorithm must evaluate—a seemingly impossible task.</p><p>Rather than evaluating each of these itemsets one by one, a smarter rule learning algorithm takes advantage of the fact that, in reality, many of the potential combinations of items are rarely, if ever, found in practice. For instance, even if a store sells both automotive items and women's cosmetics, a set of <span class="emphasis"><em>{motor oil, lipstick}</em></span> is likely to be extraordinarily uncommon. By ignoring these rare (and, perhaps, less important) combinations, it is possible to limit the scope of the search for rules to a more manageable size.</p><p>Much work has been done to identify heuristic algorithms for reducing the number of itemsets to search. Perhaps the most-widely used approach for efficiently searching large databases for rules is known as <span class="strong"><strong>Apriori</strong></span>. Introduced in 1994 by Rakesh Agrawal and <a id="id06" class="indexterm"/>Ramakrishnan Srikant, the Apriori algorithm has since become somewhat synonymous with association rule learning. The name is derived from the fact that the algorithm utilizes a simple prior (that is, <span class="emphasis"><em>a priori</em></span>) belief about the properties of frequent itemsets.</p><p>Before we discuss that in more depth, it's worth noting that this algorithm, like all learning algorithms, is not without its strengths and weaknesses. Some of these are listed as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strengths</p>
</th><th style="text-align: left" valign="bottom">
<p>Weaknesses</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Is capable of working <a id="id07" class="indexterm"/>with large amounts of transactional data</li><li class="listitem" style="list-style-type: disc">Results in rules that are easy to understand</li><li class="listitem" style="list-style-type: disc">Useful for "data mining" and discovering unexpected knowledge in databases</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Not very helpful for small datasets</li><li class="listitem" style="list-style-type: disc">Requires effort to separate the true insight from common sense</li><li class="listitem" style="list-style-type: disc">Easy to draw spurious conclusions from random patterns</li></ul></div>
</td></tr></tbody></table></div><p>As noted earlier, the Apriori algorithm employs a simple <span class="emphasis"><em>a priori</em></span> belief to reduce the association rule search space: all subsets of a frequent itemset must also be frequent. This heuristic is <a id="id008" class="indexterm"/>known as the <span class="strong"><strong>Apriori property</strong></span>. Using this astute observation, it is possible to dramatically limit the number of rules to be searched. For example, the set <span class="emphasis"><em>{motor oil, lipstick}</em></span> can only be frequent if both <span class="emphasis"><em>{motor oil}</em></span> and <span class="emphasis"><em>{lipstick}</em></span> <a id="id009" class="indexterm"/>occur frequently as well. Consequently, if either motor oil or lipstick is infrequent, any set containing these items can be excluded from the search.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note001"/>Note</h3><p>For additional details on the Apriori algorithm, refer to: Agrawal R, Srikant R. Fast algorithms for mining association rules.<span class="emphasis"><em> Proceedings of the 20th International Conference on Very Large Databases</em></span>. 1994:487-499.</p></div></div><p>To see how this principle can be applied in a more realistic setting, let's consider a simple transaction database. The following table shows five completed transactions in an imaginary hospital's gift shop:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Transaction number</p>
</th><th style="text-align: left" valign="bottom">
<p>Purchased items</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{flowers, get well card, soda}</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{plush toy bear, flowers, balloons, candy bar}</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{get well card, candy bar, flowers}</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{plush toy bear, balloons, soda}</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{flowers, get well card, soda}</em></span>
</p>
</td></tr></tbody></table></div><p>By looking at the sets of purchases, one can infer that there are a couple of typical buying patterns. A person visiting a sick friend or family member tends to buy a get well card and flowers, while visitors to new mothers tend to buy plush toy bears and balloons. Such patterns are notable because they appear frequently enough to catch our interest; we simply apply a <a id="id010" class="indexterm"/>bit of logic and subject matter experience to explain the rule.</p><p>In a similar fashion, the Apriori algorithm uses statistical measures of an itemset's "interestingness" to locate association rules in much larger transaction databases. In the sections that follow, we will discover how Apriori computes such measures of interest and how they are combined with the Apriori property to reduce the number of rules to be learned.</p></div><div class="section" title="Measuring rule interest – support and confidence"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec02"/>Measuring rule interest – support and confidence</h2></div></div></div><p>Whether or not <a id="id011" class="indexterm"/>an association rule is deemed interesting is determined by two statistical measures: support and confidence measures. By providing minimum thresholds for each of these metrics and applying the Apriori principle, it is easy to drastically limit the number of rules reported, perhaps even to the point where only the obvious or common sense rules are identified. For this reason, it is important to carefully understand the types of rules that are excluded under these criteria.</p><p>The <span class="strong"><strong>support</strong></span> of an itemset or rule measures how frequently it occurs in the data. For instance the itemset <span class="emphasis"><em>{get well card, flowers}</em></span>, has support of <span class="emphasis"><em>3 / 5 = 0.6</em></span> in the hospital gift shop data. Similarly, the support for <span class="emphasis"><em>{get well card} → {flowers}</em></span> is also 0.6. The support can be calculated for any itemset or even a single item; for instance, the support for <span class="emphasis"><em>{candy bar}</em></span> is <span class="emphasis"><em>2 / 5 = 0.4</em></span>, since candy bars appear in 40 percent of purchases. A function defining support for the itemset <span class="emphasis"><em>X</em></span> can be defined as follows:</p><div class="mediaobject"><img src="graphics/B03905_08_03.jpg" alt="Measuring rule interest – support and confidence"/></div><p>Here, <span class="emphasis"><em>N</em></span> is the number of transactions in the database and <span class="emphasis"><em>count(X)</em></span> is the number of transactions containing itemset <span class="emphasis"><em>X</em></span>.</p><p>A rule's <span class="strong"><strong>confidence</strong></span> is a measurement of its predictive power or accuracy. It is defined as the support of the itemset containing both <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> divided by the support of the itemset containing only <span class="emphasis"><em>X</em></span>:</p><div class="mediaobject"><img src="graphics/B03905_08_04.jpg" alt="Measuring rule interest – support and confidence"/></div><p>Essentially, the confidence tells us the proportion of transactions where the presence of item or itemset <span class="emphasis"><em>X</em></span> results in the presence of item or itemset <span class="emphasis"><em>Y</em></span>. Keep in mind that the confidence that <span class="emphasis"><em>X</em></span> leads to <span class="emphasis"><em>Y</em></span> is not the same as the confidence that <span class="emphasis"><em>Y</em></span> leads to <span class="emphasis"><em>X</em></span>. For example, the confidence of <span class="emphasis"><em>{flowers} → {get well card}</em></span> is <span class="emphasis"><em>0.6 / 0.8 = 0.75</em></span>. In comparison, the confidence of <span class="emphasis"><em>{get well card} → {flowers}</em></span> is <span class="emphasis"><em>0.6 / 0.6 = 1.0</em></span>. This means that a purchase involving flowers is accompanied by a purchase of a get well card 75 percent of the time, while a purchase of a get well card is associated with flowers 100 <a id="id012" class="indexterm"/>percent of the time. This information could be quite useful to the gift shop management.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip001"/>Tip</h3><p>You may have noticed similarities between support, confidence, and the Bayesian probability rules covered in <a class="link" href="ch04.html" title="Chapter 4. Probabilistic Learning – Classification Using Naive Bayes">Chapter 4</a>, <span class="emphasis"><em>Probabilistic Learning – Classification Using Naive Bayes</em></span>. In fact, <span class="emphasis"><em>support(A, B)</em></span> is the same as <span class="emphasis"><em>P(A∩B)</em></span> and <span class="emphasis"><em>confidence(A → B)</em></span> is the same as <span class="emphasis"><em>P(B|A)</em></span>. It is just the context that differs.</p></div></div><p>Rules like <span class="emphasis"><em>{get well card} → {flowers}</em></span> are known as <span class="strong"><strong>strong rules</strong></span>, because they have both high support and confidence. One way to find more strong rules would be to examine every possible combination of the items in the gift shop, measure the support and confidence value, and report back only those rules that meet certain levels of interest. However, as noted before, this strategy is generally not feasible for anything but the smallest of datasets.</p><p>In the next section, you will see how the Apriori algorithm uses the minimum levels of support and confidence with the Apriori principle to find strong rules quickly by reducing the number of rules to a more manageable level.</p></div><div class="section" title="Building a set of rules with the Apriori principle"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec03"/>Building a set of rules with the Apriori principle</h2></div></div></div><p>Recall that <a id="id013" class="indexterm"/>the Apriori <a id="id014" class="indexterm"/>principle states that all subsets of a frequent itemset must also be frequent. In other words, if <span class="emphasis"><em>{A, B}</em></span> is frequent, then <span class="emphasis"><em>{A}</em></span> and <span class="emphasis"><em>{B}</em></span> must both be frequent. Recall also that by definition, the support indicates how frequently an itemset appears in the data. Therefore, if we know that <span class="emphasis"><em>{A}</em></span> does not meet a desired support threshold, there is no reason to consider <span class="emphasis"><em>{A, B}</em></span> or any itemset containing <span class="emphasis"><em>{A}</em></span>; it cannot possibly be frequent.</p><p>The Apriori algorithm uses this logic to exclude potential association rules prior to actually evaluating them. The actual process of creating rules occurs in two phases:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Identifying all the itemsets that meet a minimum support threshold.</li><li class="listitem">Creating rules from these itemsets using those meeting a minimum confidence threshold.</li></ol></div><p>The first phase occurs in multiple iterations. Each successive iteration involves evaluating the support of a set of increasingly large itemsets. For instance, iteration 1 involves evaluating the set of 1-item itemsets (1-itemsets), iteration 2 evaluates 2-itemsets, and so on. The result of each iteration <span class="emphasis"><em>i</em></span> is a set of all the <span class="emphasis"><em>i</em></span>-itemsets that meet the minimum support threshold.</p><p>All the itemsets from iteration <span class="emphasis"><em>i</em></span> are combined in order to generate candidate itemsets for the evaluation in iteration <span class="emphasis"><em>i + 1</em></span>. But the Apriori principle can eliminate some of them even before the next round begins. If <span class="emphasis"><em>{A}</em></span>, <span class="emphasis"><em>{B}</em></span>, and <span class="emphasis"><em>{C}</em></span> are frequent in iteration 1 while <span class="emphasis"><em>{D}</em></span> is not frequent, iteration 2 will consider only <span class="emphasis"><em>{A, B}</em></span>, <span class="emphasis"><em>{A, C}</em></span>, and <span class="emphasis"><em>{B, C}</em></span>. Thus, the algorithm needs to evaluate only three itemsets rather than the six that would have been evaluated if the sets containing <span class="emphasis"><em>D</em></span> had not been eliminated <span class="emphasis"><em>a priori</em></span>.</p><p>Continuing <a id="id015" class="indexterm"/>with this thought, suppose during iteration 2, it is discovered that <span class="emphasis"><em>{A, B}</em></span> and <span class="emphasis"><em>{B, C}</em></span> are frequent, but <span class="emphasis"><em>{A, C}</em></span> is not. Although iteration 3 would normally begin by evaluating the support for <span class="emphasis"><em>{A, B, C}</em></span>, it is not mandatory that this step should occur at all. Why not? The Apriori principle states that <span class="emphasis"><em>{A, B, C}</em></span> cannot possibly be frequent, since the subset <span class="emphasis"><em>{A, C}</em></span> is not. Therefore, having generated no new itemsets in iteration 3, the algorithm may stop.</p><p>At this point, the second phase of the Apriori algorithm may begin. Given the set of frequent itemsets, association rules are generated from all possible subsets. For instance, <span class="emphasis"><em>{A, B}</em></span> would result in candidate rules for <span class="emphasis"><em>{A} → {B}</em></span> and <span class="emphasis"><em>{B} → {A}</em></span>. These are evaluated against a minimum confidence threshold, and any rule that does not meet the desired confidence level is eliminated.</p></div></div></div>
<div class="section" title="Example &#x2013; identifying frequently purchased groceries with association rules"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec02"/>Example – identifying frequently purchased groceries with association rules</h1></div></div></div><p>As noted in <a id="id016" class="indexterm"/>this chapter's introduction, market basket analysis is used behind the scenes for the recommendation systems used in many brick-and-mortar and online retailers. The learned association <a id="id017" class="indexterm"/>rules indicate the combinations of items that are often purchased together. Knowledge of these patterns provides insight into new ways a grocery chain might optimize the inventory, advertise promotions, or organize the physical layout of the store. For instance, if shoppers frequently purchase coffee or orange juice with a breakfast pastry, it may be possible to increase profit by relocating pastries closer to coffee and juice.</p><p>In this tutorial, we will perform a market basket analysis of transactional data from a grocery store. However, the techniques could be applied to many different types of problems, from movie recommendations, to dating sites, to finding dangerous interactions among medications. In doing so, we will see how the Apriori algorithm is able to efficiently evaluate a potentially massive set of association rules.</p><div class="section" title="Step 1 – collecting data"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec04"/>Step 1 – collecting data</h2></div></div></div><p>Our market <a id="id018" class="indexterm"/>basket analysis will utilize the purchase data collected from one month of operation at a real-world grocery store. The data contains 9,835 transactions or about 327 transactions per day (roughly 30 transactions per hour in a 12-hour business day), suggesting that the retailer is not particularly large, nor is it particularly small.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note002"/>Note</h3><p>The dataset used here was adapted from the <code class="literal">Groceries</code> dataset in the <code class="literal">arules</code> R package. For more information, see: Hahsler M, Hornik K, Reutterer T. Implications of probabilistic data modeling for mining association rules. In: Gaul W, Vichi M, Weihs C, ed. <span class="emphasis"><em>Studies in Classification, Data Analysis, and Knowledge Organization: from Data and Information Analysis to Knowledge Engineering</em></span>. New York: Springer; 2006:598–605.</p></div></div><p>The typical grocery store offers a huge variety of items. There might be five brands of milk, a dozen different types of laundry detergent, and three brands of coffee. Given the moderate size of the retailer, we will assume that they are not terribly concerned with finding rules that apply only to a specific brand of milk or detergent. With this in mind, all brand names can be removed from the purchases. This reduces the number of groceries to a more <a id="id019" class="indexterm"/>manageable 169 types, using broad categories such as chicken, frozen meals, margarine, and soda.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip002"/>Tip</h3><p>If you hope to identify highly specific association rules—such as whether customers prefer grape or strawberry jelly with their peanut butter—you will need a tremendous amount of transactional data. Large chain retailers use databases of many millions of transactions in order to find associations among particular brands, colors, or flavors of items.</p></div></div><p>Do you have any guesses about which types of items might be purchased together? Will wine and cheese be a common pairing? Bread and butter? Tea and honey? Let's dig into this data and see whether we can confirm our guesses.</p></div><div class="section" title="Step 2 – exploring and preparing the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec05"/>Step 2 – exploring and preparing the data</h2></div></div></div><p>Transactional <a id="id020" class="indexterm"/>data is stored in a slightly different <a id="id021" class="indexterm"/>format than that we used previously. Most of our prior analyses utilized data in the matrix form where rows indicated example instances and columns indicated features. Given the structure of the matrix format, all examples are required to have exactly the same set of features.</p><p>In comparison, transactional data is a more free form. As usual, each row in the data specifies a single example—in this case, a transaction. However, rather than having a set number of features, each record comprises a comma-separated list of any number of items, from one to many. In essence, the features may differ from example to example.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip003"/>Tip</h3><p>To follow along with this analysis, download the <code class="literal">groceries.csv</code> file from the Packt Publishing website and save it in your R working directory.</p></div></div><p>The first five rows of the raw <code class="literal">grocery.csv</code> file are as follows:</p><div class="informalexample"><pre class="programlisting">citrus fruit,semi-finished bread,margarine,ready soups
tropical fruit,yogurt,coffee
whole milk
pip fruit,yogurt,cream cheese,meat spreads
other vegetables,whole milk,condensed milk,long life bakery product</pre></div><p>These lines indicate five separate grocery store transactions. The first transaction included four items: citrus fruit, semi-finished bread, margarine, and ready soups. In comparison, the third transaction included only one item: whole milk.</p><p>Suppose we try to load the data using the <code class="literal">read.csv()</code> function as we did in the prior analyses. R <a id="id022" class="indexterm"/>would happily comply and read the data into a matrix form as follows:</p><div class="mediaobject"><img src="graphics/B03905_08_05.jpg" alt="Step 2 – exploring and preparing the data"/></div><p>You will notice that R created four columns to store the items in the transactional data: <code class="literal">V1</code>, <code class="literal">V2</code>, <code class="literal">V3</code>, and <code class="literal">V4</code>. Although this may seem reasonable this, if we use the data in this form, we will encounter problems later  may seem reasonable, R chose to create four variables because the first line had exactly four comma-separated values. However, we know that grocery purchases can contain more than four items; in the four column design such transactions will be broken across multiple rows in the matrix. We could try to remedy this by putting the transaction with the largest number of items at the top of the file, but this ignores another more problematic issue.</p><p>By structuring data this way, R has constructed a set of features that record not just the items in the transactions, but also the order in which they appear. If we imagine our learning algorithm <a id="id023" class="indexterm"/>as an attempt to find a relationship among <code class="literal">V1</code>, <code class="literal">V2</code>, <code class="literal">V3</code>, and <code class="literal">V4</code>, then whole milk in <code class="literal">V1</code> might be treated differently than the whole milk appearing in <code class="literal">V2</code>. Instead, we need a dataset that does not treat a transaction as a set of positions to be filled (or not filled) with specific items, but rather as a market basket that either contains or does not contain each particular item.</p><div class="section" title="Data preparation – creating a sparse matrix for transaction data"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec01"/>Data preparation – creating a sparse matrix for transaction data</h3></div></div></div><p>The solution to <a id="id024" class="indexterm"/>this problem utilizes a data structure called a <span class="strong"><strong>sparse matrix</strong></span>. You may recall that we used a <a id="id025" class="indexterm"/>sparse matrix to process text data in <a class="link" href="ch04.html" title="Chapter 4. Probabilistic Learning – Classification Using Naive Bayes">Chapter 4</a>, <span class="emphasis"><em>Probabilistic Learning – Classification Using Naive Bayes</em></span>. Just as with the preceding dataset, each row in the sparse matrix indicates a transaction. However, the sparse matrix has a column (that is, feature) for every item that could possibly appear in someone's shopping bag. Since there are 169 different items in our grocery store data, our sparse matrix will contain 169 columns.</p><p>Why not just store this as a data frame as we did in most of our analyses? The reason is that as additional transactions and items are added, a conventional data structure quickly becomes too large to fit in the available memory. Even with the relatively small transactional dataset used here, the matrix contains nearly 1.7 million cells, most of which contain zeros (hence, the name "sparse" matrix—there are very few nonzero values). Since there is no benefit to storing all these zero values, a sparse matrix does not actually store the full matrix in memory; it only stores the cells that are occupied by an item. This allows the structure to be more memory efficient than an equivalently sized matrix or data frame.</p><p>In order to create the sparse matrix data structure from the transactional data, we can use the functionality provided by the <code class="literal">arules</code> package. Install and load the package using the <code class="literal">install.packages("arules")</code> and <code class="literal">library(arules)</code> commands.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note003"/>Note</h3><p>For more information on the arules package, refer to: Hahsler M, Gruen B, Hornik K. arules – a computational environment for mining association rules and frequent item sets.<span class="emphasis"><em> Journal of Statistical Software. 2005; 14</em></span>.</p></div></div><p>Since we're loading the transactional data, we cannot simply use the <code class="literal">read.csv()</code> function used previously. Instead, <code class="literal">arules</code> provides a <code class="literal">read.transactions()</code> function that is similar to <code class="literal">read.csv()</code> with the exception that it results in a sparse matrix suitable for transactional data. The <code class="literal">sep = ","</code> parameter specifies that items in the input file are separated by a comma. To read the <code class="literal">groceries.csv</code> data into a sparse matrix named <code class="literal">groceries</code>, type the following line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; groceries &lt;- read.transactions("groceries.csv", sep = ",")</strong></span>
</pre></div><p>To see some <a id="id026" class="indexterm"/>basic information about the <code class="literal">groceries</code> matrix we just created, use the <code class="literal">summary()</code> function on the object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(groceries)</strong></span>
<span class="strong"><strong>transactions as itemMatrix in sparse format with</strong></span>
<span class="strong"><strong> 9835 rows (elements/itemsets/transactions) and</strong></span>
<span class="strong"><strong> 169 columns (items) and a density of 0.02609146</strong></span>
</pre></div><p>The first block of information in the output (as shown previously) provides a summary of the sparse matrix we created. The output <code class="literal">9835 rows</code> refers to the number of transactions, and the output <code class="literal">169 columns</code> refers to the 169 different items that might appear in someone's grocery basket. Each cell in the matrix is <code class="literal">1</code> if the item was purchased for the corresponding transaction, or <code class="literal">0</code> otherwise.</p><p>The <span class="strong"><strong>density</strong></span> value of <code class="literal">0.02609146</code> (2.6 percent) refers to the proportion of nonzero matrix cells. Since there are <span class="emphasis"><em>9,835 * 169 = 1,662,115</em></span> positions in the matrix, we can calculate that a total of <span class="emphasis"><em>1,662,115 * 0.02609146 = 43,367</em></span> items were purchased during the store's 30 days of operation (ignoring the fact that duplicates of the same items might have been purchased). With an additional step, we can determine that the average transaction contained <span class="emphasis"><em>43,367 / 8,835 = 4.409</em></span> distinct grocery items. Of course, if we look a little further down the output, we'll see that the mean number of items per transaction has already been provided.</p><p>The next block of the <code class="literal">summary()</code> output lists the items that were most commonly found in the transactional data. Since <span class="emphasis"><em>2,513 / 9,835 = 0.2555</em></span>, we can determine that whole milk appeared in 25.6 percent of the transactions. The other vegetables, rolls/buns, soda, and yogurt round out the list of other common items, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>most frequent items:</strong></span>
<span class="strong"><strong>      whole milk other vegetables       rolls/buns</strong></span>
<span class="strong"><strong>            2513             1903             1809</strong></span>
<span class="strong"><strong>            soda           yogurt          (Other)</strong></span>
<span class="strong"><strong>            1715             1372            34055</strong></span>
</pre></div><p>Finally, we are presented with a set of statistics about the size of the transactions. A total of 2,159 transactions contained only a single item, while one transaction had 32 items. The first quartile and median purchase sizes are two and three items, respectively, implying that 25 percent of the transactions contained two or fewer items and the transactions were split in half between those with less than three items and those with more. The mean of 4.409 items per transaction matches the value we calculated by hand.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>element (itemset/transaction) length distribution:</strong></span>
<span class="strong"><strong>sizes</strong></span>
<span class="strong"><strong>   1    2    3    4    5    6    7    8    9   10   11   12</strong></span>
<span class="strong"><strong>2159 1643 1299 1005  855  645  545  438  350  246  182  117</strong></span>
<span class="strong"><strong>  13   14   15   16   17   18   19   20   21   22   23   24</strong></span>
<span class="strong"><strong>  78   77   55   46   29   14   14    9   11    4    6    1</strong></span>
<span class="strong"><strong>  26   27   28   29   32</strong></span>
<span class="strong"><strong>   1    1    1    3    1</strong></span>

<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.</strong></span>
<span class="strong"><strong>  1.000   2.000   3.000   4.409   6.000  32.000</strong></span>
</pre></div><p>The <code class="literal">arules</code> package includes some useful features for examining transaction data. To look at the contents of the sparse matrix, use the <code class="literal">inspect()</code> function in combination with the vector operators. The first five transactions can be viewed as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; inspect(groceries[1:5])</strong></span>
<span class="strong"><strong>  items                     </strong></span>
<span class="strong"><strong>1 {citrus fruit,            </strong></span>
<span class="strong"><strong>   margarine,               </strong></span>
<span class="strong"><strong>   ready soups,             </strong></span>
<span class="strong"><strong>   semi-finished bread}     </strong></span>
<span class="strong"><strong>2 {coffee,                  </strong></span>
<span class="strong"><strong>   tropical fruit,          </strong></span>
<span class="strong"><strong>   yogurt}                  </strong></span>
<span class="strong"><strong>3 {whole milk}              </strong></span>
<span class="strong"><strong>4 {cream cheese,            </strong></span>
<span class="strong"><strong>   meat spreads,            </strong></span>
<span class="strong"><strong>   pip fruit,               </strong></span>
<span class="strong"><strong>   yogurt}                  </strong></span>
<span class="strong"><strong>5 {condensed milk,          </strong></span>
<span class="strong"><strong>   long life bakery product,</strong></span>
<span class="strong"><strong>   other vegetables,        </strong></span>
<span class="strong"><strong>   whole milk}</strong></span>
</pre></div><p>These transactions match our look at the original CSV file. To examine a particular item (that is, a column <a id="id027" class="indexterm"/>of data), it is possible use the <code class="literal">[row, column]</code> matrix notion. Using this with the <code class="literal">itemFrequency()</code> function allows us to see the proportion of transactions that contain the item. This allows us, for instance, to view the support level for the first three items in the grocery data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; itemFrequency(groceries[, 1:3])</strong></span>
<span class="strong"><strong>abrasive cleaner artif. sweetener   baby cosmetics</strong></span>
<span class="strong"><strong>    0.0035587189     0.0032536858     0.0006100661</strong></span>
</pre></div><p>Note that the items in the sparse matrix are sorted in columns by alphabetical order. Abrasive cleaner and artificial sweeteners are found in about 0.3 percent of the transactions, while baby cosmetics are found in about 0.06 percent of the transactions.</p></div><div class="section" title="Visualizing item support – item frequency plots"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec02"/>Visualizing item support – item frequency plots</h3></div></div></div><p>To present <a id="id028" class="indexterm"/>these statistics visually, use the <code class="literal">itemFrequencyPlot()</code> function. This allows you to produce a bar chart depicting the proportion of transactions containing certain items. Since the transactional data contains a very large number of items, you will often need to limit the ones appearing in the plot in order to produce a legible chart.</p><p>If you require these items to appear in a minimum proportion of transactions, use <code class="literal">itemFrequencyPlot()</code> with the <code class="literal">support</code> parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; itemFrequencyPlot(groceries, support = 0.1)</strong></span>
</pre></div><p>As shown in the following plot, this results in a histogram showing the eight items in the <code class="literal">groceries</code> data with at least 10 percent support:</p><div class="mediaobject"><img src="graphics/B03905_08_06.jpg" alt="Visualizing item support – item frequency plots"/></div><p>If you would rather limit the plot to a specific number of items, the <code class="literal">topN</code> parameter can be used with <code class="literal">itemFrequencyPlot()</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; itemFrequencyPlot(groceries, topN = 20)</strong></span>
</pre></div><p>The histogram <a id="id029" class="indexterm"/>is then sorted by decreasing support, as shown in the following diagram of the top 20 items in the <code class="literal">groceries</code> data:</p><div class="mediaobject"><img src="graphics/B03905_08_07.jpg" alt="Visualizing item support – item frequency plots"/></div></div><div class="section" title="Visualizing the transaction data – plotting the sparse matrix"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec03"/>Visualizing the transaction data – plotting the sparse matrix</h3></div></div></div><p>In addition <a id="id030" class="indexterm"/>to looking at the items, it's also possible to visualize the entire sparse matrix. To do so, use the <code class="literal">image()</code> function. The command to display the sparse matrix for the first five transactions is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; image(groceries[1:5])</strong></span>
</pre></div><p>The resulting diagram depicts a matrix with 5 rows and 169 columns, indicating the 5 transactions and 169 possible items we requested. Cells in the matrix are filled with black for transactions (rows) where the item (column) was purchased.</p><div class="mediaobject"><img src="graphics/B03905_08_08.jpg" alt="Visualizing the transaction data – plotting the sparse matrix"/></div><p>Although the preceding diagram is small and may be slightly hard to read, you can see that the first, fourth, and fifth transactions contained four items each, since their rows have four cells filled in. You can also see that rows three, five, two, and four have an item in common (on the right side of the diagram).</p><p>This visualization can be a useful tool for exploring data. For one, it may help with the identification of potential data issues. Columns that are filled all the way down could indicate items that are purchased in every transaction—a problem that could arise, perhaps, if a retailer's name or identification number was inadvertently included in the transaction dataset.</p><p>Additionally, patterns in the diagram may help reveal actionable insights within the transactions and items, particularly if the data is sorted in interesting ways. For example, if the transactions are sorted by date, the patterns in black dots could reveal seasonal effects in the number or types of items purchased. Perhaps around Christmas or Hanukkah, toys are more common; around Halloween, perhaps candies become popular. This type of visualization could be especially powerful if the items were also sorted into categories. In most cases, however, the plot will look fairly random, like static on a television screen.</p><p>Keep in mind that this visualization will not be as useful for extremely large transaction databases, because the cells will be too small to discern. Still, by combining it with the <code class="literal">sample()</code> function, you can view the sparse matrix for a randomly sampled set of transactions. The command to create random selection of 100 transactions is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; image(sample(groceries, 100))</strong></span>
</pre></div><p>This creates a matrix diagram with 100 rows and 169 columns:</p><div class="mediaobject"><img src="graphics/B03905_08_09.jpg" alt="Visualizing the transaction data – plotting the sparse matrix"/></div><p>A few columns <a id="id031" class="indexterm"/>seem fairly heavily populated, indicating some very popular items at the store. But overall, the distribution of dots seems fairly random. Given nothing else of note, let's continue with our analysis.</p></div></div><div class="section" title="Step 3 – training a model on the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec06"/>Step 3 – training a model on the data</h2></div></div></div><p>With data <a id="id032" class="indexterm"/>preparation completed, we can now work at finding associations among shopping cart items. We will use an implementation of the Apriori algorithm in the <code class="literal">arules</code> package we've been using to explore and prepare the groceries data. You'll need to install and load this package if you have not done so already. The following table shows the syntax to create sets of rules with the <code class="literal">apriori()</code> function:</p><div class="mediaobject"><img src="graphics/B03905_08_10.jpg" alt="Step 3 – training a model on the data"/></div><p>Although running the <code class="literal">apriori()</code> function is straightforward, there can sometimes be a fair amount of trial and error needed to find the <code class="literal">support</code> and <code class="literal">confidence</code> parameters that produce a reasonable number of association rules. If you set these levels too high, you might find no <a id="id033" class="indexterm"/>rules or rules that are too generic to be very useful. On the other hand, a threshold too low might result in an unwieldy number of rules, or worse, the operation might take a very long time or run out of memory during the learning phase.</p><p>In this case, if we attempt to use the default settings of <code class="literal">support = 0.1</code> and <code class="literal">confidence = 0.8</code>, we will end up with a set of zero rules:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; apriori(groceries)</strong></span>
<span class="strong"><strong>set of 0 rules</strong></span>
</pre></div><p>Obviously, we need to widen the search a bit.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip004"/>Tip</h3><p>If you think about it, this outcome should not have been terribly surprising. Because <code class="literal">support = 0.1</code> by default, in order to generate a rule, an item must have appeared in at least <span class="emphasis"><em>0.1 * 9,385 = 938.5</em></span> transactions. Since only eight items appeared this frequently in our data, it's no wonder that we didn't find any rules.</p></div></div><p>One way to approach the problem of setting a minimum support threshold is to think about the smallest number of transactions you would need before you would consider a pattern interesting. For instance, you could argue that if an item is purchased twice a day (about 60 times in a month of data), it may be an interesting pattern. From there, it is possible to calculate the support level needed to find only the rules matching at least that many transactions. Since 60 out of 9,835 equals 0.006, we'll try setting the support there first.</p><p>Setting the minimum confidence involves a delicate balance. On one hand, if confidence is too low, we might be overwhelmed with a large number of unreliable rules—such as dozens of rules indicating the items commonly purchased with batteries. How <a id="id034" class="indexterm"/>would we know where to target our advertising budget then? On the other hand, if we set confidence too high, we will be limited to the rules that are obvious or inevitable—similar to the fact that smoke detectors are always purchased in combination with batteries. In this case, moving the smoke detectors closer to the batteries is unlikely to generate additional revenue, since the two items already were almost always purchased together.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip005"/>Tip</h3><p>The appropriate minimum confidence level depends a great deal on the goals of your analysis. If you start with a conservative value, you can always reduce it to broaden the search if you aren't finding actionable intelligence.</p></div></div><p>We'll start with a confidence threshold of 0.25, which means that in order to be included in the results, the rule has to be correct at least 25 percent of the time. This will eliminate the most unreliable rules, while allowing some room for us to modify behavior with targeted promotions.</p><p>We are now ready to generate some rules. In addition to the minimum <code class="literal">support</code> and <code class="literal">confidence</code> parameters, it is helpful to set <code class="literal">minlen = 2</code> to eliminate rules that contain fewer than two items. This prevents uninteresting rules from being created simply because the item is purchased frequently, for instance, <span class="emphasis"><em>{} </em></span>
<span class="emphasis"><em>→</em></span>
<span class="emphasis"><em> whole milk</em></span>. This rule meets the minimum support and confidence because whole milk is purchased in over 25 percent of the transactions, but it isn't a very actionable insight.</p><p>The full command to find a set of association rules using the Apriori algorithm is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; groceryrules &lt;- apriori(groceries, parameter = list(support =</strong></span>
<span class="strong"><strong>                            0.006, confidence = 0.25, minlen = 2))</strong></span>
</pre></div><p>This saves our rules in a <code class="literal">rules</code> object, can take a peek into by typing its name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; groceryrules</strong></span>
<span class="strong"><strong>set of 463 rules</strong></span>
</pre></div><p>Our <code class="literal">groceryrules</code> object contains a set of 463 association rules. To determine whether any of them are useful, we'll have to dig deeper.</p></div><div class="section" title="Step 4 – evaluating model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec07"/>Step 4 – evaluating model performance</h2></div></div></div><p>To obtain a <a id="id035" class="indexterm"/>high-level overview of the association rules, we can use <code class="literal">summary()</code> as follows. The rule length distribution tells us how many rules have each count of items. In our rule set, 150 rules have only two items, while 297 have three, and 16 have four. The <code class="literal">summary</code> statistics associated with this distribution are also given:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; summary(groceryrules)</strong></span>
<span class="strong"><strong>set of 463 rules</strong></span>

<span class="strong"><strong>rule length distribution (lhs + rhs):sizes</strong></span>
<span class="strong"><strong>  2   3   4</strong></span>
<span class="strong"><strong>150 297  16</strong></span>

<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.</strong></span>
<span class="strong"><strong>  2.000   2.000   3.000   2.711   3.000   4.000</strong></span>
</pre></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip006"/>Tip</h3><p>As noted in the previous output, the size of the rule is calculated as the total of both the left-hand side (<code class="literal">lhs</code>) and right-hand side (<code class="literal">rhs</code>) of the rule. This means that a rule like <span class="emphasis"><em>{bread}</em></span>
<span class="emphasis"><em> </em></span>
<span class="emphasis"><em>→</em></span>
<span class="emphasis"><em> {butter}</em></span> is two items and <span class="emphasis"><em>{peanut butter, jelly}</em></span>
<span class="emphasis"><em> </em></span>
<span class="emphasis"><em>→</em></span>
<span class="emphasis"><em> {bread}</em></span> is three.</p></div></div><p>Next, we see the <a id="id036" class="indexterm"/>summary statistics of the rule quality measures: support, confidence, and lift. The support and confidence measures should not be very surprising, since we used these as selection criteria for the rules. We might be alarmed if most or all of the rules had support and confidence very near the minimum thresholds, as this would mean that we may have set the bar too high. This is not the case here, as there are many rules with much higher values of each.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>summary of quality measures:</strong></span>
<span class="strong"><strong>    support           confidence          lift       </strong></span>
<span class="strong"><strong> Min.   :0.006101   Min.   :0.2500   Min.   :0.9932  </strong></span>
<span class="strong"><strong> 1st Qu.:0.007117   1st Qu.:0.2971   1st Qu.:1.6229  </strong></span>
<span class="strong"><strong> Median :0.008744   Median :0.3554   Median :1.9332  </strong></span>
<span class="strong"><strong> Mean   :0.011539   Mean   :0.3786   Mean   :2.0351  </strong></span>
<span class="strong"><strong> 3rd Qu.:0.012303   3rd Qu.:0.4495   3rd Qu.:2.3565  </strong></span>
<span class="strong"><strong> Max.   :0.074835   Max.   :0.6600   Max.   :3.9565    </strong></span>
</pre></div><p>The third column is a metric we have not considered yet. The <span class="emphasis"><em>lift</em></span> of a rule measures how much more likely one item or itemset is purchased relative to its typical rate of purchase, given that you know another item or itemset has been purchased. This is defined by the following equation:</p><div class="mediaobject"><img src="graphics/B03905_08_11.jpg" alt="Step 4 – evaluating model performance"/></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip007"/>Tip</h3><p>Unlike confidence where the item order matters, <span class="emphasis"><em>lift(X → Y)</em></span> is the same as <span class="emphasis"><em>lift(Y  → X)</em></span>.</p></div></div><p>For example, suppose at a grocery store, most people purchase milk and bread. By chance alone, we would expect to find many transactions with both milk and bread. However, if <span class="emphasis"><em>lift(milk → bread)</em></span> is greater than one, it implies that the two items are found together more often than one would expect by chance. A large lift value is therefore a strong indicator that a rule is important, and reflects a true connection between the items.</p><p>In the final section of the <code class="literal">summary()</code> output, we receive mining information, telling us about how the rules were chosen. Here, we see that the <code class="literal">groceries</code> data, which contained 9,835 transactions, was used to construct rules with a minimum support of 0.0006 and minimum confidence of 0.25:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mining info:</strong></span>
<span class="strong"><strong>      data ntransactions support confidence</strong></span>
<span class="strong"><strong> groceries          9835   0.006       0.25</strong></span>
</pre></div><p>We can take a look at specific rules using the <code class="literal">inspect()</code> function. For instance, the first three rules in the <code class="literal">groceryrules</code> object can be viewed as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; inspect(groceryrules[1:3])</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03905_08_12.jpg" alt="Step 4 – evaluating model performance"/></div><p>The first rule can be read in plain language as, "if a customer buys potted plants, they will also buy whole milk." With support of 0.007 and confidence of 0.400, we can determine that this rule covers 0.7 percent of the transactions and is correct in 40 percent of purchases involving potted plants. The lift value tells us how much more likely a customer is to buy whole milk relative to the average customer, given that he or she bought a potted plant. Since we know that about 25.6 percent of the customers bought whole milk (<code class="literal">support</code>), while 40 percent of the customers buying a potted plant bought whole milk (<code class="literal">confidence</code>), we can compute the lift value as <span class="emphasis"><em>0.40 / 0.256 = 1.56</em></span>, which matches the value shown.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note004"/>Note</h3><p>Note that the column labeled <code class="literal">support</code> indicates the support value for the rule, not the support value for the <code class="literal">lhs</code> or <code class="literal">rhs</code> alone).</p></div></div><p>In spite of the fact that the confidence and lift are high, does <span class="emphasis"><em>{potted plants} </em></span>
<span class="emphasis"><em>→</em></span>
<span class="emphasis"><em> {whole milk}</em></span> seem like a very useful rule? Probably not, as there doesn't seem to be a logical reason why someone would be more likely to buy milk with a potted plant. Yet our data suggests otherwise. How can we make sense of this fact?</p><p>A common approach is to take the association rules and divide them into the following three categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Actionable</li><li class="listitem" style="list-style-type: disc">Trivial</li><li class="listitem" style="list-style-type: disc">Inexplicable</li></ul></div><p>Obviously, the goal of a market basket analysis is to find <span class="strong"><strong>actionable</strong></span> rules that provide a clear and useful insight. Some rules are clear, others are useful; it is less common to find a combination of both of these factors.</p><p>So-called <span class="strong"><strong>trivial rules</strong></span> include any rules that are so obvious that they are not worth mentioning—they are clear, but not useful. Suppose you were a marketing consultant being paid large sums of money to identify new opportunities for cross-promoting items. If you report the finding that <span class="emphasis"><em>{diapers} </em></span>
<span class="emphasis"><em>→</em></span>
<span class="emphasis"><em> {formula}</em></span>, you probably won't be invited back for another consulting job.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip008"/>Tip</h3><p>Trivial rules can also sneak in disguised as more interesting results. For instance, say you found an association between a particular brand of children's cereal and a certain DVD movie. This finding is not very insightful if the movie's main character is on the front of the cereal box.</p></div></div><p>Rules are <span class="strong"><strong>inexplicable</strong></span> if the connection between the items is so unclear that figuring out how to use the information is impossible or nearly impossible. The rule may simply be a <a id="id037" class="indexterm"/>random pattern in the data, for instance, a rule stating that <span class="emphasis"><em>{pickles} </em></span>
<span class="emphasis"><em>→</em></span>
<span class="emphasis"><em> {chocolate ice cream}</em></span> may be due to a single customer, whose pregnant wife had regular cravings for strange combinations of foods.</p><p>The best rules are hidden gems—those undiscovered insights into patterns that seem obvious once discovered. Given enough time, one could evaluate each and every rule to find the gems. However, we (the one performing the market basket analysis) may not be the best judge of whether a rule is actionable, trivial, or inexplicable. In the next section, we'll improve the utility of our work by employing methods to sort and share the learned rules so that the most interesting results might float to the top.</p></div><div class="section" title="Step 5 – improving model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec08"/>Step 5 – improving model performance</h2></div></div></div><p>Subject matter <a id="id3008" class="indexterm"/>experts may be able to identify useful rules very quickly, but it would be a poor use of their time to ask them to evaluate hundreds or thousands of rules. Therefore, it's useful to be able to sort rules according to different criteria, and get them out of R into a form that can be shared with marketing teams and examined in more depth. In this way, we can improve the performance of our rules by making the results more actionable.</p><div class="section" title="Sorting the set of association rules"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec04"/>Sorting the set of association rules</h3></div></div></div><p>Depending <a id="id0309" class="indexterm"/>upon the objectives of the market basket analysis, the most useful rules might be the ones with the highest <code class="literal">support, confidence,</code> or <code class="literal">lift.</code> The <code class="literal">arules</code> package includes a <code class="literal">sort()</code> function that can be used to reorder the list of rules so that the ones with the highest or lowest values of the quality measure come first.</p><p>To reorder the <code class="literal">groceryrules</code> object, we can apply <code class="literal">sort()</code> while specifying a <code class="literal">"support"</code>, <code class="literal">"confidence"</code>, or <code class="literal">"lift"</code> value to the by parameter. By combining the <code class="literal">sort</code> function with vector operators, we can obtain a specific number of interesting rules. For instance, the best five rules according to the lift statistic can be examined using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; inspect(sort(groceryrules, by = "lift")[1:5])</strong></span>
</pre></div><p>These output is shown as follows:</p><div class="mediaobject"><img src="graphics/B03905_08_13.jpg" alt="Sorting the set of association rules"/></div><p>These rules appear to be more interesting than the ones we looked at previously. The first rule, with a <code class="literal">lift</code> of about 3.96, implies that people who buy herbs are nearly four times more likely to buy root vegetables than the typical customer—perhaps for a stew of some sort? Rule <a id="id040" class="indexterm"/>two is also interesting. Whipped cream is over three times more likely to be found in a shopping cart with berries versus other carts, suggesting perhaps a dessert pairing?</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip009"/>Tip</h3><p>By default, the sort order is decreasing, meaning the largest values come first. To reverse this order, add an additional line, <code class="literal">parameterdecreasing = FALSE</code>.</p></div></div></div><div class="section" title="Taking subsets of association rules"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec05"/>Taking subsets of association rules</h3></div></div></div><p>Suppose <a id="id041" class="indexterm"/>that given the preceding rule, the marketing team is excited about the possibilities of creating an advertisement to promote berries, which are now in season. Before finalizing the campaign, however, they ask you to investigate whether berries are often purchased with other items. To answer this question, we'll need to find all the rules that include berries in some form.</p><p>The <code class="literal">subset()</code> function provides a method to search for subsets of transactions, items, or rules. To use it to find any rules with <code class="literal">berries</code> appearing in the rule, use the following command. It will store the rules in a new object titled <code class="literal">berryrules</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; berryrules &lt;- subset(groceryrules, items %in% "berries")</strong></span>
</pre></div><p>We can then inspect the rules as we did with the larger set:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; inspect(berryrules)</strong></span>
</pre></div><p>The result is the following set of rules:</p><div class="mediaobject"><img src="graphics/B03905_08_14.jpg" alt="Taking subsets of association rules"/></div><p>There are four rules involving berries, two of which seem to be interesting enough to be called actionable. In addition to whipped cream, berries are also purchased frequently with yogurt—a pairing that could serve well for breakfast or lunch as well as dessert.</p><p>The <code class="literal">subset()</code> function is very powerful. The criteria for choosing the subset can be defined with several keywords and operators:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The keyword <code class="literal">items</code> explained previously, matches an item appearing anywhere in the rule. To limit the subset to where the match occurs only on the left- or right-hand side, use <code class="literal">lhs</code> and <code class="literal">rhs</code> instead.</li><li class="listitem" style="list-style-type: disc">The operator <code class="literal">%in%</code> means that at least one of the items must be found in the list you defined. If you want any rules matching either berries or yogurt, you could write <code class="literal">items %in%c("berries", "yogurt”)</code>.</li><li class="listitem" style="list-style-type: disc">Additional operators are available for partial matching (<code class="literal">%pin%</code>) and complete matching (<code class="literal">%ain%</code>). Partial matching allows you to find both citrus fruit and tropical fruit using one search: <code class="literal">items %pin% "fruit"</code>. Complete matching requires that all the listed items are present. For instance, <code class="literal">items %ain% c("berries", "yogurt")</code> finds only rules with both <code class="literal">berries</code> and <code class="literal">yogurt</code>.</li><li class="listitem" style="list-style-type: disc">Subsets can also be limited by <code class="literal">support</code>, <code class="literal">confidence</code>, or <code class="literal">lift</code>. For instance, <code class="literal">confidence &gt; 0.50</code> would limit you to the rules with confidence greater than 50 percent.</li><li class="listitem" style="list-style-type: disc">Matching <a id="id042" class="indexterm"/>criteria can be combined with the standard R logical operators such as and (<code class="literal">&amp;</code>), or (<code class="literal">|</code>), and not (<code class="literal">!</code>).</li></ul></div><p>Using these options, you can limit the selection of rules to be as specific or general as you would like.</p></div><div class="section" title="Saving association rules to a file or data frame"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec06"/>Saving association rules to a file or data frame</h3></div></div></div><p>To share the <a id="id043" class="indexterm"/>results of your market basket analysis, you can save the rules to a CSV file with the <code class="literal">write()</code> function. This will produce a CSV file that can be used in most spreadsheet programs <a id="id044" class="indexterm"/>including Microsoft Excel:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; write(groceryrules, file = "groceryrules.csv",</strong></span>
<span class="strong"><strong>        sep = ",", quote = TRUE, row.names = FALSE)</strong></span>
</pre></div><p>Sometimes it is also convenient to convert the rules into an R data frame. This can be accomplished easily using the <code class="literal">as()</code> function, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; groceryrules_df &lt;- as(groceryrules, "data.frame")</strong></span>
</pre></div><p>This creates a data frame with the rules in the factor format, and numeric vectors for <code class="literal">support</code>, <code class="literal">confidence</code>, and <code class="literal">lift</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(groceryrules_df)</strong></span>
<span class="strong"><strong>'data.frame':463 obs. of 4 variables:</strong></span>
<span class="strong"><strong> $ rules     : Factor w/ 463 levels "{baking powder} =&gt; {other vegetables}",..: 340 302 207 206 208 341 402 21 139 140 ...</strong></span>
<span class="strong"><strong> $ support   : num  0.00691 0.0061 0.00702 0.00773 0.00773 ...</strong></span>
<span class="strong"><strong> $ confidence: num  0.4 0.405 0.431 0.475 0.475 ...</strong></span>
<span class="strong"><strong> $ lift      : num  1.57 1.59 3.96 2.45 1.86 ...</strong></span>
</pre></div><p>You might <a id="id045" class="indexterm"/>choose to do this if you want to perform additional processing on the rules or need to export them to another database.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec03"/>Summary</h1></div></div></div><p>Association rules are frequently used to find provide useful insights in the massive transaction databases of large retailers. As an unsupervised learning process, association rule learners are capable of extracting knowledge from large databases without any prior knowledge of what patterns to seek. The catch is that it takes some effort to reduce the wealth of information into a smaller and more manageable set of results. The Apriori algorithm, which we studied in this chapter, does so by setting minimum thresholds of interestingness, and reporting only the associations meeting these criteria.</p><p>We put the Apriori algorithm to work while performing a market basket analysis for a month's worth of transactions at a moderately sized supermarket. Even in this small example, a wealth of associations was identified. Among these, we noted several patterns that may be useful for future marketing campaigns. The same methods we applied are used at much larger retailers on databases many times this size.</p><p>In the next chapter, we will examine another unsupervised learning algorithm. Much like association rules, it is intended to find patterns within data. But unlike association rules that seek patterns within the features, the methods in the next chapter are concerned with finding connections among the examples.</p></div></body></html>
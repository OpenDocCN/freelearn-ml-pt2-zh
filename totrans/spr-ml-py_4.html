<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Advanced Topics in Supervised Machine Learning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to focus on some advanced topics. We'll cover two topics: recommender systems and neural networks. We'll start with collaborative filtering, and then we'll look at integrating content-based similarities into collaborative filtering systems. We'll get into neural networks and transfer learning. Finally, we'll introduce the math and concept behind each of these, before getting into Python code.</p>
<p>We will cover the following topics:</p>
<ul>
<li>Recommended systems and an introduction to collaborative filtering</li>
<li>Matrix factorization</li>
<li>Content-based filtering</li>
<li>Neural networks and deep learning</li>
<li>Using transfer learning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="fontstyle0">For this chapter, you will need to install the following software, if you haven't already done so:</span></p>
<ul>
<li>Jupyter Notebook</li>
<li>Anaconda</li>
<li>Python</li>
</ul>
<p class="mce-root"><span class="fontstyle0">The code files for this chapter can be found at</span><span> </span><a href="https://github.com/PacktPublishing/Supervised-Machine-Learning-with-Python" target="_blank"><span class="fontstyle2">https:/</span><span class="fontstyle3">​</span><span class="fontstyle2">/</span><span class="fontstyle3">​</span><span class="fontstyle2">github.</span><span class="fontstyle3">​</span><span class="fontstyle2">com/</span><span class="fontstyle3">​</span><span class="fontstyle2">PacktPublishing/</span><span class="fontstyle2"><br/></span><span class="fontstyle2">Supervised-Machine-Learning-with-Python</span></a><span class="fontstyle0">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recommended systems and an introduction to collaborative filtering</h1>
                </header>
            
            <article>
                
<p>In this section, we'll cover collaborative filtering and recommender systems. We'll start out by explaining what may constitute a recommender system, how users willingly share loads of data about themselves, without knowing it, and then we'll cover collaborative filtering.</p>
<p>Whether you realize it or not, you interact with numerous recommender systems on a daily basis. If you've ever purchased from Amazon, or browsed on Facebook, or watched a show on Netflix, you've been served some form of personalized content. This is how e-commerce platforms maximize conversion rates and keep you coming back for more.</p>
<p>One of the marks of a really good recommender system is that it knows what you want whether you already know it or not. A good one will make you really wonder: how did they know that? So, it turns out that humans are extraordinarily predictable in their behavior, even without having to share information about themselves, and we call that voting with our feet, meaning that a user may profess to enjoy one genre of movie, say comedy, but disproportionately consume another, say romance. So, the goal of a recommender system is simply to get you to bite; However, the secondary goal generally differs based on the platform itself. It could be to maximize revenue for the seller, create satisfaction for the customer, or any number of other metrics. But what really makes these so interesting is that they're directly consumable by human beings, whereas so many other <strong>machine learning</strong> (<strong>ML</strong>) models exist to replace an automated process.</p>
<p>Here's an example, explaining voting with your feet:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c749c0af-0f08-44c1-b582-95fa2b95603b.png" style="width:30.50em;height:11.42em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This user says he likes football, hot wings, and water skiing. And yet his ratings history shows that he's thumbed up one wing restaurant, thumbed down another, and then thumbed up a movie cinema. So, what this means is that there's something about the second wing restaurant that he didn't like. Maybe it was the ambiance, or maybe it was a wing sauce. Whatever it was, his interest in hot wings—his professed interest in hot wings—is more nuanced than he originally led us to believe. And, likewise, he's expressed an interest in movies, even though he's not disclosed it. So, the point here is that people say more with their actions than they do with their words, and they're more honest with their actions than they are with their words. We can exploit that with recommender systems to learn these nuanced patterns between items and people's interests.</p>
<p>Collaborative filtering is a common family of recommender systems. It's based on a concept known as <strong>homophily</strong>, which is basically <em>birds of a feather flock together</em>. So, that is, if you like something, people who also like that item probably share some other common interests with you; now we have a good pool of interest to start recommending things to one another.</p>
<p>In a typical collaborative filtering system, this is the format our data is going to resemble:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/614624c4-1538-437a-afc7-0bb5068263f1.png" style="width:35.00em;height:24.00em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the preceding screenshot, users are shown along the <em>y</em> axis—which are rows—and items are shown along the <em>x</em> axis—which are columns. You might have explicit ratings, which are usually continuous along this continuum, or implicit, which are commonly binary. What we're showing here is explicit. The question we seek to answer is what's the predicted rating for a user? But to get there, we have to somehow compute the similarities between the items. This is a form of collaborative filtering called item-to-item collaborative filtering, and we can only compute similarities between the items that have been mutually rated by a user. This usually works best for explicitly rated systems; it's based on a paper that was published by Amazon several years ago.</p>
<p>Computing similarities between items is straightforward. We can compute pairwise similarities using one of several common metrics, including the <strong>Pearson correlation</strong> or cosine similarity. For example, we're going to use cosine similarity as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0a4dde7f-f4c5-4c07-bd5b-7ea6512187f0.png" style="width:14.58em;height:2.92em;"/></p>
<p>It's computed in a very similar fashion to what we looked at with clustering in <a href="028b1786-df10-4e2b-96be-541675edd2cd.xhtml" target="_blank">Chapter 3</a>,<em>Working with Non-Parametric Models</em>, the <strong>Euclidean distance</strong>. However, this is computing similarity rather than spatial distance. So, it's the exact inverse of the concept, but computed in a similar fashion.</p>
<p>Since our data is so sparse, we're going to start out by putting it into a sparse CSR matrix using SciPy, and rather than having to store 32 elements, now we only have to store 14:</p>
<pre>from scipy import sparse<br/>import numpy as np<br/><br/>rows = np.array([0,0,0,0,1,1,1,2,2,2,2,3,3,3])<br/>cols = np.array([0,1,4,5,2,3,4,0,4,6,7,1,4,7])<br/>data = np.array([5.,1.,2.5,4.5,3.5,2.,3.,1.5,<br/>                 4.,4.5,4.,1.,1.,5.])<br/># Make a sparse matrix<br/>R = sparse.csr_matrix((data, (rows, cols)), shape = (4, 8))<br/>print(R.todense())</pre>
<p>The output of the preceding code is as follows:</p>
<pre class="mce-root">[[5. 1. 0. 0. 2.5 4.5 0. 0. ]<br/> [0. 0. 3.5 2. 3. 0. 0. 0. ]<br/> [1.5 0. 0. 0. 4. 0. 4.5 4. ]<br/> [0. 1. 0. 0. 1. 0. 0. 5. ]]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This is a dense matrix based on what we would actually see. So, you can imagine how handy this becomes when we have thousands of users and millions of items—as Amazon does, for instance.</p>
<p>We're simply going to compute the pairwise cosine similarities of the transpose of the matrix. We have a lot of zeros in here. It's not that a lot of these are orthogonal, which, mathematically, is what a cosine similarity would represent with a zero; it's that we're experiencing something called the item cold start, where there are several items that have never been mutually rated together. And, therefore, we cannot effectively compute the similarity on the basis of ratings alone.</p>
<p>Now we will see how to generate predictions for a given user giving their history in the computed items similarities. In the following example, we are using the same user and we're just predicting for <kbd>user_3</kbd>:</p>
<pre>from sklearn.metrics.pairwise import cosine_similarity<br/><br/># Compute the sim matrix<br/>sim = cosine_similarity(R.T).round(3)<br/>sim<br/>top_k = 3<br/>user_3 = np.array([0., 1., 0., 0., 1., 0., 0., 5.])<br/><br/># compute dot product btwn user vec and the sim matrix<br/>recommendations = user_3.dot(sim)<br/>item_indices = np.arange(recommendations.shape[0])<br/><br/># now arg sort descending (most similar items first)<br/>order = np.argsort(-recommendations)[:top_k]<br/>items = item_indices[order]<br/><br/># zip them together (item, predicted rating)<br/>list(zip(items, recommendations[order]))</pre>
<p>The output of the preceding code is as follows:</p>
<pre>[(7, 6.130000000000001), (4, 4.326), (1, 4.196)]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>So, computing predictions is easy enough in this algorithm. You just compute the dot product of that user's ratings vector and the similarities matrix. Then, <kbd>argsort</kbd> it to descending order, in a very similar fashion to how we did with nearest neighbors, but the inverse in terms of descending versus ascending. So, there are things to note here. First, the predicted rating exceeds the scale of the ground truth rating of <kbd>6.12</kbd>. We only rated up to five, but we can't guarantee bounded ratings. So, we could either call those ratings or use some other strategy, but the other two ratings are actually the ones that the user has rated before. If you look back to the ratings matrix, both of these were rated as one star by the user. So, we can see that this is not a great recommender model with its low rank and low number of users.</p>
<p>Recommender systems are technically supervised learning, but they differ in the traditional sense of the <em>x</em>, <em>y</em> pairing since our ground truth is technically our data itself. So, in our example, we could look at the ratings for item four and one, and say how far we were off from the ground truth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Item-to-item collaborative filtering</h1>
                </header>
            
            <article>
                
<p>Let's look at the code. This is item-to-item collaborative filtering. Let's start with the <kbd>base.py</kbd> file that is present in <kbd>packtml/recommendation</kbd>:</p>
<pre>class RecommenderMixin(six.with_metaclass(ABCMeta)):<br/>    """Mixin interface for recommenders.<br/><br/>    This class should be inherited by recommender algorithms. It provides an<br/>    abstract interface for generating recommendations for a user, and a<br/>    function for creating recommendations for all users.<br/>    """<br/>    @abstractmethod<br/>    def recommend_for_user(self, R, user, n=10, filter_previously_seen=False,<br/>                           return_scores=True, **kwargs):<br/>        """Generate recommendations for a user.<br/><br/>        A method that should be overridden by subclasses to create<br/>        recommendations via their own prediction strategy.<br/>        """<br/><br/>    def recommend_for_all_users(self, R, n=10,<br/>                                filter_previously_seen=False,<br/>                                return_scores=True, **kwargs):<br/>        """Create recommendations for all users."""<br/>        return (<br/>            self.recommend_for_user(<br/>                R, user, n=n, filter_previously_seen=filter_previously_seen,<br/>                return_scores=return_scores, **kwargs)<br/>            for user in xrange(R.shape[0]))</pre>
<p>This <kbd>base</kbd> class is called <kbd>RecommenderMixin</kbd>. It's simply an interface. There are two methods: one is already written for all subclasses, and that's <kbd>recommend_for_all_users</kbd>; the other is <kbd>recommended_for_user</kbd>. So, we need to override it based on the subclass. The subclass we're going to look at is item-to-item collaborative filtering.</p>
<p>In the following <kbd>itemitem.py</kbd> file, we see two parameters:</p>
<pre> def __init__(self, R, k=10):<br/>        # check the array, but don't copy if not needed<br/>        R = check_array(R, dtype=np.float32, copy=False) # type: np.ndarray<br/><br/>        # save the hyper param for later use later<br/>        self.k = k<br/>        self.similarity = self._compute_sim(R, k)<br/><br/>    def _compute_sim(self, R, k):<br/>        # compute the similarity between all the items. This calculates the<br/>        # similarity between each ITEM<br/>        sim = cosine_similarity(R.T)<br/><br/>        # Only keep the similarities of the top K, setting all others to zero<br/>        # (negative since we want descending)<br/>        not_top_k = np.argsort(-sim, axis=1)[:, k:] # shape=(n_items, k)<br/><br/>        if not_top_k.shape[1]: # only if there are cols (k &lt; n_items)<br/>            # now we have to set these to zero in the similarity matrix<br/>            row_indices = np.repeat(range(not_top_k.shape[0]),<br/>                                    not_top_k.shape[1])<br/>            sim[row_indices, not_top_k.ravel()] = 0.<br/><br/>        return sim<br/><br/>    def recommend_for_user(self, R, user, n=10,<br/>                           filter_previously_seen=False,<br/>                           return_scores=True, **kwargs):<br/>        """Generate predictions for a single user.</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>We have <kbd>R</kbd> and <kbd>k</kbd>. <kbd>R</kbd>, which is our ratings matrix, it is different from other base estimators in that we don't have the corresponding <kbd>y</kbd> value. <kbd>R</kbd> is our ground truth as well as the training array. <kbd>k</kbd> is a parameter that we can use to limit the top number of items that are similar. It helps reduce our space that we're comparing within and makes computations easier. So, for the constructor, the fit procedure is simply computing the similarity array via the <kbd>compute_sim</kbd> function. We take the <kbd>R</kbd> array, transpose it so items are along the row axis, and then we compute the cosine similarity between the rows, which are now the items. We have an <em>n x n</em> matrix, the first <em>n</em> stands for the November matrix and the second <em>n</em> is the dimensionality of the number of items. Basically, we're going to say anything that's not in <kbd>top_k</kbd>, we'll set to zero similarity. One of the strategies here is that it allows us to augment our similarity matrix in a way that, otherwise, we couldn't. And that's what we're doing: argsorting into the descending order. We want the most similar first, argsorting along the columns. We take the similarity matrix and store that in <kbd>self.similarity</kbd>. And we're going to use that when we compute predictions.</p>
<p>So, <kbd>recommend_for_user</kbd> is the function that we have to override in the super abstract interface. We can take several arguments. So, we have the <span class="CodeInTextPACKT">user</span> vector, which is an index, and <em>n</em>,<em> </em>which is the number of recommendations we want to produce. Now we get <kbd>user_vector</kbd> out of <kbd>R</kbd>:</p>
<pre># check the array and get the user vector<br/>R = check_array(R, dtype=np.float32, copy=False)<br/>user_vector = R[user, :]</pre>
<p>The recommendations—the raw recommendations—are the inner products between the user vector and the similarity matrix, which produces an <em>nD</em> or <em>1D</em> array in NumPy.</p>
<p>We get <kbd>item_indices</kbd> with the help of an <kbd>arange</kbd> method in NumPy:</p>
<pre># compute the dot product between the user vector and the similarity<br/># matrix<br/>recommendations = user_vector.dot(self.similarity) # shape=(n_items,)<br/><br/># if we're filtering previously-seen items, now is the time to do that<br/>item_indices = np.arange(recommendations.shape[0])<br/>if filter_previously_seen:<br/>    rated_mask = user_vector != 0.<br/>    recommendations = recommendations[~rated_mask]<br/>    item_indices = item_indices[~rated_mask]</pre>
<p>We're going to order this based on the descending <kbd>argsort</kbd> of the recommendations. Now we can limit them to the top <kbd>n</kbd> if we want to.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If you want to produce recommendations for everything, you can just pass <kbd>None</kbd> as <kbd>n</kbd>. We're going to return <kbd>items</kbd>, <kbd>indices</kbd>, and <kbd>recommendations</kbd>, which are the predicted ratings for each of those corresponding items, as shown here:</p>
<pre># now arg sort descending (most similar items first)<br/>order = np.argsort(-recommendations)[:n]<br/>items = item_indices[order]<br/><br/>if return_scores:<br/>   return items, recommendations[order]<br/>return items</pre>
<p>We go to the <kbd>example_item_item_recommender.py</kbd> file. We'll load up the interestingly <kbd>titled</kbd> dataset called <kbd>get_completely_fabricated_ratings_data</kbd>, which is available in the <kbd>data.py</kbd> file. Here, we've several users, as shown in the following code:</p>
<pre>    return (np.array([<br/>        # user 0 is a classic 30-yo millennial who is nostalgic for the 90s<br/>        [5.0, 3.5, 5.0, 0.0, 0.0, 0.0, 4.5, 3.0,<br/>         0.0, 2.5, 4.0, 4.0, 0.0, 1.5, 3.0],<br/><br/>        # user 1 is a 40-yo who only likes action<br/>        [1.5, 0.0, 0.0, 1.0, 0.0, 4.0, 5.0, 0.0,<br/>         2.0, 0.0, 3.0, 3.5, 0.0, 4.0, 0.0],<br/><br/>        # user 2 is a 12-yo whose parents are strict about what she watches.<br/>        [4.5, 4.0, 5.0, 0.0, 0.0, 0.0, 0.0, 4.0,<br/>         3.5, 5.0, 0.0, 0.0, 0.0, 0.0, 5.0],<br/><br/>        # user 3 has just about seen it all, and doesn't really care for<br/>        # the goofy stuff. (but seriously, who rates the Goonies 2/5???)<br/>        [2.0, 1.0, 2.0, 1.0, 2.5, 4.5, 4.5, 0.5,<br/>         1.5, 1.0, 2.0, 2.5, 3.5, 3.5, 2.0],<br/><br/>        # user 4 has just opened a netflix account and hasn't had a chance<br/>        # to watch too much<br/>        [0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0,<br/>         0.0, 0.0, 0.0, 1.5, 4.0, 0.0, 0.0],<br/>    ]), np.array(["Ghost Busters", "Ghost Busters 2",<br/>                  "The Goonies", "Big Trouble in Little China",<br/>                  "The Rocky Horror Picture Show", "A Clockwork Orange",<br/>                  "Pulp Fiction", "Bill &amp; Ted's Excellent Adventure",<br/>                  "Weekend at Bernie's", "Dumb and Dumber", "Clerks",<br/>                  "Jay &amp; Silent Bob Strike Back", "Tron", "Total Recall",<br/>                  "The Princess Bride" ]))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's say that user 0 is a classic 30-year-old millennial who loves the nostalgia of the 90s. So, they highly rate <kbd>The Princess Bride</kbd>, <kbd>Ghost Busters</kbd>, and <kbd>Ghost Busters 2</kbd>. User 1 is a 40-year-old who only likes action movies. So, they rated <kbd>Die Hard</kbd> and <kbd>Pulp Fiction</kbd>. User 2 is a 12-year-old whose parents are fairly strict, so we can assume that user 2 has not watched <kbd>Pulp Fiction</kbd> or anything like that. But user 2 has watched <kbd>Ghost Busters</kbd>, <kbd>Ghost Busters 2</kbd>, and <kbd>The Goonies</kbd>. And user 2 rated them all pretty highly. User 3 has seen it all. And user 4 has just opened a Netflix account and hasn't had the chance to watch too much. So, user 4 is probably going to be the one we're interested in producing recommendations for.</p>
<div class="packt_infobox">All this is a NumPy array. We're returning a dense array. You can return this as a sparse array.</div>
<p>In the <kbd>example_item_item_recommender.py</kbd> file that is present in <kbd>examples/recommendation</kbd>, we're going to get the <kbd>R</kbd> <span>ratings matrix </span>and <kbd>titles</kbd> from <kbd>get_completely_fabricated_ratings_data</kbd>:</p>
<pre># #############################################################################<br/># Use our fabricated data set<br/>R, titles = get_completely_fabricated_ratings_data()<br/><br/># #############################################################################<br/># Fit an item-item recommender, predict for user 0<br/>rec = ItemItemRecommender(R, k=3)<br/>user0_rec, user_0_preds = rec.recommend_for_user(<br/>    R, user=0, filter_previously_seen=True,<br/>    return_scores=True)</pre>
<p>We create a <kbd>recommender</kbd> <span>item </span>with <kbd>k=3</kbd>. We only retain the three most similar corresponding items <span>for each of the items</span>. And then we produce the recommendations for user 0.</p>
<p class="mce-root"/>
<p>Let's see what the top three rated movies are for user 0 if we run the <kbd>example_item_item_recommender.py</kbd> file:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-386 image-border" src="assets/4b81ee6d-5478-4e70-86f7-dde29b7d95dd.png" style="width:98.00em;height:11.25em;"/></p>
<p>User 0's top three rated movies are: <kbd>Ghost Busters</kbd>, <kbd>The Goonies</kbd>, and <kbd>Pulp Fiction</kbd>. This means user 0 has rated <kbd>Ghost Busters</kbd> and <kbd>The Goonies</kbd> <span>highly</span> but has not rated <kbd>Pulp Fiction</kbd>.</p>
<p>We can also see that the mean average precision is roughly 2/3. The mean average precision is a metric that we're going to use for recommender systems. It actually comes out of the information retrieval domain. It's not like, say, mean absolute error or mean squared error. What we're doing is stating <span>what proportion</span> of the ones we recommend existed in the ground truth set. In this case, it means which ones the user rated highly to begin with, which shows that the ones we produced were pretty good.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Matrix factorization</h1>
                </header>
            
            <article>
                
<p>In this section, we're going to look into recommender systems and introduce matrix factorization techniques. In typical collaborative filtering problems, we have users along one axis and items or offers along the other axis. We want to solve for the predicted rating for a user for any given item, but to get there we have to somehow compute the affinity between the users or the item. In the previous section, we looked at item-to-item collaborative filtering, where we explicitly computed the similarity matrix using the cosine similarity metric, but now we want to explore a method that's not going to explicitly compare items to items or users to users.</p>
<p>Matrix factorization is a form of collaborative filtering that focuses on the intangibles of products. At a conceptual level, every product or restaurant, for example, has intangibles that cause you to like, dislike, or remain indifferent toward them. For example, for a restaurant, maybe the atmosphere or the vibe you get outweighs the menu. Or, consider the following statement: t<span class="ItalicsPACKT">he food's terrible but the happy hour is great</span>. In this case, we're interested in learning the hidden or latent variables that underlie and manifest themselves throughout patterns in the data.</p>
<p>Matrix factorization is going to allow us to discover these latent variables by decomposing our single ratings matrix into two low-rank matrices that, 2 when multiplied, approximate the original ratings matrix. Intuitively, we're learning about these hidden factors or latent variables and learning how our users and items score against them. As shown in the following diagram, one of the low-rank matrices maps the users' affinities for the discovered factors and the other maps that item's rankings on the factors:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4934bcc4-61ec-4baf-9f3a-3d5399ace61b.png" style="width:40.50em;height:26.17em;"/></p>
<p>A drawback in matrix factorization is the lack of clarity or intuition behind what can make up a factor. It's similar to a <strong>principal component </strong>analysis (<strong>PCA</strong>) type technique, where a factor can be conceptualized as a topic. A careful, insightful analyst who has lots of subject matter expertise could feasibly extract meaning from topics, but it's very difficult to do so and, as a result, it's not typically pursued given its difficulty. For example, maybe <strong>Factor 1</strong> in the preceding diagram is a divey atmosphere. So, the wing shop is rated in varying degrees of divey-ness. As you can see on the right-hand side of the preceding diagram, there's a strong affinity between <strong>Wing Store A</strong> and the first factor, which is <strong>Dive bar</strong>. You can also assume that <strong>The Sports Bar</strong> might rate pretty highly on that scale. Then, perhaps <strong>Factor 2</strong> is a place that has some health-conscious options. So, the strength of that connection is the level at which a person or an offering ranks against the latent factor. You can see this on both the left-and the right-hand sides of the preceding diagram.</p>
<p>Essentially, we have a ratings matrix, <em>Q</em>. In different literature, it's referred to as either <em>Q</em> or <em>R</em>. We're going to call it <em>Q</em> here. We want to discover two lower rank matrices, <em>X</em> and <em>Y</em>, such that the product of the two approximate the ratings matrix. That is, <em>Q</em> or <em>Q</em> prime is approximately equal to <em>X.Y<sup>T</sup></em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9d2b83e8-9d17-4d53-9871-343eaf4841a3.png" style="width:45.00em;height:6.00em;"/></p>
<p>Our objective function is at the bottom and is basically a regularized mean squared error. So, we're looking at the mean squared error, or the reconstruction error, between <em>X</em> and <em>Y</em> and <em>Q</em> prime, and then we have the regularization term over on the other side, with lambda.</p>
<p>For the math folks, factorizing a matrix is nothing new. But doing so in the context of finding such low-rank matrices in a non-convex optimization problem might be a bit of a challenge. So, the approach we're going to see is called <strong>Alternating Least Squares</strong> (<strong>ALS</strong>).</p>
<p>The ALS algorithm is as follows:</p>
<ol>
<li>Initialize two random matrices, <em>X</em> and <em>Y</em></li>
<li>Set empty values of <em>Q</em> and <em>O</em></li>
<li>Beginning with <em>X</em>, solve the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7d8bb3e8-3235-4620-928a-0563034418ed.png" style="width:11.58em;height:1.42em;"/></p>
<ol start="4">
<li>Now solve for <em>Y</em> with the new <em>X</em>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4aa21afa-3c85-45b0-81d8-c752d7bc6bdb.png" style="width:12.33em;height:1.50em;"/></p>
<ol start="5">
<li>Iterate, alternating between <em>X</em> and <em>Y</em> until convergence</li>
</ol>
<p>Essentially, we're going to alternate between solving each respective matrix with respect to the other, and we'll eventually reach a point of convergence. So, we start out by initializing both <em>X</em> and <em>Y</em> to random values. Then, starting with <em>X</em>, we solve for <em>X</em> prime. Now that we have a more refined version of <em>X</em> prime, we can use that to solve for <em>Y</em> prime. Each matrix creates a better solution for the other at each iteration. And we can alternate like this for as many iterations as we like, or until we hit a point of diminishing returns, where we would say that we've converged.</p>
<p class="mce-root"/>
<div class="packt_infobox">A quick note on the notation here: the <em><span class="ItalicsPACKT">I</span></em> that you can see next to lambda is simply an <em>F x F</em> identity matrix, where <em>F</em> is the number of latent factors that we want to discover. We multiply that by the regularization parameter lambda. So, along the diagonal axis we have lambda, and then the rest is simply zeros.</div>
<p>Here's a hackneyed 30-line approximation of ALS in Python. We start out with defining <kbd>Q</kbd> or the ratings matrix:</p>
<pre>import numpy as np<br/>from numpy.linalg import solve<br/><br/>nan = np.nan<br/>Q = np.array([[5.0, 1.0, nan, nan, 2.5, 4.5, nan, nan],<br/>              [nan, nan, 3.5, 2.0, 3.0, nan, nan, nan],<br/>              [1.5, nan, nan, nan, 4.0, nan, 4.5, 4.0],<br/>              [nan, 1.0, nan, nan, 1.0, nan, nan, 5.0]])<br/><br/>nan_mask = np.isnan(Q) # mask applied when computing loss<br/>Q[nan_mask] = 0.<br/><br/>f = 3 # num factors<br/>n_iter = 5 # num iterations<br/>I_lambda = np.eye(f) * 0.01 # regularizing term<br/>random_state = np.random.RandomState(42)<br/><br/># initialize X, Y randomly<br/>X = random_state.rand(Q.shape[0], f)<br/>Y = random_state.rand(f, Q.shape[1])<br/>W = nan_mask.astype(int) # weights for calculating loss (0/1)<br/><br/># iterate:<br/>errors = []<br/>for i in range(n_iter):<br/>    X = solve(Y.dot(Y.T) + I_lambda, Y.dot(Q.T)).T<br/>    Y = solve(X.T.dot(X) + I_lambda, X.T.dot(Q))<br/>    errors.append(((W * (Q - X.dot(Y))) ** 2).sum())<br/>    <br/>X.dot(Y).round(3)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This is the rating that we've seen in the earlier example, and in the previous section. Now we're going to get a Boolean mask, <kbd>nan_mask</kbd>. First, we're going to set all the missing values to zero for the ensuing computations. Next, we're going to initialize <kbd>I</kbd> as our identity matrix and multiply it by lambda. We only have to do that one time, which is nice. Lambda is just 0.01 for now, but that's a hyperparameter that can be tuned using cross-validation. So, the higher lambda is, the more we'll regularize. Then, we initialize <kbd>X</kbd> and <kbd>Y</kbd> with <kbd>random_state</kbd>. <kbd>X</kbd> is going to be equal to <em>M x F</em>, that is, the number of users by the number of factors. <kbd>Y</kbd> is going to be equal to the number of factors by the number of items: <em>F x N</em>.</p>
<p>In iterating, we solve for <kbd>X</kbd>, and then we solve for <kbd>Y</kbd> given the new <kbd>X</kbd>. Then, we compute our training loss, which is again the masked version of the mean squared error, where we mask out the missing values from the original ground truth array, which is our ratings array. And then we continue to iterate until we reach convergence.</p>
<p>At the bottom of the preceding code, you can see the output of the approximation between <kbd>X</kbd> and <kbd>Y</kbd>. It is an approximation. If you look at the definition of <kbd>Q</kbd>, 3 and then the output at the bottom, it looks pretty similar. So, the way that we would create predictions at the end is that we exploit the error in the whole system, and return the highest predicted items for a user filtering the previously rated ones. So, user 4, (the very last user), would get a recommendation for the steakhouse that is <em>2.0</em>, and this is the highest non-previously rated item for that user. This is actually just a result of the multiplication error or the approximation error.</p>
<p>In the following graph, you can see how the training loss diminishes over each iteration:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-487 image-border" src="assets/7195d7ee-a2ae-4df0-a5e9-142bc00d8fb4.png" style="width:28.17em;height:20.50em;"/></p>
<p class="CDPAlignCenter CDPAlign"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Matrix factorization in Python</h1>
                </header>
            
            <article>
                
<p>In the previous section, we wanted to decompose our ratings matrix into two low-rank matrices in order to discover the intangible latent factors that drive consumers' decisions. One matrix maps the users' affinities for the discovered factors and the other maps the items' rankings on those factors.</p>
<p>So, let's look at how this can be implemented in Python. We've two files, <kbd>als.py</kbd> and <kbd>example_als_recommender</kbd>. Let's see our <kbd>als.py</kbd> file. In the last section, we saw the item-to-item collaborative filter; ALS is very similar. It's going to implement <kbd>RecommenderMixin</kbd>:</p>
<pre>def __init__(self, R, factors=0.25, n_iter=10, lam=0.001,<br/> random_state=None):</pre>
<p>We have several parameters for ALS. The first one, and the only non-optional one, is <kbd><span class="CodeInTextPACKT">R</span></kbd>, our ratings matrix. In some of the math we've seen, we've referred to this interchangeably as <kbd>R</kbd> and <kbd>Q</kbd>. Again, that's kind of a quirk of the literature. Depending on what papers you're reading, it's one or the other. And the second parameter we're going to take is <kbd><span class="CodeInTextPACKT">factors</span></kbd>.The <kbd>factors</kbd> <span>parameter </span>is the number of latent variables we want to discover. I have used float, but you can use an integer. The floating point is just going to be bound between zero and one. <kbd>n_iter</kbd> is the number of iterations. ALS, in this module, does not support early convergence or early stopping. That's something that you could absolutely write. But if you have too many iterations, what happens is you're probably going to overfit your data. Lambda is our regularization parameter, and then you can just pass <kbd>random_state</kbd> as a way for reproducibility.</p>
<p>For the first step, as always, we're going to check our array to make sure that we have only floating points:</p>
<pre># check the array<br/>R = check_array(R, dtype=np.float32) # type: np.ndarray<br/>n_users, n_items = R.shape<br/># get the random state<br/>random_state = check_random_state(random_state)</pre>
<p>We are going to allow missing data here, because missing data is natural in recommender systems. And we can almost guarantee there's always going to be missing data.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the following code, we're making sure that our factor is an integer. And if it's <kbd>float</kbd>, we figure out the number of <kbd>factors</kbd> we're going to discover:</p>
<pre># get the number of factors. If it's a float, compute it<br/>if isinstance(factors, float):<br/>    factors = min(np.ceil(factors * n_items).astype(int), n_items)</pre>
<p>So, <kbd><span class="CodeInTextPACKT">W</span></kbd> here is equal to <kbd>nan_mask</kbd>, which we looked at in the previous section:</p>
<pre>W = (R &gt; 0.).astype(np.float32)</pre>
<p>This is going to be, essentially, a weighting array that says whether or not the value was missing to begin with. And so, we use this to mask our ground truth out of the ratings matrix when we compute our mean squared error during our iterations.</p>
<p>Here, we initialize <kbd>Y</kbd>:</p>
<pre># initialize the first array, Y, and X to None<br/> Y = random_state.rand(factors, n_items)<br/> X = None<br/># the identity matrix (time lambda) is added to the XX or YY product<br/># at each iteration.<br/> I = np.eye(factors) * lam</pre>
<p>We are not initializing <kbd>X</kbd> because we know that that's going to be the first one we solve for in our iterations. So, as we have seen in the previous section, we also initialize <kbd>I</kbd> as the identity matrix—that is, <em>F x F</em>—and multiply it by our regularization parameter.</p>
<p>Now we're going to iterate, as shown in the following code:</p>
<pre class="mce-root"># for each iteration, iteratively solve for X, Y, and compute the<br/> # updated MSE<br/> for i in xrange(n_iter):<br/> X = solve(Y.dot(Y.T) + I, Y.dot(R.T)).T<br/> Y = solve(X.T.dot(X) + I, X.T.dot(R))<br/># update the training error<br/> train_err.append(mse(R, X, Y, W))<br/># now we have X, Y, which are our user factors and item factors<br/> self.X = X<br/> self.Y = Y<br/> self.train_err = train_err<br/> self.n_factors = factors<br/> self.lam = lam</pre>
<p>Begin by solving for <kbd>X</kbd>, and then solve for <kbd>Y</kbd>. At each iteration, we're going to just calculate the training error, which is the mean squared error. We append it to the list that we store as a <kbd>self</kbd> parameter in the following code.</p>
<p class="mce-root"/>
<p>The training phase is actually extraordinarily easy for ALS. Now, in the previous, section we didn't see how to concretely generate predictions. We saw the math behind it, but we haven't implemented it. If you call predict on ALS, as shown in the following code, it's simply going to compute the product of the user factors and the item factors to return the <kbd>R</kbd> prime—basically the approximation:</p>
<pre>def predict(self, R, recompute_users=False):<br/>        """Generate predictions for the test set.<br/><br/>        Computes the predicted product of ``XY`` given the fit factors.<br/>        If recomputing users, will learn the new user factors given the<br/>        existing item factors.<br/>        """<br/>        R = check_array(R, dtype=np.float32, copy=False) # type: np.ndarray<br/>        Y = self.Y # item factors<br/>        n_factors, _ = Y.shape<br/><br/>        # we can re-compute user factors on their updated ratings, if we want.<br/>        # (not always advisable, but can be useful for offline recommenders)<br/>        if recompute_users:<br/>            I = np.eye(n_factors) * self.lam<br/>            X = solve(Y.dot(Y.T) + I, Y.dot(R.T)).T<br/>        else:<br/>            X = self.X<br/><br/>        return X.dot(Y)</pre>
<p>You can pass in <kbd>R</kbd>, which would ostensibly be the test data. This is the data to include new users who weren't included in the fit originally, or it could mean that the users have updated their data. But we can recompute the user factors if we want to. So, if the users have moved on in time and our fit is about a week old, then we can recompute the user factors with respect to the existing item factors. Then, at the end, we're just returning the product of <kbd>X</kbd> and <kbd>Y</kbd>.</p>
<p>Now we'll call the <kbd>recommend_for_user</kbd> function. So, given your test matrix and the user index, we want to know what the top <kbd>n</kbd> items are to recommend for a user and we do largely the same thing:</p>
<pre>def recommend_for_user(self, R, user, n=10, recompute_user=False,<br/>                       filter_previously_seen=False,<br/>                       return_scores=True):</pre>
<p>We're going to create this prediction, but extract out the predicted user vector. So, we're using the <kbd>self.predict</kbd> method, as shown in the following code:</p>
<pre>R = check_array(R, dtype=np.float32, copy=False)<br/># compute the new user vector. Squeeze to make sure it's a vector<br/> user_vec = self.predict(R, recompute_users=recompute_user)[user, :]<br/> item_indices = np.arange(user_vec.shape[0])<br/># if we are filtering previously seen, remove the prior-rated items<br/> if filter_previously_seen:<br/> rated_mask = R[user, :] != 0.<br/> user_vec = user_vec[~rated_mask]<br/> item_indices = item_indices[~rated_mask]<br/>order = np.argsort(-user_vec)[:n] # descending order of computed scores<br/> items = item_indices[order]<br/> if return_scores:<br/> return items, user_vec[order]<br/> return items</pre>
<p>If we are interested in filtering out the ones we previously saw, we just mask those out and return the descending argsorted indices of items that we're interested in. This is very similar to what we've seen before when we were looking at spatial clustering, but here, all we're doing is computing the approximation of <kbd>X</kbd> and <kbd>Y</kbd> and argsorting the columns.</p>
<p>Let's look at an example in the <kbd>example_als_recommender.py</kbd> file:</p>
<pre># -*- coding: utf-8 -*-<br/><br/>from __future__ import absolute_import<br/><br/>from packtml.recommendation import ALS<br/>from packtml.recommendation.data import get_completely_fabricated_ratings_data<br/>from packtml.metrics.ranking import mean_average_precision<br/>from matplotlib import pyplot as plt<br/>import numpy as np<br/>import sys<br/><br/># #############################################################################<br/># Use our fabricated data set<br/>R, titles = get_completely_fabricated_ratings_data()<br/><br/># #############################################################################<br/># Fit an item-item recommender, predict for user 0<br/>n_iter = 25<br/>rec = ALS(R, factors=5, n_iter=n_iter, random_state=42, lam=0.01)<br/>user0_rec, user_0_preds = rec.recommend_for_user(<br/>    R, user=0, filter_previously_seen=True,<br/>    return_scores=True)<br/><br/># print some info about user 0<br/>top_rated = np.argsort(-R[0, :])[:3]<br/>print("User 0's top 3 rated movies are: %r" % titles[top_rated].tolist())<br/>print("User 0's top 3 recommended movies are: %r"<br/>      % titles[user0_rec[:3]].tolist())</pre>
<p>You may recall from the preceding code the recommended data. This is the completely fabricated data that we went on about <span><span>in the previous sections</span></span>. We're going to take this same data and we're going to fit ALS on it. We want to know user 0's predictions, so, before we run it, we need some information. Let's say user 0 rated <kbd>Ghost Busters</kbd> pretty highly, and rated <kbd>The Goonies</kbd> pretty highly as well. This guy knows their stuff! So, this guy is a classic 90s/late 80s millennial.</p>
<p>You'll notice, in the following screenshot, that we have activated my <kbd>packt-sml</kbd> conda environment:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-489 image-border" src="assets/bfc0e9e8-ca51-4a3a-be29-063f86051706.png" style="width:100.17em;height:8.67em;"/></p>
<p>The output of the preceding code is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-392 image-border" src="assets/8e71358d-9e6c-4a09-9a07-701681ebf40f.png" style="width:23.00em;height:20.75em;"/></p>
<p class="mce-root"/>
<p>You need to do the same. So, when we run this, we'll get the preceding graph, which is showing how the training error diminishes over the iterations, as we expect it would. As a result, we would recommend that user 0 watch <kbd>Weekend at Bernie's</kbd> as the top-rated suggestion. And that seems to make sense given <kbd>The Goonies</kbd> and <kbd>Ghost Busters</kbd>. But then <kbd>Pulp Fiction</kbd> is a bit violent, and so we also recommended <kbd>Clockwork Orange</kbd>, which also seems to jive with that. So, the mean average precision is, essentially, looking at the recommendations and then comparing them to the ground truth and saying how many of those were actually previously rated highly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Limitations of ALS</h1>
                </header>
            
            <article>
                
<p>We've been using explicit ratings. For example, on Amazon, ratings are between one and five stars. The problem here is that explicit rating systems typically have trouble getting users to rate the items, because it's easier to consume that content than it is to evaluate it from the user side. So, implicit ratings are the inverse of explicit ratings and they can be collected by a system, usually, without the user's awareness. A lot of times that's more favorable, because it doesn't require the user to interact with the system in a secondary sense to explicitly rate items, and we can get more data, which means less sparse data. So, implicit ratings might include the number of listens to a song. There's really well-known ratings dataset collected by the Last FM team that uses implicit ratings, and it's commonly used for benchmarking recommender systems. There is an implicit variation of ALS, but we only covered the explicit version. But if you check on Google for implicit ALS, there's all sorts of literature around it. We encourage you to go look it up.</p>
<p>The next challenge of recommenders is sparsity versus density. As we've seen, ratings matrices can be pretty sparse. For some systems, such as Amazon, there may only be ratings for less than approximately one percent of all items per user, and a lot of times even less than that. So, dense matrices are not usually the best solution, and oftentimes they're not even feasible. So, we either have to use sparse matrices or get really clever with how we distribute the data, so we don't totally blow up our memory.</p>
<p>Recommenders typically take a very long time to train. Like many other machine learning models, we run into that same kind of thing, but recommenders are a bit different in the sense that they have to be updated in much greater frequency, in many cases, multiple times per day, depending on the system itself. So, new items arriving in a catalog or new users beginning to consume media means that the recommender has to be refreshed. But we can't do this online or in real time, or we risk taking the system down. So, generally, recommenders are retrained on a periodic basis in an offline fashion. And the models are scored in an online or more real-time fashion.</p>
<p class="mce-root"/>
<p>In this <span><span>section</span></span>, we looked at the Python implementation of ALS in the <kbd>packtml</kbd> library and an example. Finally, we discussed some of the real-world challenges we face in recommender systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Content-based filtering</h1>
                </header>
            
            <article>
                
<p>In this section, we're going to wrap up our discussion around recommender systems by introducing an entirely separate approach to computing similarities and look at how we can use it to augment our collaborative filtering systems.</p>
<p> </p>
<p>Content-based recommenders operate similarly to the original item-to-item collaborative system that we saw earlier, but they don't use ratings data to compute the similarities. Instead, they compute the similarities directly by using provided attributes of the items in the catalog. Predictions can then be computed in the same fashion as item-to-item collaborative filtering by calculating the product of the ratings matrix and similarity matrix.</p>
<p> </p>
<p>Here's an example of how we might use content vectors to directly compute the item similarity matrix:</p>
<pre>import numpy as np<br/>from sklearn.metrics.pairwise import cosine_similarity<br/><br/>ratings = np.array(([5.0, 1.0, 0.0, 0.0, 2.5, 4.5, 0.0, 0.0],<br/>                    [0.0, 0.0, 3.5, 2.0, 3.0, 0.0, 0.0, 0.0],<br/>                    [1.5, 0.0, 0.0, 0.0, 4.0, 0.0, 4.5, 4.0],<br/>                    [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 5.0]))<br/># content vector<br/><br/>categories = ['Alcohol license',<br/>              'Healthy options',<br/>              'Burgers on menu',<br/>              'Located in downtown',<br/>              '$', '$$', '$$$', '$$$$',<br/>              'Full bar', 'Southern cooking',<br/>              'Grilled food']<br/># categories        a1   he  bu  dt  1$  2$  3$  4$  fb  sc  gf<br/>content = np.array([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.],<br/>                    [1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.],<br/>                    [0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.],<br/>                    [1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.],<br/>                    [0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.],<br/>                    [1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.],<br/>                    [1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.],<br/>                    [1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.]<br/>                   ])<br/>sim = cosine_similarity(content)<br/>ratings.dot(sim).round(3)</pre>
<p>The output of the preceding code is as follows:</p>
<pre>array([[6.337, 4.381, 6.169, 6.738, 5.703, 5.545, 4.813, 6.872],<br/> [2.997, 1.797, 7.232, 5.294, 6.904, 4.03 , 4.078, 5.587],<br/> [5.697, 4.539, 8.515, 8.305, 8.799, 5.876, 9.01 , 9.005],<br/> [2.306, 3. , 4.444, 5.169, 3.582, 4.658, 3.758, 5.916]])</pre>
<p>We're using the same ratings matrix as we have over the last few <span><span>sections,</span></span> and we've created 11 different attributes about the various restaurants. Generally, the content vectors of these dummy-encoded features indicate whether an item belongs to a given category. So, you can see the similarity is computed in exactly the same fashion. So, we just compute the cosine similarity between the rows. And then we even generate predictions in the same way. We compute the product of the similarities and the ratings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Limitations of content-based systems</h1>
                </header>
            
            <article>
                
<p>There are several notable limitations to content-based systems that make them less than ideal in most scenarios. The first of these is the manual nature of the feature engineering, which can be extraordinarily tough given that the difficulty of collecting the data about the items can be really time-consuming, and many times, the data we're presented about an item is limited to a text description. So, we're not given this nice encoded matrix and that means we have to extract the attributes from descriptions, which can be challenging and extremely time-intensive.</p>
<p>Next, we end up with the largely dummy-encoded set of content vectors, meaning it's heavily zero inflated. So, naturally, our similarity computations are going to be fairly low with respect to what we might get out of a comparable collaborative approaches computation. And, finally, as our feature matrix grows in rank, the similarity between the two given items will be orthogonal or zero, so the likelihood of that approaches 1. For more information, you can refer to <span class="MsoHyperlink"><a href="https://math.stackexchange.com/questions/995623/why-are-randomly-drawn-vectors-nearly-perpendicular-in-high-dimensions">https://math.stackexchange.com/questions/995623/why-are-randomly-drawn-vectors-nearly-perpendicular-in-high-dimensions</a></span>. It's a loose proof showing that the higher the rank, the more likely it is that you approach that orthogonality, which we don't want. All these limitations make a good case for why content-based systems are less favorable than collaboratively based systems.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>But there're also some cases where they can be really useful. One of these is called the <strong>cold-start problem</strong>, which we discussed earlier in this section, and we encounter in every collaborative filtering application. This is when a new item is added and it cannot be compared to an existing item on the basis of ratings due to its own lack of ratings. So, the challenge here, apart from being unable to compute that similarity, is that if you impute it with a 0 or some other random value, you may never present that to a consumer. You implicitly diminish the chance that you would ever recommend that item.</p>
<p>In item-to-item collaborative filtering, it also occurs in situations where there are two items that have not been mutually rated by the same user, since we can't compute the similarity. So, that's an additional case and, in this one, it's going to result in a similarity of 0 in our matrix because we impute all the missing values with 0, even though we, theoretically, have ratings on which to gauge the affinity. In these scenarios, it's useful to have a fallback plan.</p>
<p>Here, we're fitting an item-to-item collaborative filtering recommender:</p>
<pre>from packtml.recommendation import ItemItemRecommender<br/><br/>rec = ItemItemRecommender(ratings, k=5)<br/><br/>zero_mask = rec.similarity == 0<br/>rec.similarity[zero_mask] = sim[zero_mask]<br/>rec.similarity</pre>
<p>The output of the preceding code is as follows:</p>
<pre>array([[0.99999994, 0.67728543, 0.35355338, 0.26726124, 0.62405604,<br/>        0.95782626, 0.28734788, 0.31622776],<br/>       [0.67728543, 0.99999994, 0.2236068 , 0.50709254, 0.43580094,<br/>        0.70710677, 0.5477226 , 0.5521576 ],<br/>       [0.35355338, 0.2236068 , 1. , 1. , 0.52827054,<br/>        0.4472136 , 0.4082483 , 0.6708204 ],<br/>       [0.26726124, 0.50709254, 1. , 1. , 0.52827054,<br/>        0.8451542 , 0.6172134 , 0.8451542 ],<br/>       [0.62405604, 0. , 0.52827054, 0.4364358 , 1. ,<br/>        0.2581989 , 0.7043607 , 0.577514 ],<br/>       [0.95782626, 0.70710677, 0.4472136 , 0.8451542 , 0.44022545,<br/>        1. , 0.36514837, 0.8 ],<br/>       [0.28734788, 0.5477226 , 0.4082483 , 0.6172134 , 0.7043607 ,<br/>        0.36514837, 1. , 0.62469506],<br/>       [0.1795048 , 0.5521576 , 0.6708204 , 0.8451542 , 0.577514 ,<br/>        0.8 , 0.62469506, 0.99999994]], dtype=float32)</pre>
<p class="mce-root"/>
<p>From preceding code, we see several sections from the <kbd>packtml</kbd> package on our ratings data, which we've been using for the last few sections. We're going to use the content similarity computations to impute the data that suffers from the cold-start problem. When we examine the similarity matrix, you can see that there are no more 0s. So, there is a corner case where you might get a 0, and that's if you had a missing mutual similarity or a cold-start problem, and then perfect orthogonality in the actual content vectors. But we don't see that. So, ostensibly, this gets us closer to a more robust model. But you're still restricted to the limitations that we have seen before, namely, collecting the content attributes and computing those potentially orthogonal vectors.</p>
<p>So, at this point, you're familiar with the concept and you realize content-based similarities alone are not very feasible. But they can actually augment your collaborative filtering method if you have the right situation and setup. There's been a lot of research around using neural networks to automatically hybridize content-based and collaborative systems. A lot of them are using neural networks to create features from text descriptions a touch informal in an automatic sense, and then creating a separate network to factorize the matrices. So, there's a lot of hope in the future that content and collaborative systems can exist in parity.</p>
<p>The following are two papers that are pursuing this approach:</p>
<ul>
<li><em>Hybrid Collaborative Filtering with Neural Networks</em>, Florian Strub, Jeremie Mary, and Romaric Gaudel, 2016</li>
<li><em>Hybrid Recommender System Using Semi-supervised Clustering Based on Gaussian Mixture Model</em>, Cyberworlds (CW), 2016 International Conference, pp. 155-158, 2016</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks and deep learning</h1>
                </header>
            
            <article>
                
<p>This is a huge topic in machine learning, so we can't cover everything in this chapter. If you've never seen a neural network before, they look like a giant spider web. The vertices of these spider webs are called neurons, or units, and they are based on an old-school linear classifier known as a perceptron. The idea is that your vector comes in, computes a dot product with a corresponding weight vector of parameters, and then gets a bias value added to it. Then, we transform it via an activation function. A perceptron, in general, can be canonically the same as logistic regression if you're using a sigmoid transformation.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When you string a whole bunch of these together, what you get is the massive web of perceptrons feeding perceptrons: this is called a multi layer perceptron, but it's also known as a neural network. As each of these perceptrons feeds the next layer, the neurons end up learning a series of nonlinear transformations in the input space, ultimately producing a prediction in the final layer.</p>
<p>The history of these models is actually really fascinating. They were first proposed in the early 1950s, but their potential was not really unlocked for quite a long time, since they're so computationally intensive. Nowadays, though, we hear a bout deep learning everywhere, and it's really just referring to the broader family of neural networks, including some of their unsupervised and generative variants.</p>
<p>So, how does a neural network actually learn? Well, we're going to iteratively feed the data through layers of the networks in epochs. Feeding the layer forward is as simple as computing a matrix product between one layer and the next, adding the bias vector along the column axis, and then transforming the output via the activation function. There are a lot of different activation functions you can use, but some of the most common ones are the sigmoid; the hyperbolic tangent, which is similar to the sigmoid but bounds between negative one and one rather than zero and one; and <strong>rectified linear units</strong> (<strong>ReLUs</strong>), which really are just flooring functions between the value and zero. It makes sure that nothing negative comes out of the units. So, after each epoch or iteration, outside the output layer we're going to compute the error of the network, and pass the message back up through the layers and they can adjust their weights accordingly. This process is called backpropagation. We usually use gradient descent for this.</p>
<p>For our two-layer example, which is really just a single layer in the middle with an output layer at the end, we only have to compute two matrix products for each epoch. It's been found that how you initialize your weights makes a huge difference in the capacity for the network to learn. There are several approaches to the strategies for this, but the easiest way is to just initialize them to very small values. We typically pick random values between negative and positive 0.1. You can go smaller; you can get more clever. We will initialize our biases as 1 vectors. Again, there are other clever ways to do this. We're just going to use 1, and the weight matrices themselves map one layer to the next. So, going from layer 1 to layer 2, we go from three units to four. You can see that dimensionality in the number of units. Our corresponding weight matrix is going to be <em>3 x 4</em> and, likewise, for the second one it's going to be <em>4 x 2</em>.</p>
<p>Here, we're just expressing our network as a system of linear equations:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e1a9a508-461c-4283-8b21-ad4998d3eba1.png" style="width:15.17em;height:1.42em;"/></p>
<p>The first layer is passed to the second layer in that nested parentheses on the inside, and then to the last layer on the outer parentheses. And what we end up with is this real matrix in <em>m x 2</em>.</p>
<p>Here's a forward pass in a snippet of highly oversimplified Python code:</p>
<pre>import numpy as np<br/><br/># define activation function<br/>f = (lambda v: 1./ (1. + np.exp(-v)))<br/>lam = 0.01<br/># input matrix<br/>X = np.array([[1.5, 5.0, 2.5],<br/>             [0.6, 3.5, 2.8],<br/>             [2.4, 5.6, 5.6]])<br/><br/>y = np.array([1, 1, 0])<br/># initialize hidden layers, bias<br/><br/>rs = np.random.RandomState(42)<br/>H1 = rs.rand(3, 4)<br/>H2 = rs.rand(4, 2)<br/>b1, b2 = np.ones(4), np.ones(2)<br/><br/># feed forward<br/>H1_res = f(X.dot(H1) + b1)<br/>output = f(H1_res.dot(H2) + b2)</pre>
<p>We're defining our activation function. <kbd>f</kbd> is a logistic or sigmoid transformation. <kbd>lam</kbd>, or <kbd>lambda</kbd>, is going to be our learning rate, which we learned about when we talked about gradient descent. And you'll remember this from logistic regression, where we can control the rate of how we descend that gradient. After initializing <kbd>X</kbd> and <kbd>y</kbd>, which we're just using as random values, we create hidden <kbd>H1</kbd> and <kbd>H2</kbd> <span>layers,</span> and <kbd>b1</kbd> and <kbd>b2</kbd> <span>biases.</span> In this example, we created the layers using the NumPy <kbd>rand</kbd> function. But this is where you'd want to get clever and bound them between negative <kbd>0.1</kbd> and <kbd>0.1</kbd> on the positive scale. Then, the result of our hidden layer one, <kbd>H1_res</kbd>, is computed by applying our <kbd>f</kbd> <span>activation function </span>to the <kbd>AX + b</kbd> <span>linear equation.</span> So, we just compute the inner product between <kbd>X</kbd> and <kbd>H1</kbd>, and then add the bias vector along the column vectors.</p>
<p>The output is computed by applying the second hidden layer to the output of the first in the same fashion. So, we're chaining these linear systems into one another, and applying this nonlinear transformation to that output.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>So, now that we have our first epoch complete, we need to adjust the weights of the network to get an error-minimizing state because, right now, the chances are our network produced a terrible error. And so, here begins the fun of backpropagation, and if you thought we had a lot of calculus earlier in this book, you're in for a treat here. We're going to compute four derivatives: two for each layer. We use them to adjust the weight in the layer immediately above, much like we did in logistic regression. Then, the next time we do a forward pass, the weights have been adjusted and we'll, in theory, have less error in the network than we did previously.</p>
<p>Here, we're implementing backpropagation from scratch:</p>
<pre># back prop<br/>out_delta = output.copy() # get a copy of the output<br/>out_delta[range(X.shape[0]), y] -= 1.<br/>H2_d = H1_res.T.dot(out_delta)<br/>b2_d = H2_d.sum(axis=0)<br/>delta2 = out_delta.dot(H2.T) * (1. - np.power(H1_res, 2.))<br/>H1_d = X.T.dot(delta2)<br/>b1_d = delta2.sum(axis=0)<br/><br/># update weights, bias<br/><br/>H1 += -lam * H1_d<br/>b1 += -lam * b1_d<br/>H2 += -lam * H2_d<br/>b2 += -lam * b2_d</pre>
<p>We're going to compute four derivatives: the derivative and loss function with respect to each of the weights layers—that's two—and the bias layers—that's another two. The first delta is really easy to compute: it's simply the predicted probabilities, which is this matrix minus the truth indices of <kbd>y</kbd>. Next, we're going to compute the first layer's output with the delta we just computed, which is going to be a derivative with respect to the last layer, which is the output layer. And, after that, we can sum along the columns of our results to get the derivative of our second layer biases.</p>
<p>We can use the same process to compute our derivatives for the next <kbd>H1</kbd> and <kbd>b1</kbd> <span>layer. O</span>nce we have those gradients computed, we can update the weights and biases in the same fashion as we did in logistic regression, which is by multiplying each derivative by the negative learning rate, and adding that to the weights matrices and <kbd>H1</kbd> and <kbd>b1</kbd>, and <kbd>H2</kbd> and <kbd>b2</kbd> <span>bias vectors, </span>respectively. And now we've updated our weights and biases along the axis of greatest change in our function: the loss function.</p>
<p>So, if you backpropagate correctly, you're going to get error terms that converge similarly to the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b327eaec-85b2-43ee-9655-f34f1aef814b.png" style="width:24.67em;height:19.08em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tips and tricks for training a neural network</h1>
                </header>
            
            <article>
                
<p>Here are some tricks that can make your life easier when you're actually training a neural network from scratch. You can stop your training a bit early to avoid overfitting. In the preceding graph, you can see there's a long tail where the error does not decrease anymore and we're still training. It's at a point around epoch 25 or 30. We could have stopped early.</p>
<p>Regularization and dropout are ways that can prevent your network from overfitting. Now, for extremely large data, you can do partial fits per epoch, meaning that you can fit many batches through your network for each forward pass so that you don't have to hold everything in memory. It also makes backpropagation a little easier, and different activation functions are going to give you different results. So, always try them out. And, finally, always use cross-validation, as we've talked about before, to select your model hyperparameters, so that you don't inadvertently create model leakage with the validation set, or even with overfitting your training set.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks</h1>
                </header>
            
            <article>
                
<p>We're going to iteratively feed the data through layers in the network in epochs. After each iteration, we're going to compute the error of the network and the output, and pass the signal back up through the layers so they can adjust their weights accordingly. So, that's all for the theory and recaps.</p>
<p>We have two files we're going to look at. We have the source code and an example: <kbd>base.py</kbd> and <kbd>mlp.py</kbd>, which stands for multilayer perceptron. Let's start with <kbd>base.py</kbd>:</p>
<pre>def tanh(X):<br/>    """Hyperbolic tangent.<br/><br/>    Compute the tan-h (Hyperbolic tangent) activation function.<br/>    This is a very easily-differentiable activation function.<br/><br/>    Parameters<br/>    ----------<br/>    X : np.ndarray, shape=(n_samples, n_features)<br/>        The transformed X array (X * W + b).<br/>    """<br/>    return np.tanh(X)<br/><br/><br/>class NeuralMixin(six.with_metaclass(ABCMeta)):<br/>    """Abstract interface for neural network classes."""<br/>    @abstractmethod<br/>    def export_weights_and_biases(self, output_layer=True):<br/>        """Return the weights and biases of the network"""</pre>
<p>We have two functions. One function, <kbd>tanh</kbd>, is a hyperbolic tangent function we're going to use as our activation function. And this is just a wrapper for <kbd>np.tanh</kbd>. Then, we have a <kbd>NeuralMixin</kbd> class, which is kind of an abstract interface we're going to use for exporting the weights and biases of each of our networks.</p>
<p>In <kbd>mlp.py</kbd>, we're going to depend on the typical <kbd>check_X_y</kbd> from scikit-learn, <kbd>check_classification_targets</kbd>. Because we're only performing either binary or multiclass classification, we're going to use softmax, and then <kbd>check_random_state</kbd>. So, we can use a replicable <kbd>random_state</kbd> inside of our neural network.</p>
<p class="mce-root"/>
<p>There is a function outside of the class itself—<kbd>calculate_loss</kbd>:</p>
<pre>def _calculate_loss(truth, preds, weights, l2):<br/>    """Compute the log loss.<br/><br/>    Calculate the log loss between the true class labels and the predictions<br/>    generated by the softmax layer in our neural network.<br/><br/>    Parameters<br/>    ----------<br/>    truth : np.ndarray, shape=(n_samples,)<br/>        The true labels<br/><br/>    preds : np.ndarray, shape=(n_samples, n_classes)<br/>        The predicted class probabilities<br/><br/>    weights : list<br/>        The list of weights matrices. Used for computing the loss<br/>        with the L2 regularization.<br/><br/>    l2 : float<br/>        The regularization parameter<br/>    """<br/>    # get the log probs of the prediction for the true class labels<br/>    n_samples = truth.shape[0]<br/>    logprobs = -np.log(preds[range(n_samples), truth])<br/><br/>    # compute the sum of log probs<br/>    sum_logprobs = logprobs.sum()<br/><br/>    # add the L2 regularization term<br/>    sum_logprobs += l2 / 2. * sum(np.square(W).sum() for W in weights)<br/>    return 1. / n_samples * sum_logprobs</pre>
<p>Essentially, this is going to be our objective function inside of our neural network that we can compute, and backpropagate that loss up through the network. Softmax is going to be the generalization, that is, our logistic function applied to multiple classes. So, that's what we get out of this. From the <kbd>K</kbd> matrix, where <kbd>K</kbd> is the dimension of the number of classes, we have a three-class problem; we can compute probabilities for the membership of each of those classes. And that's what softmax does.</p>
<p>Now our neural net classifier is going to take a number of different parameters, as shown here:</p>
<pre> def __init__(self, X, y, hidden=(25,), n_iter=10, learning_rate=0.001,<br/>              regularization=0.01, random_state=42):</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As usual, we have our <kbd>X</kbd> and <kbd>y</kbd>, and then we have <kbd>hidden</kbd>, which is going to be a tuple or some other iterable that has positional elements indicating the number of units in each layer. So, if we wanted to have two layers, we might have <kbd><span><span>X</span></span></kbd>, <kbd>25</kbd>, where each layer would have <kbd>25</kbd> units. There is no exact science to determining how many units you want and it kind of depends on your objective. If you want to compress the dimensionality, you might make the number of units smaller than the input dimensionality. If you want to discover all sorts of nuanced features, then you might expand the number of units. The number of iterations is actually the number of epochs we're going to perform. The learning rate is the lambda that we've seen in logistic regression. Regularization is our <kbd>l2</kbd> penalty that's going to help us prevent overfitting. And <kbd>random_state</kbd>, again, is the seed that we'll use to control <kbd>random_state</kbd> so this is replicable.</p>
<p>In the constructor, all we're doing is self-assigning different attributes to the algorithm:</p>
<pre class="mce-root">self.hidden = hidden<br/>self.random_state = random_state<br/>self.n_iter = n_iter<br/>self.learning_rate = learning_rate<br/>self.regularization = regularization<br/># initialize weights, biases, etc.<br/>X, y, weights, biases = self._init_weights_biases(<br/>    X, y, hidden, random_state, last_dim=None)</pre>
<p>Then, we initialize the weights and biases. We're tracking the last dimension of the last matrix, or hidden weight matrix. So, we will start the input with <kbd>none</kbd>. We're going to use the column dimensionality as the input dimensionality of the next layer. So, we mentioned in the example that we went from three to four. Our dimensionality of the first hidden matrix or hidden layer may be <em>3 x 4</em>. We're tracking the last column dimensionality because that becomes the row dimensionality of the next layer. We return to <kbd>X</kbd>, <kbd>y</kbd>, <kbd>weights</kbd>, <kbd>biases</kbd>, and this will be used by subclasses later, as well, which is why it's a class function.</p>
<p>Now we start progressing through forward passes of our network. First, we compute the forward step:</p>
<pre>    def _forward_step(X, weights, biases):<br/>        # track the intermediate products<br/>        intermediate_results = [X]<br/><br/>        # progress through all the layers EXCEPT the very last one.<br/>        for w, b in zip(weights[:-1], biases[:-1]):<br/><br/>            # apply the activation function to the product of X and the weights<br/>            # (after adding the bias vector)<br/>            X = tanh(X.dot(w) + b)<br/><br/>            # append this layer result<br/>            intermediate_results.append(X)<br/><br/>        # we handle the very last layer a bit differently, since it's out<br/>        # output layer. First compute the product...<br/>        X = X.dot(weights[-1]) + biases[-1]<br/><br/>        # then rather than apply the activation function (tanh), we apply<br/>        # the softmax, which is essentially generalized logistic regression.<br/>        return softmax(X), intermediate_results</pre>
<p>A forward step is pretty easy. We have <kbd>X</kbd>, our weights, and our biases. We're going to ZIP our weights and biases together so we can track them together. And we're just going to compute that product of <kbd>X.dot(w)</kbd>, <kbd>w</kbd> being weight, and add biases. This is again that <kbd>AX</kbd> <span>linear system </span>plus <kbd>b</kbd>. Then, we apply this nonlinear transformation, <kbd>tanh</kbd>. But if you wanted to use sigmoid, you could do that. The last layer is slightly different. We're not running <kbd>tanh</kbd> on the last layer, we're actually running softmax. This is a classification problem, so we apply softmax to the output of <kbd>X</kbd> as opposed to <kbd>tanh</kbd>. And that's the output layer.</p>
<p>In the constructor, we've computed the first forward step and our first epoch:</p>
<pre>        # for each iteration, feed X through the network, compute the loss,<br/>        # and back-propagate the error to correct the weights.<br/>        for _ in xrange(n_iter):<br/>            # compute the product of X on the hidden layers (the output of<br/>            # the network)<br/>            out, layer_results = self._forward_step(X, weights, biases)<br/><br/>            # compute the loss on the output<br/>            loss = _calculate_loss(truth=y, preds=out, weights=weights,<br/>                                   l2=self.regularization)<br/>            train_loss.append(loss)<br/><br/>            # now back-propagate to correct the weights and biases via<br/>            # gradient descent<br/>            self._back_propagate(y, out, layer_results, weights,<br/>                                 biases, learning_rate,<br/>                                 self.regularization)</pre>
<p>Now we want to calculate the loss; the loss is just that log loss that we saw previously. We're going to track loss per epoch here in <kbd>train_loss</kbd>. If you want to speed this up, you might only calculate the loss, say, every five iterations. In the following backpropagation example, we will get a clever idea regarding how we implement these gradients in a fashion that's a bit more extensible than the two-layer example from the last one.</p>
<p class="mce-root"/>
<p>Now, in the backpropagation function, we compute delta again, which is the probabilities of each of the classes minus the truth indices:</p>
<pre class="mce-root">probas[range(n_samples), truth] -= 1.<br/># iterate back through the layers computing the deltas (derivatives)<br/> last_delta = probas<br/> for next_weights, next_biases, layer_res in \<br/> zip(weights[::-1], biases[::-1], layer_results[::-1]):<br/># the gradient for this layer is equivalent to the previous delta<br/># multiplied by the intermittent layer result<br/> d_W = layer_res.T.dot(last_delta)<br/># column sums of the (just-computed) delta is the derivative<br/># of the biases<br/> d_b = np.sum(last_delta, axis=0)<br/># set the next delta for the next iter<br/> last_delta = last_delta.dot(next_weights.T) * \<br/> (1. - np.power(layer_res, 2.))<br/># update the weights gradient with the L2 regularization term<br/> d_W += l2 * next_weights<br/># update the weights in this layer. The learning rate governs how<br/># quickly we descend the gradient<br/> next_weights += -learning_rate * d_W<br/> next_biases += -learning_rate * d_b</pre>
<p>That's our first delta. And now, iteratively, what we're going to do is compute the derivative as that layer's result times the current delta. We start out with the current delta of these probabilities that we just subtracted from. So, now that we've got our gradient, we can compute the derivative of our biases by summing over the columns in the derivative. Now we have the derivative of the biases, and we're going to compute the next delta for the next time we iterate through this. The way we use regularization is by multiplying the regularization by <kbd>next_weights</kbd>. So, <kbd>next_weights</kbd> is the weight's matrix that we will compute the gradient against. We regularize it and add that to the derivative, and then we are going to adjust the weights. So, we can add <kbd>learning_rate</kbd> times the delta, or the gradient, and we do the same for our biases. We've changed <kbd>next_weights</kbd> and <kbd>next_biases</kbd> inside of weights and biases. This is a <kbd>void</kbd> function. It doesn't return anything because it all happened in place.</p>
<p>Now, weights and biases have been iteratively updated. And the next time we progress through the iteration – the next epoch—we should see a lower error. As such, we'll continue this through the number of iterations, progress through all of our epochs, and save our weights and biases. Then, we will produce a prediction and compute those probabilities by doing a forward pass with the softmax at the very end. Take the <kbd>argmax</kbd> of the column: that's the class that has the highest probability. And that's what we return in a squashed vector.</p>
<p class="mce-root"/>
<p>In the <kbd>example_mlp_classifier</kbd> file, we use a similar dataset to what we use in decision tree classification, which are these <kbd>multivariate_normal</kbd> bubbles that are kind of clusters, in our two-dimensional space. We'll do <kbd>train_test_split</kbd> as usual:</p>
<pre># Fit a simple neural network<br/>n_iter = 4<br/>hidden = (10,)<br/>clf = NeuralNetClassifier(X_train, y_train, hidden=hidden, n_iter=n_iter,<br/>                          learning_rate=0.001, random_state=42)<br/>print("Loss per training iteration: %r" % clf.train_loss)<br/><br/>pred = clf.predict(X_test)<br/>clf_accuracy = accuracy_score(y_test, pred)<br/>print("Test accuracy (hidden=%s): %.3f" % (str(hidden), clf_accuracy))<br/><br/># #############################################################################<br/># Fit a more complex neural network<br/>n_iter2 = 150<br/>hidden2 = (25, 25)<br/>clf2 = NeuralNetClassifier(X_train, y_train, hidden=hidden2, n_iter=n_iter2,<br/>                           learning_rate=0.001, random_state=42)<br/><br/>pred2 = clf2.predict(X_test)<br/>clf_accuracy2 = accuracy_score(y_test, pred2)<br/>print("Test accuracy (hidden=%s): %.3f" % (str(hidden2), clf_accuracy2))</pre>
<p>And now we're going to train two neural networks. The first one is only going to use four iterations and have a single hidden layer of 10 units. The second one is a little more complex. We're going to do 150 iterations with two hidden layers of <kbd>25</kbd> units each.</p>
<p>So, we run the <kbd>example_mlp_classifier.py</kbd> file:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-491 image-border" src="assets/ddf90324-3d58-4c30-8efb-ca9745dcad6e.png" style="width:71.33em;height:6.33em;"/></p>
<p>We got a pretty good test accuracy with a single hidden layer of 10 units: 94.4 percent. But you can see that we almost get 100 percent if we have two hidden layers at 25 each. We also have the training iterations for the first one.</p>
<p class="mce-root"/>
<p>You can see in the following graph how the loss kind of jitters around a bit:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-488 image-border" src="assets/64669de4-b82b-4bfd-a530-913e36491b02.png" style="width:44.92em;height:30.42em;"/></p>
<p>But over time, that loss decreases. It's not guaranteed to be a perfect drop and it might jump up or drop down a bit, but we can see that, over time, our loss hits a point where it's very small. This function that we've learned here in the more complex one is a really interesting nonlinear decision boundary. It has a little bit of trouble classifying these border points, but this is how we can use a neural network to learn a function that's much more complex than something that a logistic regression can learn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using transfer learning</h1>
                </header>
            
            <article>
                
<p>In this section, we're going to take it one step further and explore the question of whether a neural network could learn from other neural networks and what they've already learned. We'll start by covering the concept of transfer learning, and then we'll get into some Python code.</p>
<p class="mce-root"/>
<p>Transfer learning is essentially the Frankenstein's monster of machine learning. The idea arose from this question: how can I take what some other network has already learned and go from there? We're basically going to do a brain splice between several different networks. This can be extremely valuable in cases where a network is trained on data that you don't have access to or the training process is the one that would have taken hours or days, as is commonly the case in text or image processing domains.</p>
<p>We don't want to retrain our model because it would take forever, but we want to take what we've already learned about the other two classes and start learning something else about the other class. Rather than retrain the whole thing, we can just use transfer learning to pick up where we left off. So, now that you have the idea and the concept behind it, let's look at how that's going to be applied to the existing multilayer perceptron framework that we're now familiar with.</p>
<p>In the <kbd>transfer.py</kbd> file, starting with <kbd>TransferLearningClassifier</kbd>, there's one more argument than there was in <kbd>MLPClassifier</kbd>: and that's the pretrained network. That can either be <kbd>NeuralNetClassifier</kbd> or <kbd>TransferLearningClassifier</kbd>. But we're just going to take <kbd>NeuralNetClassifier</kbd> for this example. Similar to the MLP constructor, we're going to spend the first few lines saving everything as self attributes, and then we're going to make sure that whatever you've passed in as the pretrained network is going to be some form of <kbd>NeuralMixin</kbd>:</p>
<pre>    def __init__(self, X, y, pretrained, hidden=(25,), n_iter=10,<br/>                 regularization=0.01, learning_rate=0.001, random_state=42):<br/><br/>        # initialize via the NN static method<br/>        self.hidden = hidden<br/>        self.random_state = random_state<br/>        self.n_iter = n_iter<br/>        self.learning_rate = learning_rate<br/>        self.regularization = regularization<br/><br/>        # this is the previous model<br/>        self.model = pretrained<br/><br/>        # assert that it's a neural net or we'll break down later<br/>        assert isinstance(pretrained, NeuralMixin), \<br/>            "Pre-trained model must be a neural network!"<br/><br/>        # initialize weights, biases, etc. for THE TRAINABLE LAYERS ONLY!<br/>        pt_w, pt_b = pretrained.export_weights_and_biases(output_layer=False)<br/>        X, y, weights, biases = NeuralNetClassifier._init_weights_biases(<br/>            X, y, hidden, random_state,<br/><br/>            # use as the last dim the column dimension of the last weights<br/>            # (the ones BEFORE the output layer, that is)<br/>            last_dim=pt_w[-1].shape[1])</pre>
<p>Because we have to have access to the weights and the biases from the previous classes, we get the pretrained weights and the pretrained biases. We only want to initialize the new weights and biases that we can kind of stack on to the end. So, if we have a network of four layers before, those are just going to be ancillary. We're not going to train those<span>—</span>we're just going to freeze them. Then, we want to stack a few layers on the end that we can train and teach new features—new characteristics about the new classes we may want to predict. We're going to do the initialized weights and biases only for the new weights and biases.</p>
<p>Epochs look slightly different; they look a lot like MLPs, but there's a little bit of difference.</p>
<p>So, for each epoch, we're going to perform one pretrained forward step. Basically, all we're going to do here is that for each of those layers in the pretrained weights and biases, we're going to compute <em>AX + b</em> with our <kbd>tanh</kbd> function on it. Notice that even on the output layer, rather than compute a softmax, we're going to compute <kbd>tanh</kbd>, because we're not interested in getting those class probabilities anymore. Now we just want to pipe it into the next layer. So, we're going to use whatever that activation function is. It could be <kbd>sigmoid</kbd> or <kbd>relu</kbd>.</p>
<p>Now we want to take a forward step on the existing or the new weights and bias layers that we do want to train:</p>
<pre>        train_loss = []<br/>        for _ in xrange(n_iter):<br/>            # first, pass the input data through the pre-trained model's<br/>            # hidden layers. Do not pass it through the last layer, however,<br/>            # since we don't want its output from the softmax layer.<br/>            X_transform = _pretrained_forward_step(X, pt_w, pt_b)<br/><br/>            # NOW we complete a forward step on THIS model's<br/>            # untrained weights/biases<br/>            out, layer_results = NeuralNetClassifier._forward_step(<br/>                X_transform, weights, biases)<br/><br/>            # compute the loss on the output<br/>            loss = _calculate_loss(truth=y, preds=out, weights=pt_w + weights,<br/>                                   l2=self.regularization)<br/>            train_loss.append(loss)<br/><br/>            # now back-propagate to correct THIS MODEL's weights and biases via<br/>            # gradient descent. NOTE we do NOT adjust the pre-trained model's<br/>            # weights!!!<br/>            NeuralNetClassifier._back_propagate(<br/>                truth=y, probas=out, layer_results=layer_results,<br/>                weights=weights, biases=biases,<br/>                learning_rate=learning_rate,<br/>                l2=self.regularization)</pre>
<p>We're going to calculate <kbd>loss</kbd>, and then we're going to backpropagate only on the new layers. So, we're not training the old weights and biases at all, but we are doing that to the new ones.</p>
<p>The predictions are slightly different:</p>
<pre>    def predict(self, X):<br/>        # compute the probabilities and then get the argmax for each class<br/>        probas = self.predict_proba(X)<br/><br/>        # we want the argmaxes of each row<br/>        return np.argmax(probas, axis=1)<br/><br/>    def predict_proba(self, X):<br/>        # Compute a forward step with the pre-trained model first:<br/>        pt_w, pt_b = self.model.export_weights_and_biases(output_layer=False)<br/>        X_transform = _pretrained_forward_step(X, pt_w, pt_b)<br/><br/>        # and then complete a forward step with the trained weights and biases<br/>        return NeuralNetClassifier._forward_step(<br/>            X_transform, self.weights, self.biases)[0]<br/><br/>    def export_weights_and_biases(self, output_layer=True):<br/>        pt_weights, pt_biases = \<br/>            self.model.export_weights_and_biases(output_layer=False)<br/>        w = pt_weights + self.weights<br/>        b = pt_biases + self.biases<br/><br/>        if output_layer:<br/>            return w, b<br/>        return w[:-1], b[:-1]</pre>
<p>Rather than just compute that single forward step, we're going to compute the pretrained forward step, again, because we don't want that softmax in the end of the other network. Then, we will compute the normal forward step with the output of the pretrained forward step, which will stack the softmax onto the end.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For the predictions, again, we're taking <kbd>argmax</kbd> of the columns. That is, getting the highest probability class from the predict probabilities.</p>
<p>Let's look at an example file. This is going to look a lot like what we set up in our previous MLP example, except we have two datasets:</p>
<pre># these are the majority classes<br/>n_obs = 1250<br/>x1 = rs.multivariate_normal(mean=[0, 0], cov=covariance, size=n_obs)<br/>x2 = rs.multivariate_normal(mean=[1, 5], cov=covariance, size=n_obs)<br/><br/># this is the minority class<br/>x3 = rs.multivariate_normal(mean=[0.85, 3.25], cov=[[1., .5], [1.25, 0.85]],<br/>                            size=n_obs // 3)<br/><br/># this is what the FIRST network will be trained on<br/>n_first = int(0.8 * n_obs)<br/>X = np.vstack((x1[:n_first], x2[:n_first])).astype(np.float32)<br/>y = np.hstack((np.zeros(n_first), np.ones(n_first))).astype(int)<br/><br/># this is what the SECOND network will be trained on<br/>X2 = np.vstack((x1[n_first:], x2[n_first:], x3)).astype(np.float32)<br/>y2 = np.hstack((np.zeros(n_obs - n_first),<br/>                np.ones(n_obs - n_first),<br/>                np.ones(x3.shape[0]) * 2)).astype(int)</pre>
<p>The first one is going to have those two blobs: the <kbd>multivariate_normal</kbd> blobs that we've been using and the majority class. The third here is going to stack this third class in between the two. Our transfer learning task is going to be learning this new class based on what it's already learned from the binary classification example.</p>
<p>Let's fit the first neural network that we'll use, which is our pretrained network:</p>
<pre># Fit the transfer network - train one more layer with a new class<br/>t_hidden = (15,)<br/>t_iter = 25<br/>transfer = TransferLearningClassifier(X2_train, y2_train, pretrained=clf,<br/>                                      hidden=t_hidden, n_iter=t_iter,<br/>                                      random_state=42)<br/><br/>t_pred = transfer.predict(X2_test)<br/>trans_accuracy = accuracy_score(y2_test, t_pred)<br/>print("Test accuracy (hidden=%s): %.3f" % (str(hidden + t_hidden),<br/>                                           trans_accuracy))</pre>
<p class="mce-root"/>
<p>This is going to be very similar to what we saw in the first example, where we have a two-layer network with <kbd>25</kbd> units in each layer. We're going to fit <kbd>75</kbd> epochs with a pretty low learning rate and we'll see how it does on learning the binary classification task.</p>
<p>Now, let's say we're predicting some type of disease, and there's type one something and type two something. I'm not going to use diabetes because there's only two types. But let's say, a third type comes out. Maybe it's a type of Zika virus, and we want to predict whether this new class is present in a patient who comes in. We don't want to retrain everything, because it's going to take forever, perhaps. So, we're going to just stack this new layer on the end that says learn these new features about this third class. And then we'll produce a new output layer for three classes rather than two. We're only going to do <kbd>25</kbd> new epochs, just based on what we've already learned from the previous binary classification task. We want to see if we can learn this new class without retraining everything. And that's all we're going to do here:</p>
<pre># Fit the transfer network - train one more layer with a new class<br/>t_hidden = (15,)<br/>t_iter = 25<br/>transfer = TransferLearningClassifier(X2_train, y2_train, pretrained=clf,<br/>                                      hidden=t_hidden, n_iter=t_iter,<br/>                                      random_state=42)<br/><br/>t_pred = transfer.predict(X2_test)<br/>trans_accuracy = accuracy_score(y2_test, t_pred)<br/>print("Test accuracy (hidden=%s): %.3f" % (str(hidden + t_hidden),<br/>                                           trans_accuracy))</pre>
<p>And then we're going to plot both out so you can see the decision boundary from both the binary and this three-class classification problem.</p>
<p>Let's run an example of transfer learning:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-395 image-border" src="assets/e8008426-04ac-43df-aef3-d7cc146fa30d.png" style="width:98.42em;height:8.67em;"/></p>
<p>Our test accuracy is down to <kbd>95.2</kbd> percent.</p>
<p class="mce-root"/>
<p>You can see in the following graph that we are able to learn a complex decision boundary in the binary classification task:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-486 image-border" src="assets/c7aee88f-511e-437d-af4c-8807b5852299.png" style="width:81.42em;height:56.50em;"/></p>
<p>And then we took that and we said let's do transfer learning with a new class, and we were still able to learn it really well. So, now we've learned the second decision boundary that we built on top of our initial decision boundary and it looks really good. So, we get 95.2 percent accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Transfer learning is a flexible concept that'll allow you to stack networks together to accomplish far more complex tasks than you thought possible. We covered recommender systems and collaborative filtering in particular, and then we looked at matrix factorization techniques and how to supplement your recommenders with content-based similarities. Lastly, we worked with neural networks and transfer learning.</p>


            </article>

            
        </section>
    </body></html>
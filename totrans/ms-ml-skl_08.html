<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;The Perceptron"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. The Perceptron</h1></div></div></div><p>In previous chapters we discussed generalized linear models that relate a linear combination of explanatory variables and model parameters to a response variable using a link function. In this chapter, we will discuss another linear model called the perceptron. The perceptron is a binary classifier that can learn from individual training instances, which can be useful for training from large datasets. More importantly, the perceptron and its limitations inspire the models that we will discuss in the final chapters.</p><p>Invented by Frank Rosenblatt at the Cornell Aeronautical Laboratory in the late 1950's, the development of the perceptron was originally motivated by efforts to simulate the human brain. A brain is composed of <a class="indexterm" id="id433"/>cells called <span class="strong"><strong>neurons</strong></span> that process information and connections <a class="indexterm" id="id434"/>between neurons called <span class="strong"><strong>synapses</strong></span> through which information is transmitted. It is estimated that human brain is composed of as many as 100 billion neurons and 100 trillion synapses. As shown in the following image, the main components of a neuron are dendrites, a body, and an axon. The dendrites receive electrical signals from other neurons. The signals are processed in the neuron's body, which then sends a signal through the axon to another neuron.</p><div class="mediaobject"><img alt="The Perceptron" src="graphics/8365OS_08_01.jpg"/></div><p>An individual neuron can be thought of as a computational unit that processes one or more inputs to produce an output. A perceptron functions analogously to a neuron; it accepts one or more inputs, processes them, and returns an output. It may seem that a model of just one of the hundreds of billions of neurons in the human brain will be of limited use. To an extent that is true; the perceptron cannot approximate some basic functions. However, we will still <a class="indexterm" id="id435"/>discuss perceptrons for two reasons. First, perceptrons are capable of online, error-driven learning; the learning algorithm can update the model's parameters using a single training instance rather than the entire batch of training instances. Online <a class="indexterm" id="id436"/>learning is useful for learning from training sets that are too large to be represented in memory. Second, understanding how the perceptron works is necessary to understand some of the more powerful models that we will discuss in subsequent chapters, including support vector machines and artificial neural networks. Perceptrons are commonly visualized using a diagram like the following one:</p><div class="mediaobject"><img alt="The Perceptron" src="graphics/8365OS_08_02.jpg"/></div><p>The circles labeled <span class="inlinemediaobject"><img alt="The Perceptron" src="graphics/8365OS_08_18.jpg"/></span>, <span class="inlinemediaobject"><img alt="The Perceptron" src="graphics/8365OS_08_19.jpg"/></span>, and <span class="inlinemediaobject"><img alt="The Perceptron" src="graphics/8365OS_08_20.jpg"/></span> are inputs units. Each input unit represents one feature. Perceptrons frequently use an additional input unit that represents a constant bias term, but this input unit is usually omitted from diagrams. The circle in the center is a computational unit or the neuron's body. The edges connecting the input units to the <a class="indexterm" id="id437"/>computational unit are analogous to dendrites. Each edge is <span class="strong"><strong>weighted</strong></span>, or associated <a class="indexterm" id="id438"/>with a parameter. The parameters can be interpreted easily; an explanatory variable that is correlated with the positive class will have a positive weight, and an explanatory variable that is correlated with the negative class will have a negative weight. The edge directed away from the computational unit returns the output and can be thought of as the axon.</p><div class="section" title="Activation functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec57"/>Activation functions</h1></div></div></div><p>The perceptron classifies instances by processing a linear combination of the explanatory variables and the model parameters <a class="indexterm" id="id439"/>using an <span class="strong"><strong>activation function</strong></span> as shown in the following equation. The linear combination of the parameters and inputs is sometimes <a class="indexterm" id="id440"/>called the perceptron's <span class="strong"><strong>preactivation</strong></span>.</p><div class="mediaobject"><img alt="Activation functions" src="graphics/8365OS_08_03.jpg"/></div><p>Here, <span class="inlinemediaobject"><img alt="Activation functions" src="graphics/8365OS_08_21.jpg"/></span> are the model's parameters, <span class="inlinemediaobject"><img alt="Activation functions" src="graphics/8365OS_08_22.jpg"/></span> is a constant bias term, and <span class="inlinemediaobject"><img alt="Activation functions" src="graphics/8365OS_08_23.jpg"/></span> is the activation function. Several different activation functions are commonly used. Rosenblatt's original perceptron used the <a class="indexterm" id="id441"/>
<span class="strong"><strong>Heaviside step</strong></span> function. Also called the unit step <a class="indexterm" id="id442"/>function, the Heaviside step function is shown in the following equation, where <span class="inlinemediaobject"><img alt="Activation functions" src="graphics/8365OS_08_24.jpg"/></span> is the weighted combination of the features:</p><div class="mediaobject"><img alt="Activation functions" src="graphics/8365OS_08_04.jpg"/></div><p>If the weighted sum of the explanatory variables and the bias term is greater than zero, the activation function returns one and the perceptron predicts that the instance is the positive class. Otherwise, the function returns zero and the perceptron predicts that the instance is the negative class. The Heaviside step activation function is plotted in the following figure:</p><div class="mediaobject"><img alt="Activation functions" src="graphics/8365OS_08_05.jpg"/></div><p>Another <a class="indexterm" id="id443"/>common activation function is the <span class="strong"><strong>logistic sigmoid</strong></span> activation function. The gradients for this activation <a class="indexterm" id="id444"/>function can be calculated efficiently, which will be important in later chapters when we construct artificial neural networks. The logistic sigmoid activation function is given by the following equation, where <span class="inlinemediaobject"><img alt="Activation functions" src="graphics/8365OS_08_24.jpg"/></span> is the sum of the weighted inputs:</p><div class="mediaobject"><img alt="Activation functions" src="graphics/8365OS_08_06.jpg"/></div><p>This model should seem familiar; it is a linear combination of the values of the explanatory variables and the model parameters processed through the logistic function. That is, this is identical to the model for logistic regression. While a perceptron with a logistic sigmoid activation function has the same model as logistic regression, it learns its parameters differently.</p><div class="section" title="The perceptron learning algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec33"/>The perceptron learning algorithm</h2></div></div></div><p>The perceptron <a class="indexterm" id="id445"/>learning algorithm begins by setting the weights to zero or to small random values. It then predicts the class for a training instance. The <a class="indexterm" id="id446"/>perceptron is an <span class="strong"><strong>error-driven</strong></span> <a class="indexterm" id="id447"/>learning algorithm; if the prediction is correct, the algorithm continues to the next instance. If the prediction is incorrect, the algorithm <a class="indexterm" id="id448"/>updates the weights. More formally, the update rule is given by the following:</p><div class="mediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_07.jpg"/></div><p>For each training instance, the value of the parameter for each explanatory variable is incremented by <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_25.jpg"/></span>, where <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_26.jpg"/></span> is the true class for instance <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_27.jpg"/></span>, <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_28.jpg"/></span> is the predicted class for instance <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_27.jpg"/></span>, <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_29.jpg"/></span> is the value of the <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_30.jpg"/></span> explanatory variable for instance <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_27.jpg"/></span>, and <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_31.jpg"/></span> is a hyperparameter that controls the learning rate. If the prediction is correct, <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_32.jpg"/></span> equals zero, and the <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_25.jpg"/></span> term equals zero. So, if the prediction is correct, the weight is not updated. If the prediction is incorrect, the weight is incremented by the product of the learning rate, <span class="inlinemediaobject"><img alt="The perceptron learning algorithm" src="graphics/8365OS_08_32.jpg"/></span>, and the value of the feature.</p><p>This update rule is similar to the update rule for gradient descent in that the weights are adjusted towards classifying the instance correctly and the size of the update is controlled by a learning rate. Each pass through the training <a class="indexterm" id="id449"/>instances is called an <span class="strong"><strong>epoch. </strong></span>The learning algorithm has converged when it completes an epoch without misclassifying any of the instances. The learning algorithm is not guaranteed to converge; later in this chapter, we will <a class="indexterm" id="id450"/>discuss linearly inseparable datasets for which convergence is impossible. For this reason, the learning algorithm also requires a <a class="indexterm" id="id451"/>hyperparameter that specifies the maximum number of epochs that can be completed before the algorithm terminates.</p></div></div></div>
<div class="section" title="Binary classification with the perceptron"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec58"/>Binary classification with the perceptron</h1></div></div></div><p>Let's work through a <a class="indexterm" id="id452"/>toy classification problem. Suppose <a class="indexterm" id="id453"/>that you wish to separate adult cats from kittens. Only two explanatory variables are available in your dataset: the proportion of the day that the animal was asleep and the proportion of the day that the animal was grumpy. Our training data consists of the following four instances:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Instance</p>
</th><th style="text-align: left" valign="bottom">
<p>Proportion of the day spent sleeping</p>
</th><th style="text-align: left" valign="bottom">
<p>Proportion of the day spent being grumpy</p>
</th><th style="text-align: left" valign="bottom">
<p>Kitten or Adult?</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0.2</p>
</td><td style="text-align: left" valign="top">
<p>0.1</p>
</td><td style="text-align: left" valign="top">
<p>Kitten</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>0.4</p>
</td><td style="text-align: left" valign="top">
<p>0.6</p>
</td><td style="text-align: left" valign="top">
<p>Kitten</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>0.5</p>
</td><td style="text-align: left" valign="top">
<p>0.2</p>
</td><td style="text-align: left" valign="top">
<p>Kitten</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>0.7</p>
</td><td style="text-align: left" valign="top">
<p>0.9</p>
</td><td style="text-align: left" valign="top">
<p>Adult</p>
</td></tr></tbody></table></div><p>The following scatter plot of the instances confirms that they are linearly separable:</p><div class="mediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_10.jpg"/></div><p>Our goal is to train a perceptron that can classify animals using the two real-valued explanatory variables. We will represent kittens with the positive class and adult cats with the negative class. The preceding network diagram describes the perceptron that we will train.</p><p>Our perceptron has three input units. <span class="inlinemediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_34.jpg"/></span> is the input unit for the bias term. <span class="inlinemediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_35.jpg"/></span> and <span class="inlinemediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_36.jpg"/></span> are input units for the two features. Our perceptron's computational unit uses a Heaviside activation function. In this example, we will set the maximum number of training epochs to ten; if the algorithm does not converge within 10 epochs, it will stop and return the current values of the weights. For simplicity, we will set the learning rate to one. Initially, we will <a class="indexterm" id="id454"/>set all of the weights to zero. Let's <a class="indexterm" id="id455"/>examine the first training epoch, which is shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Epoch 1</p>
</th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Instance</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Initial Weights</strong></span>
</p>
<p>
<span class="strong"><strong>x</strong></span>
</p>
<p>
<span class="strong"><strong>Activation</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Prediction, Target</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Correct</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Updated weights</strong></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0, 0, 0;</p>
<p>1.0, 0.2, 0.1;</p>
<p>1.0*0 + 0.2*0 + 0.1*0 = 0.0;</p>
</td><td style="text-align: left" valign="top">
<p>0, 1</p>
</td><td style="text-align: left" valign="top">
<p>False</p>
</td><td style="text-align: left" valign="top">
<p>1.0, 0.2, 0.1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1.0, 0.2, 0.1;</p>
<p>1.0, 0.4, 0.6;</p>
<p>1.0*1.0 + 0.4*0.2 + 0.6*0.1 = 1.14;</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>1.0, 0.2, 0.1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1.0, 0.2, 0.1;</p>
<p>1.0, 0.5, 0.2;</p>
<p>1.0*1.0 + 0.5*0.2 + 0.2*0.1 = 1.12;</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>1.0, 0.2, 0.1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>1.0, 0.2, 0.1;</p>
<p>1.0, 0.7, 0.9;</p>
<p>1.0*1.0 + 0.7*0.2 + 0.9*0.1 = 1.23;</p>
</td><td style="text-align: left" valign="top">
<p>1, 0</p>
</td><td style="text-align: left" valign="top">
<p>False</p>
</td><td style="text-align: left" valign="top">
<p>0, -0.5, -0.8</p>
</td></tr></tbody></table></div><p>Initially, all of the weights are equal to zero. The weighted sum of the explanatory variables for the first instance is zero, the activation function outputs zero, and the perceptron incorrectly predicts that the kitten is an adult cat. As the prediction was incorrect, we update the weights according to the update rule. We increment each of the weights by the product of the learning rate, the difference between the true and predicted labels and the value of the corresponding feature.</p><p>We then continue to the second training instance and calculate the weighted sum of its features using the updated weights. This sum equals 1.14, so the activation function outputs one. This prediction is correct, so we continue to the third training instance without updating the weights. The prediction for the third instance is also correct, so we continue to the <a class="indexterm" id="id456"/>fourth training instance. The weighted sum of the features for the fourth instance is 1.23. The activation function outputs one, incorrectly predicting that this adult cat is a kitten. Since this prediction is incorrect, we increment <a class="indexterm" id="id457"/>each weight by the product of the learning rate, the difference between the true and predicted labels, and its corresponding feature. We completed the first epoch by classifying all of the instances in the training set. The perceptron did not converge; it classified half of the training instances incorrectly. The following figure depicts the decision boundary after the first epoch:</p><div class="mediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_11.jpg"/></div><p>Note that the decision boundary moved throughout the epoch; the decision boundary formed by the weights at the <a class="indexterm" id="id458"/>end of the epoch would not <a class="indexterm" id="id459"/>necessarily have produced the same predictions seen earlier in the epoch. Since we have not exceeded the maximum number of training epochs, we will iterate through the instances again. The second training epoch is shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Epoch 2</p>
</th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Instance</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Initial Weights</strong></span>
</p>
<p>
<span class="strong"><strong>x</strong></span>
</p>
<p>
<span class="strong"><strong>Activation</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Prediction, Target</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Correct</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Updated weights</strong></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0, -0.5, -0.8</p>
<p>1.0, 0.2, 0.1</p>
<p>1.0*0 + 0.2*-0.5 + 0.1*-0.8 = -0.18</p>
</td><td style="text-align: left" valign="top">
<p>0, 1</p>
</td><td style="text-align: left" valign="top">
<p>False</p>
</td><td style="text-align: left" valign="top">
<p>1, -0.3, -0.7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1, -0.3, -0.7</p>
<p>1.0, 0.4, 0.6</p>
<p>1.0*1.0 + 0.4*-0.3 + 0.6*-0.7 = 0.46</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>1, -0.3, -0.7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1, -0.3, -0.7</p>
<p>1.0, 0.5, 0.2</p>
<p>1.0*1.0 + 0.5*-0.3 + 0.2*-0.7 = 0.71</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>1, -0.3, -0.7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>1, -0.3, -0.7</p>
<p>1.0, 0.7, 0.9</p>
<p>1.0*1.0 + 0.7*-0.3 + 0.9*-0.7 = 0.16</p>
</td><td style="text-align: left" valign="top">
<p>1, 0</p>
</td><td style="text-align: left" valign="top">
<p>False</p>
</td><td style="text-align: left" valign="top">
<p>0, -1, -1.6</p>
</td></tr></tbody></table></div><p>The second epoch begins using the values of the weights from the first epoch. Two training instances are <a class="indexterm" id="id460"/>classified incorrectly during this epoch. The <a class="indexterm" id="id461"/>weights are updated twice, but the decision boundary at the end of the second epoch is similar the decision boundary at the end of the first epoch.</p><div class="mediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_12.jpg"/></div><p>The algorithm failed to <a class="indexterm" id="id462"/>converge during this epoch, so we will continue training. The following table describes the third training epoch:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Epoch 3</p>
</th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Instance</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Initial Weights</strong></span>
</p>
<p>
<span class="strong"><strong>x</strong></span>
</p>
<p>
<span class="strong"><strong>Activation</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Prediction, Target</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Correct</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Updated Weights</strong></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0, -1, -1.6</p>
<p>1.0, 0.2, 0.1</p>
<p>1.0*0 + 0.2*-1.0 + 0.1*-1.6 = -0.36</p>
</td><td style="text-align: left" valign="top">
<p>0, 1</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">False</code>
</p>
</td><td style="text-align: left" valign="top">
<p>1,-0.8, -1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1,-0.8, -1.5</p>
<p>1.0, 0.4, 0.6</p>
<p>1.0*1.0 + 0.4*-0.8 + 0.6*-1.5 = -0.22</p>
</td><td style="text-align: left" valign="top">
<p>0, 1</p>
</td><td style="text-align: left" valign="top">
<p>False</p>
</td><td style="text-align: left" valign="top">
<p>2, -0.4, -0.9</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2, -0.4, -0.9</p>
<p>1.0, 0.5, 0.2</p>
<p>1.0*2.0 + 0.5*-0.4 + 0.2*-0.9 = 1.62</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>2, -0.4, -0.9</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>2, -0.4, -0.9</p>
<p>1.0, 0.7, 0.9</p>
<p>1.0*2.0 + 0.7*-0.4 + 0.9*-0.9 = 0.91</p>
</td><td style="text-align: left" valign="top">
<p>1, 0</p>
</td><td style="text-align: left" valign="top">
<p>False</p>
</td><td style="text-align: left" valign="top">
<p>1, -1.1, -1.8</p>
</td></tr></tbody></table></div><p>The perceptron <a class="indexterm" id="id463"/>classified more instances incorrectly <a class="indexterm" id="id464"/>during this epoch than during previous epochs. The following figure depicts the decision boundary at the end of the third epoch:</p><div class="mediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_13.jpg"/></div><p>The perceptron continues to update its weights throughout the fourth and fifth training epochs, and it continues to <a class="indexterm" id="id465"/>classify training instances <a class="indexterm" id="id466"/>incorrectly. During the sixth epoch the perceptron classified all of the instances correctly; it converged on a set of weights that separates the two classes. The following table describes the sixth training epoch:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Epoch 6</p>
</th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom"> </th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Instance</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Initial Weights</strong></span>
</p>
<p>
<span class="strong"><strong>x</strong></span>
</p>
<p>
<span class="strong"><strong>Activation</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Prediction, Target</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Correct</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Updated weights</strong></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
<p>1.0, 0.2, 0.1</p>
<p>1.0*2 + 0.2*-1 + 0.1*-1.5 = 1.65</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
<p>1.0, 0.4, 0.6</p>
<p>1.0*2 + 0.4*-1 + 0.6*-1.5 = 0.70</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
<p>1.0, 0.5, 0.2</p>
<p>1.0*2 + 0.5*-1 + 0.2*-1.5 = 1.2</p>
</td><td style="text-align: left" valign="top">
<p>1, 1</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
<p>1.0, 0.7, 0.9</p>
<p>1.0*2 + 0.7*-1 + 0.9*-1.5 = -0.05</p>
</td><td style="text-align: left" valign="top">
<p>0, 0</p>
</td><td style="text-align: left" valign="top">
<p>True</p>
</td><td style="text-align: left" valign="top">
<p>2, -1, -1.5</p>
</td></tr></tbody></table></div><p>The decision boundary at the <a class="indexterm" id="id467"/>end of the sixth training <a class="indexterm" id="id468"/>epoch is shown in the following figure:</p><div class="mediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_16.jpg"/></div><p>The following figure shows <a class="indexterm" id="id469"/>the decision boundary <a class="indexterm" id="id470"/>throughout all the training epochs.</p><div class="mediaobject"><img alt="Binary classification with the perceptron" src="graphics/8365OS_08_17.jpg"/></div><div class="section" title="Document classification with the perceptron"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec34"/>Document classification with the perceptron</h2></div></div></div><p>scikit-learn <a class="indexterm" id="id471"/>provides an implementation of the perceptron. As with the other implementations that we used, the constructor for the <code class="literal">Perceptron</code> class accepts keyword arguments that set the algorithm's hyperparameters. <code class="literal">Perceptron</code> similarly exposes the <code class="literal">fit_transform()</code> and <code class="literal">predict()</code> methods. <code class="literal">Perceptron</code> also provides a <code class="literal">partial_fit()</code> method, which allows the classifier to train and make predictions for streaming data.</p><p>In this <a class="indexterm" id="id472"/>example, we train a perceptron to classify documents from the 20 newsgroups dataset. The dataset consists of approximately 20,000 documents sampled from 20 Usenet newsgroups. The dataset is commonly used in document classification and clustering experiments; scikit-learn provides a convenience function to download and read the dataset. We will train a perceptron to classify documents from three newsgroups: <code class="literal">rec.sports.hockey</code>, <code class="literal">rec.sports.baseball</code>, and <code class="literal">rec.auto</code>. scikit-learn's <code class="literal">Perceptron</code> natively supports multiclass classification; it will use the one versus all strategy to train a classifier for each of the classes in the training data. We will represent the documents as TF-IDF-weighted bags of words. The <a class="indexterm" id="id473"/>
<code class="literal">partial_fit()</code> method could be used in conjunction with <code class="literal">HashingVectorizer</code> to train from large or streaming data in a memory-constrained setting:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.datasets import fetch_20newsgroups
&gt;&gt;&gt; from sklearn.metrics.metrics import f1_score, classification_report
&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; from sklearn.linear_model import Perceptron

&gt;&gt;&gt; categories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']
&gt;&gt;&gt; newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))
&gt;&gt;&gt; newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))

&gt;&gt;&gt; vectorizer = TfidfVectorizer()
&gt;&gt;&gt; X_train = vectorizer.fit_transform(newsgroups_train.data)
&gt;&gt;&gt; X_test = vectorizer.transform(newsgroups_test.data)

&gt;&gt;&gt; classifier = Perceptron(n_iter=100, eta0=0.1)
&gt;&gt;&gt; classifier.fit_transform(X_train, newsgroups_train.target )
&gt;&gt;&gt; predictions = classifier.predict(X_test)
&gt;&gt;&gt; print classification_report(newsgroups_test.target, predictions)</pre></div><p>The following <a class="indexterm" id="id474"/>is the output of the script:</p><div class="informalexample"><pre class="programlisting">             precision    recall  f1-score   support

          0       0.89      0.87      0.88       396
          1       0.87      0.78      0.82       397
          2       0.79      0.88      0.83       399

avg / total       0.85      0.85      0.85      1192</pre></div><p>First, we download and read the dataset using the <code class="literal">fetch_20newsgroups()</code> function. Consistent with <a class="indexterm" id="id475"/>other built-in datasets, the function returns an object with <code class="literal">data</code>, <code class="literal">target</code>, and <code class="literal">target_names</code> fields. We also specify that the documents' headers, footers, and quotes should be removed. Each of the newsgroups used different conventions in the headers and footers; retaining these explanatory variables makes classifying the documents artificially easy. We produce TF-IDF vectors using <code class="literal">TfifdVectorizer</code>, train the perceptron, and evaluate it on the test set. Without hyperparameter optimization, the perceptron's average precision, recall, and F1 score are 0.85.</p></div></div>
<div class="section" title="Limitations of the perceptron"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec59"/>Limitations of the perceptron</h1></div></div></div><p>While the <a class="indexterm" id="id476"/>perceptron classified the instances in our example well, the model has limitations. Linear models like the perceptron with a Heaviside activation <a class="indexterm" id="id477"/>function are not <span class="strong"><strong>universal function approximators</strong></span>; they cannot represent some functions. Specifically, linear models can only learn to approximate the functions for <span class="strong"><strong>linearly separable</strong></span> datasets. The linear classifiers that we <a class="indexterm" id="id478"/>have examined find a hyperplane that separates the positive classes from the negative classes; if no hyperplane exists that can separate the classes, the problem is not linearly separable.</p><p>A simple example of a function that is linearly inseparable is the logical operation <span class="strong"><strong>XOR</strong></span>, or exclusive disjunction. The <a class="indexterm" id="id479"/>output of XOR is one when one of its inputs is equal to one and the other is equal to zero. The inputs and outputs of XOR are plotted in two dimensions in the following graph. When XOR outputs <span class="strong"><strong>1</strong></span>, the instance is marked with a circle; when XOR outputs <span class="strong"><strong>0</strong></span>, the instance is marked with a diamond, as shown in the following figure:</p><div class="mediaobject"><img alt="Limitations of the perceptron" src="graphics/8365OS_08_09.jpg"/></div><p>It is impossible to separate the circles from the diamonds using a single straight line. Suppose that the instances are pegs on a board. If you were to stretch a rubber band around both of the positive instances, and stretch a second rubber band around both of the negative instances, the bands would intersect in the middle of the board. The rubber bands represent <span class="strong"><strong>convex </strong></span>
<a class="indexterm" id="id480"/>
<span class="strong"><strong>hulls</strong></span>, or the envelope that contains all of the points within the set and all of the points along any line connecting a pair points within the set. Feature representations are more likely to be linearly separable in higher dimensional spaces <a class="indexterm" id="id481"/>than lower dimensional spaces. For instance, text classification problems tend to be linearly separable when high-dimensional representations like the bag-of-words are used.</p><p>In the next two chapters, we will discuss techniques that can be used to model linearly inseparable data. The <a class="indexterm" id="id482"/>first technique, called <span class="strong"><strong>kernelization</strong></span>, projects linearly inseparable data to a higher dimensional space in which it is linearly separable. Kernelization can be used in many models, including perceptrons, but it is particularly associated with support vector machines, which we will discuss in the next chapter. Support vector machines also support techniques that can find the hyperplane that separates linearly inseparable classes with the fewest errors. The second technique creates a directed graph of perceptrons. The <a class="indexterm" id="id483"/>resulting model, called an <span class="strong"><strong>artificial neural network</strong></span>, is a universal function approximator; we will discuss artificial neural networks in <a class="link" href="ch10.html" title="Chapter 10. From the Perceptron to Artificial Neural Networks">Chapter 10</a>, <span class="emphasis"><em>From the Perceptron to Artificial Neural Networks</em></span>.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec60"/>Summary</h1></div></div></div><p>In this chapter, we discussed the perceptron. Inspired by neurons, the perceptron is a linear model for binary classification. The perceptron classifies instances by processing a linear combination of the explanatory variables and weights with an activation function. While a perceptron with a logistic sigmoid activation function is the same model as logistic regression, the perceptron learns its weights using an online, error-driven algorithm. The perceptron can be used effectively in some problems. Like the other linear classifiers that we have discussed, the perceptron is not a universal function approximator; it can only separate the instances of one class from the instances of the other using a hyperplane. Some datasets are not linearly separable; that is, no possible hyperplane can classify all of the instances correctly. In the following chapters, we will discuss two models that can be used with linearly inseparable data: the artificial neural network, which creates a universal function approximator from a graph of perceptrons and the support vector machine, which projects the data onto a higher dimensional space in which it is linearly separable.</p></div></body></html>
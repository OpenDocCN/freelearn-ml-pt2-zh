<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Support Vector Machines</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will cover these recipes:</p>
<ul>
<li>Classifying data with a linear SVM</li>
<li>Optimizing an SVM</li>
<li>Multiclass classification with SVM</li>
<li>Support vector regression</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will start by using a <strong>support vector machine</strong> (<strong>SVM</strong>) with a linear kernel to get a rough idea of how SVMs work. They create a hyperplane, or linear surface in several dimensions, which best separates the data.</p>
<p>In two dimensions, this is easy to see: the hyperplane is a line that separates the data. We will see the array of coefficients and intercept of the SVM. Together they <span>uniquely </span><span>describe a scikit-learn linear SVC predictor.</span></p>
<p>In the rest of the chapter, the SVMs have a <strong>radial basis function</strong> (<strong>RBF</strong>) kernel. They are nonlinear, but with smooth separating surfaces. In practice, SVMs work well with many datasets and thus are an integral part of the <kbd>scikit-learn</kbd> library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying data with a linear SVM</h1>
                </header>
            
            <article>
                
<p>In the first chapter, we saw some examples of classification with SVMs. We focused on SVMs' slightly superior classification performance compared to logistic regression, but for the most part, we left SVMs alone.</p>
<p>Here, we will focus on them more closely. While SVMs do not have an easy probabilistic interpretation, they do have an easy visual-geometric one. The main idea behind linear SVMs is to separate two classes with the best possible plane.</p>
<p>Let's linearly separate two classes with an SVM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Let us start by loading and visualizing the iris dataset available in scikit-learn:</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Load the data</h1>
                </header>
            
            <article>
                
<p>Load part of the iris dataset. This will allow for easy comparison with the first chapter:</p>
<pre><strong>#load the libraries we have been using</strong><br/><strong>import numpy as np      </strong><br/><strong>import pandas as pd     </strong><br/><strong>import matplotlib.pyplot as plt  #Library for visualization</strong><br/><br/><strong>from sklearn import datasets</strong><br/><br/><strong>iris = datasets.load_iris()</strong><br/><strong>X_w = iris.data[:, :2]  #load the first two features of the iris data </strong><br/><strong>y_w = iris.target         #load the target of the iris data</strong></pre>
<p>Now, we will use a NumPy mask to focus on the first two classes:</p>
<pre><strong>#select only the first two classes for both the feature set and target set</strong><br/><strong>#the first two classes of the iris dataset: Setosa (0), Versicolour (1)</strong><br/><br/><strong>X = X_w[y_w &lt; 2]   </strong><br/><strong>y = y_w[y_w &lt; 2]</strong><br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualize the two classes</h1>
                </header>
            
            <article>
                
<p>Plot the classes <kbd>0</kbd> and <kbd>1</kbd> with matplotlib. Recall that the notation <kbd>X_0[:,0]</kbd> refers to the first column of a NumPy array.</p>
<p>In the following code, <kbd>X_0</kbd> refers to the subset of inputs <kbd>X</kbd> that correspond to the target <kbd>y</kbd> being <kbd>0</kbd> and <kbd>X_1</kbd> is a subset with a matching target value of <kbd>1</kbd>:</p>
<pre><strong>X_0 = X[y == 0]</strong><br/><strong>X_1 = X[y == 1]</strong><br/><br/><strong>#to visualize within IPython:</strong><br/><strong>%matplotlib inline </strong><br/><strong>plt.figure(figsize=(10,7)) #change figure-size for easier viewing</strong><br/><strong>plt.scatter(X_0[:,0],X_0[:,1], color = 'red')</strong><br/><strong>plt.scatter(X_1[:,0],X_1[:,1], color = 'blue')</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e3e10b08-ae0c-441c-a69c-3b72d5ab0389.png"/></div>
<p>From the graph, it is clear that we could find a straight line to separate these two classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The process of finding the SVM line is straightforward. It is the same process as with any scikit-learn supervised learning estimator:</p>
<ol>
<li>Create training and testing sets.</li>
<li>Create an SVM model instance.</li>
<li>Fit the SVM model to the loaded data.</li>
<li>Predict with the SVM model and measure the performance of the model in preparation for predictions of unseen data.</li>
</ol>
<p>Let's begin:</p>
<ol>
<li>Split the dataset of the first two features of the first two classes. Stratify the target set:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)</strong></pre>
<ol start="2">
<li>Create an SVM model instance. Set the kernel to be linear, as we want a line to separate the two classes that are involved in this example:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.svm import SVC</strong><br/><br/><strong>svm_inst = SVC(kernel='linear')</strong></pre>
<ol start="3">
<li>Fit the model (train the model):</li>
</ol>
<pre style="padding-left: 60px"><strong>svm_inst.fit(X_train,y_train)</strong></pre>
<ol start="4">
<li>Predict using the test set:</li>
</ol>
<pre style="padding-left: 60px"><strong>y_pred = svm_inst.predict(X_test)</strong></pre>
<ol start="5">
<li>Measure the performance of the SVM on the test set:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.metrics import accuracy_score</strong><br/><br/><strong>accuracy_score(y_test, y_pred) </strong><br/><br/><strong>1.0</strong></pre>
<p style="padding-left: 60px">It did perfectly on the test set. This is not surprising, because when we visualized each class, they were easy to visually separate.</p>
<ol start="6">
<li>Visualize the decision boundary, the line separating the classes, by using the estimator on a two-dimensional grid:</li>
</ol>
<pre style="padding-left: 60px"><strong>from itertools import product</strong><br/><br/><strong>#Minima and maxima of both features</strong><br/><strong>xmin, xmax = np.percentile(X[:, 0], [0, 100])</strong><br/><strong>ymin, ymax = np.percentile(X[:, 1], [0, 100])</strong><br/><br/><strong>#Grid/Cartesian product with itertools.product</strong><br/><strong>test_points = np.array([[xx, yy] for xx, yy in product(np.linspace(xmin, xmax), np.linspace(ymin, ymax))])</strong><br/><br/><strong>#Predictions on the grid</strong><br/><strong>test_preds = svm_inst.predict(test_points)</strong></pre>
<ol start="7">
<li>Plot the grid by coloring the predictions. Note that we have amended the previous visualization to include SVM predictions:</li>
</ol>
<pre style="padding-left: 60px"><strong>X_0 = X[y == 0]</strong><br/><strong>X_1 = X[y == 1]</strong><br/><br/><strong>%matplotlib inline</strong><br/><strong>plt.figure(figsize=(10,7))   #change figure-size for easier viewing</strong><br/><strong>plt.scatter(X_0[:,0],X_0[:,1], color = 'red')</strong><br/><strong>plt.scatter(X_1[:,0],X_1[:,1], color = 'blue')</strong><br/><br/><strong>colors = np.array(['r', 'b'])</strong><br/><strong>plt.scatter(test_points[:, 0], test_points[:, 1], color=colors[test_preds], alpha=0.25)</strong><br/><strong>plt.scatter(X[:, 0], X[:, 1], color=colors[y])</strong><br/><strong>plt.title("Linearly-separated classes")</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img height="325" width="444" src="assets/fe389784-e987-4481-9a99-a48e372f59b2.png"/></div>
<p>We fleshed out the SVM linear decision boundary by predicting on a two-dimensional grid.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>At times, it could be computationally expensive to predict on a whole grid of points, especially if the SVM is predicting many classes in many dimensions. In these cases, you will need access to the geometric information of the SVM decision boundary.</p>
<p>A linear decision boundary, a hyperplane, is uniquely specified by a vector normal to the hyperplane and an intercept. The normal vectors are contained in the SVM instance's <kbd>coef_ data</kbd> attribute. The intercepts are contained in the SVM instance's <kbd>intercept_ data</kbd> attribute. View these two attributes:</p>
<pre><strong>svm_inst.coef_</strong><br/><br/><strong>array([[ 2.22246001, -2.2213921 ]])</strong><br/><br/><strong>svm_inst.intercept_</strong><br/><br/><strong>array([-5.00384439])</strong></pre>
<p>You might be able to quickly see that the <kbd>coef_[0]</kbd> vector is perpendicular to the line we drew to separate both of the iris classes we have been viewing.</p>
<p>Every time, these two NumPy arrays, <kbd>svm_inst.coef_</kbd> and <kbd>svm_inst.intercept_</kbd>, will have the same number of rows. Each row corresponds to each plane separating the classes involved. In the example, there are two classes linearly separated by one hyperplane. The particular SVM type, SVC in this case, implements a one-versus-one classifier: it will draw a unique plane separating every pair of classes involved.</p>
<p>If we were trying to separate three classes, there would be three possible combinations, 3 x 2/2 = 3. For <em>n</em> classes, the number of planes provided by SVC is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="475" class="fm-editor-equation" src="assets/85dc7ab5-c651-4f01-8127-93b16db2445c.png"/></div>
<p>The number of columns in the <kbd>coef_ data</kbd> attribute is the number of features in the data, which in this case is two.</p>
<p>To find the decision in regards to a point in space, solve the following equation for zero:</p>
<div class="CDPAlignCenter CDPAlign"><img height="290" width="405" src="assets/a977e6ac-9fac-498b-8618-c28efc373378.png"/></div>
<p>If you only desire the uniqueness of the plane, store the tuple <kbd>(coef_, intercept_)</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Additionally, you can view the the parameters of the instance to learn more about it:</p>
<pre><strong>svm_inst</strong><br/><br/><strong>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</strong><br/><strong>  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',</strong><br/><strong>  max_iter=-1, probability=False, random_state=None, shrinking=True,</strong><br/><strong>  tol=0.001, verbose=False)</strong></pre>
<p>Traditionally, the SVC prediction performance is optimized over the following parameters: C, gamma, and the shape of the kernel. C describes the margin of the SVM and is set to one by default. The margin is the empty space on either side of the hyperplane with no class examples. If your dataset has many noisy observations, try higher Cs with cross-validation. C is proportional to error on the margin, and as C gets higher in value, the SVM will try to make the margin smaller.</p>
<p>A final note on SVMs is that we could re-scale the data and test that scaling with cross-validation. Conveniently, the iris dataset has units of cms for all of the inputs so re-scaling is not necessary but for an arbitrary dataset you should look into it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing an SVM</h1>
                </header>
            
            <article>
                
<p>For this example we will continue with the iris dataset, but will use two classes that are harder to tell apart, the Versicolour and Virginica iris species.</p>
<p>In this section we will focus on the following:</p>
<ul>
<li><strong>Setting up a scikit-learn pipeline</strong>: A chain of transformations with a predictive model at the end</li>
<li><strong>A grid search</strong>: A performance scan of several versions of SVMs with varying parameters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Load two classes and two features of the iris dataset:</p>
<pre><strong>#load the libraries we have been using</strong><br/><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><strong>import matplotlib.pyplot as plt</strong><br/><br/><strong>from sklearn import datasets</strong><br/><br/><strong>iris = datasets.load_iris()</strong><br/><strong>X_w = iris.data[:, :2]  #load the first two features of the iris data </strong><br/><strong>y_w = iris.target       #load the target of the iris data</strong><br/><br/><strong>X = X_w[y_w != 0]</strong><br/><strong>y = y_w[y_w != 0]</strong><br/><br/><strong>X_1 = X[y == 1]</strong><br/><strong>X_2 = X[y == 2]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Begin by splitting the data into training and testing sets:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Construct a pipeline</h1>
                </header>
            
            <article>
                
<ol start="2">
<li>Then construct a pipeline with two steps: a scaling step and an SVM step. It is best to scale the data before passing it to an SVM:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.svm import SVC</strong><br/><strong>from sklearn.pipeline import Pipeline</strong><br/><strong>from sklearn.preprocessing import StandardScaler</strong><br/><br/><strong>svm_est = Pipeline([('scaler',StandardScaler()),('svc',SVC())])</strong></pre>
<div style="padding-left: 60px" class="packt_infobox">Note that in the pipeline, the scaling step has the name <kbd>scaler</kbd> and the SVM has the name <kbd>svc</kbd>. The names will be crucial in the next step. Note that the default SVM is an RBF SVM, which is nonlinear.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Construct a parameter grid for a pipeline</h1>
                </header>
            
            <article>
                
<ol start="3">
<li>Vary the relevant RBF parameters, C and gamma, logarithmically, varying by one order of magnitude at a time:</li>
</ol>
<pre style="padding-left: 60px"><strong>Cs = [0.001, 0.01, 0.1, 1, 10]</strong><br/><strong>gammas = [0.001, 0.01, 0.1, 1, 10]</strong></pre>
<ol start="4">
<li>Finally, construct the parameter grid by making it into a dictionary. The SVM parameter dictionary key names begin with <kbd>svc__</kbd>, taking the pipeline SVM name and adding two underscores. This is followed by the parameter name within the SVM estimator, <kbd>C</kbd> and <kbd>gamma</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>param_grid = dict(svc__gamma=gammas, svc__C=Cs)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Provide a cross-validation scheme</h1>
                </header>
            
            <article>
                
<ol start="5">
<li>The following is a stratified and shuffled split. The <kbd>n_splits</kbd> parameter refers to the number of splits, or tries, the dataset will be split into. The <kbd>test_size</kbd> parameter is how much data will be left out for testing within the fold. The estimator will be scored using the test set on each fold:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import StratifiedShuffleSplit</strong><br/><br/><strong>cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=7)</strong></pre>
<p style="padding-left: 60px">The most important element of the stratified shuffle is that each fold preserves the proportion of samples for each class.</p>
<ol start="6">
<li>For a plain cross-validation scheme, set <kbd>cv</kbd> to an integer representing the number of folds:</li>
</ol>
<pre style="padding-left: 60px"><strong>cv = 10</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Perform a grid search</h1>
                </header>
            
            <article>
                
<p>There are three required elements for a grid search:</p>
<ul>
<li>An estimator</li>
<li>A parameter grid</li>
<li>A cross-validation scheme</li>
</ul>
<ol start="7">
<li>We have those three elements. Set up the grid search. Run it on the training set:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import GridSearchCV</strong><br/><br/><strong>grid_cv = GridSearchCV(svm_est, param_grid=param_grid, cv=cv)</strong><br/><strong>grid_cv.fit(X_train, y_train)</strong></pre>
<ol start="8">
<li>Look up the best parameters found with the grid search:</li>
</ol>
<pre style="padding-left: 60px"><strong>grid_cv.best_params_</strong><br/><br/><strong>{'svc__C': 10, 'svc__gamma': 0.1}</strong></pre>
<ol start="9">
<li>Look up the best score, that pertains to the best estimator:</li>
</ol>
<pre style="padding-left: 60px"><strong>grid_cv.best_score_</strong><br/><br/><strong>0.71250000000000002</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Let us look at additional perspectives of SVM for classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Randomized grid search alternative</h1>
                </header>
            
            <article>
                
<p>scikit-learn's <kbd>GridSearchCV</kbd> performs a full scan for the best set of parameters for the estimator. In this case, it searches the 5 x 5 = 25 (C, gamma) pairs specified by the <kbd>param_grid</kbd> parameter.</p>
<p>An alternative would have been using <kbd>RandomizedSearchCV</kbd>, by using the following line instead of the one used with <kbd>GridSearchCV</kbd>:</p>
<pre><strong>from sklearn.model_selection import RandomizedSearchCV</strong><br/><br/><strong>rand_grid = RandomizedSearchCV(svm_est, param_distributions=param_grid, cv=cv,n_iter=10)</strong><br/><strong>rand_grid.fit(X_train, y_train)</strong></pre>
<p>It yields the same <kbd>C</kbd> and <kbd>gamma</kbd>:</p>
<pre><strong>rand_grid.best_params_</strong><br/><br/><strong>{'svc__C': 10, 'svc__gamma': 0.001}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualize the nonlinear RBF decision boundary</h1>
                </header>
            
            <article>
                
<p>Visualize the RBF decision boundary with code similar to the previous recipe. First, create a grid and predict to which class each point on the grid corresponds:</p>
<pre><strong>from itertools import product</strong><br/><br/><strong>#Minima and maxima of both features</strong><br/><strong>xmin, xmax = np.percentile(X[:, 0], [0, 100])</strong><br/><strong>ymin, ymax = np.percentile(X[:, 1], [0, 100])</strong><br/><br/><strong>#Grid/Cartesian product with itertools.product</strong><br/><strong>test_points = np.array([[xx, yy] for xx, yy in product(np.linspace(xmin, xmax), np.linspace(ymin, ymax))])</strong><br/><br/><strong>#Predictions on the grid</strong><br/><strong>test_preds = grid_cv.predict(test_points)</strong></pre>
<p>Now visualize the grid:</p>
<pre><strong>X_1 = X[y == 1]</strong><br/><strong>X_2 = X[y == 2]</strong><br/><br/><strong>%matplotlib inline</strong><br/><strong>plt.figure(figsize=(10,7))   #change figure-size for easier viewing</strong><br/><strong>plt.scatter(X_2[:,0],X_2[:,1], color = 'red')</strong><br/><strong>plt.scatter(X_1[:,0],X_1[:,1], color = 'blue')</strong><br/><br/><strong>colors = np.array(['r', 'b'])</strong><br/><strong>plt.scatter(test_points[:, 0], test_points[:, 1], color=colors[test_preds-1], alpha=0.25)</strong><br/><strong>plt.scatter(X[:, 0], X[:, 1], color=colors[y-1]) </strong><br/><strong>plt.title("RBF-separated classes")</strong></pre>
<p>Note that in the resulting graph, the RBF curve looks quite straight, but it really corresponds to a slight curve. This is an SVM with gamma = 0.1 and C = 0.001:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fc24acfb-8367-4673-86d1-c8af1c559b6e.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More meaning behind C and gamma</h1>
                </header>
            
            <article>
                
<p>More intuitively, the gamma parameter determines how influential a single example can be per units of distance. If gamma is low, examples have an influence at long distances. If gamma is high, their influence is only over short distances. The SVM selects support vectors in its implementation, and gamma is inversely proportional to the radius of influence of these vectors.</p>
<p>With regard to C, a low C makes the decision surface smoother, while a high C makes the SVM try to classify all the examples correctly and leads to surfaces that are not as smooth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiclass classification with SVM</h1>
                </header>
            
            <article>
                
<p>We begin expanding the previous recipe to classify all iris flower types based on two features. This is not a binary classification problem, but a multiclass classification problem. These steps expand on the previous recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The SVC classifier (scikit's SVC) can be changed slightly in the case of multiclass classifications. For this, we will use all three classes of the iris dataset.</p>
<p>Load two features for each class:</p>
<pre><strong>#load the libraries we have been using</strong><br/><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><strong>import matplotlib.pyplot as plt</strong><br/><br/><strong>from sklearn import datasets</strong><br/><br/><strong>iris = datasets.load_iris()</strong><br/><strong>X = iris.data[:, :2]  #load the first two features of the iris data </strong><br/><strong>y = iris.target       #load the target of the iris data</strong><br/><br/><strong>X_0 = X[y == 0]</strong><br/><strong>X_1 = X[y == 1]</strong><br/><strong>X_2 = X[y == 2]</strong></pre>
<p>Split the data into training and testing sets:</p>
<pre><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7,stratify=y)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OneVsRestClassifier</h1>
                </header>
            
            <article>
                
<ol>
<li>Load <kbd>OneVsRestClassifier</kbd> within a pipeline:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.svm import SVC</strong><br/><strong>from sklearn.pipeline import Pipeline</strong><br/><strong>from sklearn.preprocessing import StandardScaler</strong><br/><strong>from sklearn.multiclass import OneVsRestClassifier</strong><br/><br/><strong>svm_est = Pipeline([('scaler',StandardScaler()),('svc',OneVsRestClassifier(SVC()))])</strong></pre>
<ol start="2">
<li>Set up a parameter grid:</li>
</ol>
<pre style="padding-left: 60px"><strong>Cs = [0.001, 0.01, 0.1, 1, 10]</strong><br/><strong>gammas = [0.001, 0.01, 0.1, 1, 10]</strong></pre>
<ol start="3">
<li>Construct the parameter grid. Note the very special syntax to denote the <kbd>OneVsRestClassifier</kbd> SVC. The parameter key names within the dictionary start with <kbd>svc__estimator__</kbd> when named <kbd>svc</kbd> within the pipeline:</li>
</ol>
<pre style="padding-left: 60px"><strong>param_grid = dict(svc__estimator__gamma=gammas, svc__estimator__C=Cs)</strong></pre>
<ol start="4">
<li>Load a randomized hyperparameter search. Fit it:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import RandomizedSearchCV</strong><br/><strong>from sklearn.model_selection import StratifiedShuffleSplit</strong><br/><br/><strong>cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=7)</strong><br/><strong>rand_grid = RandomizedSearchCV(svm_est, param_distributions=param_grid, cv=cv,n_iter=10)</strong><br/><strong>rand_grid.fit(X_train, y_train)</strong></pre>
<ol start="5">
<li>Look up the best parameters:</li>
</ol>
<pre style="padding-left: 60px"><strong>rand_grid.best_params_</strong><br/><br/><strong>{'svc__estimator__C': 10, 'svc__estimator__gamma': 0.1}</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualize it</h1>
                </header>
            
            <article>
                
<p>We are going to predict the category of every point in a two-dimensional grid by calling the trained SVM to predict along the grid:</p>
<pre><strong>%matplotlib inline</strong><br/><strong>from itertools import product</strong><br/><br/><strong>#Minima and maxima of both features</strong><br/><strong>xmin, xmax = np.percentile(X[:, 0], [0, 100])</strong><br/><strong>ymin, ymax = np.percentile(X[:, 1], [0, 100])</strong><br/><br/><br/><strong>#Grid/Cartesian product with itertools.product</strong><br/><strong>test_points = np.array([[xx, yy] for xx, yy in product(np.linspace(xmin, xmax,100), np.linspace(ymin, ymax,100))])</strong><br/><br/><strong>#Predictions on the grid</strong><br/><strong>test_preds = rand_grid.predict(test_points)</strong><br/><br/><strong>plt.figure(figsize=(15,9))   #change figure-size for easier viewing</strong><br/><br/><strong>plt.scatter(X_0[:,0],X_0[:,1], color = 'green')</strong><br/><strong>plt.scatter(X_1[:,0],X_1[:,1], color = 'blue')</strong><br/><strong>plt.scatter(X_2[:,0],X_2[:,1], color = 'red')</strong><br/><br/><strong>colors = np.array(['g', 'b', 'r'])</strong><br/><strong>plt.tight_layout()</strong><br/><strong>plt.scatter(test_points[:, 0], test_points[:, 1], color=colors[test_preds], alpha=0.25)</strong><br/><strong>plt.scatter(X[:, 0], X[:, 1], color=colors[y])</strong></pre>
<div class="CDPAlignCenter CDPAlign"><br/>
<img src="assets/efceac4a-81bf-44db-ba2e-15889c0a8e6c.png"/></div>
<div class="packt_tip"><span>The boundaries generated by SVM tend to be  smooth curves, very different from the tree-based boundaries we will see in upcoming chapters.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The <kbd>OneVsRestClassifier</kbd> creates many binary SVC classifiers: one for each class versus the rest of the classes. In this case, three decision boundaries will be computed because there are three classes. This type of classifier is easy to conceptualize because there are fewer decision boundaries and surfaces.</p>
<p>If there were 10 classes, there would be 10 x 9/2 = 45 surfaces if SVC was the default <kbd>OneVsOneClassifier</kbd>. On the other hand, there would be 10 surfaces for the <kbd>OneVsAllClassifier</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector regression</h1>
                </header>
            
            <article>
                
<p>We will capitalize on the SVM classification recipes by performing support vector regression on scikit-learn's diabetes dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Load the diabetes dataset:</p>
<pre><strong>#load the libraries we have been using</strong><br/><strong>import numpy as np</strong><br/><strong>import pandas as pd</strong><br/><strong>import matplotlib.pyplot as plt</strong><br/> <br/><strong>from sklearn import datasets</strong><br/> <br/><strong>diabetes = datasets.load_diabetes()</strong><br/><br/><strong>X = diabetes.data</strong><br/><strong>y = diabetes.target</strong></pre>
<p>Split the data in training and testing sets. There is no stratification for regression in this case:</p>
<pre><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a <kbd>OneVsRestClassifier</kbd> within a pipeline and <strong>s</strong><span><strong>upport vector regression</strong> (</span><strong>SVR</strong>) from <kbd>sklearn.svm</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.svm import SVR</strong><br/><strong>from sklearn.pipeline import Pipeline</strong><br/><strong>from sklearn.preprocessing import StandardScaler</strong><br/><strong>from sklearn.multiclass import OneVsRestClassifier</strong><br/> <br/><strong>svm_est = Pipeline([('scaler',StandardScaler()),('svc',OneVsRestClassifier(SVR()))])</strong></pre>
<ol start="2">
<li>Create a parameter grid:</li>
</ol>
<pre style="padding-left: 60px"><strong>Cs = [0.001, 0.01, 0.1, 1]</strong><br/><strong>gammas = [0.001, 0.01, 0.1]</strong><br/> <br/><strong>param_grid = dict(svc__estimator__gamma=gammas, svc__estimator__C=Cs)</strong></pre>
<ol start="3">
<li>Perform a randomized search of the best hyperparameters, C and gamma:</li>
</ol>
<pre style="padding-left: 60px"><strong>from sklearn.model_selection import RandomizedSearchCV</strong><br/><strong>from sklearn.model_selection import StratifiedShuffleSplit</strong><br/> <br/><strong>rand_grid = RandomizedSearchCV(svm_est, param_distributions=param_grid, cv=5,n_iter=5,scoring='neg_mean_absolute_error')</strong><br/><strong>rand_grid.fit(X_train, y_train)</strong></pre>
<ol start="4">
<li>Look at the best parameters:</li>
</ol>
<pre style="padding-left: 60px"><strong>rand_grid.best_params_</strong><br/><br/><strong>{'svc__estimator__C': 10, 'svc__estimator__gamma': 0.1}</strong></pre>
<ol start="5">
<li>Look at the best score:</li>
</ol>
<p class="mce-root"/>
<pre style="padding-left: 60px"><strong>rand_grid.best_score_</strong><br/><br/><strong>-58.059490084985839</strong></pre>
<p>The score does not seem very good. Try different algorithms with different score setups and see which one performs best.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>
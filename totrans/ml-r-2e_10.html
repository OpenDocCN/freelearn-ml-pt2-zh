<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;Evaluating Model Performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Evaluating Model Performance</h1></div></div></div><p>When only the wealthy could afford education, tests and exams did not evaluate students' potential. Instead, teachers were judged for parents who wanted to know whether their children had learned enough to justify the instructors' wages. Obviously, this has changed over the years. Now, such evaluations are used to distinguish between high- and low-achieving students, filtering them into careers and other opportunities.</p><p>Given the significance of this process, a great deal of effort is invested in developing accurate student assessments. Fair assessments have a large number of questions that cover a wide breadth of topics and reward true knowledge over lucky guesses. They also require students to think about problems they have never faced before. Correct responses therefore indicate that students can generalize their knowledge more broadly.</p><p>The process of evaluating machine learning algorithms is very similar to the process of evaluating students. Since algorithms have varying strengths and weaknesses, tests should distinguish among the learners. It is also important to forecast how a learner will perform on future data.</p><p>This chapter provides the information needed to assess machine learners, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The reasons why predictive accuracy is not sufficient to measure performance, and the performance measures you might use instead</li><li class="listitem" style="list-style-type: disc">Methods to ensure that the performance measures reasonably reflect a model's ability to predict or forecast unseen cases</li><li class="listitem" style="list-style-type: disc">How to use R to apply these more useful measures and methods to the predictive models covered in the previous chapters</li></ul></div><p>Just as the best way to learn a topic is to attempt to teach it to someone else, the process of teaching and evaluating machine learners will provide you with greater insight into the methods you've learned so far.</p><div class="section" title="Measuring performance for classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec44"/>Measuring performance for classification</h1></div></div></div><p>In the previous <a id="id814" class="indexterm"/>chapters, we measured classifier accuracy by dividing the proportion of correct predictions by the total number of predictions. This indicates the percentage of cases in which the learner is right or wrong. For example, suppose that for 99,990 out of 100,000 newborn babies a classifier correctly predicted whether they were a carrier of a treatable but potentially fatal genetic defect. This would imply an accuracy of 99.99 percent and an error rate of only 0.01 percent.</p><p>At first glance, this appears to be an extremely accurate classifier. However, it would be wise to collect additional information before trusting your child's life to the test. What if the genetic defect is found in only 10 out of every 100,000 babies? A test that predicts <span class="emphasis"><em>no defect</em></span> regardless of the circumstances will be correct for 99.99 percent of all cases, but incorrect for 100 percent of the cases that matter most. In other words, even though the predictions are extremely accurate, the classifier is not very useful to prevent treatable birth defects.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip112"/>Tip</h3><p>This is <a id="id815" class="indexterm"/>one consequence of the <span class="strong"><strong>class imbalance problem</strong></span>, which refers to the trouble associated with data having a large majority of records belonging to a single class.</p></div></div><p>Though there are many ways to measure a classifier's performance, the best measure is always the one that captures whether the classifier is successful at its intended purpose. It is crucial to define the performance measures for utility rather than raw accuracy. To this end, we will begin exploring a variety of alternative performance measures derived from the confusion matrix. Before we get started, however, we need to consider how to prepare a classifier for evaluation.</p><div class="section" title="Working with classification prediction data in R"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec108"/>Working with classification prediction data in R</h2></div></div></div><p>The goal of <a id="id816" class="indexterm"/>evaluating a classification model is to have a better understanding of how its <a id="id817" class="indexterm"/>performance will extrapolate to future cases. Since it is usually unfeasible to test a still-unproven model in a live environment, we typically simulate future conditions by asking the model to classify a dataset made of cases that resemble what it will be asked to do in the future. By observing the learner's responses to this examination, we can learn about its strengths and weaknesses.</p><p>Though we've evaluated classifiers in the prior chapters, it's worth reflecting on the types of data at our disposal:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Actual class values</li><li class="listitem" style="list-style-type: disc">Predicted class values</li><li class="listitem" style="list-style-type: disc">Estimated probability of the prediction</li></ul></div><p>The actual and predicted class values may be self-evident, but they are the key to evaluation. Just like a teacher uses an answer key to assess the student's answers, we need to know the correct answer for a machine learner's predictions. The goal is to maintain two vectors of data: one holding the correct or actual class values, and the other holding the predicted class values. Both vectors must have the same number of values stored in the same order. The predicted and actual values may be stored as separate R vectors or columns in a single R data frame.</p><p>Obtaining this data is easy. The actual class values come directly from the target feature in the test dataset. Predicted class values are obtained from the classifier built upon the training data, and applied to the test data. For most machine learning packages, this involves applying the <code class="literal">predict()</code> function to a model object and a data frame of test data, such as: <code class="literal">predicted_outcome &lt;- predict(model, test_data)</code>.</p><p>Until now, we have only examined classification predictions using these two vectors of data. Yet most models can supply another piece of useful information. Even though the classifier makes a single prediction about each example, it may be more confident about some decisions than others. For instance, a classifier may be 99 percent certain that an SMS with the words "free" and "ringtones" is spam, but is only 51 percent certain that an SMS with the word "tonight" is spam. In both cases, the classifier classifies the message as spam, but it is far more certain about one decision than the other.</p><div class="mediaobject"><img src="graphics/B03905_10_01.jpg" alt="Working with classification prediction data in R"/></div><p>Studying these internal prediction probabilities provides useful data to evaluate a model's performance. If two models make the same number of mistakes, but one is more capable of accurately assessing its uncertainty, then it is a smarter model. It's ideal to fid a learner that is extremely confident when making a correct prediction, but timid in the face of doubt. The balance between confidence and caution is a key part of model evaluation.</p><p>Unfortunately, obtaining internal prediction probabilities can be tricky because the method to do so varies across classifiers. In general, for most classifiers, the <code class="literal">predict()</code> function is used to specify the desired type of prediction. To obtain a single predicted class, such as spam or ham, you typically set the <code class="literal">type = "class"</code> parameter. To obtain the prediction probability, the <code class="literal">type</code> parameter should be set to one of <code class="literal">"prob"</code>, <code class="literal">"posterior"</code>, <code class="literal">"raw"</code>, or <code class="literal">"probability"</code> depending on the classifier used.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip113"/>Tip</h3><p>Nearly all of the classifiers presented in this book will provide prediction probabilities. The <code class="literal">type</code> parameter is included in the syntax box introducing each model.</p></div></div><p>For example, to output the predicted probabilities for the C5.0 classifier built in <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span>, use the <code class="literal">predict()</code> function with <code class="literal">type = "prob"</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; predicted_prob &lt;- predict(credit_model, credit_test, type = "prob")</strong></span>
</pre></div><p>To further illustrate the process of evaluating learning algorithms, let's look more closely at the performance of the SMS spam classification model developed in <a class="link" href="ch04.html" title="Chapter 4. Probabilistic Learning – Classification Using Naive Bayes">Chapter 4</a>, <span class="emphasis"><em>Probabilistic Learning – Classification Using Naive Bayes</em></span>. To output the naive Bayes predicted probabilities, use <code class="literal">predict()</code> with <code class="literal">type = "raw"</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sms_test_prob &lt;- predict(sms_classifier, sms_test, type = "raw")</strong></span>
</pre></div><p>In most cases, the <code class="literal">predict()</code> function returns a probability for each category of the outcome. For example, in the case of a two-outcome model like the SMS classifier, the predicted probabilities <a id="id818" class="indexterm"/>might be a matrix or data frame as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(sms_test_prob)</strong></span>
<span class="strong"><strong>              ham         spam</strong></span>
<span class="strong"><strong>[1,] 9.999995e-01 4.565938e-07</strong></span>
<span class="strong"><strong>[2,] 9.999995e-01 4.540489e-07</strong></span>
<span class="strong"><strong>[3,] 9.998418e-01 1.582360e-04</strong></span>
<span class="strong"><strong>[4,] 9.999578e-01 4.223125e-05</strong></span>
<span class="strong"><strong>[5,] 4.816137e-10 1.000000e+00</strong></span>
<span class="strong"><strong>[6,] 9.997970e-01 2.030033e-04</strong></span>
</pre></div><p>Each line in this output <a id="id819" class="indexterm"/>shows the classifier's predicted probability of <code class="literal">spam</code> and <code class="literal">ham</code>, which always sum up to 1 because these are the only two outcomes. While constructing an evaluation dataset, it is important to ensure that you are using the correct probability for the class level of interest. To avoid confusion, in the case of a binary outcome, you might even consider dropping the vector for one of the two alternatives.</p><p>For convenience during the evaluation process, it can be helpful to construct a data frame containing the predicted class values, actual class values, as well as the estimated probabilities of interest.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip114"/>Tip</h3><p>The steps required to construct the evaluation dataset have been omitted for brevity, but are included in this chapter's code on the Packt Publishing website. To follow along with the examples here, download the <code class="literal">sms_results.csv</code> file, and load to a data frame using the <code class="literal">sms_results &lt;- read.csv("sms_results.csv")</code> command.</p></div></div><p>The <code class="literal">sms_results</code> data frame is simple. It contains four vectors of 1,390 values. One vector contains values indicating the actual type of SMS message (<code class="literal">spam</code> or <code class="literal">ham</code>), one vector indicates the naive Bayes model's predicted type, and the third and fourth vectors indicate the <a id="id820" class="indexterm"/>probability that the message was <code class="literal">spam</code> or <code class="literal">ham</code>, respectively:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(sms_results)</strong></span>
<span class="strong"><strong>  actual_type predict_type prob_spam prob_ham</strong></span>
<span class="strong"><strong>1         ham          ham   0.00000  1.00000</strong></span>
<span class="strong"><strong>2         ham          ham   0.00000  1.00000</strong></span>
<span class="strong"><strong>3         ham          ham   0.00016  0.99984</strong></span>
<span class="strong"><strong>4         ham          ham   0.00004  0.99996</strong></span>
<span class="strong"><strong>5        spam         spam   1.00000  0.00000</strong></span>
<span class="strong"><strong>6         ham          ham   0.00020  0.99980</strong></span>
</pre></div><p>For these six test cases, the predicted and actual SMS message types agree; the model predicted their status correctly. Furthermore, the prediction probabilities suggest that model was extremely confident about these predictions, because they all fall close to zero or one.</p><p>What happens when the <a id="id821" class="indexterm"/>predicted and actual values are further from zero and one? Using the <code class="literal">subset()</code> function, we can identify a few of these records. The following output shows test cases where the model estimated the probability of <code class="literal">spam</code> somewhere between 40 and 60 percent:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(subset(sms_results, prob_spam &gt; 0.40 &amp; prob_spam &lt; 0.60))</strong></span>
<span class="strong"><strong>     actual_type predict_type prob_spam prob_ham</strong></span>
<span class="strong"><strong>377         spam          ham   0.47536  0.52464</strong></span>
<span class="strong"><strong>717          ham         spam   0.56188  0.43812</strong></span>
<span class="strong"><strong>1311         ham         spam   0.57917  0.42083</strong></span>
</pre></div><p>By the model's own admission, these were cases in which a correct prediction was virtually a coin flip. Yet all three predictions were wrong—an unlucky result. Let's look at a few more cases where the model was wrong:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(subset(sms_results, actual_type != predict_type))</strong></span>
<span class="strong"><strong>    actual_type predict_type prob_spam prob_ham</strong></span>
<span class="strong"><strong>53         spam          ham   0.00071  0.99929</strong></span>
<span class="strong"><strong>59         spam          ham   0.00156  0.99844</strong></span>
<span class="strong"><strong>73         spam          ham   0.01708  0.98292</strong></span>
<span class="strong"><strong>76         spam          ham   0.00851  0.99149</strong></span>
<span class="strong"><strong>184        spam          ham   0.01243  0.98757</strong></span>
<span class="strong"><strong>332        spam          ham   0.00003  0.99997</strong></span>
</pre></div><p>These cases illustrate the important fact that a model can be extremely confident and yet it can be extremely wrong. All six of these test cases were <code class="literal">spam</code> that the classifier believed to have no less than a 98 percent chance of being <code class="literal">ham</code>.</p><p>In spite of such mistakes, is <a id="id822" class="indexterm"/>the model still useful? We can answer this question by applying various error <a id="id823" class="indexterm"/>metrics to the evaluation data. In fact, many such metrics are based on a tool we've already used extensively in the previous chapters.</p></div><div class="section" title="A closer look at confusion matrices"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec109"/>A closer look at confusion matrices</h2></div></div></div><p>A <span class="strong"><strong>confusion </strong></span><a id="id824" class="indexterm"/>
<span class="strong"><strong>matrix</strong></span> is a table that categorizes predictions according to whether they match the actual value. One of the table's dimensions indicates the possible categories of predicted values, while the other dimension indicates the same for actual values. Although we have only seen 2 x 2 confusion matrices so far, a matrix can be created for models that predict any number of class values. The following figure depicts the familiar confusion matrix for a two-class binary model as well as the 3 x 3 confusion matrix for a three-class model.</p><p>When the predicted value is the same as the actual value, it is a correct classification. Correct predictions fall on the diagonal in the confusion matrix (denoted by <span class="strong"><strong>O</strong></span>). The off-diagonal matrix cells (denoted by <span class="strong"><strong>X</strong></span>) indicate the cases where the predicted value differs from the actual value. These are incorrect predictions. The performance measures for classification models are based on the counts of predictions falling on and off the diagonal in these tables:</p><div class="mediaobject"><img src="graphics/B03905_10_02.jpg" alt="A closer look at confusion matrices"/></div><p>The most common performance measures consider the model's ability to discern one class versus all others. The class of interest is known as the <span class="strong"><strong>positive </strong></span>class, while all others are known as <span class="strong"><strong>negative</strong></span>.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip115"/>Tip</h3><p>The use of the terms positive and negative is not intended to imply any value judgment (that is, good versus bad), nor does it necessarily suggest that the outcome is present or absent (such as birth defect versus none). The choice of the positive outcome can even be arbitrary, as in cases where a model is predicting categories such as sunny versus rainy or dog versus cat.</p></div></div><p>The relationship between the positive class and negative class predictions can be depicted as a 2 x 2 <a id="id825" class="indexterm"/>confusion matrix that tabulates whether predictions fall into one of the four categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True Positive (TP)</strong></span>: Correctly classified as the class of interest</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True Negative (TN)</strong></span>: Correctly classified as not the class of interest</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False Positive (FP)</strong></span>: Incorrectly classified as the class of interest</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False Negative (FN)</strong></span>: Incorrectly classified as not the class of interest</li></ul></div><p>For the spam classifier, the positive class is <code class="literal">spam</code>, as this is the outcome we hope to detect. We can then imagine the confusion matrix as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B03905_10_03.jpg" alt="A closer look at confusion matrices"/></div><p>The confusion matrix, presented in this way, is the basis for many of the most important measures of model's performance. In the next section, we'll use this matrix to have a better understanding of what is meant by accuracy.</p></div><div class="section" title="Using confusion matrices to measure performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec110"/>Using confusion matrices to measure performance</h2></div></div></div><p>With the <a id="id826" class="indexterm"/>2 x 2 confusion matrix, we can formalize our definition of prediction <span class="strong"><strong>accuracy</strong></span> (sometimes called the <span class="strong"><strong>success rate</strong></span>) as:</p><div class="mediaobject"><img src="graphics/B03905_10_04.jpg" alt="Using confusion matrices to measure performance"/></div><p>In this <a id="id827" class="indexterm"/>formula, the terms <span class="emphasis"><em>TP</em></span>, <span class="emphasis"><em>TN</em></span>, <span class="emphasis"><em>FP</em></span>, and <span class="emphasis"><em>FN </em></span>refer to the number of times the model's predictions fell into each of these categories. The accuracy is therefore a proportion that represents the number of true positives and true negatives, divided by the total number of predictions.</p><p>The <span class="strong"><strong>error rate</strong></span> or the proportion of the incorrectly classified examples is specified as:</p><div class="mediaobject"><img src="graphics/B03905_10_05.jpg" alt="Using confusion matrices to measure performance"/></div><p>Notice that the error rate can be calculated as one minus the accuracy. Intuitively, this makes sense; a model that is correct 95 percent of the time is incorrect 5 percent of the time.</p><p>An easy way to tabulate a classifier's predictions into a confusion matrix is to use R's <code class="literal">table()</code> function. The command to create a confusion matrix for the SMS data is shown as follows. The counts in this table could then be used to calculate accuracy and other statistics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; table(sms_results$actual_type, sms_results$predict_type)</strong></span>

<span class="strong"><strong>        ham spam</strong></span>
<span class="strong"><strong>  ham  1203    4</strong></span>
<span class="strong"><strong>  spam   31  152</strong></span>
</pre></div><p>If you like to create a confusion matrix with a more informative output, the <code class="literal">CrossTable()</code> function in the <code class="literal">gmodels</code> package offers a customizable solution. If you recall, we first used this function in <a class="link" href="ch02.html" title="Chapter 2. Managing and Understanding Data">Chapter 2</a>, <span class="emphasis"><em>Managing and Understanding Data</em></span>. If you didn't install the package at that time, you will need to do so using the <code class="literal">install.packages("gmodels")</code> command.</p><p>By default, the <code class="literal">CrossTable()</code> output includes proportions in each cell that indicate the cell count as a percentage of table's row, column, or overall total counts. The output also includes row and column totals. As shown in the following code, the syntax is similar to the <code class="literal">table()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(gmodels)</strong></span>
<span class="strong"><strong>&gt; CrossTable(sms_results$actual_type, sms_results$predict_type)</strong></span>
</pre></div><p>The result is a <a id="id828" class="indexterm"/>confusion <a id="id829" class="indexterm"/>matrix with a wealth of additional detail:</p><div class="mediaobject"><img src="graphics/B03905_10_06.jpg" alt="Using confusion matrices to measure performance"/></div><p>We've used <code class="literal">CrossTable()</code> in several of the previous chapters, so by now you should be familiar with the output. If you ever forget how to interpret the output, simply refer to the key (labeled <code class="literal">Cell Contents</code>), which provides the definition of each number in the table cells.</p><p>We can use the confusion matrix to obtain the accuracy and error rate. Since the accuracy is <span class="emphasis"><em>(TP + TN) / (TP + TN + FP + FN)</em></span>, we can calculate it using following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; (152 + 1203) / (152 + 1203 + 4 + 31)</strong></span>
<span class="strong"><strong>[1] 0.9748201</strong></span>
</pre></div><p>We can also calculate the error rate <span class="emphasis"><em>(FP + FN) / (TP + TN + FP + FN)</em></span> as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; (4 + 31) / (152 + 1203 + 4 + 31)</strong></span>
<span class="strong"><strong>[1] 0.02517986</strong></span>
</pre></div><p>This is the same as one minus accuracy:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; 1 - 0.9748201</strong></span>
<span class="strong"><strong>[1] 0.0251799</strong></span>
</pre></div><p>Although these calculations may seem simple, it is important to practice thinking about how the <a id="id830" class="indexterm"/>components of the confusion matrix relate to one another. In the next section, you will see <a id="id831" class="indexterm"/>how these same pieces can be combined in different ways to create a variety of additional performance measures.</p></div><div class="section" title="Beyond accuracy – other measures of performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec111"/>Beyond accuracy – other measures of performance</h2></div></div></div><p>Countless <a id="id832" class="indexterm"/>performance measures have been developed and used for specific purposes in disciplines as diverse as medicine, information retrieval, marketing, and signal detection theory, among others. Covering all of them could fill hundreds of pages and makes a comprehensive description infeasible here. Instead, we'll consider only some of the most useful and commonly cited measures in the machine learning literature.</p><p>The <a id="id833" class="indexterm"/>Classification and Regression Training package <code class="literal">caret</code> by Max Kuhn includes functions to compute many such performance measures. This package provides a large number of tools to prepare, train, evaluate, and visualize machine learning models and data. In addition to its use here, we will also employ <code class="literal">caret</code> extensively in <a class="link" href="ch11.html" title="Chapter 11. Improving Model Performance">Chapter 11</a>, <span class="emphasis"><em>Improving Model Performance</em></span>. Before proceeding, you will need to install the package using the <code class="literal">install.packages("caret")</code> command.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip116"/>Tip</h3><p>For more information on <code class="literal">caret</code>, please refer to: Kuhn M. Building predictive models in R using the caret package. <span class="emphasis"><em>Journal of Statistical</em></span> Software. 2008; 28.</p></div></div><p>The <code class="literal">caret</code> package adds yet another function to create a confusion matrix. As shown in the following command, the syntax is similar to <code class="literal">table()</code>, but with a minor difference. Because <code class="literal">caret</code> provides measures of model performance that consider the ability to classify the positive class, a <code class="literal">positive</code> parameter should be specified. In this case, since the SMS classifier is intended to detect <code class="literal">spam</code>, we will set <code class="literal">positive = "spam"</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; confusionMatrix(sms_results$predict_type,</strong></span>
<span class="strong"><strong>    sms_results$actual_type, positive = "spam")</strong></span>
</pre></div><p>This results in the <a id="id834" class="indexterm"/>following output:</p><div class="mediaobject"><img src="graphics/B03905_10_07.jpg" alt="Beyond accuracy – other measures of performance"/></div><p>At the top of the output is a confusion matrix much like the one produced by the <code class="literal">table()</code> function, but transposed. The output also includes a set of performance measures. Some of these, like accuracy, are familiar, while many others are new. Let's take a look at few of the most important metrics.</p><div class="section" title="The kappa statistic"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec56"/>The kappa statistic</h3></div></div></div><p>The <span class="strong"><strong>kappa statistic</strong></span> (labeled <code class="literal">Kappa</code> in the previous output) adjusts accuracy by accounting for the possibility of a correct prediction by chance alone. This is especially important for datasets with a severe class imbalance, because a classifier can obtain high accuracy simply by always guessing the most frequent class. The kappa statistic <a id="id835" class="indexterm"/>will only reward the classifier if it is correct more often than this simplistic strategy.</p><p>Kappa values <a id="id836" class="indexterm"/>range from 0 to a maximum of 1, which indicates perfect agreement between the model's predictions and the true values. Values less than one indicate imperfect agreement. Depending on how a model is to be used, the interpretation of the kappa statistic might vary. One common interpretation is shown as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Poor agreement = less than 0.20</li><li class="listitem" style="list-style-type: disc">Fair agreement = 0.20 to 0.40</li><li class="listitem" style="list-style-type: disc">Moderate agreement = 0.40 to 0.60</li><li class="listitem" style="list-style-type: disc">Good agreement = 0.60 to 0.80</li><li class="listitem" style="list-style-type: disc">Very good agreement = 0.80 to 1.00</li></ul></div><p>It's important to note that these categories are subjective. While a "good agreement" may be more than adequate to predict someone's favorite ice cream flavor, "very good agreement" may not suffice if your goal is to identify birth defects.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip117"/>Tip</h3><p>For more information on the previous scale, refer to: Landis JR, Koch GG. The measurement of observer agreement for categorical data. <span class="emphasis"><em>Biometrics</em></span>. 1997; 33:159-174.</p></div></div><p>The following is the formula to calculate the kappa statistic. In this formula, <span class="emphasis"><em>Pr(a)</em></span> refers to the proportion of the actual agreement and <span class="emphasis"><em>Pr(e)</em></span> refers to the expected agreement between the classifier and the true values, under the assumption that they were chosen at random:</p><div class="mediaobject"><img src="graphics/B03905_10_08.jpg" alt="The kappa statistic"/></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip118"/>Tip</h3><p>There is more than one way to define the kappa statistic. The most common method described here uses <span class="strong"><strong>Cohen's kappa coefficient</strong></span>, as described in the paper: Cohen J. A coefficient of agreement for nominal scales. <span class="emphasis"><em>Education and Psychological Measurement</em></span>. 1960; 20:37-46.</p></div></div><p>These proportions are easy to obtain from a confusion matrix once you know where to look. Let's consider the confusion matrix for the SMS classification model created with the <code class="literal">CrossTable()</code> function, which is repeated here for convenience:</p><div class="mediaobject"><img src="graphics/B03905_10_09.jpg" alt="The kappa statistic"/></div><p>Remember that the bottom value in each cell indicates the proportion of all instances falling into that cell. Therefore, to calculate the observed agreement <span class="emphasis"><em>Pr(a)</em></span>, we simply add the proportion of all instances where the predicted type and actual SMS type agree. Thus, we can calculate <span class="emphasis"><em>Pr(a)</em></span> as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; pr_a &lt;- 0.865 + 0.109</strong></span>
<span class="strong"><strong>&gt; pr_a</strong></span>
<span class="strong"><strong>[1] 0.974</strong></span>
</pre></div><p>For this classifier, the <a id="id837" class="indexterm"/>observed and actual values agree 97.4 percent of the time—you will note that this is the same as the accuracy. The kappa statistic adjusts the accuracy relative to the <a id="id838" class="indexterm"/>expected agreement <span class="emphasis"><em>Pr(e)</em></span>, which is the probability that the chance alone would lead the predicted and actual values to match, under the assumption that both are selected randomly according to the observed proportions.</p><p>To find these observed proportions, we can use the probability rules we learned in <a class="link" href="ch04.html" title="Chapter 4. Probabilistic Learning – Classification Using Naive Bayes">Chapter 4</a>, <span class="emphasis"><em>Probabilistic Learning – Classification Using Naive Bayes</em></span>. Assuming two events are independent (meaning that one does not affect the other), probability rules note that the probability of both occurring is equal to the product of the probabilities of each one occurring. For instance, we know that the probability of both choosing ham is:</p><p>
<span class="emphasis"><em>Pr(actual type is ham) * Pr(predicted type is ham)</em></span>
</p><p>The probability of both choosing spam is:</p><p>
<span class="emphasis"><em>Pr(actual type is spam) * Pr(predicted type is spam)</em></span>
</p><p>The probability that the predicted or actual type is spam or ham can be obtained from the row or column totals. For instance, <span class="emphasis"><em>Pr(actual type is ham) = 0.868</em></span> and <span class="emphasis"><em>Pr(predicted type is ham) = 0.888</em></span>.</p><p>
<span class="emphasis"><em>Pr(e) </em></span>is calculated as the sum of the probabilities that by chance the predicted and actual values agree that the message is either spam or ham. Recall that for mutually exclusive events (events that cannot happen simultaneously), the probability of either occurring is equal to the sum of their probabilities. Therefore, to obtain the final <span class="emphasis"><em>Pr(e)</em></span>, we simply add both products, as shown in the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; pr_e &lt;- 0.868 * 0.888 + 0.132 * 0.112</strong></span>
<span class="strong"><strong>&gt; pr_e</strong></span>
<span class="strong"><strong>[1] 0.785568</strong></span>
</pre></div><p>Since <span class="emphasis"><em>Pr(e) </em></span>is 0.786, by chance alone, we would expect the observed and actual values to agree about 78.6 percent of the time.</p><p>This means that we now have all the information needed to complete the kappa formula. Plugging the <span class="emphasis"><em>Pr(a)</em></span> and <span class="emphasis"><em>Pr(e)</em></span> values into the kappa formula, we find:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; k &lt;- (pr_a - pr_e) / (1 - pr_e)</strong></span>
<span class="strong"><strong>&gt; k</strong></span>
<span class="strong"><strong>[1] 0.8787494</strong></span>
</pre></div><p>The kappa is about <a id="id839" class="indexterm"/>0.88, which agrees with the previous <code class="literal">confusionMatrix()</code> output from <code class="literal">caret</code> (the small difference is due to rounding). Using the suggested interpretation, we note that there is very good agreement between the classifier's predictions and the actual values.</p><p>There are a couple of <a id="id840" class="indexterm"/>R functions to calculate kappa automatically. The <code class="literal">Kappa()</code> function (be sure to note the capital 'K') in the Visualizing Categorical Data (<code class="literal">vcd)</code> package uses a confusion matrix of predicted and actual values. After installing the package by typing <code class="literal">install.packages("vcd")</code>, the following commands can be used to obtain kappa:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(vcd)</strong></span>
<span class="strong"><strong>&gt; Kappa(table(sms_results$actual_type, sms_results$predict_type))</strong></span>
<span class="strong"><strong>               value        ASE</strong></span>
<span class="strong"><strong>Unweighted 0.8825203 0.01949315</strong></span>
<span class="strong"><strong>Weighted   0.8825203 0.01949315</strong></span>
</pre></div><p>We're interested in the unweighted kappa. The value 0.88 matches what we expected.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip119"/>Tip</h3><p>The weighted kappa is used when there are varying degrees of agreement. For example, using a scale of cold, cool, warm, and hot, a value of warm agrees more with hot than it does with the value of cold. In the case of a two-outcome event, such as spam and ham, the weighted and unweighted kappa statistics will be identical.</p></div></div><p>The <code class="literal">kappa2()</code> function in the <a id="id841" class="indexterm"/>Inter-Rater Reliability (<code class="literal">irr</code>) package can be used to calculate kappa from the vectors of predicted and actual values stored in a data frame. After installing the package using the <code class="literal">install.packages("irr")</code> command, the following commands can be used to obtain kappa:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; kappa2(sms_results[1:2])</strong></span>
<span class="strong"><strong> Cohen's Kappa for 2 Raters (Weights: unweighted)</strong></span>

<span class="strong"><strong> Subjects = 1390 </strong></span>
<span class="strong"><strong>   Raters = 2 </strong></span>
<span class="strong"><strong>    Kappa = 0.883 </strong></span>

<span class="strong"><strong>        z = 33 </strong></span>
<span class="strong"><strong>  p-value = 0</strong></span>
</pre></div><p>The <code class="literal">Kappa()</code> and <code class="literal">kappa2()</code> functions report the same kappa statistic, so use whichever option you are more <a id="id842" class="indexterm"/>comfortable with.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip120"/>Tip</h3><p>Be careful not to use the built-in <code class="literal">kappa()</code> function. It is completely unrelated to the kappa statistic reported previously!</p></div></div></div><div class="section" title="Sensitivity and specificity"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec57"/>Sensitivity and specificity</h3></div></div></div><p>Finding a useful <a id="id843" class="indexterm"/>classifier often involves a balance between predictions that are overly conservative and overly aggressive. For example, an e-mail filter could guarantee to eliminate <a id="id844" class="indexterm"/>every spam message by aggressively eliminating nearly every ham message at the same time. On the other hand, guaranteeing that no ham message is inadvertently filtered might require us to allow an unacceptable amount of spam to pass through the filter. A pair of performance measures captures this tradeoff: sensitivity and specificity.</p><p>The <span class="strong"><strong>sensitivity</strong></span> of a <a id="id845" class="indexterm"/>model (also called the <span class="strong"><strong>true positive rate</strong></span>) measures the proportion of positive examples that were correctly classified. Therefore, as shown in the following formula, it is calculated as the number of true positives divided by the total number of positives, both correctly classified (the true positives) as well as incorrectly classified (the false negatives):</p><div class="mediaobject"><img src="graphics/B03905_10_10.jpg" alt="Sensitivity and specificity"/></div><p>The <span class="strong"><strong>specificity </strong></span>of a <a id="id846" class="indexterm"/>model (also called the <span class="strong"><strong>true negative rate</strong></span>) measures the proportion of negative examples that were correctly classified. As with sensitivity, this is computed as the number of true negatives, divided by the total number of negatives—the true negatives plus the false positives:</p><div class="mediaobject"><img src="graphics/B03905_10_11.jpg" alt="Sensitivity and specificity"/></div><p>Given the confusion matrix for the SMS classifier, we can easily calculate these measures by hand. Assuming that spam is the positive class, we can confirm that the numbers in the <code class="literal">confusionMatrix()</code> output are correct. For example, the calculation for sensitivity is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sens &lt;- 152 / (152 + 31)</strong></span>
<span class="strong"><strong>&gt; sens</strong></span>
<span class="strong"><strong>[1] 0.8306011</strong></span>
</pre></div><p>Similarly, for specificity we can calculate:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; spec &lt;- 1203 / (1203 + 4)</strong></span>
<span class="strong"><strong>&gt; spec</strong></span>
<span class="strong"><strong>[1] 0.996686</strong></span>
</pre></div><p>The <code class="literal">caret</code> package provides functions to calculate sensitivity and specificity directly from the vectors of predicted and actual values. Be careful that you specify the <code class="literal">positive</code> or <code class="literal">negative</code> parameter appropriately, as shown in the following lines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; sensitivity(sms_results$predict_type, sms_results$actual_type,</strong></span>
<span class="strong"><strong>              positive = "spam")</strong></span>
<span class="strong"><strong>[1] 0.8306011</strong></span>

<span class="strong"><strong>&gt; specificity(sms_results$predict_type, sms_results$actual_type,</strong></span>
<span class="strong"><strong>              negative = "ham")</strong></span>
<span class="strong"><strong>[1] 0.996686</strong></span>
</pre></div><p>Sensitivity and specificity range from 0 to 1, with values close to 1 being more desirable. Of course, it is important to find an appropriate balance between the two—a task that is often quite context-specific.</p><p>For example, in this case, the <a id="id847" class="indexterm"/>sensitivity of 0.831 implies that 83.1 percent of the spam messages were correctly <a id="id848" class="indexterm"/>classified. Similarly, the specificity of 0.997 implies that 99.7 percent of the nonspam messages were correctly classified or, alternatively, 0.3 percent of the valid messages were rejected as spam. The idea of rejecting 0.3 percent of valid SMS messages may be unacceptable, or it may be a reasonable trade-off given the reduction in spam.</p><p>Sensitivity and specificity provide tools for thinking about such trade-offs. Typically, changes are made to the model and different models are tested until you find one that meets a desired sensitivity and specificity threshold. Visualizations, such as those discussed later in this chapter, can also assist with understanding the trade-off between sensitivity and specificity.</p></div><div class="section" title="Precision and recall"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec58"/>Precision and recall</h3></div></div></div><p>Closely related to <a id="id849" class="indexterm"/>sensitivity and specificity are two other performance measures related to compromises made in classification: precision and recall. Used primarily in the context of information retrieval, these statistics are intended to provide an indication of how interesting and relevant a model's results are, or whether the predictions are diluted by meaningless noise.</p><p>The <a id="id850" class="indexterm"/>
<span class="strong"><strong>precision</strong></span> (also known as the <a id="id851" class="indexterm"/>
<span class="strong"><strong>positive predictive value</strong></span>) is defined as the proportion of positive examples that are truly positive; in other words, when a model predicts the positive class, how often is it correct? A precise model will only predict the positive class in cases that are very likely to be positive. It will be very trustworthy.</p><p>Consider what would happen if the model was very imprecise. Over time, the results would be less likely to be trusted. In the context of information retrieval, this would be similar to a search engine such as Google returning unrelated results. Eventually, users would switch to a competitor like Bing. In the case of the SMS spam filter, high precision means that the model is able to carefully target only the spam while ignoring the ham.</p><div class="mediaobject"><img src="graphics/B03905_10_12.jpg" alt="Precision and recall"/></div><p>On the other hand, <span class="strong"><strong>recall</strong></span> is a measure of how complete the results are. As shown in the following formula, this is defined as the number of true positives over the total number of positives. You may have already recognized this as the same as sensitivity. However, in this case, the interpretation differs slightly. A model with a high recall captures a large <a id="id852" class="indexterm"/>portion of the positive examples, meaning that it has wide breadth. For example, a search engine with a high recall returns a large number of documents pertinent to the search query. Similarly, the SMS spam filter has a high recall if the majority of spam messages are correctly identified.</p><div class="mediaobject"><img src="graphics/B03905_10_13.jpg" alt="Precision and recall"/></div><p>We can calculate precision and recall from the confusion matrix. Again, assuming that <code class="literal">spam</code> is the positive class, the precision is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; prec &lt;- 152 / (152 + 4)</strong></span>
<span class="strong"><strong>&gt; prec</strong></span>
<span class="strong"><strong>[1] 0.974359</strong></span>
</pre></div><p>The recall is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; rec &lt;- 152 / (152 + 31)</strong></span>
<span class="strong"><strong>&gt; rec</strong></span>
<span class="strong"><strong>[1] 0.8306011</strong></span>
</pre></div><p>The <code class="literal">caret</code> package can be used to compute either of these measures from the vectors of predicted and actual classes. Precision uses the <code class="literal">posPredValue()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; posPredValue(sms_results$predict_type, sms_results$actual_type,</strong></span>
<span class="strong"><strong>               positive = "spam")</strong></span>
<span class="strong"><strong>[1] 0.974359</strong></span>
</pre></div><p>While recall uses the <code class="literal">sensitivity()</code> function that we used earlier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; sensitivity(sms_results$predict_type, sms_results$actual_type, </strong></span>
<span class="strong"><strong>              positive = "spam")</strong></span>
<span class="strong"><strong>[1] 0.8306011</strong></span>
</pre></div><p>Similar to the inherent trade-off between sensitivity and specificity, for most of the real-world problems, it is difficult to build a model with both high precision and high recall. It is easy to be <a id="id853" class="indexterm"/>precise if you target only the low-hanging fruit—the easy to classify examples. Similarly, it is easy for a model to have high recall by casting a very wide net, meaning that the model is overly aggressive in identifying the positive cases. In contrast, having both high precision and recall at the same time is very challenging. It is therefore important to test a variety of models in order to find the combination of precision and recall that will meet the needs of your project.</p></div><div class="section" title="The F-measure"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec59"/>The F-measure</h3></div></div></div><p>A measure of model <a id="id854" class="indexterm"/>performance that combines precision and recall into a single number is known as the <a id="id855" class="indexterm"/>
<span class="strong"><strong>F-measure</strong></span> (also sometimes called the <span class="strong"><strong>F<sub>1</sub> score</strong></span> or <span class="strong"><strong>F-score</strong></span>). The F-measure combines <a id="id856" class="indexterm"/>precision and <a id="id857" class="indexterm"/>recall using the <span class="strong"><strong>harmonic mean</strong></span>, a type of average that is used for rates of change. The harmonic mean is used rather than the common arithmetic mean since both precision and recall are expressed as proportions between zero and one, which can be interpreted as rates. The following is the formula for the F-measure:</p><div class="mediaobject"><img src="graphics/B03905_10_14.jpg" alt="The F-measure"/></div><p>To calculate the F-measure, use the precision and recall values computed previously:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; f &lt;- (2 * prec * rec) / (prec + rec)</strong></span>
<span class="strong"><strong>&gt; f</strong></span>
<span class="strong"><strong>[1] 0.8967552</strong></span>
</pre></div><p>This comes out exactly the same as using the counts from the confusion matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; f &lt;- (2 * 152) / (2 * 152 + 4 + 31)</strong></span>
<span class="strong"><strong>&gt; f</strong></span>
<span class="strong"><strong>[1] 0.8967552</strong></span>
</pre></div><p>Since the F-measure describes the model performance in a single number, it provides a convenient way to compare several models side by side. However, this assumes that equal weight should be assigned to precision and recall, an assumption that is not always valid. It is possible to calculate F-scores using different weights for precision and recall, but choosing the <a id="id858" class="indexterm"/>weights could be tricky at the best and arbitrary at worst. A better practice is to use measures such as the F-score in combination with methods that consider a model's strengths and weaknesses more globally, such as those described in the next section.</p></div></div><div class="section" title="Visualizing performance trade-offs"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec112"/>Visualizing performance trade-offs</h2></div></div></div><p>Visualizations are <a id="id859" class="indexterm"/>helpful to understand the performance of machine learning algorithms in greater detail. Where statistics such as sensitivity and specificity or precision and recall attempt to boil model performance down to a single number, visualizations depict how a learner performs across a wide range of conditions.</p><p>Because learning algorithms have different biases, it is possible that two models with similar accuracy could have drastic differences in how they achieve their accuracy. Some models may struggle with certain predictions that others make with ease, while breezing through the cases that others cannot get right. Visualizations provide a method to understand these trade-offs, by comparing learners side by side in a single chart.</p><p>The <code class="literal">ROCR</code> package provides an easy-to-use suite of functions for visualizing for visualizing the performance of classification models. It includes functions for computing large set of the most common performance measures and visualizations. The <code class="literal">ROCR</code> website at <a class="ulink" href="http://rocr.bioinf.mpi-sb.mpg.de/">http://rocr.bioinf.mpi-sb.mpg.de/</a> includes a list of the full set of features as well as several examples on visualization capabilities. Before continuing, install the package using the <code class="literal">install.packages("ROCR")</code> command.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip121"/>Tip</h3><p>For more information on the development of ROCR, see : Sing T, Sander O, Beerenwinkel N, Lengauer T. ROCR: visualizing classifier performance in R. <span class="emphasis"><em>Bioinformatics</em></span>. 2005; 21:3940-3941.</p></div></div><p>To create visualizations with <code class="literal">ROCR</code>, two vectors of data are needed. The first must contain the predicted class values, and the second must contain the estimated probability of the positive class. These are used to create a prediction object that can be examined with the plotting functions of <code class="literal">ROCR</code>.</p><p>The prediction object for the SMS classifier requires the classifier's estimated spam probabilities and the actual class labels. These are combined using the <code class="literal">prediction()</code> function in the following lines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(ROCR)</strong></span>
<span class="strong"><strong>&gt; pred &lt;- prediction(predictions = sms_results$prob_spam,</strong></span>
<span class="strong"><strong>                     labels = sms_results$actual_type)</strong></span>
</pre></div><p>Next, the <code class="literal">performance()</code> function will allow us to compute measures of performance from the <a id="id860" class="indexterm"/>
<code class="literal">prediction</code> object we just created, which can then be visualized using the R <code class="literal">plot()</code> function. Given these three steps, a large variety of useful visualizations can be created.</p><div class="section" title="ROC curves"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec60"/>ROC curves</h3></div></div></div><p>The <span class="strong"><strong>Receiver Operating Characteristic (ROC) curve</strong></span> is commonly used to examine the trade-off <a id="id861" class="indexterm"/>between the detection of true positives, while avoiding the false positives. As you might suspect from the name, ROC curves were developed by engineers in the field of communications. Around the time of World War II, radar and radio operators used ROC curves to measure a receiver's ability to discriminate between true signals and false alarms. The same technique is useful today to visualize the efficacy of machine learning models.</p><p>The characteristics of a typical ROC diagram are depicted in the following plot. Curves are defined on a plot with the proportion of true positives on the vertical axis and the proportion of false positives on the horizontal axis. Because these values are equivalent to sensitivity and (1 – specificity), respectively, the diagram is also known as a sensitivity/specificity plot:</p><div class="mediaobject"><img src="graphics/B03905_10_15.jpg" alt="ROC curves"/></div><p>The points comprising ROC curves indicate the true positive rate at varying false positive thresholds. To create the curves, a classifier's predictions are sorted by the model's estimated probability of the positive class, with the largest values first. Beginning at the origin, each prediction's impact on the true positive rate and false positive rate will result in a curve tracing vertically (for a correct prediction) or horizontally (for an incorrect prediction).</p><p>To illustrate this concept, three hypothetical classifiers are contrasted in the previous plot. First, the diagonal line from the bottom-left to the top-right corner of the diagram represents a <span class="strong"><strong>classifier with no predictive value</strong></span>. This type of classifier detects true positives and false positives at exactly the same rate, implying that the classifier cannot discriminate <a id="id862" class="indexterm"/>between the two. This is the baseline by which other classifiers may be judged. ROC curves falling close to this line indicate models that are not very useful. The <span class="strong"><strong>perfect classifier</strong></span> has a curve that passes through the point at a 100 percent true positive rate and 0 percent false positive rate. It is able to correctly identify all of the positives before it incorrectly classifies any negative result. Most real-world classifiers are similar to the test classifier and they fall somewhere in the zone between perfect and useless.</p><p>The closer the curve is to the perfect classifier, the better it is at identifying positive values. This can <a id="id863" class="indexterm"/>be measured using a statistic known as the <span class="strong"><strong>area under the ROC curve</strong></span> (abbreviated <span class="strong"><strong>AUC</strong></span>). The AUC treats the ROC diagram as a two-dimensional square and measures the total area under the ROC curve. AUC ranges from 0.5 (for a classifier with no predictive value) to 1.0 (for a perfect classifier). A convention to interpret AUC scores uses a system similar to academic letter grades:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>A</strong></span>: Outstanding = 0.9 to 1.0</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>B</strong></span>: Excellent/good = 0.8 to 0.9</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>C</strong></span>: Acceptable/fair = 0.7 to 0.8</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>D</strong></span>: Poor = 0.6 to 0.7</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>E</strong></span>: No discrimination = 0.5 to 0.6</li></ul></div><p>As with most scales similar to this, the levels may work better for some tasks than others; the categorization is somewhat subjective.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip122"/>Tip</h3><p>It's also worth noting that two ROC curves may be shaped very differently, yet have an identical AUC. For this reason, an AUC alone can be misleading. The best practice is to use AUC in combination with qualitative examination of the ROC curve.</p></div></div><p>Creating ROC curves <a id="id864" class="indexterm"/>with the <code class="literal">ROCR</code> package involves building a <code class="literal">performance</code> object from the <code class="literal">prediction</code> object we computed earlier. Since ROC curves plot true positive rates versus false positive rates, we simply call the <code class="literal">performance()</code> function while specifying the <code class="literal">tpr</code> and <code class="literal">fpr</code> measures, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; perf &lt;- performance(pred, measure = "tpr", x.measure = "fpr")</strong></span>
</pre></div><p>Using the <code class="literal">perf</code> object, we can visualize the ROC curve with R's <code class="literal">plot()</code> function. As shown in the following code lines, many of the standard parameters to adjust the visualization can be used, such as <code class="literal">main</code> (to add a title), <code class="literal">col</code> (to change the line color), and <code class="literal">lwd</code> (to adjust the line width):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; plot(perf, main = "ROC curve for SMS spam filter",</strong></span>
<span class="strong"><strong>       col = "blue", lwd = 3)</strong></span>
</pre></div><p>Although the <code class="literal">plot()</code> command is sufficient to create a valid ROC curve, it is helpful to add a reference line to indicate the performance of a classifier with no predictive value.</p><p>To plot such a line, we'll use the <code class="literal">abline()</code> function. This function can be used to specify a line in the slope-intercept form, where <code class="literal">a</code> is the intercept and <code class="literal">b</code> is the slope. Since we need an identity line that passes through the origin, we'll set the intercept to <code class="literal">a=0</code> and the slope to <code class="literal">b=1</code>, as shown in the following plot. The <code class="literal">lwd</code> parameter adjusts the line thickness, while the <code class="literal">lty</code> parameter adjusts the type of line. For example, <code class="literal">lty = 2</code> indicates a dashed line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; abline(a = 0, b = 1, lwd = 2, lty = 2)</strong></span>
</pre></div><p>The end result is an ROC plot with a dashed reference line:</p><div class="mediaobject"><img src="graphics/B03905_10_16.jpg" alt="ROC curves"/></div><p>Qualitatively, we can see that this ROC curve appears to occupy the space at the top-left corner of the diagram, which suggests that it is closer to a perfect classifier than the dashed line representing a useless classifier. To confirm this quantitatively, we can use the ROCR package to calculate the AUC. To do so, we first need to create another <code class="literal">performance</code> object, this time specifying <code class="literal">measure = "auc"</code> as shown in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; perf.auc &lt;- performance(pred, measure = "auc")</strong></span>
</pre></div><p>Since <code class="literal">perf.auc</code> is an R <a id="id865" class="indexterm"/>object (specifically known as an S4 object), we need to use a special type of notation to access the values stored within. S4 objects hold information in positions known as slots. The <code class="literal">str()</code> function can be used to see all of an object's slots:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(perf.auc)</strong></span>
<span class="strong"><strong>Formal class 'performance' [package "ROCR"] with 6 slots</strong></span>
<span class="strong"><strong>  ..@ x.name      : chr "None"</strong></span>
<span class="strong"><strong>  ..@ y.name      : chr "Area under the ROC curve"</strong></span>
<span class="strong"><strong>  ..@ alpha.name  : chr "none"</strong></span>
<span class="strong"><strong>  ..@ x.values    : list()</strong></span>
<span class="strong"><strong>  ..@ y.values    :List of 1</strong></span>
<span class="strong"><strong>  .. ..$ : num 0.984</strong></span>
<span class="strong"><strong>  ..@ alpha.values: list()</strong></span>
</pre></div><p>Notice that slots are prefixed with the <code class="literal">@</code> symbol. To access the AUC value, which is stored as a list in the <code class="literal">y.values</code> slot, we can use the <code class="literal">@</code> notation along with the <code class="literal">unlist()</code> function, which simplifies lists to a vector of numeric values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; unlist(perf.auc@y.values)</strong></span>
<span class="strong"><strong>[1] 0.9835862</strong></span>
</pre></div><p>The AUC for the SMS classifier is 0.98, which is extremely high. But how do we know whether the model is just as likely to perform well for another dataset? In order to answer such questions, we need to have a better understanding of how far we can extrapolate a model's predictions beyond the test data.</p></div></div></div></div>
<div class="section" title="Estimating future performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec45"/>Estimating future performance</h1></div></div></div><p>Some R machine <a id="id866" class="indexterm"/>learning packages present confusion matrices and performance measures during the model building process. The purpose of these statistics is to provide insight about the <a id="id867" class="indexterm"/>model's <span class="strong"><strong>resubstitution error</strong></span>, which occurs when the training data is incorrectly predicted in spite of the model being built directly from this data. This information can be used as a rough diagnostic to identify obviously poor performers.</p><p>The resubstitution error is not a very useful marker of future performance. For example, a model that used rote memorization to perfectly classify every training instance with zero resubstitution error would be unable to generalize its predictions to data it has never seen before. For this reason, the error rate on the training data can be extremely optimistic about a model's future performance.</p><p>Instead of relying on resubstitution error, a better practice is to evaluate a model's performance on data it has not yet seen. We used this approach in previous chapters when we split the available data into a set for training and a set for testing. In some cases, however, it is not always ideal to create training and test datasets. For instance, in a situation where you have only a small pool of data, you might not want to reduce the sample any further.</p><p>Fortunately, there are other ways to estimate a model's performance on unseen data. The <code class="literal">caret</code> package we used to calculate performance measures also offers a number of functions to estimate future performance. If you are following the R code examples and haven't already installed the <code class="literal">caret</code> package, please do so. You will also need to load the package to the R session, using the <code class="literal">library(caret)</code> command.</p><div class="section" title="The holdout method"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec113"/>The holdout method</h2></div></div></div><p>The <a id="id868" class="indexterm"/>procedure of partitioning data into training and test datasets that we used in previous chapters is known as the <a id="id869" class="indexterm"/>
<span class="strong"><strong>holdout method</strong></span>. As shown in the following diagram, the <span class="strong"><strong>training dataset </strong></span>is used to generate the model, which is then applied to the <span class="strong"><strong>test dataset </strong></span>to generate predictions for evaluation. Typically, about one-third of the data is held out for testing, and two-thirds is used for training, but this proportion can vary depending on the amount of available data. To ensure that the training and test data do not have systematic differences, their examples are randomly divided into the two groups.</p><div class="mediaobject"><img src="graphics/B03905_10_17.jpg" alt="The holdout method"/></div><p>For the holdout method to result in a truly accurate estimate of the future performance, at no time should the performance on the test dataset be allowed to influence the model. It is easy to unknowingly violate this rule by choosing the best model based upon the results of repeated testing. For example, suppose we built several models on the training data, and selected the one with the highest accuracy on the test data. Because we have cherry-picked the best result, the test performance is not an unbiased measure of the performance on unseen data.</p><p>To avoid this <a id="id870" class="indexterm"/>problem, it is better to divide the original data so that in addition to the training datasets and the test <a id="id871" class="indexterm"/>datasets, a <span class="strong"><strong>validation dataset</strong></span> is available. The validation dataset would be used for iterating and refining the model or models chosen, leaving the test dataset to be used only once as a final step to report an estimated error rate for future predictions. A typical split between training, test, and validation would be 50 percent, 25 percent, and 25 percent, respectively.</p><div class="mediaobject"><img src="graphics/B03905_10_18.jpg" alt="The holdout method"/></div><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip123"/>Tip</h3><p>A keen reader will note that holdout test data was used in the previous chapters to both evaluate models and improve model performance. This was done for illustrative purposes, but it would indeed violate the rule as stated previously. Consequently, the model performance statistics shown were not valid estimates of future performance on unseen data and the process could have been more accurately termed validation.</p></div></div><p>A simple method to create holdout samples uses random number generators to assign records to partitions. This technique was first used in <a class="link" href="ch05.html" title="Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules">Chapter 5</a>, <span class="emphasis"><em>Divide and Conquer – Classification Using Decision Trees and Rules</em></span> to create training and test datasets.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip124"/>Tip</h3><p>If you'd like to follow along with the following examples, download the <code class="literal">credit.csv</code> dataset from the Packt Publishing website, and load to a data frame using the <code class="literal">credit &lt;- read.csv("credit.csv")</code> command.</p></div></div><p>Suppose we have a <a id="id872" class="indexterm"/>data frame named credit with 1000 rows of data. We can divide it into three partitions as follows. First, we create a vector of randomly ordered row IDs from 1 to 1000 using the <code class="literal">runif()</code> function, which by default generates a specified number of random values between 0 and 1. The <code class="literal">runif()</code> function gets its name from the random uniform distribution, which was discussed in <a class="link" href="ch02.html" title="Chapter 2. Managing and Understanding Data">Chapter 2</a>, <span class="emphasis"><em>Managing and Understanding Data</em></span>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; random_ids &lt;- order(runif(1000))</strong></span>
</pre></div><p>The <code class="literal">order()</code> used here returns a vector indicating the rank order of the 1,000 random numbers. For example, <code class="literal">order(c(0.5, 0.25, 0.75, 0.1))</code> returns the sequence <code class="literal">4 2 1 3</code> because the smallest number (0.1) appears fourth, the second smallest (0.25) appears second, and so on.</p><p>We can use the resulting random IDs to divide the <code class="literal">credit</code> data frame into 500, 250, and 250 records comprising the training, validation, and test datasets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_train &lt;- credit[random_ids[1:500], ]</strong></span>
<span class="strong"><strong>&gt; credit_validate &lt;- credit[random_ids[501:750], ]</strong></span>
<span class="strong"><strong>&gt; credit_test &lt;- credit[random_ids[751:1000], ]</strong></span>
</pre></div><p>One problem with holdout sampling is that each partition may have a larger or smaller proportion of some classes. In certain cases, particularly those in which a class is a very small proportion of the dataset, this can lead a class to be omitted from the training dataset. This is a significant problem, because the model will not be able to learn this class.</p><p>In order to reduce the chance of this occurring, a technique called <span class="strong"><strong>stratified random sampling</strong></span> can be used. Although in the long run a random sample should contain roughly the same proportion of each class value as the full dataset, stratified random sampling guarantees that the random partitions have nearly the same proportion of each class as the full dataset, even when some classes are small.</p><p>The <code class="literal">caret</code> package provides a <code class="literal">createDataPartition()</code> function that will create partitions based on stratified holdout sampling. The code to create a stratified sample of training and test data for the <code class="literal">credit</code> dataset is shown in the following commands. To use the function, a vector of the class values must be specified (here, <code class="literal">default</code> refers to whether a loan went into default) in addition to a parameter <code class="literal">p</code>, which specifies the proportion of instances to be included in the partition. The <code class="literal">list = FALSE</code> parameter prevents the result from being stored in the list format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; in_train &lt;- createDataPartition(credit$default, p = 0.75,</strong></span>
<span class="strong"><strong>    list = FALSE)</strong></span>
<span class="strong"><strong>&gt; credit_train &lt;- credit[in_train, ]</strong></span>
<span class="strong"><strong>&gt; credit_test &lt;- credit[-in_train, ]</strong></span>
</pre></div><p>The <code class="literal">in_train</code> vector indicates row numbers included in the training sample. We can use these row numbers to select examples for the <code class="literal">credit_train</code> data frame. Similarly, by using a negative symbol, we can use the rows not found in the <code class="literal">in_train</code> vector for the <code class="literal">credit_test</code> dataset.</p><p>Although it distributes <a id="id873" class="indexterm"/>the classes evenly, stratified sampling does not guarantee other types of representativeness. Some samples may have too many or few difficult cases, easy-to-predict cases, or outliers. This is especially true for smaller datasets, which may not have a large enough portion of such cases to be divided among training and test sets.</p><p>In addition to potentially biased samples, another problem with the holdout method is that substantial portions of <a id="id874" class="indexterm"/>data must be reserved to test and validate the model. Since these data cannot be used to train the model until its performance has been measured, the performance estimates are likely to be overly conservative.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip125"/>Tip</h3><p>Since models trained on larger datasets generally perform better, a common practice is to retrain the model on the full set of data (that is, training plus test and validation) after a final model has been selected and evaluated.</p></div></div><p>A technique called <span class="strong"><strong>repeated holdout</strong></span> is sometimes used to mitigate the problems of randomly composed training datasets. The repeated holdout method is a special case of the holdout method that uses the average result from several random holdout samples to evaluate a model's performance. As multiple holdout samples are used, it is less likely that the model is trained or tested on nonrepresentative data. We'll expand on this idea in the next section.</p><div class="section" title="Cross-validation"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec61"/>Cross-validation</h3></div></div></div><p>The repeated holdout is the basis of a technique known as <span class="strong"><strong>k-fold cross-validation</strong></span> (or <span class="strong"><strong>k-fold CV</strong></span>), which has become the industry standard for estimating model performance. <a id="id875" class="indexterm"/>Rather than taking repeated random samples that could potentially use the same <a id="id876" class="indexterm"/>record more than once, k-fold CV randomly divides the data into <span class="emphasis"><em>k</em></span> to completely separate random partitions called <span class="strong"><strong>folds</strong></span>.</p><p>Although <span class="emphasis"><em>k</em></span> can be set to any number, by far, the most common convention is to use <span class="strong"><strong>10-fold cross-validation </strong></span>(10-fold CV). Why 10 folds? The reason is that the empirical evidence suggests that there is little added <a id="id877" class="indexterm"/>benefit in using a greater number. For each of the 10 folds (each comprising 10 percent of the total data), a machine learning model is built on the remaining 90 percent of data. The fold's matching 10 percent sample is then used for model evaluation. After the process of training and evaluating the model has occurred for 10 times (with 10 different training/testing combinations), the average performance across <a id="id878" class="indexterm"/>all the folds is reported.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip126"/>Tip</h3><p>An extreme case of <a id="id879" class="indexterm"/>k-fold CV is the <span class="strong"><strong>leave-one-out method</strong></span>, which performs k-fold CV using a fold for each of the data's examples. This ensures that the greatest amount of data is used to train the model. Although this may seem useful, it is so computationally expensive that it is rarely used in practice.</p></div></div><p>Datasets for cross-validation can be created using the <code class="literal">createFolds()</code> function in the <code class="literal">caret</code> package. Similar to the stratified random holdout sampling, this function will attempt to maintain the same class <a id="id880" class="indexterm"/>balance in each of the folds as in the original dataset. The following is the command to create 10 folds:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; folds &lt;- createFolds(credit$default, k = 10)</strong></span>
</pre></div><p>The result of the <code class="literal">createFolds()</code> function is a list of vectors storing the row numbers for each of the requested <code class="literal">k = 10</code> folds. We can peek at the contents, using <code class="literal">str()</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(folds)</strong></span>
<span class="strong"><strong>List of 10</strong></span>
<span class="strong"><strong> $ Fold01: int [1:100] 1 5 12 13 19 21 25 32 36 38 ...</strong></span>
<span class="strong"><strong> $ Fold02: int [1:100] 16 49 78 81 84 93 105 108 128 134 ...</strong></span>
<span class="strong"><strong> $ Fold03: int [1:100] 15 48 60 67 76 91 102 109 117 123 ...</strong></span>
<span class="strong"><strong> $ Fold04: int [1:100] 24 28 59 64 75 85 95 97 99 104 ...</strong></span>
<span class="strong"><strong> $ Fold05: int [1:100] 9 10 23 27 29 34 37 39 53 61 ...</strong></span>
<span class="strong"><strong> $ Fold06: int [1:100] 4 8 41 55 58 103 118 121 144 146 ...</strong></span>
<span class="strong"><strong> $ Fold07: int [1:100] 2 3 7 11 14 33 40 45 51 57 ...</strong></span>
<span class="strong"><strong> $ Fold08: int [1:100] 17 30 35 52 70 107 113 129 133 137 ...</strong></span>
<span class="strong"><strong> $ Fold09: int [1:100] 6 20 26 31 42 44 46 63 79 101 ...</strong></span>
<span class="strong"><strong> $ Fold10: int [1:100] 18 22 43 50 68 77 80 88 106 111 ...</strong></span>
</pre></div><p>Here, we see that the first fold is named <code class="literal">Fold01</code> and stores <code class="literal">100</code> integers, indicating the 100 rows in the credit data frame for the first fold. To create training and test datasets to build and evaluate a model, an additional step is needed. The following commands show how to create data for the first fold. We'll assign the selected 10 percent to the test dataset, and use the negative symbol to assign the remaining 90 percent to the training dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit01_test &lt;- credit[folds$Fold01, ]</strong></span>
<span class="strong"><strong>&gt; credit01_train &lt;- credit[-folds$Fold01, ]</strong></span>
</pre></div><p>To perform the full 10-fold CV, this step would need to be repeated a total of 10 times; building a model and then calculating the model's performance each time. At the end, the performance measures would be averaged to obtain the overall performance. Thankfully, we can automate this task by applying several of the techniques we've learned earlier.</p><p>To demonstrate the <a id="id881" class="indexterm"/>process, we'll estimate the kappa statistic for a C5.0 decision tree model of the credit data using 10-fold CV. First, we need to load some R packages: <code class="literal">caret</code> (to create the folds), <code class="literal">C50</code> (for the decision tree), and <code class="literal">irr</code> (to calculate kappa). The latter two packages were <a id="id882" class="indexterm"/>chosen for illustrative purposes; if you desire, you can use a different model or a different <a id="id883" class="indexterm"/>performance measure along with the same series of steps.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; library(C50)</strong></span>
<span class="strong"><strong>&gt; library(irr)</strong></span>
</pre></div><p>Next, we'll create a list of 10 folds as we have done previously. The <code class="literal">set.seed()</code> function is used here to ensure that the results are consistent if the same code is run again:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set.seed(123)</strong></span>
<span class="strong"><strong>&gt; folds &lt;- createFolds(credit$default, k = 10)</strong></span>
</pre></div><p>Finally, we will apply a series of identical steps to the list of folds using the <code class="literal">lapply()</code> function. As shown in the following code, because there is no existing function that does exactly what we need, we must define our own function to pass to <code class="literal">lapply()</code>. Our custom function divides the credit data frame into training and test data, builds a decision tree using the <code class="literal">C5.0()</code> function on the training data, generates a set of predictions from the test data, and compares the predicted and actual values using the <code class="literal">kappa2()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; cv_results &lt;- lapply(folds, function(x) {</strong></span>
<span class="strong"><strong>    credit_train &lt;- credit[-x, ]</strong></span>
<span class="strong"><strong>    credit_test &lt;- credit[x, ]</strong></span>
<span class="strong"><strong>    credit_model &lt;- C5.0(default ~ ., data = credit_train)</strong></span>
<span class="strong"><strong>    credit_pred &lt;- predict(credit_model, credit_test)</strong></span>
<span class="strong"><strong>    credit_actual &lt;- credit_test$default</strong></span>
<span class="strong"><strong>    kappa &lt;- kappa2(data.frame(credit_actual, credit_pred))$value</strong></span>
<span class="strong"><strong>    return(kappa)</strong></span>
<span class="strong"><strong>  })</strong></span>
</pre></div><p>The resulting <a id="id884" class="indexterm"/>kappa statistics are <a id="id885" class="indexterm"/>compiled into a list stored in the <code class="literal">cv_results</code> object, which we can examine using <code class="literal">str()</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(cv_results)</strong></span>
<span class="strong"><strong>List of 10</strong></span>
<span class="strong"><strong> $ Fold01: num 0.343</strong></span>
<span class="strong"><strong> $ Fold02: num 0.255</strong></span>
<span class="strong"><strong> $ Fold03: num 0.109</strong></span>
<span class="strong"><strong> $ Fold04: num 0.107</strong></span>
<span class="strong"><strong> $ Fold05: num 0.338</strong></span>
<span class="strong"><strong> $ Fold06: num 0.474</strong></span>
<span class="strong"><strong> $ Fold07: num 0.245</strong></span>
<span class="strong"><strong> $ Fold08: num 0.0365</strong></span>
<span class="strong"><strong> $ Fold09: num 0.425</strong></span>
<span class="strong"><strong> $ Fold10: num 0.505</strong></span>
</pre></div><p>There's just one more step remaining in the 10-fold CV process: we must calculate the average of these 10 values. Although you will be tempted to type <code class="literal">mean(cv_results)</code>, because <code class="literal">cv_results</code> is not a numeric vector, the result would be an error. Instead, use the <code class="literal">unlist()</code> function, which eliminates the list structure, and reduces <code class="literal">cv_results</code> to a numeric vector. From here, we can calculate the mean kappa as expected:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mean(unlist(cv_results))</strong></span>
<span class="strong"><strong>[1] 0.283796</strong></span>
</pre></div><p>This kappa statistic is <a id="id886" class="indexterm"/>fairly low, corresponding to "fair" on the interpretation scale, which suggests that the credit <a id="id887" class="indexterm"/>scoring model performs only marginally better than random chance. In the next chapter, we'll examine automated methods based on 10-fold CV that can assist us in <a id="id888" class="indexterm"/>improving the performance of this model.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip127"/>Tip</h3><p>Perhaps the current gold standard method to reliably estimate model performance is <span class="strong"><strong>repeated k-fold CV</strong></span>. As you might guess from the name, this involves repeatedly applying k-fold CV and averaging the results. A common strategy is to perform 10-fold CV ten times. Although it is computationally intensive, it provides a very robust estimate.</p></div></div></div><div class="section" title="Bootstrap sampling"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec62"/>Bootstrap sampling</h3></div></div></div><p>A slightly <a id="id889" class="indexterm"/>less frequently used alternative to k-fold CV is known as <span class="strong"><strong>bootstrap sampling</strong></span>, the <span class="strong"><strong>bootstrap</strong></span> or <span class="strong"><strong>bootstrapping</strong></span> for short. Generally speaking, these refer to the statistical <a id="id890" class="indexterm"/>methods of using random samples of data to estimate the properties of a larger set. When this principle is applied to machine learning model performance, it implies the creation of several randomly selected training and test datasets, which are then used to estimate performance statistics. The results from the various random datasets are then averaged to obtain a final estimate of future performance.</p><p>So, what makes this procedure different from k-fold CV? Whereas cross-validation divides the data into separate partitions in which each example can appear only once, the bootstrap allows examples to be selected multiple times through a process of <span class="strong"><strong>sampling with replacement</strong></span>. This means that from the original dataset of <span class="emphasis"><em>n</em></span> examples, the bootstrap procedure will create one or more new training datasets that will also contain <span class="emphasis"><em>n</em></span> examples, some of which are repeated. The corresponding test datasets are then constructed from the set of examples that were not selected for the respective training datasets.</p><p>Using sampling with replacement as described previously, the probability that any given instance is included in the training dataset is 63.2 percent. Consequently, the probability of any instance being in the test dataset is 36.8 percent. In other words, the training data represents only 63.2 percent of available examples, some of which are repeated. In contrast to 10-fold CV, which uses 90 percent of the examples for training, the bootstrap sample is less representative of the full dataset.</p><p>Because a model trained on only 63.2 percent of the training data is likely to perform worse than a model trained on a larger training set, the bootstrap's performance estimates may be substantially lower than what would be obtained when the model is later trained on the <a id="id891" class="indexterm"/>full dataset. A special case of bootstrapping known as the <span class="strong"><strong>0.632 bootstrap</strong></span> accounts for this by calculating the final performance measure as a function of performance on <a id="id892" class="indexterm"/>both the training data (which is overly optimistic) and the test data (which is overly pessimistic). The final error rate is then estimated as:</p><div class="mediaobject"><img src="graphics/B03905_10_19.jpg" alt="Bootstrap sampling"/></div><p>One advantage of bootstrap over cross-validation is that it tends to work better with very small datasets. Additionally, bootstrap sampling has applications beyond performance measurement. In particular, in the next chapter we'll learn how the principles of bootstrap sampling can be used to improve model performance.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec46"/>Summary</h1></div></div></div><p>This chapter presented a number of the most common measures and techniques for evaluating the performance of machine learning classification models. Although accuracy provides a simple method to examine how often a model is correct, this can be misleading in the case of rare events because the real-life cost of such events may be inversely proportional to how frequently they appear.</p><p>A number of measures based on confusion matrices better capture the balance among the costs of various types of errors. Closely examining the tradeoffs between sensitivity and specificity, or precision and recall can be a useful tool for thinking about the implications of errors in the real world. Visualizations such as the ROC curve are also helpful to this end.</p><p>It is also worth mentioning that sometimes the best measure of a model's performance is to consider how well it meets, or doesn't meet, other objectives. For instance, you may need to explain a model's logic in simple language, which would eliminate some models from consideration. Additionally, even if it performs very well, a model that is too slow or difficult to scale to a production environment is completely useless.</p><p>An obvious extension of measuring performance is to identify automated ways to find the best models for a particular task. In the next chapter, we will build upon our work so far to investigate ways to make smarter models by systematically iterating, refining, and combining learning algorithms.</p></div></body></html>
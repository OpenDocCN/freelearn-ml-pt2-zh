<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Create a Custom Newsfeed</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>I read</span> <em>a lot</em><span>. Some might even say compulsively</span><span>.</span> <span>I've been known to consume more than a hundred articles on some days. But despite this, I frequently find myself searching for more to read. I suffer from this sneaking suspicion that I have missed something interesting, and will forever suffer a gap in my knowledge!</span></p>
<p><span class="HeaderFooterPACKT">If you suffer from similar symptoms, fear not, because in this chapter, I'm going to reveal one simple trick to finding all the articles you want to read without having to dig through the dozens that you don't.</span></p>
<p><span class="HeaderFooterPACKT">By the end of this chapter, you'll have learned how to build a system that understands your taste in news, and will send you a personally tailored newsletter each day.</span></p>
<p><span class="HeaderFooterPACKT">Here's what we'll cover in this chapter:</span></p>
<ul>
<li>Creating a supervised training set with the Pocket app</li>
<li>Leveraging the Pocket API to retrieve stories</li>
<li>Using the Embedly API to extract story bodies</li>
<li>Natural language processing basics</li>
<li>Support vector machines</li>
<li>IFTTT integration with RSS feeds and Google Sheets</li>
<li>Setting up a daily personal newsletter</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a supervised training set with Pocket</h1>
                </header>
            
            <article>
                
<p>Before we can create a model of our taste in news articles, we need training data. This training data will be fed into our model in order to teach it to discriminate between the articles we'd be interested in and those we would not. To build this corpus, we will need to annotate a large number of articles to correspond to these interests. We'll label each article either <kbd>y</kbd> or <kbd>n</kbd>, indicating whether it is the type of article we would want to have sent to us in our daily digest or not.</p>
<p>To simplify this process, we'll use the Pocket app. Pocket is an application that allows you to save stories to read later. You simply install the browser extension, and then click on the Pocket icon in your browser's toolbar when you wish to save a story. The article is saved to your personal repository. One of the great features of Pocket for our purposes is the ability to save the article with a tag of your choosing. We'll use this to mark interesting articles as <kbd>y</kbd> and non-interesting articles as <kbd>n</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the Pocket Chrome Extension</h1>
                </header>
            
            <article>
                
<p>I'm using Google Chrome for this, but other browsers should work similarly. Follow the steps for installing the Pocket Chrome Extention:</p>
<ol>
<li>For Chrome, go to the Google app store and look for the <span class="packt_screen">Extensions</span> section:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1193 image-border" src="assets/bec36363-4072-441d-9ee3-5e8d34d870e2.png" style="width:56.75em;height:20.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Pocket Chrome Extention</span></div>
<ol start="2">
<li>Click on <span class="packt_screen">Add to Chrome</span>. If you already have an account, log in, and, if not, go ahead and sign up (it's free).</li>
<li>Once that is complete, you should see the Pocket icon in the upper-right corner of your browser.</li>
<li>It will be grayed out, but once there is an article you wish to save, you can click it. It will turn red once the article has been saved:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1194 image-border" src="assets/0600e60d-722d-40bc-ae27-2ac2aac6598e.png" style="width:68.42em;height:21.92em;"/></p>
<p>The saved page can be seen as below:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f0eba099-38cb-46dc-b437-6676aa2cbf4d.png" style="width:77.58em;height:32.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The New York Times saved page</div>
<p>Now the fun part! As you go through your day, start saving articles you'd like to read, as well as those you wouldn't. Tag the interesting ones with <kbd>y</kbd>, and the non-interesting ones with <kbd>n</kbd>. This is going to take some work. Your end results will only be as good as your training set, so you're going to need to do this for hundreds of articles. If you forget to tag an article when you save it, you can always go to the site, <span class="URLPACKT"><a href="https://getpocket.com/">http://www.get.pocket.com</a>,</span> to tag it there.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Pocket API to retrieve stories</h1>
                </header>
            
            <article>
                
<p>Now that you've diligently saved your articles to Pocket, the next step is to retrieve them. To accomplish this, we'll use the Pocket API. You can sign up for an account at <a href="https://getpocket.com/developer/apps/new" target="_blank"><span class="URLPACKT">https://getpocket.com/developer/apps/new</span></a>. Follow the steps to achieve that:</p>
<ol>
<li>Click on <span class="packt_screen">Create a New App</span> in the upper-left corner and fill in the details to get your API key.</li>
</ol>
<ol start="2">
<li>Make sure to click all of the permissions so that you can add, change, and retrieve articles:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d70f782e-626c-411d-b2d7-111535a45741.png" style="width:59.92em;height:55.17em;"/></p>
<p class="mce-root"/>
<ol start="3">
<li>Once you have that filled in and submitted, you will receive your <strong>consumer key</strong>.</li>
<li>You can find that in the upper-left corner, under <span class="packt_screen">My Apps</span>. It will look like the following screenshot, but obviously with a real key:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2f8553e5-5a88-4939-8c58-1e115884f87e.png"/></p>
<ol start="5">
<li>Once that is set, you are ready to move on to the next step, which is to set up authorizations. We'll do that now.</li>
</ol>
<ol start="6">
<li>It requires you to input your consumer key and a redirect URL. The redirect URL can be anything. Here, I have used my Twitter account:</li>
</ol>
<pre style="padding-left: 60px"><strong>import requests 
import pandas as pd 
import json 
pd.set_option('display.max_colwidth', 200) 
 
CONSUMER_KEY = 'enter_your_consumer_key_here 
 
auth_params = {'consumer_key': CONSUMER_KEY, 'redirect_uri': 'https://www.twitter.com/acombs'} 
 
tkn = requests.post('https://getpocket.com/v3/oauth/request', data=auth_params) 
 
tkn.text</strong> </pre>
<p style="padding-left: 60px"><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e651dd33-b0ae-439a-8d0e-512fcdacdde3.png" style="width:18.75em;height:2.08em;"/></p>
<ol start="7">
<li>The output will have the code you'll need for the next step. Place the following in your browser bar:</li>
</ol>
<p style="padding-left: 60px"><a href="https://getpocket.com/auth/authorize?request_token=some_long_access_code&amp;redirect_uri=https%3A//www.twitter.com/acombs" target="_blank"><span class="URLPACKT">https://getpocket.com/auth/authorize?request_token=some_long_access_code&amp;amp;redirect_uri=https%3A//www.twitter.com/acombs</span></a></p>
<ol start="8">
<li>If you change the redirect URL to one of your own, make sure to URL encode it (that's the <kbd>%3A</kbd> type stuff you see in the preceding URL).</li>
<li>At this point, you should be presented with an authorization screen. Go ahead and approve it, and then we can move on to the next step:</li>
</ol>
<pre style="padding-left: 60px"><strong># below we parse out the access code from the tkn.text string 
ACCESS_CODE = tkn.text.split('=')[1] 
 
usr_params = {'consumer_key': CONSUMER_KEY, 'code': ACCESS_CODE} 
 
usr = requests.post('https://getpocket.com/v3/oauth/authorize', data=usr_params) 
 
usr.text </strong></pre>
<p style="padding-left: 60px"><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4c276fc-3893-4910-b350-f332d3d02864.png"/></p>
<ol start="10">
<li>We'll use the output code here, to move on to retrieving the stories. First, we retrieve the stories tagged <kbd>n</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong># below we parse out the access token from the usr.text string 
ACCESS_TOKEN = usr.text.split('=')[1].split('&amp;amp;')[0] 
 
no_params = {'consumer_key': CONSUMER_KEY, 
'access_token': ACCESS_TOKEN, 
'tag': 'n'} 
 
no_result = requests.post('https://getpocket.com/v3/get', data=no_params) 
 
no_result.text</strong> </pre>
<p style="padding-left: 60px"><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1195 image-border" src="assets/0ed35aed-75d2-4df9-8091-fc87965664d3.png" style="width:124.92em;height:15.75em;"/></p>
<p style="padding-left: 60px">You'll notice that we have a long JSON string on all the articles that we tagged <kbd>n</kbd>. There are several keys in this, but we are really only interested in the URL at this point.</p>
<ol start="11">
<li>We'll go ahead and create a list of all the URLs from this:</li>
</ol>
<pre style="padding-left: 60px"><strong>no_jf = json.loads(no_result.text) 
no_jd = no_jf['list'] 
 
no_urls=[] 
for i in no_jd.values(): 
    no_urls.append(i.get('resolved_url')) 
 
</strong><strong>no_urls</strong> </pre>
<p class="mce-root"/>
<p style="padding-left: 60px"><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1196 image-border" src="assets/bac52869-eb17-41e6-99f6-e15668064bfd.png" style="width:56.17em;height:9.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">List of URLs</div>
<ol start="12">
<li>This list contains all the URLs of stories we aren't interested in. Let's now put that in a DataFrame and tag it as such:</li>
</ol>
<pre style="padding-left: 60px"><strong>no_uf = pd.DataFrame(no_urls, columns=['urls']) 
no_uf = no_uf.assign(wanted = lambda x: 'n') 
 
</strong><strong>no_uf</strong> </pre>
<p style="padding-left: 60px"><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7b330ba0-fb54-44e7-b899-a03304214597.png" style="width:54.67em;height:16.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Tagging the URLs</div>
<ol start="13">
<li>Now we're all set with the unwanted stories. Let's do the same thing with those stories we are interested in:</li>
</ol>
<pre style="padding-left: 60px"><strong>yes_params = {'consumer_key': CONSUMER_KEY, 
'access_token': ACCESS_TOKEN, 
'tag': 'y'} 
yes_result = requests.post('https://getpocket.com/v3/get', data=yes_params) 
 
yes_jf = json.loads(yes_result.text) 
yes_jd = yes_jf['list'] 
 
yes_urls=[] 
for i in yes_jd.values(): 
    yes_urls.append(i.get('resolved_url')) 
 
yes_uf = pd.DataFrame(yes_urls, columns=['urls']) 
yes_uf = yes_uf.assign(wanted = lambda x: 'y') 
 
yes_uf</strong> </pre>
<p style="padding-left: 60px"><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e0cf64cd-f8cc-4071-9d3b-ee31e97bdd51.png" style="width:62.17em;height:18.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Tagging the URLs of stories we are interested in</div>
<ol start="14">
<li>Now that we have both types of stories for our training data, let's join them together into a single DataFrame:</li>
</ol>
<pre style="padding-left: 60px"><strong>df = pd.concat([yes_uf, no_uf]) 
 
df.dropna(inplace=True) 
 
df</strong> </pre>
<p class="mce-root"/>
<p style="padding-left: 60px"><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7a12a25e-1a50-471b-b1af-6836de25339a.png" style="width:65.08em;height:20.25em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Joining the URLs- both interested and not interested</div>
<p>Now that we're set with all our URLs and their corresponding tags in a single frame, we'll move on to downloading the HTML for each article. We'll use another free service for this, called Embedly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Embedly API to download story bodies</h1>
                </header>
            
            <article>
                
<p>We have all the URLs for our stories, but, unfortunately, this isn't enough to train on; we'll need the full article body. This in itself could become a huge challenge if we want to roll our own scraper, especially if we are going to be pulling stories from dozens of sites. We would need to write code to target the article body while carefully avoiding all the other site gunk that surrounds it. Fortunately, as far as we are concerned, there are a number of free services that will do this for us. I'm going to be using Embedly to do this, but there are a number of other services that you could use instead.</p>
<p>The first step is to sign up for Embedly API access. You can do that at <a href="https://app.embed.ly/signup" target="_blank"><span class="URLPACKT">https://app.embed.ly/signup</span></a>. It is a straightforward process. Once you confirm your registration, you will receive an API key. That's really all you'll need. You'll just use that key in your HTTP request. Let's do that now:</p>
<pre><strong>import urllib 
 
EMBEDLY_KEY = 'your_embedly_api_key_here' 
 
def get_html(x): 
    try: 
        qurl = urllib.parse.quote(x) 
        rhtml = requests.get('https://api.embedly.com/1/extract?url=' + qurl + '&amp;amp;key=' + EMBEDLY_KEY) 
        ctnt = json.loads(rhtml.text).get('content') 
    except: 
        return None 
    return ctnt</strong> </pre>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6b4b9a5b-6e6e-4345-8a5c-766a71a49269.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">HTTP requests</div>
<p>And with that, we have the HTML of each story.</p>
<p>Since the content is embedded in HTML markup, and we want to feed plain text into our model, we'll use a parser to strip out the markup tags:</p>
<pre><strong>from bs4 import BeautifulSoup 
 
def get_text(x): 
    soup = BeautifulSoup(x, 'html5lib') 
    text = soup.get_text() 
    return text 
 
df.loc[:,'text'] = df['html'].map(get_text) 
 
df</strong> </pre>
<p class="mce-root"/>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fb4aa1e9-f5f4-4a26-923c-b4cfa49d89e4.png"/></p>
<p>And with that, we have our training set ready. We can now move on to a discussion of how to transform our text into something that a model can work with.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basics of Natural Language Processing</h1>
                </header>
            
            <article>
                
<p>If machine learning models only operate on numerical data, how can we transform our text into a numerical representation? That is exactly the focus of <strong>Natural Language Processing</strong> (<strong>NLP</strong>). Let's take a brief look at how this is done.</p>
<p>We'll begin with a small corpus of three sentences:</p>
<ol>
<li>The new kitten played with the other kittens</li>
<li>She ate lunch</li>
<li>She loved her kitten</li>
</ol>
<p>We'll first convert our corpus into a <strong>bag-of-words</strong> (<strong>BOW</strong>) representation. We'll skip preprocessing for now. Converting our corpus into a BOW representation involves taking each word and its count to create what's called a <strong>term-document matrix</strong>. In a term-document matrix, each unique word is assigned to a column, and each document is assigned to a row. At the intersection of the two is the count:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Sr. no.</strong> </p>
</td>
<td>
<p><strong>the</strong></p>
</td>
<td>
<p><strong>new</strong></p>
</td>
<td>
<p><strong>kitten</strong></p>
</td>
<td>
<p><strong>played</strong></p>
</td>
<td>
<p><strong>with</strong></p>
</td>
<td>
<p><strong>other</strong></p>
</td>
<td>
<p><strong>kittens</strong></p>
</td>
<td>
<p><strong>she</strong></p>
</td>
<td>
<p><strong>ate</strong></p>
</td>
<td>
<p><strong>lunch</strong></p>
</td>
<td>
<p><strong>loved</strong></p>
</td>
<td>
<p><strong>her</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p>Notice that, for these three short sentences, we already have 12 features. As you might imagine, if we were dealing with actual documents, such as news articles or even books, the number of features would explode into the hundreds of thousands. To mitigate this explosion, we can take a number of steps to remove features that add little to no informational value to our analysis.</p>
<p>The first step we can take is to remove <strong>stop words</strong>. These are words that are so common that they typically tell you nothing about the content of the document. Common examples of English stop words are <em>the</em>, <em>is</em>, <em>at</em>, <em>which</em>, and <em>on</em>. We'll remove those, and recompute our term-document matrix:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Sr. no.</strong></td>
<td>
<p><strong>new</strong></p>
</td>
<td>
<p><strong>kitten</strong></p>
</td>
<td>
<p><strong>played</strong></p>
</td>
<td>
<p><strong>kittens</strong></p>
</td>
<td>
<p><strong>ate</strong></p>
</td>
<td>
<p><strong>lunch</strong></p>
</td>
<td>
<p><strong>loved</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As you can see, the number of features was reduced from 12 to 7. This is great, but we can take it even further. We can perform <strong>stemming</strong> or <strong>lemmatization</strong> to reduce the features <span>further</span>. Notice that in our matrix, we have both <em>kitten</em> and <em>kittens</em>. By using stemming or lemmatization, we can consolidate that into just <em>kitten</em>:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Sr. no.</strong></td>
<td>
<p><strong>new</strong></p>
</td>
<td>
<p><strong>kitten</strong></p>
</td>
<td>
<p><strong>play</strong></p>
</td>
<td>
<p><strong>eat</strong></p>
</td>
<td>
<p><strong>lunch</strong></p>
</td>
<td>
<p><strong>love</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Our new matrix consolidated <em>kittens</em> and <em>kitten</em>, but something else happened as well. We lost the suffixes to <em>played</em> and <em>loved</em>, and <em>ate</em> was transformed to <em>eat</em>. Why? This is what lemmatization does. If you remember your grade school grammar classes, we've gone from the inflectional form to the base form of the word. If that is lemmatization, what is stemming? Stemming has the same goal, but uses a less sophisticated approach. This approach can sometimes produce pseudo-words rather than the actual base form. For example, in lemmatization, if you were to reduce <em>ponies</em>, you would get <em>pony</em>, but with stemming, you'd get <em>poni</em>.</p>
<p>Let's now go further to apply another transformation to our matrix. So far, we have used a simple count of each word, but we can apply an algorithm that will act like a filter on our data to enhance the words that are unique to each document. This algorithm is called <strong>term frequency-inverse document frequency</strong> (<strong>tf-idf</strong>)<strong>.</strong></p>
<p>We calculate this tf-idf ratio for each term in our matrix. Let's calculate it for a couple of examples. For the word <em>new</em> in document one, the term frequency is just the count, which is <kbd>1</kbd>. The inverse document frequency is calculated as the log of the number of documents in the corpus over the number of documents the term appears in. For <em>new</em>, this is <em>log (3/1)</em>, or .4471. So, for the complete tf-idf value, we have <em>tf * idf</em>, or, here, it is <em>1 x .4471</em>, or just .4471. For the word <em>kitten</em> in document one, the tf-idf is <em>2 * log (3/2)</em>, or .3522.</p>
<p>Completing this for the remainder of the terms and documents, we have the following:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Sr. no.</strong></td>
<td>
<p><strong>new</strong></p>
</td>
<td>
<p><strong>kitten</strong></p>
</td>
<td>
<p><strong>play</strong></p>
</td>
<td>
<p><strong>eat</strong></p>
</td>
<td>
<p><strong>lunch</strong></p>
</td>
<td>
<p><strong>love</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>.4471</p>
</td>
<td>
<p>.3522</p>
</td>
<td>
<p>.4471</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>.4471</p>
</td>
<td>
<p>.4471</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>.1761</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>.4471</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Why do all of this? Let's say, for example, we have a corpus of documents about many subjects (medicine, computing, food, animals, and so on) and we want to classify them into topics. Very few documents would contain the word <em>sphygmomanometer</em>, which is the device used to measure blood pressure; and all the documents that did would likely concern the topic of medicine. And obviously, the more times this word appears in a document, the more likely it is to be about medicine. So a term that occurs rarely across our entire corpus, but that is present many times in a document, makes it likely that this term is tied closely to the topic of that document. In this way, documents can be said to be represented by those terms with high tf-idf values.</p>
<p>With the help of this framework, we'll now convert our training set into a tf-idf matrix:</p>
<pre><strong>from sklearn.feature_extraction.text import TfidfVectorizer 
 
vect = TfidfVectorizer(ngram_range=(1,3), stop_words='english', min_df=3) 
 
tv = vect.fit_transform(df['text'])</strong> </pre>
<p>With those three lines, we have converted all our documents into a tf-idf vector. We passed in a number of parameters: <kbd>ngram_range</kbd>, <kbd>stop_words</kbd>, and <kbd>min_df</kbd>. Let's discuss each.</p>
<p>First, <kbd>ngram_range</kbd> is how the document is tokenized. In our previous examples, we used each word as a token, but here, we are using all one- to three-word sequences as tokens. Let's take our second sentence, <em>She ate lunch</em>. We'll ignore stop words for the moment. The n-grams for this sentence would be: <em>she</em>, <em>she ate</em>, <em>she ate lunch</em>, <em>ate</em>, <em>ate lunch</em>, and <em>lunch</em>.</p>
<p>Next, we have <kbd>stop_words</kbd>. We pass in <kbd>english</kbd> for this to remove all the English stop words. As discussed previously, this removes all terms that lack informational content.</p>
<p>And finally, we have <kbd>min_df</kbd>. This removes all words from consideration that don't appear in at least three documents. Adding this removes very rare terms and reduces the size of our matrix.</p>
<p>Now that our article corpus is in a workable numerical format, we'll move on to feeding it to our classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support Vector Machines</h1>
                </header>
            
            <article>
                
<p>We're going to be utilizing a new classifier in this chapter, a linear <strong>Support Vector Machine</strong> (<strong>SVM</strong>). An SVM is an algorithm that attempts to linearly separate data points into classes using a <strong>maximum-margin hyperplane</strong>. That's a mouthful, so let's look at what it really means.</p>
<p>Suppose we have two classes of data, and we want to separate them with a line. (We'll just deal with two features, or dimensions, here.) What is the most effective way to place that line? Lets have a look at an illustration:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1147 image-border" src="assets/13ff1cec-f4de-4170-b1c7-e2feadd686af.png" style="width:27.33em;height:23.58em;"/></p>
<p class="mce-root"/>
<p>In the preceding diagram, line <strong>H<sub>1</sub></strong> does not effectively discriminate between the two classes, so we can eliminate that one. Line <strong>H<sub>2</sub></strong> is able to discriminate between them cleanly, but <strong>H<sub>3</sub></strong> is the maximum-margin line. This means that the line is centered between the two nearest points of each class, which are known as the <strong>support vectors</strong>. These can be seen as the dotted lines in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1148 image-border" src="assets/30e96749-12d3-4f34-be03-81b0701fff2c.png" style="width:31.08em;height:33.50em;"/></p>
<p>What if the data isn't able to be separated into classes so neatly? What if there is an overlap between the points? In that situation, there are still options. One is to use what's called a <strong>soft-margin SVM</strong>. This formulation still maximizes the margin, but with the trade-off being a penalty for points that fall on the wrong side of the margin. The other option is to use what's called the <strong>kernel trick</strong>. This method transforms the data into a higher dimensional space where the data can be linearly separated. An example is provided here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bb67ef6e-3a1e-4060-8da9-6e6fcfd5ea5f.png" style="width:30.92em;height:3.17em;"/></p>
<p class="mce-root"/>
<p>The two-dimensional representation is a follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1149 image-border" src="assets/74a17d51-a26b-4487-9d36-644d544e09fd.png" style="width:28.92em;height:19.17em;"/></p>
<p>We have taken a one-dimensional feature space and mapped it onto a two-dimensional feature space. The mapping simply takes each <em>x</em> value and maps it to <em>x</em>, <em>x<sup>2</sup></em>. Doing so allows us to add a linear separating plane.</p>
<p>With that covered, let's now feed our tf-idf matrix into our SVM:</p>
<pre><strong>from sklearn.svm import LinearSVC 
 
clf = LinearSVC() 
model = clf.fit(tv, df['wanted'])</strong> </pre>
<p><kbd>tv</kbd> is our matrix, and <kbd>df['wanted']</kbd> is our list of labels. Remember this is either <kbd>y</kbd> or <kbd>n</kbd>, denoting whether we are interested in the article. Once that runs, our model is trained.</p>
<p>One thing we aren't doing in this chapter is formally evaluating our model. You should almost always have a hold-out set to evaluate your model against, but because we are going to be continuously updating our model, and evaluating it daily, we'll skip that step for this chapter. Just remember that this is generally a terrible idea.</p>
<p>Let's now move on to setting up our daily feed of news items.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">IFTTT integration with feeds, Google Sheets, and email</h1>
                </header>
            
            <article>
                
<p>We used Pocket to build our training set, but now we need a streaming feed of articles to run our model against. To set this up, we'll use IFTTT once again, as well as Google Sheets, and a Python library that will allow us to work with Google Sheets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up news feeds and Google Sheets through IFTTT</h1>
                </header>
            
            <article>
                
<p>Hopefully, you have an IFTTT account set up at this point, but if not, go ahead and set that up now. Once that is done, you'll need to set up integration with feeds and with Google Sheets:</p>
<ol>
<li>First, search for feeds in the search box on the home page, then click on <span class="packt_screen">Services</span>, and click to set that up:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1197 image-border" src="assets/fea512df-fbd0-44dd-a31d-fe6600f32ce9.png" style="width:162.50em;height:66.67em;"/></p>
<ol start="2">
<li>You'll just need to click <span class="packt_screen">Connect</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6389031a-ce09-45cd-bf3d-41080cd1cc73.png" style="width:35.92em;height:20.67em;"/></p>
<ol start="3">
<li>Next, search for <kbd>Google Drive</kbd> under <span class="packt_screen">Services</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1198 image-border" src="assets/03e1346f-8188-45e6-9a4a-3e44ae1bab67.png" style="width:162.50em;height:64.83em;"/></p>
<ol start="4">
<li>Click on that. It should take you to a page where you select the Google account you want to connect to. Choose the account and then click <span class="packt_screen">Allow</span> to enable IFTTT to access your Google Drive account. Once that's done, you should see the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f2445244-6fc6-4787-90a3-0ac7823c8d23.png" style="width:32.25em;height:14.17em;"/></p>
<ol start="5">
<li>Now, with our channels connected, we can set up our feed. Click on <span class="packt_screen">New Applet</span> in the dropdown under your username in the right-hand corner. This will bring you here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9bf9d660-7dba-4d1b-b951-8ebb2d523f60.png" style="width:15.75em;height:6.08em;"/></p>
<ol start="6">
<li>Click on <span class="packt_screen">+this</span>. Search for <kbd>RSS Feed</kbd>, and then click on that. That should bring you here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1150 image-border" src="assets/edc23225-750c-4932-bf9f-5aeaf1001c77.png" style="width:35.08em;height:20.08em;"/></p>
<ol start="7">
<li>From here, click on <span class="packt_screen">New feed item</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1151 image-border" src="assets/fe70de1e-5a10-47a3-8aea-e12bedde546d.png" style="width:25.00em;height:23.75em;"/></p>
<ol start="8">
<li>Then, add the URL to the box and click <span class="packt_screen">Create trigger</span>. Once that is done, you'll be brought back to add the <span class="packt_screen">+that</span> action:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1152 image-border" src="assets/bff82527-32fb-4ed2-8e67-fcebfc39e102.png" style="width:10.75em;height:1.83em;"/></p>
<ol start="9">
<li>Click on <span class="packt_screen">+that</span>, search for <kbd>Sheets</kbd>, and then click on its icon. Once that is done, you'll find yourself here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1153 image-border" src="assets/89fbc4e5-1aad-42d7-84c8-39773626cec5.png" style="width:28.50em;height:16.50em;"/></p>
<ol start="10">
<li>We want our news items to flow into a Google Drive spreadsheet, so click on <span class="packt_screen">Add row to spreadsheet</span>. You'll then have an opportunity to customize the spreadsheet:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1154 image-border" src="assets/e9c7a419-8c0c-486a-97ad-22ed50fafff1.png" style="width:18.58em;height:38.58em;"/></p>
<p>I gave the spreadsheet the name <kbd>NewStories</kbd>, and placed it in a Google Drive folder called <kbd>IFTTT</kbd>. Click <span class="packt_screen">Create Action</span> to finish the recipe, and soon you'll start seeing news items flow into your Google Drive spreadsheet. Note that it will only add new items as they come in, not items that existed at the time you created the sheet. I recommend adding a number of feeds. You will need to create individual recipes for each. It is best if you add feeds for the sites that are in your training set, in other words, the ones you saved with Pocket.</p>
<p class="mce-root"/>
<p>Give those stories a day or two to build up in the sheet, and then it should look something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1205 image-border" src="assets/4a6b042c-a32e-41a0-93a5-f9d4b27e1efd.png" style="width:162.50em;height:55.33em;"/></p>
<p>Fortunately, the full article HTML body is included. This means we won't have to use Embedly to download it for each article. We will still need to download the articles from Google Sheets, and then process the text to strip out the HTML tags, but this can all be done rather easily.</p>
<p>To pull down the articles, we'll use a Python library called <kbd>gspread</kbd>. This can be pip installed. Once that is installed, you need to follow the direction for setting up <strong>OAuth 2</strong>. That can be found at <a href="http://gspread.readthedocs.org/en/latest/oauth2.html" target="_blank"><span class="URLPACKT">http://gspread.readthedocs.org/en/latest/oauth2.html</span></a>. You will end up downloading a JSON credentials file. It is critical that, once you have that file, you find the email address in it with the <kbd>client_email</kbd> key. You then need to share the <kbd>NewStories</kbd> spreadsheet you are sending the stories to with that email. Just click on the blue <span class="packt_screen">Share</span> button in the upper-right corner of the sheet, and paste the email in there. You will end up receiving a <em>failed to send</em> message in your Gmail account, but that is expected. Make sure to swap in your path to the file and the name of the file in the following code:</p>
<pre><strong>import gspread 
 
from oauth2client.service_account import ServiceAccountCredentials 
JSON_API_KEY = 'the/path/to/your/json_api_key/here' 
 
scope = ['https://spreadsheets.google.com/feeds', 
         'https://www.googleapis.com/auth/drive'] 
 
credentials = ServiceAccountCredentials.from_json_keyfile_name(JSON_API_KEY, scope) 
gc = gspread.authorize(credentials)</strong> </pre>
<p class="mce-root"/>
<p>Now, if everything went well, it should run without errors. Next, you can download the stories:</p>
<pre><strong>ws = gc.open("NewStories") 
sh = ws.sheet1 
 
zd = list(zip(sh.col_values(2),sh.col_values(3), sh.col_values(4))) 
 
zf = pd.DataFrame(zd, columns=['title','urls','html']) 
zf.replace('', pd.np.nan, inplace=True) 
zf.dropna(inplace=True) 
 
zf</strong> </pre>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cb48abf5-89de-4090-8fc2-47ddc3a33bf0.png"/></p>
<p>With that, we downloaded all of the articles from our feed and placed them into a DataFrame. We now need to strip out the HTML tags. We can use the function we used earlier to retrieve the text. We'll then transform it using our tf-idf vectorizer:</p>
<pre><strong>zf.loc[:,'text'] = zf['html'].map(get_text) 
 
zf.reset_index(drop=True, inplace=True) 
 
test_matrix = vect.transform(zf['text']) 
 
test_matrix</strong> </pre>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0d864c05-e9fc-4aad-b78f-902084ce2524.png" style="width:43.67em;height:3.58em;"/></p>
<p>Here, we see that our vectorization was successful. Let's now pass it into our model to get back the results:</p>
<pre><strong>results = pd.DataFrame(model.predict(test_matrix), columns=['wanted']) 
 
results</strong> </pre>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1155 image-border" src="assets/2292ef3e-aa9e-4b12-9703-f4e5ec5b44cc.png" style="width:6.25em;height:22.58em;"/></p>
<p>We see here that we have results for each of the stories. Let's now join them with the stories themselves so that we can evaluate the results:</p>
<pre><strong>rez = pd.merge(results,zf, left_index=True, right_index=True) 
 
rez</strong> </pre>
<p class="mce-root"/>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c5dd103e-6d29-4d96-afaf-3271f3696e49.png"/></p>
<p>At this point, we can improve the model by going through the results and correcting the errors. You'll need to do this for yourself, but here is how I made changes to my own:</p>
<pre><strong>change_to_no = [130, 145, 148, 163, 178, 199, 219, 222, 223, 226, 235, 279, 348, 357, 427, 440, 542, 544, 546, 568, 614, 619, 660, 668, 679, 686, 740, 829] 
 
change_to_yes = [0, 9, 29, 35, 42, 71, 110, 190, 319, 335, 344, 371, 385, 399, 408, 409, 422, 472, 520, 534, 672] 
 
for i in rez.iloc[change_to_yes].index: 
    rez.iloc[i]['wanted'] = 'y' 
 
for i in rez.iloc[change_to_no].index: 
    rez.iloc[i]['wanted'] = 'n' 
 
rez</strong> </pre>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/25c89735-a506-4343-834a-8ea97965d1c2.png"/></p>
<p>This may look like a lot of changes, but of the over 900 articles evaluated, I had to change very few. By making these corrections, we can now feed this back into our model to improve it even more. Let's add these results to our earlier training data and then rebuild the model:</p>
<pre><strong>combined = pd.concat([df[['wanted', 'text']], rez[['wanted', 'text']]]) 
 
</strong><strong>combined</strong> </pre>
<p><span class="fontstyle0">The preceding code results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1156 image-border" src="assets/b97a3db6-da79-4fbc-9a5d-6216fbde7bd7.png" style="width:28.92em;height:21.33em;"/></p>
<p>Retrain the model with following code:</p>
<pre><strong>tvcomb = vect.fit_transform(combined['text'], combined['wanted']) 
 
model = clf.fit(tvcomb, combined['wanted'])</strong> </pre>
<p>Now we have retrained our model with all the available data. You may want to do this a number of times as you get more results over the days and weeks. The more you add, the better your results will be.</p>
<p>We'll assume you have a well-trained model at this point, and are ready to begin using it. Let's now see how we can deploy this to set up a personalized news feed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up your daily personal newsletter</h1>
                </header>
            
            <article>
                
<p>In order to set up a personal email with news stories, we're going to utilize IFTTT again. As before, in <a href="73ff80b5-e844-4791-bf95-b678215c00a8.xhtml" target="_blank">Chapter 3</a><a href="73ff80b5-e844-4791-bf95-b678215c00a8.xhtml" target="_blank"/>, <em>Build an App to Find Cheap Airfares</em>, we'll use the Webhooks channel to send a <kbd>POST</kbd> request. But this time, the payload will be our news stories. If you haven't set up the Webhooks channel, do so now. Instructions can be found in <a href="73ff80b5-e844-4791-bf95-b678215c00a8.xhtml" target="_blank">Chapter 3</a>, <em>Build an App to Find Cheap Airfares</em>. You should also set up the Gmail channel. Once that is complete, we'll add a recipe to combine the two. Follow the steps to set up IFTTT:</p>
<ol>
<li>First, click <span class="packt_screen">New Applet</span> from the IFTTT home page and then click <span class="packt_screen">+this</span>. Then, search for the Webhooks channel:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1157 image-border" src="assets/9dfbd355-91a9-4624-9ccf-155e0796cb1b.png" style="width:30.92em;height:21.75em;"/></p>
<ol start="2">
<li>Select that, and then select <span class="packt_screen">Receive a web request</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1158 image-border" src="assets/35ed4dcf-c87b-42ae-b804-c13f352ac26a.png" style="width:27.58em;height:29.75em;"/></p>
<ol start="3">
<li>Then, give the request a name. I'm using <kbd>news_event</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1159 image-border" src="assets/aaa279e1-2abc-42e7-97cd-4d2ff759aa9f.png" style="width:21.67em;height:24.17em;"/></p>
<ol start="4">
<li>Finish by clicking <span class="packt_screen">Create trigger</span>. Next, click on <span class="packt_screen">+that</span> to set up the email piece. Search for Gmail and click on that:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1160 image-border" src="assets/0f4c3ac5-e8a7-4d0e-9eef-9e4da3764925.png" style="width:27.50em;height:20.42em;"/></p>
<p class="mce-root"/>
<ol start="5">
<li>Once you have clicked <span class="packt_screen">Gmail</span>, click <span class="packt_screen">Send yourself an email</span>. From there, you can customize your email message:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1161 image-border" src="assets/36ee5b08-8d0d-45a2-a692-936f5e2b9b74.png" style="width:20.50em;height:36.75em;"/></p>
<p>Input a subject line, and include <kbd>{{Value1}}</kbd> in the email body. We will pass our story title and link into this with our <kbd>POST</kbd> request. Click on <span class="packt_screen">Create action</span> and then <span class="packt_screen">Finish</span> to finalize it.</p>
<p>Now, we're ready to generate the script that will run on a schedule, automatically sending us articles of interest. We're going to create a separate script for this, but one last thing we need to do in our existing code is serialize our vectorizer and our model, as demonstrated in the following code block:</p>
<pre><strong>import pickle 
 
pickle.dump(model, open(r'/input/a/path/here/to/news_model_pickle.p', 'wb')) 
 
pickle.dump(vect, open(r'/input/a/path/here/to/news_vect_pickle.p', 'wb'))</strong> </pre>
<p>With that, we have saved everything we need from our model. In our new script, we will read those in to generate our new predictions. We're going to use the same scheduling library to run the code as we used in <a href="73ff80b5-e844-4791-bf95-b678215c00a8.xhtml" target="_blank">Chapter 3</a>, <em>Build an App to Find Cheap Airfares</em>. Putting it all together, we have the following script:</p>
<pre><strong>import pandas as pd 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.svm import LinearSVC 
import schedule 
import time 
import pickle 
import json 
import gspread 
from oauth2client.service_account import ServiceAccountCredentials 
import requests 
from bs4 import BeautifulSoup 
 
 
def fetch_news(): 
 
    try: 
        vect = pickle.load(open(r'/your/path/to/news_vect_pickle.p', 'rb')) 
        model = pickle.load(open(r'/your/path/to /news_model_pickle.p', 'rb')) 
 
        JSON_API_KEY = r'/your/path/to/API KEY.json' 
 
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive'] 
 
        credentials = ServiceAccountCredentials.from_json_keyfile_name(JSON_API_KEY, scope) 
        gc = gspread.authorize(credentials) 
 
        ws = gc.open("NewStories") 
        sh = ws.sheet1 
        zd = list(zip(sh.col_values(2),sh.col_values(3), sh.col_values(4))) 
        zf = pd.DataFrame(zd, columns=['title','urls','html']) 
        zf.replace('', pd.np.nan, inplace=True) 
        zf.dropna(inplace=True) 
 
</strong> <strong>
        def get_text(x): 
            soup = BeautifulSoup(x, 'html5lib') 
            text = soup.get_text() 
            return text 
 
        zf.loc[:,'text'] = zf['html'].map(get_text) 
 
        tv = vect.transform(zf['text']) 
        res = model.predict(tv) 
 
        rf = pd.DataFrame(res, columns=['wanted']) 
        rez = pd.merge(rf, zf, left_index=True, right_index=True) 
 
        rez = rez.iloc[:20,:] 
 
        news_str = ''</strong> 
<strong>        for t, u in zip(rez[rez['wanted']=='y']['title'], rez[rez['wanted']=='y']['urls']): 
            news_str = news_str + t + '\n' + u + '\n' 
 
        payload = {"value1" : news_str} 
        r = requests.post('https://maker.ifttt.com/trigger/news_event/with/key/bNHFwiZx0wMS7EnD425n3T', data=payload) 
 
        # clean up worksheet 
        lenv = len(sh.col_values(1)) 
        cell_list = sh.range('A1:F' + str(lenv)) 
        for cell in cell_list: 
            cell.value = "" 
        sh.update_cells(cell_list) 
        print(r.text)</strong> 
 
    <strong>except: 
        print('Action Failed') 
 
schedule.every(480).minutes.do(fetch_news) 
 
while 1: 
    schedule.run_pending() 
    time.sleep(1)</strong> </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>What this script will do is run every 4 hours, pull down the news stories from Google Sheets, run the stories through the model, generate an email by sending a <kbd>POST</kbd> request to IFTTT for those stories that are predicted to be of interest, and then, finally, it will clear out the stories in the spreadsheet so only new stories get sent in the next email.</p>
<p>Congratulations! You now have your own personalized news feed!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've learned how to work with text data when training machine learning models. We've also learned the basics of NLP and of SVMs.</p>
<p>In the next chapter, we'll develop these skills further and attempt to predict what sort of content will go viral.</p>


            </article>

            
        </section>
    </body></html>
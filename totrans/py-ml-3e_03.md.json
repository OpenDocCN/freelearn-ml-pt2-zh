["```py\n>>> from sklearn import datasets\n>>> import numpy as np\n>>> iris = datasets.load_iris()\n>>> X = iris.data[:, [2, 3]]\n>>> y = iris.target\n>>> print('Class labels:', np.unique(y))\nClass labels: [0 1 2] \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, y_train, y_test = train_test_split(\n...     X, y, test_size=0.3, random_state=1, stratify=y) \n```", "```py\n>>> print('Labels counts in y:', np.bincount(y))\nLabels counts in y: [50 50 50]\n>>> print('Labels counts in y_train:', np.bincount(y_train))\nLabels counts in y_train: [35 35 35]\n>>> print('Labels counts in y_test:', np.bincount(y_test))\nLabels counts in y_test: [15 15 15] \n```", "```py\n>>> from sklearn.preprocessing import StandardScaler\n>>> sc = StandardScaler()\n>>> sc.fit(X_train)\n>>> X_train_std = sc.transform(X_train)\n>>> X_test_std = sc.transform(X_test) \n```", "```py\n>>> from sklearn.linear_model import Perceptron\n>>> ppn = Perceptron(eta0=0.1, random_state=1)\n>>> ppn.fit(X_train_std, y_train) \n```", "```py\n>>> y_pred = ppn.predict(X_test_std)\n>>> print('Misclassified examples: %d' % (y_test != y_pred).sum())\nMisclassified examples: 1 \n```", "```py\n>>> from sklearn.metrics import accuracy_score\n>>> print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\nAccuracy: 0.978 \n```", "```py\n>>> print('Accuracy: %.3f' % ppn.score(X_test_std, y_test))\nAccuracy: 0.978 \n```", "```py\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\ndef plot_decision_regions(X, y, classifier, test_idx=None,\n                          resolution=0.02):\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=colors[idx],\n                    marker=markers[idx], label=cl,\n                    edgecolor='black')\n\n    # highlight test examples\n    if test_idx:\n        # plot all examples\n        X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0], X_test[:, 1],\n                    c='', edgecolor='black', alpha=1.0,\n                    linewidth=1, marker='o',\n                    s=100, label='test set') \n```", "```py\n>>> X_combined_std = np.vstack((X_train_std, X_test_std))\n>>> y_combined = np.hstack((y_train, y_test))\n>>> plot_decision_regions(X=X_combined_std,\n...                       y=y_combined,\n...                       classifier=ppn,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('petal length [standardized]')\n>>> plt.ylabel('petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> def sigmoid(z):\n...     return 1.0 / (1.0 + np.exp(-z))\n>>> z = np.arange(-7, 7, 0.1)\n>>> phi_z = sigmoid(z)\n>>> plt.plot(z, phi_z)\n>>> plt.axvline(0.0, color='k')\n>>> plt.ylim(-0.1, 1.1)\n>>> plt.xlabel('z')\n>>> plt.ylabel('$\\phi (z)$')\n>>> # y axis ticks and gridline\n>>> plt.yticks([0.0, 0.5, 1.0])\n>>> ax = plt.gca()\n>>> ax.yaxis.grid(True)\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n:\n```", "```py\n>>> def cost_1(z):\n...     return - np.log(sigmoid(z))\n>>> def cost_0(z):\n...     return - np.log(1 - sigmoid(z))\n>>> z = np.arange(-10, 10, 0.1)\n>>> phi_z = sigmoid(z)\n>>> c1 = [cost_1(x) for x in z]\n>>> plt.plot(phi_z, c1, label='J(w) if y=1')\n>>> c0 = [cost_0(x) for x in z]\n>>> plt.plot(phi_z, c0, linestyle='--', label='J(w) if y=0')\n>>> plt.ylim(0.0, 5.1)\n>>> plt.xlim([0, 1])\n>>> plt.xlabel('$\\phi$(z)')\n>>> plt.ylabel('J(w)')\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nclass LogisticRegressionGD(object):\n    \"\"\"Logistic Regression Classifier using gradient descent.\n\n    Parameters\n    ------------\n    eta : float\n        Learning rate (between 0.0 and 1.0)\n    n_iter : int\n        Passes over the training dataset.\n    random_state : int\n        Random number generator seed for random weight\n        initialization.\n\n    Attributes\n    -----------\n    w_ : 1d-array\n        Weights after fitting.\n    cost_ : list\n        Logistic cost function value in each epoch.\n\n    \"\"\"\n    def __init__(self, eta=0.05, n_iter=100, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n            Training vectors, where n_examples is the number of\n            examples and n_features is the number of features.\n        y : array-like, shape = [n_examples]\n            Target values.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01,\n                              size=1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n\n            # note that we compute the logistic `cost` now\n            # instead of the sum of squared errors cost\n            cost = (-y.dot(np.log(output)) -\n                        ((1 - y).dot(np.log(1 - output))))\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, z):\n        \"\"\"Compute logistic sigmoid activation\"\"\"\n        return 1\\. / (1\\. + np.exp(-np.clip(z, -250, 250)))\n\n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.net_input(X) >= 0.0, 1, 0)\n        # equivalent to:\n        # return np.where(self.activation(self.net_input(X))\n        #                 >= 0.5, 1, 0) \n```", "```py\n>>> X_train_01_subset = X_train[(y_train == 0) | (y_train == 1)]\n>>> y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]\n>>> lrgd = LogisticRegressionGD(eta=0.05,\n...                             n_iter=1000,\n...                             random_state=1)\n>>> lrgd.fit(X_train_01_subset,\n...          y_train_01_subset)\n>>> plot_decision_regions(X=X_train_01_subset,\n...                       y=y_train_01_subset,\n...                       classifier=lrgd)\n>>> plt.xlabel('petal length [standardized]')\n>>> plt.ylabel('petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> lr = LogisticRegression(C=100.0, random_state=1,\n...                         solver='lbfgs', multi_class='ovr')\n>>> lr.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined,\n...                       classifier=lr,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('petal length [standardized]')\n>>> plt.ylabel('petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> lr.predict_proba(X_test_std[:3, :]) \n```", "```py\narray([[3.81527885e-09, 1.44792866e-01, 8.55207131e-01],\n       [8.34020679e-01, 1.65979321e-01, 3.25737138e-13],\n       [8.48831425e-01, 1.51168575e-01, 2.62277619e-14]]) \n```", "```py\n>>> lr.predict_proba(X_test_std[:3, :]).argmax(axis=1) \n```", "```py\narray([2, 0, 0]) \n```", "```py\n>>> lr.predict(X_test_std[:3, :])\narray([2, 0, 0]) \n```", "```py\n>>> lr.predict(X_test_std[0, :].reshape(1, -1))\narray([2]) \n```", "```py\n>>> weights, params = [], []\n>>> for c in np.arange(-5, 5):\n...     lr = LogisticRegression(C=10.**c, random_state=1,\n...                             solver='lbfgs', multi_class='ovr')\n...     lr.fit(X_train_std, y_train)\n...     weights.append(lr.coef_[1])\n...     params.append(10.**c)\n>>> weights = np.array(weights)\n>>> plt.plot(params, weights[:, 0],\n...          label='petal length')\n>>> plt.plot(params, weights[:, 1], linestyle='--',\n...          label='petal width')\n>>> plt.ylabel('weight coefficient')\n>>> plt.xlabel('C')\n>>> plt.legend(loc='upper left')\n>>> plt.xscale('log')\n>>> plt.show() \n```", "```py\n>>> from sklearn.svm import SVC\n>>> svm = SVC(kernel='linear', C=1.0, random_state=1)\n>>> svm.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined,\n...                       classifier=svm,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('petal length [standardized]')\n>>> plt.ylabel('petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.linear_model import SGDClassifier\n>>> ppn = SGDClassifier(loss='perceptron')\n>>> lr = SGDClassifier(loss='log')\n>>> svm = SGDClassifier(loss='hinge') \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> np.random.seed(1)\n>>> X_xor = np.random.randn(200, 2)\n>>> y_xor = np.logical_xor(X_xor[:, 0] > 0,\n...                        X_xor[:, 1] > 0)\n>>> y_xor = np.where(y_xor, 1, -1)\n>>> plt.scatter(X_xor[y_xor == 1, 0],\n...             X_xor[y_xor == 1, 1],\n...             c='b', marker='x',\n...             label='1')\n>>> plt.scatter(X_xor[y_xor == -1, 0],\n...             X_xor[y_xor == -1, 1],\n...             c='r',\n...             marker='s',\n...             label='-1')\n>>> plt.xlim([-3, 3])\n>>> plt.ylim([-3, 3])\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)\n>>> svm.fit(X_xor, y_xor)\n>>> plot_decision_regions(X_xor, y_xor, classifier=svm)\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> svm = SVC(kernel='rbf', random_state=1, gamma=0.2, C=1.0)\n>>> svm.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined, classifier=svm,\n...                       test_idx=range(105,150))\n>>> plt.xlabel('petal length [standardized]')\n>>> plt.ylabel('petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> svm = SVC(kernel='rbf', random_state=1, gamma=100.0, C=1.0)\n>>> svm.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std,\n...                       y_combined, classifier=svm,\n...                       test_idx=range(105,150))\n>>> plt.xlabel('petal length [standardized]')\n>>> plt.ylabel('petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> def gini(p):\n...     return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))\n>>> def entropy(p):\n...     return - p*np.log2(p) - (1 - p)*np.log2((1 - p))\n>>> def error(p):\n...     return 1 - np.max([p, 1 - p])\n>>> x = np.arange(0.0, 1.0, 0.01)\n>>> ent = [entropy(p) if p != 0 else None for p in x]\n>>> sc_ent = [e*0.5 if e else None for e in ent]\n>>> err = [error(i) for i in x]\n>>> fig = plt.figure()\n>>> ax = plt.subplot(111)\n>>> for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err],\n...                           ['Entropy', 'Entropy (scaled)',\n...                            'Gini impurity',\n...                            'Misclassification error'],\n...                           ['-', '-', '--', '-.'],\n...                           ['black', 'lightgray',\n...                            'red', 'green', 'cyan']):\n...     line = ax.plot(x, i, label=lab,\n...                   linestyle=ls, lw=2, color=c)\n>>> ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n...           ncol=5, fancybox=True, shadow=False)\n>>> ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n>>> ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n>>> plt.ylim([0, 1.1])\n>>> plt.xlabel('p(i=1)')\n>>> plt.ylabel('impurity index') \n```", "```py\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> tree_model = DecisionTreeClassifier(criterion='gini',\n...                                     max_depth=4,\n...                                     random_state=1)\n>>> tree_model.fit(X_train, y_train)\n>>> X_combined = np.vstack((X_train, X_test))\n>>> y_combined = np.hstack((y_train, y_test))\n>>> plot_decision_regions(X_combined,\n...                       y_combined,\n...                       classifier=tree_model,\n...                       test_idx=range(105, 150))\n>>> plt.xlabel('petal length [cm]')\n>>> plt.ylabel('petal width [cm]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn import tree\n>>> tree.plot_tree(tree_model)\n>>> plt.show() \n```", "```py\n> pip3 install pydotplus \n```", "```py\npip3 install graphviz\npip3 install pyparsing \n```", "```py\n>>> from pydotplus import graph_from_dot_data\n>>> from sklearn.tree import export_graphviz\n>>> dot_data = export_graphviz(tree_model,\n...                            filled=True,\n...                            rounded=True,\n...                            class_names=['Setosa',\n...                                         'Versicolor',\n...                                         'Virginica'],\n...                            feature_names=['petal length',\n...                                           'petal width'],\n...                            out_file=None)\n>>> graph = graph_from_dot_data(dot_data)\n>>> graph.write_png('tree.png') \n```", "```py\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> forest = RandomForestClassifier(criterion='gini',\n...                                 n_estimators=25,\n...                                 random_state=1,\n...                                 n_jobs=2)\n>>> forest.fit(X_train, y_train)\n>>> plot_decision_regions(X_combined, y_combined,\n...                       classifier=forest, test_idx=range(105,150))\n>>> plt.xlabel('petal length [cm]')\n>>> plt.ylabel('petal width [cm]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> knn = KNeighborsClassifier(n_neighbors=5, p=2,\n...                            metric='minkowski')\n>>> knn.fit(X_train_std, y_train)\n>>> plot_decision_regions(X_combined_std, y_combined,\n...                       classifier=knn, test_idx=range(105,150))\n>>> plt.xlabel('petal length [standardized]')\n>>> plt.ylabel('petal width [standardized]')\n>>> plt.legend(loc='upper left')\n>>> plt.tight_layout()\n>>> plt.show() \n```"]
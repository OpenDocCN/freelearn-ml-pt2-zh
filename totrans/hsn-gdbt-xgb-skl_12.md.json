["```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import cross_val_score\n    from xgboost import XGBClassifier, XGBRFClassifier\n    from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split, StratifiedKFold\n    from sklearn.metrics import accuracy_score\n    from sklearn.ensemble import VotingClassifier\n    import warnings\n    warnings.filterwarnings('ignore')\n    ```", "```py\n    df = pd.read_csv('cab_rides.csv', nrows=10000)\n    df.head()\n    ```", "```py\n    df.info()\n    ```", "```py\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 10000 entries, 0 to 9999\n    Data columns (total 10 columns):\n     #   Column            Non-Null Count  Dtype  \n    ---  ------            --------------  -----  \n     0   distance          10000 non-null  float64\n     1   cab_type          10000 non-null  object \n     2   time_stamp        10000 non-null  int64  \n     3   destination       10000 non-null  object \n     4   source            10000 non-null  object \n     5   price             9227 non-null   float64\n     6   surge_multiplier  10000 non-null  float64\n     7   id                10000 non-null  object \n     8   product_id        10000 non-null  object \n     9   name              10000 non-null  object \n    dtypes: float64(3), int64(1), object(6)\n    memory usage: 781.4+ KB\n    ```", "```py\n    df[df.isna().any(axis=1)]\n    ```", "```py\n    df.dropna(inplace=True)\n    ```", "```py\n    df['date'] = pd.to_datetime(df['time_stamp'])\n    df.head()\n    ```", "```py\n    df['date'] = pd.to_datetime(df['time_stamp']*(10**6))\n    df.head()\n    ```", "```py\n    import datetime as dt\n    df['month'] = df['date'].dt.month\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    ```", "```py\n    def weekend(row):\n        if row['dayofweek'] in [5,6]:\n            return 1\n        else:\n            return 0\n    ```", "```py\n    df['weekend'] = df.apply(weekend, axis=1)\n    ```", "```py\n    def rush_hour(row):\n        if (row['hour'] in [6,7,8,9,15,16,17,18]) & \n            (row['weekend'] == 0):\n            return 1\n        else:\n            return 0\n    ```", "```py\n    df['rush_hour'] = df.apply(rush_hour, axis=1)\n    ```", "```py\n    df.tail()\n    ```", "```py\n    df['cab_type'].value_counts()\n    ```", "```py\n    Uber    4654\n    Lyft    4573\n    Name: cab_type, dtype: int64\n    ```", "```py\n    df['cab_freq'] = df.groupby('cab_type')['cab_type'].transform('count')\n    ```", "```py\n    df['cab_freq'] = df['cab_freq']/len(df)\n    ```", "```py\n    df.tail()\n    ```", "```py\n    pip install --upgrade category_encoders\n    from category_encoders.target_encoder import TargetEncoder\n    ```", "```py\n    encoder = TargetEncoder()\n    ```", "```py\n    df['cab_type_mean'] = encoder.fit_transform(df['cab_type'], df['price'])\n    ```", "```py\n    df.tail()\n    ```", "```py\n    from sklearn.datasets import load_breast_cancer\n    ```", "```py\n    X, y = load_breast_cancer(return_X_y=True)\n    ```", "```py\n    kfold = StratifiedKFold(n_splits=5)\n    ```", "```py\n    def classification_model(model):\n        scores = cross_val_score(model, X, y, cv=kfold)\n        return scores.mean()\n    ```", "```py\n    classification_model(XGBClassifier())\n    ```", "```py\n    0.9771619313771154\n    ```", "```py\n    classification_model(XGBClassifier(booster='gblinear'))\n    ```", "```py\n    0.5782952957615277\n    ```", "```py\n    classification_model(XGBClassifier(booster='dart', one_drop=True))\n    ```", "```py\n    0.9736376339077782\n    ```", "```py\n    classification_model(RandomForestClassifier(random_state=2))\n    ```", "```py\n    0.9666356155876418\n    ```", "```py\n    classification_model(LogisticRegression(max_iter=10000))\n    ```", "```py\n    0.9490451793199813\n    ```", "```py\nclassification_model(XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1))\n```", "```py\n0.9701133364384411\n```", "```py\n    def y_pred(model):\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        score = accuracy_score(y_pred, y_test)\n        print(score)\n        return y_pred\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n    ```", "```py\n    y_pred_gbtree = y_pred(XGBClassifier())\n    ```", "```py\n    0.951048951048951\n    ```", "```py\n    y_pred_dart = y_pred(XGBClassifier(booster='dart', one_drop=True))\n    ```", "```py\n    0.951048951048951\n    ```", "```py\n    y_pred_forest = y_pred(RandomForestClassifier())\n    ```", "```py\n    0.9370629370629371\n    ```", "```py\n    y_pred_logistic = y_pred(LogisticRegression(max_iter=10000))\n    ```", "```py\n    0.9370629370629371\n    y_pred_xgb = y_pred(XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1))\n    ```", "```py\n    0.965034965034965\n    ```", "```py\n    df_pred = pd.DataFrame(data= np.c_[y_pred_gbtree, y_pred_dart, y_pred_forest, y_pred_logistic, y_pred_xgb], columns=['gbtree', 'dart','forest', 'logistic', 'xgb'])\n    ```", "```py\n    df_pred.corr()\n    ```", "```py\n    estimators = []\n    ```", "```py\n    logistic_model = LogisticRegression(max_iter=10000)\n    ```", "```py\n    estimators.append(('logistic', logistic_model))\n    ```", "```py\n    xgb_model = XGBClassifier(max_depth=2, n_estimators=500, learning_rate=0.1)\n    estimators.append(('xgb', xgb_model))\n    rf_model = RandomForestClassifier(random_state=2)\n    estimators.append(('rf', rf_model))\n    ```", "```py\n    ensemble = VotingClassifier(estimators)\n    ```", "```py\n    scores = cross_val_score(ensemble, X, y, cv=kfold)\n    print(scores.mean())\n    ```", "```py\n    0.9754075454122031\n    ```", "```py\n    base_models = []\n    ```", "```py\n    base_models.append(('lr', LogisticRegression()))\n    base_models.append(('xgb', XGBClassifier()))\n    base_models.append(('rf', RandomForestClassifier(random_state=2)))\n    ```", "```py\n    meta_model = LogisticRegression()\n    ```", "```py\n    clf = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n    ```", "```py\n    scores = cross_val_score(clf, X, y, cv=kfold)\n    print(scores.mean())\n    ```", "```py\n    0.9789318428815401\n    ```"]
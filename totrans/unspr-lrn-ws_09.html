<html><head></head><body>
		<div>
			<div id="_idContainer266" class="Content">
			</div>
		</div>
		<div id="_idContainer267" class="Content">
			<h1 id="_idParaDest-166"><a id="_idTextAnchor169"/>8. Market Basket Analysis</h1>
		</div>
		<div id="_idContainer322" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will explore market basket analysis, which is an algorithm originally designed to help retailers understand and improve their businesses. It is not, however, exclusive to retail, as we will discuss throughout the chapter. Market basket analysis unlocks the underlying relationships between the items that customers purchase. By the end of this chapter, you should have a solid grasp of transaction data, the basic metrics that define the relationship between two items, the Apriori algorithm, and association rules.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor170"/>Introduction</h1>
			<p>Most data science practitioners would agree that natural language processing, including topic modeling, is toward the cutting edge of data science and is an active research area. We now understand that topic models can, and should, be leveraged wherever text data could potentially drive insights or growth, including in social media analyses, recommendation engines, and news filtering. The preceding chapter featured an exploration of the fundamental features of topic models and two of the major algorithms. In this chapter, we are going to change direction entirely.</p>
			<p>This chapter takes us into the retail space to explore a foundational and reliable algorithm for analyzing transaction data. While this algorithm might not be on the cutting edge or in the catalog of the most popular machine learning algorithms, it is ubiquitous and undeniably impactful in the retail space. The insights it drives are easily interpretable, immediately actionable, and instructive for determining analytical next steps. If you work in the retail space or with transaction data, you would be well-served to dive deep into market basket analysis. Market basket analysis is important because it provides insight into why people buy certain items together and whether those item combinations can be leveraged to hasten growth and or increase profitability.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor171"/>Market Basket Analysis</h1>
			<p>Imagine you work for a retailer that sells dozens of products and your boss comes to you and asks the following questions:</p>
			<ul>
				<li>What products are purchased together most frequently? </li>
				<li>How should the products be organized and positioned in the store? </li>
				<li>How do we identify the best products to discount via coupons? </li>
			</ul>
			<p>You might reasonably respond with complete bewilderment, as those questions are very diverse and do not immediately seem answerable using a single algorithm and dataset. However, the answer to all those questions and many more is <strong class="bold">market basket analysis</strong>. The general idea behind market basket analysis is to identify and quantify which items, or groups of items, are purchased together frequently enough to drive insight into customer behavior and product relationships.</p>
			<p>Before we dive into the analytics, it is worth defining the term <em class="italic">market basket</em>. A market basket is a permanent set of products in an economic system. Permanent does not necessarily mean permanent in the traditional sense. It means that until such time as the product is taken out of the catalog, it will consistently be available for purchase. The product referenced in the preceding definition is any good, service, or element of a group, including a bicycle, having your house painted, or a website. Lastly, an economic system could be a company, a collection of activities, or a country. The easiest example of a market basket is a grocery store, which is a system made up of a collection of food and drink items:</p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<img src="image/B15923_08_01.jpg" alt="Figure 8.1: An example market basket &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1: An example market basket </p>
			<p>Even without using any models or analyses, certain product relationships are obvious. Let's take the relationship between meat and vegetables. Typically, market basket analysis models return relationships more specific than meat and vegetables, but, for argument's sake, we will generalize to meat and vegetables. Okay, there is a relationship between meat and vegetables. So what? Well, we know these are staple items that are frequently purchased together. We can leverage this information by putting the vegetables and meats on opposite sides of the store, which you will notice is often the positioning of those two items, forcing customers to walk the full distance of the store, and thereby increasing the likelihood that they will buy additional items that they might not have bought had they not traversed the whole store.</p>
			<p>One of the things retail companies struggle with is how to discount items effectively. Let's consider another obvious relationship: peanut butter and jelly. In the United States, peanut butter and jelly sandwiches are incredibly popular, especially among children. When peanut butter is in a shopping basket, the chance jelly is also there can be assumed to be quite high. Since we know peanut butter and jelly are purchased together, it does not make sense to discount them both. If we want customers to buy both items, we can just discount one of the items, knowing that if we can get the customers to buy the discounted item, they will probably buy the other item too, even if it is full price. Just like the topic models in the preceding chapter, market basket analysis is all about identifying frequently occurring groups. The following figure presents an example of such groups:</p>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<img src="image/B15923_08_02.jpg" alt="Figure 8.2: A visualization of market basket analysis&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2: A visualization of market basket analysis</p>
			<p>In market basket analysis, we are looking for frequently occurring groups of products, whereas in topic models, we were looking for frequently occurring groups of words. Thus, as it was to topic models, the word <em class="italic">clustering</em> could be applied to market basket analysis. The major differences are that the clusters in market basket analysis are micro – only a few products per cluster – and the order of the items in the cluster matters when it comes to computing probabilistic metrics. We will dive much deeper into these metrics and how they are calculated later in this chapter.</p>
			<p>What has clearly been implied by the previous two examples is that, in market basket analysis, retailers can discover the relationships – obvious and surprising – between the products that customers buy. Once uncovered, the relationships can be used to inform and improve the decision-making process. A great aspect of market basket analysis is that while this analysis was developed in relation to, discussed in terms of, and mostly applied to, the retail world, it can be applied to many different types of businesses.</p>
			<p>The only requirement for performing this type of analysis is that the data is a list of collections of items. In the retail case, this would be a list of transactions where each transaction is a group of purchased products. One example of an alternative application is analyzing website traffic. With website traffic, we consider the products to be websites, so each element of the list is the collection of websites visited by an individual over a specified time period. Needless to say, the applications of market basket analysis extend well beyond the initial retail application.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor172"/>Use Cases</h2>
			<p>There are three principal use cases in the traditional retail application: pricing enhancement, coupon and discount recommendations, and store layout. As was briefly mentioned previously, by using the product associations uncovered by the model, retailers can strategically place products in their stores to get customers to buy more items and thus spend more money. If any relationship between two or more products is sufficiently strong, meaning the product grouping occurs often in the dataset and the individual products in the grouping appear separate from the group infrequently, then the products could be placed far away from one another in the store without significantly jeopardizing the odds of the customer purchasing both products. By forcing the customer to traverse the whole store to get both products, the retailer increases the chances that the customer will notice and purchase additional products. Likewise, retailers can increase the chances of customers purchasing two weakly related or non-staple products by placing the two items next to each other. Obviously, there are a lot of factors that drive store layout, but market basket analysis is definitely one of those factors:</p>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="image/B15923_08_03.jpg" alt="Figure 8.3: Product associations that can help inform efficient and lucrative store layouts&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3: Product associations that can help inform efficient and lucrative store layouts</p>
			<p>Pricing enhancement and coupon and discount recommendations are two sides of the same coin. They can simply be interpreted as where to raise and where to lower prices. Consider the case of two strongly related items. These two items are most likely going to be purchased in the same transaction, so one way to increase the profitability of that transaction would be to increase the price of one of the items. If the association between the two items is sufficiently strong, the price increase can be made with little to no risk of the customer not purchasing both items. In a similar way, retailers can encourage customers to purchase an item weakly associated with another through discounting or couponing. </p>
			<p>For example, retailers could compare the purchase history of individual customers with the results of market basket analysis done on all transactions and find where some of the items certain customers are purchasing are weakly associated with items those customers are not currently purchasing. Using this comparison, retailers could offer discounts to the customers for the as-yet-unpurchased items the model suggested were related to the items previously purchased by those customers. If you have ever had coupons print out with your receipt at the end of a transaction, the chances are high that those items were found to be related to the items involved in your just-completed transaction.</p>
			<p>A non-traditional, but viable, use of market basket analysis would be to enhance online advertising and search engine optimization. Imagine we had access to lists of websites visited by individuals. Using market basket analysis, we could find relationships between websites and use those relationships to both strategically order and group the websites resulting from a search engine query. In many ways, this is similar to the store layout use case.</p>
			<p>With a general sense of what market basket analysis is all about and a clear understanding of its use cases, let's dig into the data used in these models.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor173"/>Important Probabilistic Metrics</h2>
			<p>Market basket analysis is built upon the computation of several probabilistic metrics. The five major metrics covered here are <strong class="bold">support</strong>, <strong class="bold">confidence</strong>, <strong class="bold">lift</strong>, <strong class="bold">leverage</strong>, and <strong class="bold">conviction</strong>. Before digging into transaction data and the specific market basket analysis models, including the <strong class="bold">Apriori algorithm</strong> and <strong class="bold">association rules</strong>, we should spend some time defining and exploring these metrics using a small, made-up set of transactions. We begin by making up some data to use.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor174"/>Exercise 8.01: Creating Sample Transaction Data</h2>
			<p>Since this is the first exercise of the chapter, let's set the environment. This chapter will use the same environment requirements that were used in <em class="italic">Chapter 7</em>, <em class="italic">Topic Modeling</em>. If any of the packages do not load, as happened in the preceding chapter, use <strong class="source-inline">pip</strong> to install them via the command line. One of the libraries we will use is <strong class="source-inline">mlxtend</strong>, which may be unfamiliar to you. It is a machine learning extensions library that contains useful supplemental tools, including ensembling, stacking, and of course, market basket analysis models. This exercise does not have any real output. We will simply create a sample transaction dataset for use in subsequent exercises:</p>
			<ol>
				<li>Open a Jupyter notebook with Python 3.</li>
				<li>Install the following libraries: <strong class="source-inline">matplotlib.pyplot</strong>, which is used to plot the results of the models; <strong class="source-inline">mlxtend.frequent_patterns</strong>, which is used to run the models; <strong class="source-inline">mlxtend.preprocessing</strong>, which is used to encode and prep the data for the models; <strong class="source-inline">numpy</strong>, which is used to work with arrays; and <strong class="source-inline">pandas</strong>, which is used to work with DataFrames.<p class="callout-heading">Note</p><p class="callout">To install <strong class="source-inline">mlxtend</strong>, go to the Anaconda prompt and execute <strong class="source-inline">pip install mlxtend</strong>.</p><p>The code is as follows:</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import mlxtend.frequent_patterns</p><p class="source-code">import mlxtend.preprocessing</p><p class="source-code">import numpy</p><p class="source-code">import pandas</p></li>
				<li>Create 10 fake transactions featuring grocery store items, and then print out the transactions. The data will take the form of a list of lists, a data structure that will be relevant later when discussing formatting transaction data for the models:<p class="source-code">example = [['milk', 'bread', 'apples', 'cereal', 'jelly', \</p><p class="source-code">            'cookies', 'salad', 'tomatoes'], \</p><p class="source-code">           ['beer', 'milk', 'chips', 'salsa', 'grapes', \</p><p class="source-code">            'wine', 'potatoes', 'eggs', 'carrots'], \</p><p class="source-code">           ['diapers', 'baby formula', 'milk', 'bread', \</p><p class="source-code">            'chicken', 'asparagus', 'cookies'], \</p><p class="source-code">           ['milk', 'cookies', 'chicken', 'asparagus', \</p><p class="source-code">            'broccoli', 'cereal', 'orange juice'], \</p><p class="source-code">           ['steak', 'asparagus', 'broccoli', 'chips', \</p><p class="source-code">            'salsa', 'ketchup', 'potatoes', 'salad'], \</p><p class="source-code">           ['beer', 'salsa', 'asparagus', 'wine', 'cheese', \</p><p class="source-code">            'crackers', 'strawberries', 'cookies'],\</p><p class="source-code">           ['chocolate cake', 'strawberries', 'wine', 'cheese', \</p><p class="source-code">            'beer', 'milk', 'orange juice'],\</p><p class="source-code">           ['chicken', 'peas', 'broccoli', 'milk', 'bread', \</p><p class="source-code">            'eggs', 'potatoes', 'ketchup', 'crackers'],\</p><p class="source-code">           ['eggs', 'bread', 'cheese', 'turkey', 'salad', \</p><p class="source-code">            'tomatoes', 'wine', 'steak', 'carrots'],\</p><p class="source-code">           ['bread', 'milk', 'tomatoes', 'cereal', 'chicken', \</p><p class="source-code">            'turkey', 'chips', 'salsa', 'diapers']]</p><p class="source-code">print(example)</p><p>The output is as follows:</p><div id="_idContainer271" class="IMG---Figure"><img src="image/B15923_08_04.jpg" alt="Figure 8.4: Printing the transactions&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.4: Printing the transactions</p>
			<p>Now that we have created our dataset, we will explore several probabilistic metrics that quantify the relationship between pairs of items.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fhf9bS">https://packt.live/3fhf9bS</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/303vIBJ">https://packt.live/303vIBJ</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor175"/>Support</h2>
			<p><strong class="bold">Support</strong> is simply the probability that a given item set appears in the data, which can be calculated by counting the number of transactions in which the item set appears and dividing that count by the total number of transactions.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">An item set can be a single item or a group of items.</p>
			<p>Support is an important metric, despite being very simple, as it is one of the primary metrics used to determine the believability and strength of association between groups of items. For example, it is possible to have two items that only occur with each other, suggesting that their association is very strong, but in a dataset containing 100 transactions, only appearing twice is not very impressive. Because the item set appears in only 2% of the transactions, and 2% is small in terms of the raw number of appearances, the association cannot be considered significant and, therefore, is probably unusable in decision making.</p>
			<p>Note that since support is a probability, it will fall in the range [0,1]. The formula takes the following form if the item set is two items, X and Y, and N is the total number of transactions:</p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/B15923_08_05.jpg" alt="Figure 8.5: Formula for support&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5: Formula for support</p>
			<p>While working with market basket analysis, if the support for an item or group of items is lower than a pre-defined threshold, then the purchase of that item or group of items is rare enough to not be actionable. Let's return momentarily to the made-up data from <em class="italic">Exercise 8.01</em>, <em class="italic">Creating Sample Transaction Data</em>, and define an item set as being milk and bread. We can easily look through the 10 transactions and count the number of transactions in which this milk and bread item set occurs – that would be 4 times. Given that there are 10 transactions, the support of milk and bread is 4 divided by 10, or 0.4. Whether this is large enough support depends on the dataset itself, which we will get into in a later section.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor176"/>Confidence</h2>
			<p>The <strong class="bold">confidence</strong> metric can be thought of in terms of conditional probability, as it is basically the probability that product B is purchased given the purchase of product A. Confidence is typically notated as A <img src="image/B15923_08_Formula_01.png" alt="and expressed "/> B, and expressed as the proportion of transactions containing A that also contain B. Hence, confidence is found by filtering the full set of transactions down to those containing A, and then computing the proportion of those transactions that contain B. Like support, confidence is a probability, so its range is [0,1]. Using the same variable definitions as in the <em class="italic">Support</em> section, the following is the formula for confidence:</p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="image/B15923_08_06.jpg" alt="Figure 8.6: Formula for confidence&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6: Formula for confidence</p>
			<p>To demonstrate confidence, we will use the items beer and wine. Specifically, let's compute the confidence of Beer <img src="image/B15923_08_Formula_01.png" alt="Formula"/> Wine. To begin, we need to identify the transactions that contain beer. There are three of them, and they are transactions 2, 6, and 7. Now, of those transactions, how many contain wine? The answer is all of them. Thus, the confidence of Beer <img src="image/B15923_08_Formula_01.png" alt="Formula"/> Wine is 1. Every time a customer bought beer, they also bought wine. It might be obvious, but for identifying actionable associations, higher confidence values are better.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor177"/>Lift and Leverage</h2>
			<p>We will discuss the next two metrics, lift and leverage, simultaneously, since despite being calculated differently, both seek to answer the same question. Like confidence, <strong class="bold">lift</strong> and <strong class="bold">leverage</strong> are notated as A <img src="image/B15923_08_Formula_01.png" alt="Formula"/> B. The question to which we seek an answer is, can one item, say A, be used to determine anything about another item, say B? Stated another way, if product A is bought by an individual, can we say anything about whether they will or will not purchase product B with some level of confidence? These questions are answered by comparing the support of A and B under the standard case when A and B are not assumed to be independent with the case where the two products are assumed to be independent. Lift calculates the ratio of these two cases, so its range is [0, Infinity]. When lift equals one, the two products are independent and, hence, no conclusions can be made about product B when product A is purchased:</p>
			<div>
				<div id="_idContainer278" class="IMG---Figure">
					<img src="image/B15923_08_07.jpg" alt="Figure 8.7: Formula for lift&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7: Formula for lift</p>
			<p>Leverage calculates the difference between the two cases, so its range is [-1, 1]. Leverage equaling zero can be interpreted the same way as lift equaling one:</p>
			<div>
				<div id="_idContainer279" class="IMG---Figure">
					<img src="image/B15923_08_08.jpg" alt="Figure 8.8: Formula for leverage&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8: Formula for leverage</p>
			<p>The values of the metrics measure the degree and orientation (in other words, positive or negative) of the relationship between the items. A value of lift other than 1 means that some dependency exists between the items. When the value is greater than 1, the second item is more likely to be purchased if the first item is purchased. Likewise, when the value is less than 1, the second item is less likely to be purchased if the first item is purchased. If the lift value is 0.1, we could say that the relationship between the two items is strong in the negative direction. That is, it could be said that when one product is purchased, the chance the second product is purchased is diminished. A lift of 1 indicates that the products are independent of one another. In the case of leverage, a positive value implies a positive association, while a negative value indicates a negative association. The positive and negative associations are separated by the points of independence, which, as stated earlier, are 1 for lift and 0 for leverage, and the further away the value gets from these points, the stronger the association.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor178"/>Conviction</h2>
			<p>The last metric to be discussed is conviction, which is a bit less intuitive than the other metrics. Conviction is the ratio of the expected frequency that X occurs without Y, given that X and Y are independent of the frequency of incorrect predictions. The frequency of incorrect predictions is defined as 1 minus the confidence of X <img src="image/B15923_08_Formula_01.png" alt="Formula"/> Y. Remember that confidence can be defined as <em class="italic">P(Y|X)</em>, which means <em class="italic">1 – P(Y|X) = P(Not Y|X)</em>. The numerator could also be thought of as <em class="italic">1 – P(Y|X) = P(Not Y|X)</em>. The only difference between the two is that the numerator has the assumption of independence between X and Y, while the denominator does not. A value greater than 1 is ideal because that means the association between products or item sets X and Y is incorrect more often if the association between X and Y is random (in other words, X and Y are independent). To reiterate, this stipulates that the association between X and Y is meaningful. A value of 1 applies independence, and a value of less than 1 signifies that the random chance X and Y relationship is correct more often than the X and Y relationship that has been defined as X <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Y. Under this situation, the relationship might go the other way (in other words, Y <img src="image/B15923_08_Formula_06.png" alt="Formula"/> X). Conviction has the range [0, Infinity] and the following form:</p>
			<div>
				<div id="_idContainer283" class="IMG---Figure">
					<img src="image/B15923_08_09.jpg" alt="Figure 8.9: Formula for conviction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9: Formula for conviction</p>
			<p>Let's again return to the products beer and wine, but for this explanation, we will consider the opposite association of Wine <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Beer. Support(Y) or, in this case, Support(Beer) is 3/10, and Confidence X <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Y, or, in this case, Confidence(Wine <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Beer) is 3/4. Thus, the Conviction(Wine <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Beer) is (1-3/10) / (1-3/4) = (7/10) * (4/1). We can conclude by saying that Wine <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Beer would be incorrect 2.8 times as often if wine and beer were independent. Thus, the previously articulated association between wine and beer is legitimate.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor179"/>Exercise 8.02: Computing Metrics</h2>
			<p>In this exercise, we'll use the fake data from <em class="italic">Exercise 8.01</em>, <em class="italic">Creating Sample Transaction Data</em>, to compute the five previously described metrics, which we will use again in the covering of the Apriori algorithm and association rules. The association on which these metrics will be evaluated is Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All exercises in this chapter need to be performed in the same Jupyter notebook.</p>
			<ol>
				<li value="1">Define and print the frequencies that are the basis of all five metrics, which would be Frequency(Milk), Frequency(Bread), and Frequency(Milk, Bread). Also, define <strong class="source-inline">N</strong> as the total number of transactions in the dataset:<p class="source-code"># the number of transactions</p><p class="source-code">N = len(example)</p><p class="source-code"># the frequency of milk</p><p class="source-code">f_x = sum(['milk' in i for i in example])</p><p class="source-code"># the frequency of bread</p><p class="source-code">f_y = sum(['bread' in i for i in example]) </p><p class="source-code"># the frequency of milk and bread</p><p class="source-code">f_x_y = sum([all(w in i for w in ['milk', 'bread']) \</p><p class="source-code">             for i in example])</p><p class="source-code"># print out the metrics computed above</p><p class="source-code">print("N = {}\n".format(N) + "Freq(x) = {}\n".format(f_x) \</p><p class="source-code">      + "Freq(y) = {}\n".format(f_y) \</p><p class="source-code">      + "Freq(x, y) = {}".format(f_x_y))</p><p>The output is as follows:</p><p class="source-code">N = 10</p><p class="source-code">Freq(x) = 7</p><p class="source-code">Freq(y) = 5</p><p class="source-code">Freq(x, y) = 4</p></li>
				<li>Calculate and print Support(Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread):<p class="source-code">support = f_x_y / N</p><p class="source-code">print("Support = {}".format(round(support, 4)))</p><p>The support of <strong class="source-inline">x</strong> to <strong class="source-inline">y</strong> is <strong class="source-inline">0.4</strong>. From experience, if we were working with a full transaction dataset, this support value would be considered very large in many cases.</p></li>
				<li>Calculate and print Confidence(Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread):<p class="source-code">confidence = support / (f_x / N)</p><p class="source-code">print("Confidence = {}".format(round(confidence, 4)))</p><p>The confidence of <strong class="source-inline">x</strong> to <strong class="source-inline">y</strong> is <strong class="source-inline">0.5714</strong>. This means that the probability of Y being purchased given that <strong class="source-inline">x</strong> was purchased is just slightly higher than 50%. </p></li>
				<li>Calculate and print Lift(Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread):<p class="source-code">lift = confidence / (f_y / N)</p><p class="source-code">print("Lift = {}".format(round(lift, 4)))</p><p>The lift of <strong class="source-inline">x</strong> to <strong class="source-inline">y</strong> is <strong class="source-inline">1.1429</strong>.</p></li>
				<li>Calculate and print Leverage(Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread):<p class="source-code">leverage = support - ((f_x / N) * (f_y / N))</p><p class="source-code">print("Leverage = {}".format(round(leverage, 4)))</p><p>The leverage of <strong class="source-inline">x</strong> to <strong class="source-inline">y</strong> is <strong class="source-inline">0.05</strong>. Both lift and leverage can be used to say that the association <strong class="source-inline">x</strong> to <strong class="source-inline">y</strong> is positive (in other words, <strong class="source-inline">x</strong> implies <strong class="source-inline">y</strong>) but weak. The values for lift and leverage are close to 1 and 0, respectively.</p></li>
				<li>Calculate and print Conviction(Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread):<p class="source-code">conviction = (1 - (f_y / N)) / (1 - confidence)</p><p class="source-code">print("Conviction = {}".format(round(conviction, 4)))</p><p>The conviction value of <strong class="source-inline">1.1667</strong> can be interpreted by saying the Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread association would be incorrect <strong class="source-inline">1.1667</strong> times as often if milk and bread were independent.</p><p>In this exercise, we explored a series of probabilistic metrics designed to quantify the relationship between two items. The five metrics are support, confidence, lift, leverage, and conviction. We will use these metrics again as they are the foundation of both the Apriori algorithm and association rules.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fhf9bS">https://packt.live/3fhf9bS</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/303vIBJ">https://packt.live/303vIBJ</a>.</p><p class="callout">You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>Before diving into the Apriori algorithm and association rule learning on actual data, we will explore transaction data and get some retail data loaded and prepped for modeling.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor180"/>Characteristics of Transaction Data</h1>
			<p>The data used in market basket analysis is transaction data or any type of data that resembles transaction data. In its most basic form, transaction data has some sort of transaction identifier, such as an invoice or transaction number, and a list of products associated with said identifier. It just so happens that these two base elements are all that is needed to perform market basket analysis. However, transaction data rarely – it is probably even safe to say never – comes in this basic form. Transaction data typically includes pricing information, dates and times, and customer identifiers, among many other things. Here is how each product is mapped to multiple invoices:</p>
			<div>
				<div id="_idContainer296" class="IMG---Figure">
					<img src="image/B15923_08_10.jpg" alt="Figure 8.10: Each available product is going to map back to multiple invoice numbers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10: Each available product is going to map back to multiple invoice numbers</p>
			<p>Due to the complexity of transaction data, data cleaning is crucial. The goal of data cleaning in the context of market basket analysis is to filter out all the unnecessary information, which includes removing variables in the data that are not relevant and filtering out problematic transactions. The techniques used to complete these two cleaning steps vary, depending on the particular transaction data file. In an attempt to not get bogged down in data cleaning, the exercises from here on out will use a subset of an online retail dataset from the UCI Machine Learning Repository, and the activities will use the entire dataset. This both limits the data cleaning discussion, but also gives us an opportunity to discuss how the results change when the size of the dataset changes. This is important because if you work for a retailer and run market basket analysis, it will be important to understand and be able to clearly articulate the fact that, as more data is received, product relationships can, and most likely will, shift. Before discussing the specific cleaning process required for this dataset, let's load the online retail dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In all subsequent exercises and activities, there could be slight differences in the output from what is shown in the following. This is due to one of two things: issues with data loading (in other words, rows getting shuffled) or the fact that <strong class="source-inline">mlxtend</strong> does not have a seed setting option to guarantee consistency across executions.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor181"/>Exercise 8.03: Loading Data</h2>
			<p>In this exercise, we will load and view an example online retail dataset. This dataset is originally sourced from the UCI Machine Learning Repository and can be found at <a href="https://packt.live/2XeT6ft">https://packt.live/2XeT6ft</a>. Once you have downloaded the dataset, save it and note the path. Now, let's proceed with the exercise. The output of this exercise is the transaction data that will be used in future modeling exercises and some exploratory figures to help us better understand the data with which we are working.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="http://archive.ics.uci.edu/ml/datasets/online+retail#">http://archive.ics.uci.edu/ml/datasets/online+retail#</a> (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science).</p>
			<p class="callout">Citation: This is a subset of the online retail dataset obtained from the UCI Machine Learning repository. Daqing Chen, Sai Liang Sain, and Kun Guo, <em class="italic">Data mining for the online retail industry</em>: <em class="italic">A case study of RFM model-based customer segmentation using data mining</em>, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012.</p>
			<p class="callout">It can be downloaded from <a href="https://packt.live/2XeT6ft">https://packt.live/2XeT6ft</a>.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Using the <strong class="source-inline">read_excel</strong> function from <strong class="source-inline">pandas</strong>, load the data. Note that we can define the first row as containing column names by adding <strong class="source-inline">header=0</strong> to the <strong class="source-inline">read_excel</strong> function:<p class="source-code">online = pandas.read_excel(io="./Online Retail.xlsx", \</p><p class="source-code">                           sheet_name="Online Retail", \</p><p class="source-code">                           header=0)</p><p class="callout-heading">Note</p><p class="callout">The path to <strong class="source-inline">Online Retail.xlsx</strong> should be changed as per the location of the file on your system.</p></li>
				<li>Print out the first 10 rows of the DataFrame. Notice that the data contains some columns that will not be relevant to market basket analysis:<p class="source-code">online.head(10)</p><p>The output is as follows:</p><div id="_idContainer297" class="IMG---Figure"><img src="image/B15923_08_11.jpg" alt="Figure 8.11: The raw online retail data &#13;&#10;"/></div><p class="figure-caption">Figure 8.11: The raw online retail data </p></li>
				<li>Print out the data type for each column in the DataFrame. This information will come in handy when trying to perform specific cleaning tasks. Columns need to be of the correct type in order for filtering and computing to execute as expected:<p class="source-code">online.dtypes</p><p>The output is as follows:</p><p class="source-code">InvoiceNo              object</p><p class="source-code">StockCode              object</p><p class="source-code">Description            object</p><p class="source-code">Quantity                int64</p><p class="source-code">InvoiceDate    datetime64[ns]</p><p class="source-code">UnitPrice             float64</p><p class="source-code">CustomerID            float64</p><p class="source-code">Country                object</p><p class="source-code">dtype: object</p></li>
				<li>Get the dimensions of the DataFrame, as well as the number of unique invoice numbers and customer identifications:<p class="source-code">print("Data dimension (row count, col count): {dim}" \</p><p class="source-code">      .format(dim=online.shape))</p><p class="source-code">print("Count of unique invoice numbers: {cnt}" \</p><p class="source-code">      .format(cnt=online.InvoiceNo.nunique()))</p><p class="source-code">print("Count of unique customer ids: {cnt}" \</p><p class="source-code">      .format(cnt=online.CustomerID.nunique()))</p><p>The output is as follows:</p><p class="source-code">Data dimension (row count, col count): (541909, 8)</p><p class="source-code">Count of unique invoice numbers: 25900</p><p class="source-code">Count of unique customer ids: 4372</p></li>
			</ol>
			<p>From the preceding output, we can say that we successfully loaded the data and obtained some key information which will be further used as we progress with exercises in this chapter.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fhf9bS">https://packt.live/3fhf9bS</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/303vIBJ">https://packt.live/303vIBJ</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor182"/>Data Cleaning and Formatting</h2>
			<p>With the dataset now loaded, let's delve into the specific data cleaning processes to be performed. Since we are going to filter the data down to just the invoice numbers and items, we focus the data cleaning on these two columns of the dataset. Remember that market basket analysis looks to identify associations between the items purchased by all customers over time. As such, the main focus of data cleaning involves removing transactions with a non-positive number of items. This could happen when the transaction involves voiding another transaction, when items are returned, or when the transaction is an administrative task. These types of transactions will be filtered out in two ways. The first is that canceled transactions have invoice numbers that are prefixed with "C," so we will identify those specific invoice numbers and remove them from the data. The other approach is to remove all transactions with either zero or a negative number of items. After performing these two steps, the data will be subset down to just the invoice number and item description columns, and any row of the now two-column dataset with at least one missing value is removed.</p>
			<p>The next stage of the data cleaning exercise involves putting the data in the appropriate format for modeling. In this and subsequent exercises, we will use a subset of the full data. The subset will be created by taking the first 5,000 unique invoice numbers. Once we have cut the data down to the first 5,000 unique invoice numbers, we change the data structure to the structure needed to run the models. Note that the data currently features one item per row, so transactions with multiple items take up multiple rows. The desired format is a list of lists, like the made-up data from earlier in the chapter. Each subset list represents a unique invoice number, so in this case, the outer list should contain 5,000 sub-lists. The elements of the sub-lists are all the items belonging to the invoice number that that sub-list represents. With the cleaning process described, let's proceed to the exercise.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor183"/>Exercise 8.04: Data Cleaning and Formatting</h2>
			<p>In this exercise, we will perform the cleaning steps described previously. As we work through the process, the evolution of the data will be monitored by printing out the current state of the data and computing some basic summary metrics. Be sure to perform data cleaning in the same notebook in which the data is loaded:</p>
			<ol>
				<li value="1">Create an indicator column stipulating whether the invoice number begins with "<strong class="source-inline">C</strong>":<p class="source-code"># create new column called IsCPresent</p><p class="source-code">online['IsCPresent']  = (# looking for C in InvoiceNo column \</p><p class="source-code">                         .astype(str)\</p><p class="source-code">                         .apply(lambda x: 1 if x.find('C') \</p><p class="source-code">                                != -1 else 0))</p></li>
				<li>Filter out all transactions having either zero or a negative number of items (in other words, items were returned), remove all invoice numbers starting with "<strong class="source-inline">C</strong>" using the column created in step one, subset the DataFrame down to <strong class="source-inline">InvoiceNo</strong> and <strong class="source-inline">Description</strong>, and lastly, drop all rows with at least one missing value. Rename the DataFrame <strong class="source-inline">online1</strong>:<p class="source-code">online1 = (online\</p><p class="source-code">           # filter out non-positive quantity values\</p><p class="source-code">           .loc[online["Quantity"] &gt; 0]\</p><p class="source-code">           # remove InvoiceNos starting with C\</p><p class="source-code">           .loc[online['IsCPresent'] != 1]\</p><p class="source-code">           # column filtering\</p><p class="source-code">           .loc[:, ["InvoiceNo", "Description"]]\</p><p class="source-code">           # dropping all rows with at least one missing value\</p><p class="source-code">           .dropna())</p></li>
				<li>Print out the first 10 rows of the filtered DataFrame, <strong class="source-inline">online1</strong>:<p class="source-code">online1.head(10)</p><p>The output is as follows:</p><div id="_idContainer298" class="IMG---Figure"><img src="image/B15923_08_12.jpg" alt="Figure 8.12: The cleaned online retail dataset&#13;&#10;"/></div><p class="figure-caption">Figure 8.12: The cleaned online retail dataset</p></li>
				<li>Print out the dimensions of the cleaned DataFrame and the number of unique invoice numbers using the <strong class="source-inline">nunique</strong> function, which counts the number of unique values in a DataFrame column:<p class="source-code">print("Data dimension (row count, col count): {dim}"\</p><p class="source-code">      .format(dim=online1.shape)\</p><p class="source-code">)</p><p class="source-code">print("Count of unique invoice numbers: {cnt}"\</p><p class="source-code">      .format(cnt=online1.InvoiceNo.nunique())\</p><p class="source-code">)</p><p>The output is as follows:</p><p class="source-code">Data dimension (row count, col count): (530693, 2)</p><p class="source-code">Count of unique invoice numbers: 20136</p><p>Notice that we have already removed approximately 10,000 rows and 5,800 invoice numbers.</p></li>
				<li>Extract the invoice numbers from the DataFrame as a list. Remove duplicate elements to create a list of unique invoice numbers. Confirm that the process was successful by printing the length of the list of unique invoice numbers. Compare with the output of <em class="italic">Step 4</em>:<p class="source-code">invoice_no_list = online1.InvoiceNo.tolist()</p><p class="source-code">invoice_no_list = list(set(invoice_no_list))</p><p class="source-code">print("Length of list of invoice numbers: {ln}" \</p><p class="source-code">      .format(ln=len(invoice_no_list)))</p><p>The output is as follows:</p><p class="source-code">Length of list of invoice numbers: 20136</p></li>
				<li>Take the list from <em class="italic">Step 5</em> and cut it to only include the first 5,000 elements. Print out the length of the new list to confirm that it is, in fact, the expected length of 5,000:<p class="source-code">subset_invoice_no_list = invoice_no_list[0:5000]</p><p class="source-code">print("Length of subset list of invoice numbers: {ln}"\</p><p class="source-code">      .format(ln=len(subset_invoice_no_list)))</p><p>The output is as follows:</p><p class="source-code">Length of subset list of invoice numbers: 5000</p></li>
				<li>Filter the <strong class="source-inline">online1</strong> DataFrame down by only keeping the invoice numbers in the list from the preceding step:<p class="source-code">online1 = online1.loc[online1["InvoiceNo"]\</p><p class="source-code">                     .isin(subset_invoice_no_list)]</p></li>
				<li>Print out the first 10 rows of <strong class="source-inline">online1</strong>:<p class="source-code">online1.head(10)</p><p>The output is as follows:</p><div id="_idContainer299" class="IMG---Figure"><img src="image/B15923_08_13.jpg" alt="Figure 8.13: The cleaned dataset with only 5,000 unique invoice numbers&#13;&#10;"/></div><p class="figure-caption">Figure 8.13: The cleaned dataset with only 5,000 unique invoice numbers</p></li>
				<li>Print out the dimensions of the DataFrame and the number of unique invoice numbers to confirm that the filtering and cleaning process was successful:<p class="source-code">print("Data dimension (row count, col count): {dim}"\</p><p class="source-code">      .format(dim=online1.shape))</p><p class="source-code">print("Count of unique invoice numbers: {cnt}"\</p><p class="source-code">      .format(cnt=online1.InvoiceNo.nunique()))</p><p>The output is as follows:</p><p class="source-code">Data dimension (row count, col count): (133315, 2)</p><p class="source-code">Count of unique invoice numbers: 5000</p></li>
				<li>Transform the data in <strong class="source-inline">online1</strong> into the aforementioned list of lists called <strong class="source-inline">invoice_item_list</strong>. The process for doing this is to iterate over the unique invoice numbers and, at each iteration, extract the item descriptions as a list and append that list to the larger <strong class="source-inline">invoice_item_list</strong> list. Print out elements one through four of the list:<p class="source-code">invoice_item_list = []</p><p class="source-code">for num in list(set(online1.InvoiceNo.tolist())):</p><p class="source-code">    # filter dataset down to one invoice number</p><p class="source-code">    tmp_df = online1.loc[online1['InvoiceNo'] == num]</p><p class="source-code">    # extract item descriptions and convert to list</p><p class="source-code">    tmp_items = tmp_df.Description.tolist()</p><p class="source-code">    # append list invoice_item_list</p><p class="source-code">    invoice_item_list.append(tmp_items)</p><p class="source-code">print(invoice_item_list[1:5])</p><p>The output is as follows:</p><div id="_idContainer300" class="IMG---Figure"><img src="image/B15923_08_14.jpg" alt="Figure 8.14: Four elements of the list of lists&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.14: Four elements of the list of lists</p>
			<p>In the preceding list of lists, each sub-list contains all the items belonging to an individual invoice.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This step can take some minutes to complete.</p>
			<p>In this exercise, the DataFrame was filtered and subset to only the needed columns and the relevant rows. We then cut the full dataset down to the first 5,000 unique invoice numbers. The full dataset will be used in the forthcoming activities. The last step converted the DataFrame to a list of lists, which is the format the data needs to be in for the encoder that will be discussed next.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fhf9bS">https://packt.live/3fhf9bS</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/303vIBJ">https://packt.live/303vIBJ</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor184"/>Data Encoding</h2>
			<p>While cleaning the data is crucial, the most important part of the data preparation process is molding the data into the correct form. Before running the models, the data, currently in the list of lists form, needs to be encoded and recast as a DataFrame. To do this, we will leverage <strong class="source-inline">TransactionEncoder</strong> from the <strong class="source-inline">preprocessing</strong> module of <strong class="source-inline">mlxtend</strong>. The output from the encoder is a multidimensional array, where each row is the length of the total number of unique items in the transaction dataset and the elements are Boolean variables, indicating whether that particular item is linked to the invoice number that row represents. With the data encoded, we can recast it as a DataFrame where the rows are the invoice numbers and the columns are the unique items in the transaction dataset.</p>
			<p>In the following exercise, the data encoding will be done using <strong class="source-inline">mlxtend</strong>, but it is very easy to encode the data without using a package. The first step is to unlist the list of lists and return one list with every value from the original list of lists. Next, the duplicate products are filtered out and, if preferred, the data is sorted in alphabetical order. Before doing the actual encoding, we initialize the final DataFrame by having all elements equal to false, the number of rows equal to the number of invoice numbers in the dataset, and column names equal to the non-duplicated list of product names. </p>
			<p>In this case, we have 5,000 transactions and over 3,100 unique products. Thus, the DataFrame has over 15,000,000 elements. The actual encoding is done by looping over each transaction and each item in each transaction. Change the row <em class="italic">i</em> and column <em class="italic">j</em> cell values in the initialized dataset from <strong class="source-inline">false</strong> to <strong class="source-inline">true</strong> if the <em class="italic">i</em><span class="superscript">th</span> transaction contains the <em class="italic">j</em><span class="superscript">th</span> product. This double loop is not fast as we need to iterate over 15,000,000 cells. There are ways to improve performance, including some that have been implemented in <strong class="source-inline">mlxtend</strong>, but to better understand the process, it is helpful to work through the double loop methodology. The following is an example function to do the encoding from scratch without the assistance of a package other than <strong class="source-inline">pandas</strong>:</p>
			<p class="source-code">def manual_encoding(ll):</p>
			<p class="source-code">    # unlist the list of lists input</p>
			<p class="source-code">    # result is one list with all the elements of the sublists</p>
			<p class="source-code">    list_dup_unsort_items = \</p>
			<p class="source-code">    [element for sub in ll for element in sub]</p>
			<p class="source-code">    # two cleaning steps:</p>
			<p class="source-code">    """</p>
			<p class="source-code">    1. remove duplicate items, only want one of each item in list</p>
			<p class="source-code">    """</p>
			<p class="source-code">    #     2. sort items in alphabetical order</p>
			<p class="source-code">    list_nondup_sort_items = \</p>
			<p class="source-code">    sorted(list(set(list_dup_unsort_items)))</p>
			<p class="source-code">    # initialize DataFrame with all elements having False value</p>
			<p class="source-code">    # name the columns the elements of list_dup_unsort_items</p>
			<p class="source-code">    manual_df = pandas.DataFrame(False, \</p>
			<p class="source-code">                                 index=range(len(ll)), \</p>
			<p class="source-code">                                 columns=list_dup_unsort_items)</p>
			<p class="source-code">    """</p>
			<p class="source-code">    change False to True if element is </p>
			<p class="source-code">    in individual transaction list</p>
			<p class="source-code">    """</p>
			<p class="source-code">    """</p>
			<p class="source-code">    each row is represents the contains of an individual transaction</p>
			<p class="source-code">    """</p>
			<p class="source-code">    # (sublist from the original list of lists)</p>
			<p class="source-code">    for i in range(len(ll)):</p>
			<p class="source-code">        for j in ll[i]:</p>
			<p class="source-code">            manual_df.loc[i, j] = True</p>
			<p class="source-code">    # return the True/False DataFrame</p>
			<p class="source-code">    return manual_df</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor185"/>Exercise 8.05: Data Encoding</h2>
			<p>In this exercise, we'll continue the data preparation process by taking the list of lists generated in the preceding exercise and encoding the data in the specific way required to run the models:</p>
			<ol>
				<li value="1">Initialize and fit the transaction encoder. Print out an example of the resulting data:<p class="source-code">online_encoder = mlxtend.preprocessing.TransactionEncoder()</p><p class="source-code">online_encoder_array = online_encoder\</p><p class="source-code">                       .fit_transform(invoice_item_list)</p><p class="source-code">print(online_encoder_array)</p><p>The output is as follows:</p><p class="source-code">[[False False False ... False False False]</p><p class="source-code"> [False False False ... False False False]</p><p class="source-code"> [False False False ... False False False]</p><p class="source-code"> ...</p><p class="source-code"> [False False False ... False False False]</p><p class="source-code"> [False False False ... False False False]</p><p class="source-code"> [False False False ... False False False]]</p><p>The preceding array contains the Boolean variables indicating the product presence in each transaction.</p></li>
				<li>Recast the encoded array as a DataFrame named <strong class="source-inline">online_encoder_df</strong>. Print the predefined subset of the DataFrame that features both <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong> values:<p class="source-code">online_encoder_df = pandas.DataFrame(online_encoder_array, \</p><p class="source-code">                                     columns=online_encoder\</p><p class="source-code">                                     .columns_)</p><p class="source-code">"""</p><p class="source-code">this is a very big table, so for more </p><p class="source-code">easy viewing only a subset is printed</p><p class="source-code">"""</p><p class="source-code">online_encoder_df.loc[4970:4979, \</p><p class="source-code">                      online_encoder_df.columns.tolist()[0:8]]</p><p>The output will be similar to the following:</p><div id="_idContainer301" class="IMG---Figure"><img src="image/B15923_08_15.jpg" alt="Figure 8.15: A small section of the encoded data recast as a DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 8.15: A small section of the encoded data recast as a DataFrame</p></li>
				<li>Print out the dimensions of the encoded DataFrame. It should have 5,000 rows because the data used to generate it was previously filtered down to 5,000 unique invoice numbers:<p class="source-code">print("Data dimension (row count, col count): {dim}"\</p><p class="source-code">      .format(dim=online_encoder_df.shape))</p><p>The output will be similar to the following:</p><p class="source-code">Data dimension (row count, col count): (5000, 3135)</p></li>
			</ol>
			<p>The data is now prepared for modeling, which we will perform in <em class="italic">Exercise 8.06</em>, <em class="italic">Executing the Apriori Algorithm</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fhf9bS">https://packt.live/3fhf9bS</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/303vIBJ">https://packt.live/303vIBJ</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor186"/>Activity 8.01: Loading and Preparing Full Online Retail Data</h2>
			<p>In this activity, we are charged with loading and preparing a large transaction dataset for modeling. The final output will be an appropriately encoded dataset that has one row for each unique transaction in the dataset, and one column for each unique item in the dataset. If an item appears in an individual transaction, that element of the DataFrame will be marked <strong class="source-inline">true</strong>.</p>
			<p>This activity will largely repeat the last few exercises but will use the complete online retail dataset file. No new downloads need to be executed, but you will need the path to the file downloaded previously. Perform this activity in a separate Jupyter notebook.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Load the online retail dataset file.<p class="callout-heading">Note</p><p class="callout">This dataset is sourced from <a href="http://archive.ics.uci.edu/ml/datasets/online+retail#">http://archive.ics.uci.edu/ml/datasets/online+retail#</a> (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science).</p><p class="callout">Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, <em class="italic">Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining</em>, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012.</p><p class="callout">It can be downloaded from <a href="https://packt.live/39Nx3iQ">https://packt.live/39Nx3iQ</a>.</p></li>
				<li>Clean and prepare the data for modeling, including turning the cleaned data into a list of lists.</li>
				<li>Encode the data and recast it as a DataFrame.<p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found on page 490.</p></li>
			</ol>
			<p>The output will be similar to the following:</p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<img src="image/B15923_08_16.jpg" alt="Figure 8.16: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16: A subset of the cleaned, encoded, and recast DataFrame built from the complete online retail dataset</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor187"/>The Apriori Algorithm</h1>
			<p>The <strong class="bold">Apriori</strong> algorithm is a data mining methodology for identifying and quantifying frequent item sets in transaction data and is the foundational component of association rule learning. Extending the results of the Apriori algorithm to association rule learning will be discussed in the next section. The threshold for an item set being frequent is an input (in other words, a hyperparameter) of the model and, as such, is adjustable. Frequency is quantified here as support, so the value input into the model is the minimum support acceptable for the analysis being done. The model then identifies all item sets whose support is greater than, or equal to, the minimum support provided to the model. </p>
			<p class="callout-heading">Note </p>
			<p class="callout">The minimum support hyperparameter is not a value that can be optimized via grid search because there is no evaluation metric for the Apriori algorithm. Instead, the minimum support parameter is set based on the data, the use case, and domain expertise.</p>
			<p>The main idea behind the Apriori algorithm is the Apriori principle: any subset of a frequent item set must itself be frequent.</p>
			<p>Another aspect worth mentioning is the corollary: no superset of an infrequent item set can be frequent.</p>
			<p>Let's take some examples. If the item set {hammer, saw, and nail} is frequent, then, according to the Apriori principle, and what is hopefully obvious, any less complex item set derived from it, say {hammer, saw}, is also frequent. On the contrary, if that same item set, {hammer, saw, nail}, is infrequent, then adding complexity, such as incorporating wood into the item set {hammer, saw, nail, wood}, is not going to result in the item set becoming frequent.</p>
			<p>It might seem straightforward to calculate the support value for every item set in a transactional database and only return those item sets whose support is greater than or equal to the pre-specified minimum support threshold, but it is not because of the number of computations that need to happen. For example, take an item set with 10 unique items. This would result in 1,023 individual item sets for which support would need to be calculated. Now, try to extrapolate out to our working dataset that has 3,135 unique items. That is going to be an enormous number of item sets for which we need to compute a support value. Computational efficiency is a major issue:</p>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<img src="image/B15923_08_17.jpg" alt="Figure 8.17: Representation of the computational efficiency issue&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.17: Representation of the computational efficiency issue</p>
			<p>The preceding diagram shows a mapping of how item sets are built and how the Apriori principle can greatly decrease the computational requirements (all the grayed-out nodes are infrequent).</p>
			<p>In order to address the computational demands, the Apriori algorithm is defined as a bottom-up model that has two steps. These steps involve generating candidate item sets by adding items to already existing frequent item sets and testing these candidate item sets against the dataset to determine whether these candidate item sets are also frequent. No support value is computed for item sets that contain infrequent item sets. This process repeats until no further candidate item sets exist:</p>
			<div>
				<div id="_idContainer304" class="IMG---Figure">
					<img src="image/B15923_08_18.jpg" alt="Figure 8.18: General Apriori algorithm structure&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18: General Apriori algorithm structure</p>
			<p>Assuming a minimum support threshold of 0.4, the preceding diagram shows the general Apriori algorithm structure.</p>
			<p><em class="italic">Figure 8.20</em> includes establishing an item set, computing support values, filtering out infrequent item sets, creating new item sets, and repeating the process.</p>
			<p>There is a clear tree-like structure that serves as the path for identifying candidate item sets. The specific search technique used, which was built for traversing tree-like data structures, is called a breadth-first search, which means that each step of the search process focuses on completely searching one level of the tree before moving on instead of searching branch by branch.</p>
			<p>The high-level steps of the algorithm are designed to do the following:</p>
			<ol>
				<li value="1">Define the set of frequent items (in other words, choose only those items that have support greater than the pre-defined threshold). To start, this is typically the set of individual items.</li>
				<li>Derive candidate item sets by combining frequent item sets. Move up in size one item at a time. That is, go from item sets with one item to two, two to three, and so on.</li>
				<li>Compute the support value for each candidate item set.</li>
				<li>Create a new frequent item set made up of the candidate item sets whose support value exceeds the specified threshold. </li>
			</ol>
			<p>Repeat <em class="italic">Steps 1</em> to <em class="italic">4</em> until there are no more frequent item sets; that is, until we have worked through all the combinations.</p>
			<p>The pseudo code for the Apriori algorithm is as follows:</p>
			<p class="source-code">L<span class="subscript">1</span> = {frequent items}</p>
			<p class="source-code">k = 1</p>
			<p class="source-code">L = {}</p>
			<p class="source-code">while L<span class="subscript">k</span>.Length is not an empty set do</p>
			<p class="source-code">    C<span class="subscript">k+1</span> = candidate item sets derived from L<span class="subscript">k</span></p>
			<p class="source-code">    For each transaction t in the dataset do</p>
			<p class="source-code">        Increment the count of the candidates \</p>
			<p class="source-code">            in C<span class="subscript">k+1</span> that appear in t</p>
			<p class="source-code">    Compute the support for the candidates in C<span class="subscript">k+1</span> \</p>
			<p class="source-code">        using the appearance counts</p>
			<p class="source-code">    L<span class="subscript">k+1</span> = the candidates in C<span class="subscript">k+1</span> meeting \</p>
			<p class="source-code">          the minimum support requirement</p>
			<p class="source-code">    L.append(L<span class="subscript">k</span>)</p>
			<p class="source-code">    k = k + 1</p>
			<p class="source-code">End</p>
			<p class="source-code">Return L = all frequent item sets with corresponding support values</p>
			<p>Despite the Apriori principle, this algorithm can still face significant computational challenges depending on the size of the transaction dataset. There are several strategies currently accepted to further reduce the computational demands.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor188"/>Computational Fixes</h2>
			<p>Transaction reduction is an easy way to reduce the computational load. Note that after each candidate set of items is generated, the entirety of the transaction data needs to be scanned in order to count the number of appearances of each candidate item set. If we could shrink the size of the transaction dataset, the size of the dataset scan would decrease dramatically. The shrinking of the transaction dataset is done by realizing that any transaction containing no frequent item sets in the <em class="italic">i</em><span class="superscript">th </span>iteration is not going to contain any frequent item sets in subsequent iterations. Therefore, once each transaction contains no frequent item sets, it can be removed from the transaction dataset used for future scans.</p>
			<p>Sampling the transaction dataset and testing each candidate item set against it is another approach to reducing the computational requirements associated with scanning the transaction dataset to calculate the support of each item set. When this approach is implemented, it is important to lower the minimum support requirement to guarantee that no item sets that should be present in the final data are left out. Given that the sampled transaction dataset will naturally cause the support values to be smaller, leaving the minimum support at its original value will incorrectly remove what should be frequent item sets from the output of the model.</p>
			<p>A similar approach is partitioning. In this case, the dataset is randomly partitioned into several individual datasets on which the evaluation of each candidate item set is executed. Item sets are deemed frequent in the full transaction dataset if frequent in one of the partitions. Each partition is scanned consecutively until the frequency for an item set is established. Like sampling, partitioning is just another way to avoid testing each item set on the full dataset, which could be very computationally expensive if the full dataset is really big. If the frequency is established on the first partition, then we have established it for the whole dataset without testing it against a majority of the partitions.</p>
			<p>Regardless of whether or not one of these techniques is employed, the computational requirements are always going to be fairly substantial when it comes to the Apriori algorithm. As should now be clear, the essence of the algorithm, the computation of support, is not as complex as other models discussed in this text.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor189"/>Exercise 8.06: Executing the Apriori Algorithm</h2>
			<p>The execution of the Apriori algorithm is made easy with <strong class="source-inline">mlxtend</strong>. As a result, this exercise will focus on how to manipulate the output dataset and to interpret the results. You will recall that the cleaned and encoded transaction data was defined as <strong class="source-inline">online_encoder_df</strong>: </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Perform this exercise in the same notebook that all previous exercises were run in as we will continue using the environment, data, and results already established in that notebook. (So, you should be using the notebook that contains the reduced dataset of 5,000 invoices, not the full dataset as was used in the activity.)</p>
			<ol>
				<li value="1">Run the Apriori algorithm using <strong class="source-inline">mlxtend</strong> without changing any of the default parameter values:<p class="source-code">mod = mlxtend.frequent_patterns.apriori(online_encoder_df)</p><p class="source-code">mod</p><p>The output is an empty DataFrame. The default minimum support value is set to 0.5, so since an empty DataFrame was returned, we know that all item sets have a support value of less than 0.5. Depending on the number of transactions and the diversity of available items, having no item set with a plus 0.5 support value is not unusual.</p></li>
				<li>Rerun the Apriori algorithm, but with the minimum support set to 0.01: <p class="source-code">mod_minsupport = mlxtend.frequent_patterns\</p><p class="source-code">                 .apriori(online_encoder_df, \</p><p class="source-code">                 min_support=0.01)</p><p class="source-code">mod_minsupport.loc[0:6]</p><p>This minimum support value is the same as saying that when analyzing 5,000 transactions, we need an item set to appear 50 times to be considered frequent. As mentioned previously, the minimum support can be set to any value in the range [0,1]. There is no best minimum support value; the setting of this value is entirely subjective. Many businesses have their own specific thresholds for significance, but there is no industry standard or method for optimizing this value.</p><p>The output will be similar to the following:</p><div id="_idContainer305" class="IMG---Figure"><img src="image/B15923_08_19.jpg" alt="Figure 8.19: Basic output of the Apriori algorithm run using mlxtend&#13;&#10;"/></div><p class="figure-caption">Figure 8.19: Basic output of the Apriori algorithm run using mlxtend</p><p>Notice that the item sets are designated numerically in the output, which makes the results hard to interpret.</p></li>
				<li>Rerun the Apriori algorithm with the same minimum support as in <em class="italic">Step 2</em>, but this time set <strong class="source-inline">use_colnames</strong> to <strong class="source-inline">True</strong>. This will replace the numerical designations with the actual item names:<p class="source-code">mod_colnames_minsupport = mlxtend.frequent_patterns\</p><p class="source-code">                          .apriori(online_encoder_df, \</p><p class="source-code">                                   min_support=0.01, \</p><p class="source-code">                                   use_colnames=True)</p><p class="source-code">mod_colnames_minsupport.loc[0:6]</p><p>The output will be similar to the following:</p><div id="_idContainer306" class="IMG---Figure"><img src="image/B15923_08_20.jpg" alt="Figure 8.20: The output of the Apriori algorithm with the actual item names instead of numerical designations&#13;&#10;"/></div><p class="figure-caption">Figure 8.20: The output of the Apriori algorithm with the actual item names instead of numerical designations</p><p>This DataFrame contains every item set whose support value is greater than the specified minimum support value. That is, these item sets occur with sufficient frequency to potentially be meaningful and therefore actionable.</p></li>
				<li>Add an additional column to the output of <em class="italic">Step 3</em> that contains the size of the item set (in other words, how many items are in the set), which will help with filtering and further analysis:<p class="source-code">mod_colnames_minsupport['length'] = \</p><p class="source-code">(mod_colnames_minsupport['itemsets'].apply(lambda x: len(x)))</p><p class="source-code">mod_colnames_minsupport.loc[0:6]</p><p>The output will be similar to the following:</p><div id="_idContainer307" class="IMG---Figure"><img src="image/B15923_08_21.jpg" alt="Figure 8.21: The Apriori algorithm output plus an additional column containing the lengths of the item sets&#13;&#10;"/></div><p class="figure-caption">Figure 8.21: The Apriori algorithm output plus an additional column containing the lengths of the item sets</p></li>
				<li>Find the support of the item set containing <strong class="source-inline">10 COLOUR SPACEBOY PEN</strong>:<p class="source-code">mod_colnames_minsupport[mod_colnames_minsupport['itemsets'] \</p><p class="source-code">                        == frozenset({'10 COLOUR SPACEBOY PEN'})]</p><p>The output is as follows:</p><div id="_idContainer308" class="IMG---Figure"><img src="image/B15923_08_22.jpg" alt="Figure 8.22: The output DataFrame filtered down to a single item set&#13;&#10;"/></div><p class="figure-caption">Figure 8.22: The output DataFrame filtered down to a single item set</p><p>This single-row DataFrame gives us the support value for this specific item set that contains one item. The support value says that this specific item set appears in 1.78% of the transactions.</p></li>
				<li>Return all item sets of length 2 whose support is in the range [0.02, 0.021]:<p class="source-code">mod_colnames_minsupport[(mod_colnames_minsupport['length'] == 2) \</p><p class="source-code">                         &amp; (mod_colnames_minsupport\</p><p class="source-code">                            ['support'] &gt;= 0.02) \</p><p class="source-code">                         &amp; (mod_colnames_minsupport\</p><p class="source-code">                            ['support'] &lt; 0.021)]</p><p>The output will be similar to the following:</p><div id="_idContainer309" class="IMG---Figure"><img src="image/B15923_08_23.jpg" alt="Figure 8.23: The Apriori algorithm output DataFrame filtered by length and support&#13;&#10;"/></div><p class="figure-caption">Figure 8.23: The Apriori algorithm output DataFrame filtered by length and support</p><p>This DataFrame contains all the item sets (pairs of items bought together) whose support value is in the range specified at the start of the step. Each of these item sets appears in between 2.0% and 2.1% of transactions.</p><p>Note that when filtering on <strong class="source-inline">support</strong>, it is wise to specify a range instead of a specific value since it is quite possible to pick a value for which there are no item sets. The preceding output has 32 item sets; only a subset is shown. Keep note of the particular items in the item sets because we will be running this same filter when we scale up to the full data and we will want to execute a comparison.</p></li>
				<li>Plot the support values. Note that this plot will have no support values less than <strong class="source-inline">0.01</strong> because that was the value used as the minimum support in <em class="italic">Step 2</em>:<p class="source-code">mod_colnames_minsupport.hist("support", grid=False, bins=30)</p><p class="source-code">plt.xlabel("Support of item")</p><p class="source-code">plt.ylabel("Number of items")</p><p class="source-code">plt.title("Frequency distribution of Support")</p><p class="source-code">plt.show()</p><p>The output will be similar to the following plot:</p><div id="_idContainer310" class="IMG---Figure"><img src="image/B15923_08_24.jpg" alt="Figure 8.24: Distribution of the support values returned by the Apriori algorithm&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.24: Distribution of the support values returned by the Apriori algorithm</p>
			<p>The maximum support value is approximately 0.14, which is approximately 700 transactions. What might appear to be a small value may not be, given the number of products available. Larger numbers of products tend to result in lower support values because the variability of item combinations increases.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fhf9bS">https://packt.live/3fhf9bS</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/303vIBJ">https://packt.live/303vIBJ</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<p>Hopefully, you can think of more ways in which this data could be used. We will generate even more useful information in the next section by using the Apriori algorithm results to generate association rules.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor190"/>Activity 8.02: Running the Apriori Algorithm on the Complete Online Retail Dataset</h2>
			<p>Imagine you work for an online retailer. You are given all the transaction data from the last month and told to find all the item sets appearing in at least 1% of the transactions. Once the qualifying item sets are identified, you are subsequently told to identify the distribution of the support values. The distribution of support values will tell all interested parties whether groups of items exist that are purchased together with high probability as well as the mean of the support values. Let's collect all the information for the company's leadership and strategists.</p>
			<p>In this activity, you will run the Apriori algorithm on the full online retail dataset. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This dataset is sourced from <a href="http://archive.ics.uci.edu/ml/datasets/online+retail#">http://archive.ics.uci.edu/ml/datasets/online+retail#</a> (UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science).</p>
			<p class="callout">Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, <em class="italic">Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining</em>, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012.</p>
			<p class="callout">It can be downloaded from <a href="https://packt.live/39Nx3iQ">https://packt.live/39Nx3iQ</a>.</p>
			<p>Ensure that you complete this activity in the same notebook as the preceding activity (in other words, the notebook that uses the full dataset, not the notebook that uses the subset of 5,000 invoices that you're using for the exercises).</p>
			<p>This will also provide you with an opportunity to compare the results with those generated using only 5,000 transactions. This is an interesting activity, as it provides some insight into the ways in which the data may change as more data is collected, as well as some insight into how support values change when the partitioning technique is employed. Note that what was done in the exercises is not a perfect representation of the partitioning technique because 5,000 was an arbitrary number of transactions to sample.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the activities in this chapter need to be performed in the same notebook.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Run the Apriori algorithm on the full data with reasonable parameter settings.</li>
				<li>Filter the results down to the item set containing <strong class="source-inline">10 COLOUR SPACEBOY PEN</strong>. Compare the support value to that of <em class="italic">Exercise 8.06</em>, <em class="italic">Executing the Apriori Algorithm</em>.</li>
				<li>Add another column containing the item set length. Then, filter down to those item sets whose length is two and whose support is in the range [0.02, 0.021]. Compare this to the result from <em class="italic">Exercise 8.06</em>, <em class="italic">Executing the Apriori Algorithm</em>.</li>
				<li>Plot the <strong class="source-inline">support</strong> values.<p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found on page 492.</p></li>
			</ol>
			<p>The output of this activity will be similar to the following:</p>
			<div>
				<div id="_idContainer311" class="IMG---Figure">
					<img src="image/B15923_08_25.jpg" alt="Figure 8.25: Distribution of support values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.25: Distribution of support values</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor191"/>Association Rules</h1>
			<p>Association rule learning is a machine learning model that seeks to unearth the hidden patterns (in other words, relationships) in transaction data that describe the shopping habits of the customers of any retailer. The definition of an association rule was hinted at when the common probabilistic metrics were defined and explained earlier in the chapter. </p>
			<p>Consider the imaginary frequent item set {Milk, Bread}. Two association rules can be formed from that item set: Milk <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Bread and Bread <img src="image/B15923_08_Formula_06.png" alt="Formula"/> Milk. For simplicity, the first item set in the association rule is referred to as the antecedent, while the second item set in the association rule is referred to as the consequent. Once the association rules have been identified, all the previously discussed metrics can be computed to evaluate the validity of the association rules, determining whether or not the rules can be leveraged in the decision-making process.</p>
			<p>The establishment of an association rule is based on support and confidence. Support, as we discussed in the last section, identifies which item sets are frequent, while confidence measures the frequency of truthfulness for a particular rule. Confidence is typically referred to as one of the measures of interestingness, as it is one of the metrics that determines whether an association should be formed. Thus, the establishment of an association rule is a two-step process. Identify frequent datasets and then evaluate the confidence of a candidate association rule and, if that confidence value exceeds some arbitrary threshold, the result is an association rule.</p>
			<p>A major issue of association rule learning is the discovery of spurious associations, which are highly likely given the huge numbers of potential rules. Spurious associations are defined as associations that occur with surprising regularity in the data, given that the association occurs entirely by chance. To clearly articulate the idea, assume we are in a situation where we have 100 candidate rules. If we run a statistical test for independence at the 0.05 significance level, we are still faced with a 5% chance that an association is found when no association exists. Let's further assume that all 100 candidate rules are not valid associations. Given the 5% chance, we should still expect to find 5 valid association rules. Now, scale the imaginary candidate rule list up to millions or billions, so that 5% amounts to an enormous number of associations. This problem is not unlike the issue of statistical significance and error faced by virtually every model. It is worth calling out that some techniques exist to combat the spurious association issue, but they are neither consistently incorporated into the frequently used association rule libraries nor in the scope of this chapter.</p>
			<p>Let's now apply our working knowledge of association rule learning to the online retail dataset.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor192"/>Exercise 8.07: Deriving Association Rules</h2>
			<p>In this exercise, we will derive association rules for the online retail dataset and explore the associated metrics.  </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Ensure that you complete this exercise in the same notebook as the previous exercises (in other words, the notebook that uses the 5,000-invoice subset, not the full dataset from the activities).</p>
			<ol>
				<li value="1">Use the <strong class="source-inline">mlxtend</strong> library to derive association rules for the online retail dataset. Use confidence as the measure of interestingness, set the minimum threshold to <strong class="source-inline">0.6</strong>, and return all the metrics, not just support:<p class="source-code">rules = mlxtend.frequent_patterns\</p><p class="source-code">        .association_rules(mod_colnames_minsupport, \</p><p class="source-code">        metric="confidence", \</p><p class="source-code">        min_threshold=0.6,  \</p><p class="source-code">        support_only=False)</p><p class="source-code">rules.loc[0:6]</p><p>The output is similar to the following:</p><div id="_idContainer314" class="IMG---Figure"><img src="image/B15923_08_26.jpg" alt="Figure 8.26: The first seven rows of the association rules generated using only 5,000 transactions&#13;&#10;"/></div><p class="figure-caption">Figure 8.26: The first seven rows of the association rules generated using only 5,000 transactions</p></li>
				<li>Print the number of associations:<p class="source-code">print("Number of Associations: {}".format(rules.shape[0]))</p><p><strong class="source-inline">1,064</strong> association rules were found.</p><p class="callout-heading">Note</p><p class="callout">The number of association rules may differ.</p></li>
				<li>Try running another version of the model. Choose any minimum threshold and any measure of interestingness. Explore the returned rules:<p class="source-code">rules2 = mlxtend.frequent_patterns\</p><p class="source-code">         .association_rules(mod_colnames_minsupport, \</p><p class="source-code">         metric="lift", \</p><p class="source-code">         min_threshold=50,\</p><p class="source-code">         support_only=False)</p><p class="source-code">rules2.loc[0:6]</p><p>The output is as follows:</p><div id="_idContainer315" class="IMG---Figure"><img src="image/B15923_08_27.jpg" alt="Figure 8.27: The first seven rows of the association rules&#13;&#10;"/></div><p class="figure-caption">Figure 8.27: The first seven rows of the association rules</p></li>
				<li>Print the number of associations:<p class="source-code">print("Number of Associations: {}".format(rules2.shape[0]))</p><p>The number of association rules found using the lift metric and the minimum threshold value of <strong class="source-inline">50</strong> is <strong class="source-inline">176</strong>, which is significantly lower than in <em class="italic">Step 2</em>. We will see in a future step that <strong class="source-inline">50</strong> is quite a high threshold value, so it is not surprising that we returned fewer association rules.</p></li>
				<li>Plot confidence against support and identify specific trends in the data:<p class="source-code">rules.plot.scatter("support", "confidence", \</p><p class="source-code">                   alpha=0.5, marker="*")</p><p class="source-code">plt.xlabel("Support")</p><p class="source-code">plt.ylabel("Confidence")</p><p class="source-code">plt.title("Association Rules")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer316" class="IMG---Figure"><img src="image/B15923_08_28.jpg" alt="Figure 8.28: A plot of confidence against support&#13;&#10;"/></div><p class="figure-caption">Figure 8.28: A plot of confidence against support</p><p>Notice that there are no association rules with both extremely high confidence and extremely high support. This should hopefully make sense. If an item set has high support, the items are likely to appear with many other items, making the chances of high confidence very low.</p></li>
				<li>Look at the distribution of confidence:<p class="source-code">rules.hist("confidence", grid=False, bins=30)</p><p class="source-code">plt.xlabel("Confidence of item")</p><p class="source-code">plt.ylabel("Number of items")</p><p class="source-code">plt.title("Frequency distribution of Confidence")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer317" class="IMG---Figure"><img src="image/B15923_08_29.jpg" alt="Figure 8.29: The distribution of confidence values&#13;&#10;"/></div><p class="figure-caption">Figure 8.29: The distribution of confidence values</p></li>
				<li>Now, look at the distribution of lift:<p class="source-code">rules.hist("lift", grid=False, bins=30)</p><p class="source-code">plt.xlabel("Lift of item")</p><p class="source-code">plt.ylabel("Number of items")</p><p class="source-code">plt.title("Frequency distribution of Lift")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer318" class="IMG---Figure"><img src="image/B15923_08_30.jpg" alt="Figure 8.30: The distribution of lift values&#13;&#10;"/></div><p class="figure-caption">Figure 8.30: The distribution of lift values</p><p>As mentioned previously, this plot shows that <strong class="source-inline">50</strong> is a high threshold value in that there are not many points above that value.</p></li>
				<li>Now, look at the distribution of leverage:<p class="source-code">rules.hist("leverage", grid=False, bins=30)</p><p class="source-code">plt.xlabel("Leverage of item")</p><p class="source-code">plt.ylabel("Number of items")</p><p class="source-code">plt.title("Frequency distribution of Leverage")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer319" class="IMG---Figure"><img src="image/B15923_08_31.jpg" alt="Figure 8.31: The distribution of leverage values&#13;&#10;"/></div><p class="figure-caption">Figure 8.31: The distribution of leverage values</p></li>
				<li>Now, look at the distribution of conviction:<p class="source-code">plt.hist(rules[numpy.isfinite(rules['conviction'])]\</p><p class="source-code">                              .conviction.values, bins = 30)</p><p class="source-code">plt.xlabel("Coviction of item")</p><p class="source-code">plt.ylabel("Number of items")</p><p class="source-code">plt.title("Frequency distribution of Conviction")</p><p class="source-code">plt.show()</p><p>The output is as follows:</p><div id="_idContainer320" class="IMG---Figure"><img src="image/B15923_08_32.jpg" alt="Figure 8.32: The distribution of conviction values&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.32: The distribution of conviction values</p>
			<p>What is interesting about the four distributions is that spikes of varying sizes appear at the upper ends of the plots, implying that there are a few very strong association rules. The distribution of confidence tails off as the confidence values get larger, but at the very end, around the highest values, the distribution jumps up a little. The lift distribution has the most obvious spike. The conviction distribution plot shows a small spike, perhaps more accurately described as a bump, around 50. Lastly, the leverage distribution does not really show any spike in the higher values, but it does feature a long tail with some very high leverage values.</p>
			<p>Take some time to explore the association rules found by the model. Do the product pairings make sense to you? What happened to the number of association rules when you changed the model parameter values? Do you appreciate the impact that these rules would have when attempting to improve any retail business?</p>
			<p>In the preceding exercise, we built and plotted association rules. Association rules can be difficult to interpret and are heavily dependent on the thresholds and metrics used to create them. The questions in the preceding paragraph are meant to get you thinking creatively about how the algorithm works and how the rules can be used. Let's go through the questions one by one. There are obviously many rules, so the question regarding whether or not the rules make sense is hard to answer as a whole. Spot-checking the pairs seems to suggest that the rules are reasonable. For example, three of the pairings include children's cups and bowls, teacups and plates, and a playhouse kitchen and living room, all of which make sense. When the metrics and parameters changed, so did the results. As is the case in almost all modeling exercises, the ideal course of action is to look at the results under various circumstances and leverage all the findings to make the best decisions.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fhf9bS">https://packt.live/3fhf9bS</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/303vIBJ">https://packt.live/303vIBJ</a>.</p>
			<p class="callout">You must execute the entire Notebook in order to get the desired result.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor193"/>Activity 8.03: Finding the Association Rules on the Complete Online Retail Dataset</h2>
			<p>Let's pick up the scenario set out in <em class="italic">Activity 8.02</em>, <em class="italic">Running the Apriori Algorithm on the Complete Online Retail Dataset</em>. The company leadership comes back to you and says it is great that we know how frequently each item set occurs in the dataset, but which item sets can we act upon? Which item sets can we use to change the store layout or adjust pricing? To find these answers, we derive the full association rules.</p>
			<p>In this activity, let's derive association rules from the complete online retail transaction dataset. Ensure that you complete this activity in the notebook that uses the full dataset (in other words, the notebook with the complete retail dataset, not the notebook from the exercises that use the 5,000-invoice subset).</p>
			<p>These steps will help us to perform the activity:</p>
			<ol>
				<li value="1">Fit the association rule model on the full dataset. Use the confidence metric and a minimum threshold of <strong class="source-inline">0.6</strong>.</li>
				<li>Count the number of association rules. Is the number different from that found in <em class="italic">Step 1</em> of <em class="italic">Exercise 8.07</em>, <em class="italic">Deriving Association Rules</em>?</li>
				<li>Plot confidence against support.</li>
				<li>Look at the distributions of confidence, lift, leverage, and conviction.</li>
			</ol>
			<p>Expected association rules output the following:</p>
			<div>
				<div id="_idContainer321" class="IMG---Figure">
					<img src="image/B15923_08_33.jpg" alt="Figure 8.33: Expected association rules&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.33: Expected association rules</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 496.</p>
			<p>By the end of this activity, you will have plots of lift, leverage, and conviction.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor194"/>Summary</h1>
			<p>Market basket analysis is used to analyze and extract insights from transaction or transaction-like data that can be used to help drive growth in many industries, most famously the retail industry. These decisions can include how to lay out the retail space, what products to discount, and how to price products. One of the central pillars of market basket analysis is the establishment of association rules. Association rule learning is a machine learning approach to uncovering the associations between the products individuals purchase that are strong enough to be leveraged for business decisions. Association rule learning relies on the Apriori algorithm to find frequent item sets in a computationally efficient way. These models are atypical of machine learning models because no prediction is being done, the results cannot really be evaluated using any one metric, and the parameter values are selected not by grid search, but by domain requirements specific to the question of interest. That being said, the goal of pattern extraction that is at the heart of all machine learning models is most definitely present here. </p>
			<p>At the conclusion of this chapter, you should feel comfortable evaluating and interpreting probabilistic metrics, be able to run and adjust the Apriori algorithm and association rule learning models using <strong class="source-inline">mlxtend</strong> and know how these models are applied in business. Know that there is a decent chance that the positioning and pricing of items in your neighborhood grocery store were chosen based on the past actions made by you and many other customers in that store!</p>
			<p>In the next chapter, we will explore hotspot analysis using kernel density estimation, arguably one of the most frequently used algorithms in all of statistics and machine learning.</p>
		</div>
	</body></html>
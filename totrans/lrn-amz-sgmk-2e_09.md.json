["```py\n    %%sh\n    wget -N https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\n    unzip -o bank-additional.zip\n    ```", "```py\n    import sagemaker\n    sess   = sagemaker.Session()\n    bucket = sess.default_bucket()                     \n    prefix = 'xgboost-direct-marketing'\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    data = pd.read_csv('./bank-additional/bank-additional-full.csv')\n    data = pd.get_dummies(data)\n    data = data.drop(['y_no'], axis=1)\n    data = pd.concat([data['y_yes'], \n                     data.drop(['y_yes'], axis=1)], \n                     axis=1)\n    data = data.sample(frac=1, random_state=123)\n    train_data, val_data = train_test_split(\n        data, test_size=0.05)\n    train_data.to_csv(\n        'training.csv', index=False, header=False)\n    val_data.to_csv(\n        'validation.csv', index=False, header=False)\n    ```", "```py\n    training = sess.upload_data(path='training.csv', \n               key_prefix=prefix + '/training')\n    validation = sess.upload_data(path='validation.csv', \n                 key_prefix=prefix + \"/validation\")\n    output   = 's3://{}/{}/output/'.format(bucket,prefix)\n    ```", "```py\n    from sagemaker import TrainingInput\n    train_input = TrainingInput(\n        training_path, content_type='text/csv')\n    val_input = TrainingInput(\n        validation_path, content_type='text/csv')\n    ```", "```py\n    from sagemaker.xgboost import XGBoost\n    xgb_estimator = XGBoost(\n        role= sagemaker.get_execution_role(),\n        entry_point='xgb-dm.py',\n        instance_count=1, \n        instance_type='ml.m5.large',\n        framework_version='1.2-2',\n        output_path=output,\n        hyperparameters={\n            'num_round': 100,\n            'early_stopping_rounds': 10,\n            'max-depth': 5,\n            'eval-metric': 'auc'}\n    )\n    ```", "```py\n    xgb_estimator.fit({'train':training, \n                       'validation':validation})\n    ```", "```py\n    from time import strftime,gmtime\n    xgb_endpoint_name = \n        prefix+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n    xgb_predictor = xgb_estimator.deploy(\n        endpoint_name=xgb_endpoint_name,\n        initial_instance_count=1,\n        instance_type='ml.t2.medium')\n    ```", "```py\n    payload = val_data[:10].drop(['y_yes'], axis=1) \n    payload = payload.to_csv(header=False, \n              index=False).rstrip('\\n')\n    xgb_predictor.serializer =\n        sagemaker.serializers.CSVSerializer()\n    xgb_predictor.deserializer = \n        sagemaker.deserializers.CSVDeserializer()\n    response = xgb_predictor.predict(payload)\n    print(response)\n    ```", "```py\n    [['0.07206538'], ['0.02661967'], ['0.16043524'], ['4.026455e-05'], ['0.0002120432'], ['0.52123886'], ['0.50755614'], ['0.00015006188'], ['3.1439096e-05'], ['9.7614546e-05']]\n    ```", "```py\n    xgb_predictor.delete_endpoint()\n    ```", "```py\n    #role = sagemaker.get_execution_role()\n    role = 'arn:aws:iam::0123456789012:role/Sagemaker-fullaccess'\n    ```", "```py\n    training = 'file://training.csv'\n    validation = 'file://validation.csv'\n    output = 'file:///tmp'\n    ```", "```py\nInvoking script with the following command:\n/miniconda3/bin/python3 -m xgb-dm --early-stopping-rounds 10 \n--eval-metric auc --max-depth 5 \n```", "```py\nparser = argparse.ArgumentParser()\nparser.add_argument('--max-depth', type=int, default=4)\nparser.add_argument('--early-stopping-rounds', type=int, \n                    default=10)\nparser.add_argument('--eval-metric', type=str, \n                    default='error')\n```", "```py\nSM_CHANNEL_TRAIN=/opt/ml/input/data/train\nSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\nSM_MODEL_DIR=/opt/ml/model\n```", "```py\nparser.add_argument('--model-dir', type=str, \n    default=os.environ['SM_MODEL_DIR'])\nparser.add_argument('--training-dir', type=str, \n    default=os.environ['SM_CHANNEL_TRAIN'])\nparser.add_argument('--validation', type=str, \n    default=os.environ['SM_CHANNEL_VALIDATION'])\n```", "```py\n# Scikit-learn\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \n                                   'model.joblib'))\n    return clf\n# XGBoost\ndef model_fn(model_dir):\n    model = xgb.Booster()\n    model.load_model(os.path.join(model_dir, 'xgb.model'))\n    return model\n```", "```py\nsk = SKLearn(entry_point='myscript.py',\n             source_dir='src',\n             . . .\n```", "```py\nimport subprocess, sys\ndef install(package):\n    subprocess.call([sys.executable, \"-m\", \n                    \"pip\", \"install\", package])\nif __name__=='__main__':\n    install('gensim')\n    import gensim\n    . . . \n```", "```py\n$ mkdir lib\n$ pip install mylib -t lib\n```", "```py\nsk = SKLearn(entry_point='myscript.py',\n             dependencies=['lib/mylib'],\n             . . .\n```", "```py\nsk = SKLearn(entry_point='myscript.py', image_uri= '123456789012.dkr.ecr.eu-west-1.amazonaws.com/my-sklearn' \n. . .\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport joblib\ndata = pd.read_csv('housing.csv')\nlabels = data[['medv']]\nsamples = data.drop(['medv'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(\nsamples, labels, test_size=0.1, random_state=123)\nregr = LinearRegression(normalize=True)\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\nprint('Mean squared error: %.2f' \n       % mean_squared_error(y_test, y_pred))\nprint('Coefficient of determination: %.2f' \n       % r2_score(y_test, y_pred))\njoblib.dump(regr, 'model.joblib')\n```", "```py\n    import argparse\n    if __name__ == '__main__':\n      parser = argparse.ArgumentParser()\n      parser.add_argument('--normalize', type=bool, \n                          default=False)\n      parser.add_argument('--test-size', type=float, \n                          default=0.1)\n      parser.add_argument('--random-state', type=int, \n                          default=123)\n      args, _ = parser.parse_known_args()\n      normalize = args.normalize\n      test_size = args.test_size\n      random_state = args.random_state\n      data = pd.read_csv('housing.csv')\n      labels = data[['medv']]\n      samples = data.drop(['medv'], axis=1)\n      X_train, X_test, y_train, y_test = train_test_split(\n        samples, labels,test_size=test_size, \n        random_state=random_state)\n      . . . \n    ```", "```py\n    import os\n    if __name__ == '__main__':\n      . . .    \n      parser.add_argument('--model-dir', type=str, \n        default=os.environ['SM_MODEL_DIR'])\n      parser.add_argument('--training', type=str, \n        default=os.environ['SM_CHANNEL_TRAINING'])\n      . . .\n      model_dir = args.model_dir\n      training_dir = args.training\n      . . . \n      filename = os.path.join(training_dir, 'housing.csv')\n      data = pd.read_csv(filename)\n      . . .\n      model = os.path.join(model_dir, 'model.joblib')\n      dump(regr, model)\n    ```", "```py\n    def model_fn(model_dir):\n      model = joblib.load(os.path.join(model_dir, \n                                       'model.joblib'))\n      return model\n    ```", "```py\n$ source activate python3\n$ export SM_CHANNEL_TRAINING=\n$ export SM_MODEL_DIR=\n$ python sklearn-boston-housing.py --normalize True –test-ration 0.1 --training . --model-dir .\nMean squared error: 41.82\nCoefficient of determination: 0.63\n```", "```py\n    role = 'arn:aws:iam::0123456789012:role/Sagemaker-fullaccess'\n    sk = SKLearn(entry_point='sklearn-boston-housing.py',\n      role=role,\n      framework_version='0.23-1',\n      instance_count=1,\n      instance_type='local',\n      output_path=output_path,\n      hyperparameters={'normalize': True, \n                       'test-size': 0.1})\n    sk.fit({'training':training_path})\n    ```", "```py\n    /miniconda3/bin/python -m sklearn-boston-housing --normalize True --test-size 0.1\n    . . . \n    Mean squared error: 41.82\n    Coefficient of determination: 0.63\n    ```", "```py\n    sk_predictor = sk.deploy(initial_instance_count=1, \n                             instance_type='local')\n    data = pd.read_csv('housing.csv')\n    payload = data[:10].drop(['medv'], axis=1) \n    payload = payload.to_csv(header=False, index=False)\n    sk_predictor.serializer = \n        sagemaker.serializers.CSVSerializer()\n    sk_predictor.deserializer =\n        sagemaker.deserializers.CSVDeserializer()\n    response = sk_predictor.predict(payload)\n    print(response)\n    ```", "```py\n    [['29.801388899699845'], ['24.990809475886074'], ['30.7379654455552'], ['28.786967125316544'], ['28.1421501991961'], ['25.301714533101716'], ['22.717977231840184'], ['19.302415613883348'], ['11.369520911229536'], ['18.785593532977657']]\n    ```", "```py\nsess = sagemaker.Session()\nbucket = sess.default_bucket()                     \nprefix = 'sklearn-boston-housing'\ntraining_path = sess.upload_data(path='housing.csv', \n           key_prefix=prefix + \"/training\")\noutput_path = 's3://{}/{}/output/'.format(bucket,prefix)\nsk = SKLearn(. . ., instance_type='ml.m5.large')\nsk.fit({'training':training_path})\n. . .\nsk_predictor = sk.deploy(initial_instance_count=1, \n                         instance_type='ml.t2.medium')\n```", "```py\n    import tensorflow as tf\n    import numpy as np\n    import argparse, os\n    from model import FMNISTModel\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--learning-rate', type=float,  \n                        default=0.01)\n    parser.add_argument('--batch-size', type=int, \n                        default=128)\n    ```", "```py\n    parser.add_argument('--training', type=str, \n        default=os.environ['SM_CHANNEL_TRAINING'])\n    parser.add_argument('--validation', type=str,\n        default=os.environ['SM_CHANNEL_VALIDATION'])\n    parser.add_argument('--model-dir', type=str, \n        default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--gpu-count', type=int, \n        default=os.environ['SM_NUM_GPUS'])\n    ```", "```py\n    x_train = np.load(os.path.join(training_dir, \n              'training.npz'))['image']\n    y_train = np.load(os.path.join(training_dir, \n              'training.npz'))['label']\n    x_val = np.load(os.path.join(validation_dir, \n            'validation.npz'))['image']\n    y_val = np.load(os.path.join(validation_dir, \n            'validation.npz'))['label']\n    ```", "```py\n    model.save(os.path.join(model_dir, '1'))\n    ```", "```py\n    import os\n    import numpy as np\n    import keras\n    from keras.datasets import fashion_mnist\n    (x_train, y_train), (x_val, y_val) =  \n        fashion_mnist.load_data()\n    os.makedirs(\"./data\", exist_ok = True)\n    np.savez('./data/training', image=x_train,       \n             label=y_train)\n    np.savez('./data/validation', image=x_val, \n             label=y_val)\n    prefix = 'tf2-fashion-mnist'\n    training_input_path = sess.upload_data(\n        'data/training.npz', \n        key_prefix=prefix+'/training')\n    validation_input_path = sess.upload_data(\n        'data/validation.npz',   \n        key_prefix=prefix+'/validation')\n    ```", "```py\n    from sagemaker.tensorflow import TensorFlow\n    tf_estimator = TensorFlow(entry_point='fmnist.py',\n        source_dir='.',\n        role=sagemaker.get_execution_role(),\n        instance_count=1,\n        instance_type='ml.p3.2xlarge', \n        framework_version='2.4.1',\n        py_version='py37',\n        hyperparameters={'epochs': 10})\n    ```", "```py\n    from time import strftime,gmtime\n    tf_estimator.fit(\n        {'training': training_input_path, \n         'validation': validation_input_path})\n    tf_endpoint_name = 'tf2-fmnist-'+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n    tf_predictor = tf_estimator.deploy(\n                   initial_instance_count=1,\n                   instance_type='ml.m5.large',\n                   endpoint_name=tf_endpoint_name)\n    ```", "```py\n    response = tf_predictor.predict(payload)\n    prediction = np.array(reponse['predictions'])\n    predicted_label = prediction.argmax(axis=1)\n    print('Predicted labels are: \n        {}'.format(predicted_label))\n    ```", "```py\n    tf_predictor.delete_endpoint()\n    ```", "```py\n[('0', '8'), ('1', '17'), ('24', '31'), . . .\n```", "```py\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\nsess = sagemaker.Session()\nprefix = 'dgl-karate-club'\ntraining_input_path = sess.upload_data('edge_list.pickle', \nkey_prefix=prefix+'/training')\nestimator = PyTorch(role=sagemaker.get_execution_role(),\n    entry_point='karate_club_sagemaker.py',\n    hyperparameters={'node_count': 34, 'epochs': 30},\n    framework_version='1.5.0',\n    py_version='py3',\n    instance_count=1,\n    instance_type='ml.m5.large')\nestimator.fit({'training': training_input_path})\n```", "```py\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--epochs', type=int, default=30)\n    parser.add_argument('--node_count', type=int)\n    args, _    = parser.parse_known_args()\n    epochs     = args.epochs\n    node_count = args.node_count\n    training_dir = os.environ['SM_CHANNEL_TRAINING']\n    model_dir    = os.environ['SM_MODEL_DIR']\n    with open(os.path.join(training_dir, 'edge_list.pickle'), \n    'rb') as f:\n        edge_list = pickle.load(f)\n    # Build the graph and the model\n    . . .\n    # Train the model\n    . . .\n    # Print predicted classes\n    last_epoch = all_preds[epochs-1].detach().numpy()\n    predicted_class = np.argmax(last_epoch, axis=-1)\n    print(predicted_class)\n    # Save the model\n    torch.save(net.state_dict(), os.path.join(model_dir, \n    'karate_club.pt'))\n```", "```py\n[0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1]\n```", "```py\n{'correct': 0, 'review_star': 4, 'translation': {'en': \"I had a hard time finding a case for my new LG Lucid 2 but finally found this one on amazon. The colors are really pretty and it works just as well as, if not better than the otterbox. Hopefully there will be more available by next Xmas season. Overall, very cute case. I love cheetah's. :)\", 'th': 'ฉันมีปัญหาในการหาเคสสำหรับ LG Lucid 2 ใหม่ของฉันแต่ในที่สุดก็พบเคสนี้ใน Amazon สีสวยมากและใช้งานได้ดีเช่นเดียวกับถ้าไม่ดีกว่านากหวังว่าจะมีให้มากขึ้นในช่วงเทศกาลคริสต์มาสหน้าโดยรวมแล้วน่ารักมากๆฉันรักเสือชีตาห์ :)'}}\n```", "```py\n{'labels': 1,\n 'text': \"I had a hard time finding a case for my new LG Lucid 2 but finally found this one on amazon. The colors are really pretty and it works just as well as, if not better than the otterbox. Hopefully there will be more available by next Xmas season. Overall, very cute case. I love cheetah's. :)\"}\n```", "```py\n    !pip -q install \"transformers>=4.4.2\" \"datasets[s3]==1.5.0\" --upgrade\n    ```", "```py\n    from datasets import load_dataset\n    train_dataset, valid_dataset = load_dataset('generated_reviews_enth', \n                 split=['train', 'validation'])\n    ```", "```py\n    def map_stars_to_sentiment(row):\n        return {\n            'labels': 1 if row['review_star'] >= 4 else 0\n        }\n    train_dataset = \n        train_dataset.map(map_stars_to_sentiment)\n    valid_dataset = \n        valid_dataset.map(map_stars_to_sentiment)\n    ```", "```py\n    train_dataset = train_dataset.flatten()\n    valid_dataset = valid_dataset.flatten()\n    ```", "```py\n    train_dataset = train_dataset.remove_columns(\n        ['correct', 'translation.th', 'review_star'])\n    valid_dataset = valid_dataset.remove_columns(\n        ['correct', 'translation.th', 'review_star'])\n    train_dataset = train_dataset.rename_column(\n        'translation.en', 'text')\n    valid_dataset = valid_dataset.rename_column(\n        'translation.en', 'text')\n    ```", "```py\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        'distilbert-base-uncased')\n    def tokenize(batch):\n        return tokenizer(batch['text'], \n        padding='max_length', truncation=True)\n    ```", "```py\n    train_dataset = train_dataset.map(tokenize, \n        batched=True, batch_size=len(train_dataset))\n    valid_dataset = valid_dataset.map(tokenize, \n        batched=True, batch_size=len(valid_dataset))\n    ```", "```py\n    train_dataset = train_dataset.remove_columns(['text'])\n    valid_dataset = valid_dataset.remove_columns(['text'])\n    ```", "```py\n    '{\"attention_mask\": [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,<zero padding>], \"input_ids\": [101,1045, 2018,1037,2524,2051,4531,1037,2553,2005,2026,2047,1048,2290,12776,3593,1016,2021,2633,2179,2023,2028,2006,9733,1012,1996,6087,2024,2428,3492,1998,2009,2573,2074,2004,2092,2004,1010,2065,2025,2488,2084,1996,22279,8758,1012,11504,2045,2097,2022,2062,2800,2011,2279,1060,9335,2161,1012,3452,1010,2200,10140,2553,1012,1045,2293,18178,12928,2232,1005,1055,1012,1024,1007,102,<zero padding>], \"labels\": 1}'\n    ```", "```py\n    import sagemaker\n    from datasets.filesystems import S3FileSystem\n    bucket = sagemaker.Session().default_bucket()\n    s3_prefix = 'hugging-face/sentiment-analysis'\n    train_input_path = \n        f's3://{bucket}/{s3_prefix}/training'\n    valid_input_path =  \n        f's3://{bucket}/{s3_prefix}/validation'\n    s3 = S3FileSystem()\n    train_dataset.save_to_disk(train_input_path, fs=s3)\n    valid_dataset.save_to_disk(valid_input_path, fs=s3)\n    ```", "```py\n    hyperparameters={\n        'epochs': 1,\n        'train_batch_size': 32,\n        'model_name':'distilbert-base-uncased'\n    }\n    from sagemaker.huggingface import HuggingFace\n    huggingface_estimator = HuggingFace(\n        role=sagemaker.get_execution_role(),\n        entry_point='train.py',\n        hyperparameters=hyperparameters,\n        transformers_version='4.4.2',\n        pytorch_version='1.6.0',\n        py_version='py36',\n        instance_type='ml.p3.2xlarge',\n        instance_count=1\n    )\n    ```", "```py\n    huggingface_estimator.fit(\n        {'train': train_input_path, \n         'valid': valid_input_path})\n    ```", "```py\n    from sagemaker.pytorch import PyTorchModel\n    from sagemaker.serializers import JSONSerializer\n    from sagemaker.deserializers import JSONDeserializer\n    model = PyTorchModel(\n        model_data=huggingface_estimator.model_data,\n        role=sagemaker.get_execution_role(), \n        entry_point='torchserve-predictor.py',\n        framework_version='1.6.0',\n        py_version='py36')\n    ```", "```py\n    def model_fn(model_dir):\n      config_path = '{}/config.json'.format(model_dir)\n      model_path ='{}/pytorch_model.bin'.format(model_dir)\n      config = AutoConfig.from_pretrained(config_path)\n      model = DistilBertForSequenceClassification\n              .from_pretrained(model_path, config=config)\n      return model\n    ```", "```py\n    tokenizer = AutoTokenizer.from_pretrained(\n                'distilbert-base-uncased')\n    CLASS_NAMES = ['negative', 'positive']\n    def predict_fn(input_data, model):\n        inputs = tokenizer(input_data['text'],  \n                           return_tensors='pt')\n        outputs = model(**inputs)\n        logits = outputs.logits\n        _, prediction = torch.max(logits, dim=1)\n        return CLASS_NAMES[prediction]\n    ```", "```py\n    predictor = model.deploy(\n        initial_instance_count=1,\n        instance_type='ml.m5.xlarge')\n    ```", "```py\n    predictor.serializer = JSONSerializer()\n    predictor.deserializer = JSONDeserializer()\n    sample = {'text':'This camera is really amazing!}\n    prediction = predictor.predict(test_data)\n    print(prediction)\n    ['positive']\n    ```", "```py\n    predictor.delete_endpoint()\n    ```", "```py\n    $ aws s3 cp ham s3://sagemaker-eu-west-1-123456789012\n    $ aws s3 cp spam s3://sagemaker-eu-west-1-123456789012\n    ```", "```py\n    val spam = sc.textFile(\n    \"s3://sagemaker-eu-west-1-123456789012/spam\")\n    .map(l => l.toLowerCase())\n    .map(l => l.replaceAll(\"[^ a-z]\", \"\"))\n    .map(l => l.trim())\n    val ham = sc.textFile(\n    \"s3://sagemaker-eu-west-1-123456789012/ham\")\n    .map(l => l.toLowerCase())\n    .map(l => l.replaceAll(\"[^ a-z]\", \"\"))\n    .map(l => l.trim())\n    ```", "```py\n    val tf = new HashingTF(numFeatures = 200)\n    val spamFeatures = spam.map(\n                       m => tf.transform(m.split(\" \")))\n    val hamFeatures = ham.map(\n                      m => tf.transform(m.split(\" \")))\n    ```", "```py\n    Array((200,[15,83,96,188],[1.0,1.0,2.0,2.0]))\n    ```", "```py\n    val positiveExamples = spamFeatures.map(\n        features => LabeledPoint(1, features))\n    val negativeExamples = hamFeatures.map(\n        features => LabeledPoint(0, features))\n    ```", "```py\n    val data = positiveExamples.union(negativeExamples)\n    val data_libsvm =  \n        MLUtils.convertVectorColumnsToML(data.toDF)\n    ```", "```py\n    Array([1.0,(200,[2,41,99,146,172,181],[2.0,1.0,1.0,1.0,1.0])])\n    ```", "```py\n    val Array(trainingData, testData) = \n        data_libsvm.randomSplit(Array(0.8, 0.2))\n    ```", "```py\n    val roleArn = \"arn:aws:iam:YOUR_SAGEMAKER_ROLE\"\n    val xgboost_estimator = new XGBoostSageMakerEstimator(\n        trainingInstanceType=\"ml.m5.large\",\n        trainingInstanceCount=1,\n        endpointInstanceType=\"ml.t2.medium\",\n        endpointInitialInstanceCount=1,\n        sagemakerRole=IAMRole(roleArn))\n    xgboost_estimator.setObjective(\"binary:logistic\")\n    xgboost_estimator.setNumRound(25)\n    ```", "```py\n    val xgboost_model =   \n        xgboost_estimator.fit(trainingData_libsvm)\n    ```", "```py\n    val transformedData = \n        xgboost_model.transform(testData_libsvm)\n    val accuracy = 1.0*transformedData.filter(\n        $\"label\"=== $\"prediction\")\n        .count/transformedData.count()\n    ```", "```py\n    val cleanup = new SageMakerResourceCleanup(\n                      xgboost_model.sagemakerClient)\n    cleanup.deleteResources(\n        xgboost_model.getCreatedResources)\n    ```"]
<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 11. Dimensionality Reduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Dimensionality Reduction</h1></div></div></div><p>Garbage in, garbage out—throughout the book, we saw this pattern also holds true when applying machine learning methods to training data. Looking back, we realize that the most interesting machine learning challenges always involved some sort of feature engineering, where we tried to use our insight into the problem to carefully crafted additional features that the machine learner hopefully picks up.</p><p>In this chapter, we will go in the opposite direction with <a id="id554" class="indexterm"/>dimensionality reduction involving cutting away features that are irrelevant or redundant. Removing features might seem counter-intuitive at first thought, as more information should always be better than less information. Also, even if we had redundant features in our dataset, would not the learning algorithm be able to quickly figure it out and set their weights to 0? The following are several good reasons that are still in practice for trimming down the dimensions as much as possible:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Superfluous features can irritate or mislead the learner. This is not the case with all machine learning methods (for example, Support Vector Machines love high dimensional spaces). However, most of the models feel safer with fewer dimensions.</li><li class="listitem" style="list-style-type: disc">Another argument against high dimensional feature spaces is that more features mean more parameters to tune and a higher risk to overfit.</li><li class="listitem" style="list-style-type: disc">The data we retrieved to solve our task might have just artificially high dimensionality, whereas the real dimension might be small.</li><li class="listitem" style="list-style-type: disc">Fewer dimensions = faster training = more parameter variations to try out in the same time frame = better end result.</li><li class="listitem" style="list-style-type: disc">Visualization—if we want to visualize the data we are restricted to two or three dimensions.</li></ul></div><p>So, here we will show how to get rid of the garbage within our data while keeping the real valuable part of it.</p><div class="section" title="Sketching our roadmap"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec62"/>Sketching our roadmap</h1></div></div></div><p>Dimensionality reduction<a id="id555" class="indexterm"/> can be roughly grouped into feature selection and feature extraction methods. We already employed some kind of feature selection in almost every chapter when we invented, analyzed, and then probably dropped some features. In this chapter, we will present some ways that use statistical methods, namely correlation and mutual information, to be able to do so in vast feature spaces. Feature extraction tries to transform the original feature space into a lower-dimensional feature space. This is especially useful when we cannot get rid of features using selection methods, but still we have too many features for our learner. We will demonstrate this<a id="id556" class="indexterm"/> using <a id="id557" class="indexterm"/>
<span class="strong"><strong>principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>), <span class="strong"><strong>linear discriminant analysis</strong></span> (<span class="strong"><strong>LDA</strong></span>), and<a id="id558" class="indexterm"/> <span class="strong"><strong>multidimensional scaling</strong></span> (<span class="strong"><strong>MDS</strong></span>).</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Selecting features"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec63"/>Selecting features</h1></div></div></div><p>If we want <a id="id559" class="indexterm"/>to be nice to our machine learning algorithm, we provide it with features that are not dependent on each other, yet highly dependent on the value to be predicted. This means that each feature adds salient information. Removing any of the features will lead to a drop in performance.</p><p>If we have <a id="id560" class="indexterm"/>only a handful of features, we could draw a matrix of scatter plots (one scatter plot for every feature pair combination). Relationships between the features could then be easily spotted. For every feature pair showing an obvious dependence, we would then think of whether we should remove one of them or better design a newer, cleaner feature out of both.</p><p>Most of the time, however, we have more than a handful of features to choose from. Just think of the classification task where we had a bag of words to classify the quality of an answer, which would require a 1,000 by 1,000 scatter plot. In this case, we need a more automated way to detect overlapping features and to resolve them. We will present two general ways to do so in the following subsections, namely filters and wrappers.</p><div class="section" title="Detecting redundant features using filters"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec91"/>Detecting redundant features using filters</h2></div></div></div><p>Filters try to <a id="id561" class="indexterm"/>clean up the feature forest independent of any later used machine learning method. They rely on statistical methods to find which of the features are redundant or irrelevant. In case of redundant features, it keeps only one per redundant feature group. Irrelevant features will simply be removed. In general, the filter works as depicted in the following workflow:</p><div class="mediaobject"><img src="images/2772OS_11_06.jpg" alt="Detecting redundant features using filters"/></div><div class="section" title="Correlation"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec16"/>Correlation</h3></div></div></div><p>Using<a id="id562" class="indexterm"/> correlation, we can easily see linear relationships between pairs of features. In the following graphs, we can see different degrees of correlation, together with a potential linear dependency plotted as a red-dashed line (fitted 1-dimensional polynomial). The correlation coefficient <span class="inlinemediaobject"><img src="images/2772OS_11_14.jpg" alt="Correlation"/></span> at the top of the individual graphs is calculated using the common Pearson correlation coefficient (Pearson <code class="literal">r</code> value) by means of the <code class="literal">pearsonr()</code> function of <code class="literal">scipy.stat</code>.</p><p>Given two equal-sized data series, it returns a tuple of the correlation coefficient value and the p-value. The p-value describes how likely it is that the data series has been generated by an uncorrelated system. In other words, the higher the p-value, the less we should trust the correlation coefficient:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from scipy.stats import pearsonr</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pearsonr([1,2,3], [1,2,3.1])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; (0.99962228516121843, 0.017498096813278487)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pearsonr([1,2,3], [1,20,6])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; (0.25383654128340477, 0.83661493668227405)</strong></span>
</pre></div><p>In the first case, we have a clear indication that both series are correlated. In the second case, we still have a clearly non-zero <span class="inlinemediaobject"><img src="images/2772OS_11_15.jpg" alt="Correlation"/></span> value.</p><p>However, the p-value of 0.84 tells us that the correlation coefficient is not significant and we should not pay too close attention to it. Have a look at the following graphs:</p><div class="mediaobject"><img src="images/2772OS_11_01.jpg" alt="Correlation"/></div><p>In the first<a id="id563" class="indexterm"/> three cases that have high correlation coefficients, we would probably want to throw out either <span class="inlinemediaobject"><img src="images/2772OS_11_16.jpg" alt="Correlation"/></span> or <span class="inlinemediaobject"><img src="images/2772OS_11_17.jpg" alt="Correlation"/></span> because they seem to convey similar, if not the same, information.</p><p>In the last case, however, we should keep both features. In our application, this decision would, of course, be driven by this p-value.</p><p>Although, it <a id="id564" class="indexterm"/>worked nicely in the preceding example, reality is seldom nice to us. One big disadvantage of correlation-based feature selection is that it only detects linear relationships (a relationship that can be modelled by a straight line). If we use correlation on a non-linear data, we see the problem. In the following example, we have a quadratic relationship:</p><div class="mediaobject"><img src="images/2772OS_11_02.jpg" alt="Correlation"/></div><p>Although, the human eye immediately sees the relationship between X<sub>1</sub> and X<sub>2</sub> in all but the bottom-right graph, the correlation coefficient does not. It's obvious that correlation is useful to detect <a id="id565" class="indexterm"/>linear relationships, but fails for everything else. Sometimes, it already helps to apply simple transformations to get a linear relationship. For instance, in the preceding plot, we would have got a high correlation coefficient if we had drawn X<sub>2</sub> over X<sub>1</sub> squared. Normal data, however, does not often offer this opportunity.</p><p>Luckily, for non-linear relationships, mutual information comes to the rescue.</p></div><div class="section" title="Mutual information"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec17"/>Mutual information</h3></div></div></div><p>When looking <a id="id566" class="indexterm"/>at the feature selection, we should not focus on the type of relationship as we did in the previous section (linear relationships). Instead, we should think in terms of how much information one feature provides (given that we already have another).</p><p>To understand this, let's pretend that we want to use features from <code class="literal">house_size</code>, <code class="literal">number_of_levels</code>, and <code class="literal">avg_rent_price</code> feature set to train a classifier that outputs whether the house has an elevator or not. In this example, we intuitively see that knowing <code class="literal">house_size</code> we don't need to know <code class="literal">number_of_levels</code> anymore, as it contains, somehow, redundant information. With <code class="literal">avg_rent_price</code>, it's different because we cannot infer the value of rental space simply from the size of the house or the number of levels it has. Thus, it would be wise to keep only one of them in addition to the average price of rental space.</p><p>Mutual information formalizes the aforementioned reasoning by calculating how much information two features have in common. However, unlike correlation, it does not rely on a sequence of data, but on the distribution. To understand how it works, we have to dive a bit into information entropy.</p><p>Let's assume we have a fair coin. Before we flip it, we will have maximum uncertainty as to whether it will show heads or tails, as both have an equal probability of 50 percent. This uncertainty can be measured by means of Claude Shannon's information entropy:</p><div class="mediaobject"><img src="images/2772OS_11_18.jpg" alt="Mutual information"/></div><p>In our fair coin case, we have two cases: Let <span class="inlinemediaobject"><img src="images/2772OS_11_19.jpg" alt="Mutual information"/></span> be the case of head and <span class="inlinemediaobject"><img src="images/2772OS_11_20.jpg" alt="Mutual information"/></span> the case of tail with <span class="inlinemediaobject"><img src="images/2772OS_11_21.jpg" alt="Mutual information"/></span>.</p><p>Thus, it concludes to:</p><div class="mediaobject"><img src="images/2772OS_11_22.jpg" alt="Mutual information"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip21"/>Tip</h3><p>For convenience, we can also use <code class="literal">scipy.stats.entropy([0.5, 0.5], base=2)</code>. We set the base parameter to <code class="literal">2</code> to get the same result as earlier. Otherwise, the function will use the natural logarithm via <code class="literal">np.log()</code>. In general, the base does not matter (as long as you use it consistently).</p></div></div><p>Now, imagine <a id="id567" class="indexterm"/>we knew upfront that the coin is actually not that fair with heads having a chance of 60 percent showing up after flipping:</p><div class="mediaobject"><img src="images/2772OS_11_23.jpg" alt="Mutual information"/></div><p>We see that this situation is less uncertain. The uncertainty will decrease the farther away we get from 0.5 reaching the extreme value of 0 for either 0 percent or 100 percent of heads showing up, as we can see in the following graph:</p><div class="mediaobject"><img src="images/2772OS_11_03.jpg" alt="Mutual information"/></div><p>We will now <a id="id568" class="indexterm"/>modify entropy <span class="inlinemediaobject"><img src="images/2772OS_11_24.jpg" alt="Mutual information"/></span> by applying it to two features instead of one in such a way that it measures how much uncertainty is removed from X when we learn about Y. Then, we can catch how one feature reduces the uncertainty of another.</p><p>For example, without having any further information about the weather, we are totally uncertain whether it's raining outside or not. If we now learn that the grass outside is wet, the uncertainty has been reduced (we will still have to check whether the sprinkler had been turned on).</p><p>More formally, mutual information is defined as:</p><div class="mediaobject"><img src="images/2772OS_11_25.jpg" alt="Mutual information"/></div><p>This looks a bit intimidating, but is really not more than sums and products. For instance, the calculation of <span class="inlinemediaobject"><img src="images/2772OS_11_26.jpg" alt="Mutual information"/></span> is done by binning the feature values and then calculating the fraction of values in each bin. In the following plots, we have set the number of bins to ten.</p><p>In order to <a id="id569" class="indexterm"/>restrict mutual information to the interval [0,1], we have to divide it by their added individual entropy, which gives us the normalized mutual information:</p><div class="mediaobject"><img src="images/2772OS_11_27.jpg" alt="Mutual information"/></div><p>The nice thing about mutual information is that unlike correlation, it does not look only at linear relationships, as we can see in the following graphs:</p><div class="mediaobject"><img src="images/2772OS_11_04.jpg" alt="Mutual information"/></div><p>As we can see, mutual information<a id="id570" class="indexterm"/> is able to indicate the strength of a linear relationship. The following diagram shows that, it also works for squared relationships:</p><div class="mediaobject"><img src="images/2772OS_11_05.jpg" alt="Mutual information"/></div><p>So, what we <a id="id571" class="indexterm"/>would have to do is to calculate the normalized mutual information for all feature pairs. For every pair having too high value (we would have to determine what this means), we would then drop one of them. In case of regression, we could drop this feature that has too low mutual information with the desired result value.</p><p>This might work for a not too-big set of features. At some point, however, this procedure can be really expensive, because the amount of calculation grows quadratically (as we are computing the mutual information between feature pairs).</p><p>Another huge disadvantage of filters is that they drop features that seem to be not useful in isolation. More<a id="id572" class="indexterm"/> often than not, there are a handful of features that seem to be totally independent of the target variable, yet when combined together they rock. To keep these, we need wrappers.</p></div></div><div class="section" title="Asking the model about the features using wrappers"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec92"/>Asking the model about the features using wrappers</h2></div></div></div><p>While filters can <a id="id573" class="indexterm"/>help tremendously in getting rid of useless features, they can go only so far. After all the filtering, there might still be some features that are independent among themselves and show some degree of dependence with the result variable, but yet they are totally useless from the model's point of view. Just think of the following data that describes the XOR function. Individually, neither <code class="literal">A</code> nor <code class="literal">B</code> would show any signs of dependence on <code class="literal">Y</code>, whereas together they clearly do:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>A</p>
</th><th style="text-align: left" valign="bottom">
<p>B</p>
</th><th style="text-align: left" valign="bottom">
<p>Y</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr></tbody></table></div><p>So, why not ask the model itself to give its vote on the individual features? This is what wrappers do, as we can see in the following process chart:</p><div class="mediaobject"><img src="images/2772OS_11_07.jpg" alt="Asking the model about the features using wrappers"/></div><p>Here, we pushed the calculation of feature importance to the model training process. Unfortunately (but understandably), feature importance is not determined as a binary, but as a ranking value. So, we still have to specify where to make the cut, what part of the features are we willing to take, and what part do we want to drop?</p><p>Coming back to<a id="id574" class="indexterm"/> scikit-learn, we find various excellent wrapper classes in the <code class="literal">sklearn.feature_selection</code> package. A real workhorse in this field is <code class="literal">RFE</code>, which stands for recursive feature elimination. It takes an estimator and the desired number of features to keep as parameters and then trains the estimator with various feature sets as long as it has found a subset of features that is small enough. The <code class="literal">RFE</code> instance itself pretends to be like an estimator, thereby, indeed, wrapping the provided estimator.</p><p>In the following example, we create an artificial classification problem of 100 samples using datasets' convenient <code class="literal">make_classification()</code> function. It lets us specify the creation of 10 features, out of which only three are really valuable to solve the classification problem:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.feature_selection import RFE</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.datasets import make_classification</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; X,y = make_classification(n_samples=100, n_features=10, n_informative=3, random_state=0)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; clf = LogisticRegression()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; clf.fit(X, y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; selector = RFE(clf, n_features_to_select=3)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; selector = selector.fit(X, y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(selector.support_)</strong></span>
<span class="strong"><strong>[False  True False  True False False False False  True False]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(selector.ranking_)</strong></span>
<span class="strong"><strong>[4 1 3 1 8 5 7 6 1 2]</strong></span>
</pre></div><p>The problem in real-world scenarios is, of course, how can we know the right value for <code class="literal">n_features_to_select</code>? Truth is, we can't. However, most of the time we can use a sample of the data and play with it using different settings to quickly get a feeling for the right ballpark.</p><p>The good thing is that we don't have to be that exact using wrappers. Let's try different values for <code class="literal">n_features_to_select</code> to see how <code class="literal">support_</code> and <code class="literal">ranking_</code> change:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>n_features_</p>
<p>to_select</p>
</th><th style="text-align: left" valign="bottom">
<p>support_</p>
</th><th style="text-align: left" valign="bottom">
<p>ranking_</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>[False False False  True False False False False False False]</p>
</td><td style="text-align: left" valign="top">
<p>[ 6  3  5  1 10  7  9  8  2  4]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>[False False False  True False False False False  True False]</p>
</td><td style="text-align: left" valign="top">
<p>[5 2 4 1 9 6 8 7 1 3]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>[False  True False  True False False False False  True False]</p>
</td><td style="text-align: left" valign="top">
<p>[4 1 3 1 8 5 7 6 1 2]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>[False  True False  True False False False False  True  True]</p>
</td><td style="text-align: left" valign="top">
<p>[3 1 2 1 7 4 6 5 1 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>[False  True  True  True False False False False  True  True]</p>
</td><td style="text-align: left" valign="top">
<p>[2 1 1 1 6 3 5 4 1 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>[ True  True  True  True False False False False  True  True]</p>
</td><td style="text-align: left" valign="top">
<p>[1 1 1 1 5 2 4 3 1 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>[ True  True  True  True False  True False False  True  True]</p>
</td><td style="text-align: left" valign="top">
<p>[1 1 1 1 4 1 3 2 1 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>[ True  True  True  True False  True False  True  True  True]</p>
</td><td style="text-align: left" valign="top">
<p>[1 1 1 1 3 1 2 1 1 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>[ True  True  True  True False  True  True  True  True  True]</p>
</td><td style="text-align: left" valign="top">
<p>[1 1 1 1 2 1 1 1 1 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>[ True  True  True  True  True  True  True  True  True  True]</p>
</td><td style="text-align: left" valign="top">
<p>[1 1 1 1 1 1 1 1 1 1]</p>
</td></tr></tbody></table></div><p>We see that the result is very stable. Features that have been used when requesting smaller feature sets<a id="id575" class="indexterm"/> keep on getting selected when letting more features in. At last, we rely on our train/test set splitting to warn us when we go the wrong way.</p></div><div class="section" title="Other feature selection methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec93"/>Other feature selection methods</h2></div></div></div><p>There are <a id="id576" class="indexterm"/>several other feature selection methods that you will discover while reading through machine learning literature. Some even don't look like being a feature selection method because they are embedded into the learning process (not to be confused with the aforementioned wrappers). Decision trees, for instance, have a feature selection mechanism implanted deep in their core. Other learning methods employ some kind of regularization that punishes model complexity, thus driving the learning process towards good performing models that are still "simple". They do this by decreasing the less impactful features importance to zero and then dropping them (L1-regularization).</p><p>So watch out! Often, the power of machine learning methods has to be attributed to their implanted feature selection method to a great degree.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Feature extraction"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec64"/>Feature extraction</h1></div></div></div><p>At some<a id="id577" class="indexterm"/> point, after we have removed redundant features and dropped irrelevant ones, we, often, still find that we have too many features. No<a id="id578" class="indexterm"/> matter what learning method we use, they all perform badly and given the huge feature space we understand that they actually cannot do better. We realize that we have to cut living flesh; we have to get rid of features, for which all common sense tells us that they are valuable. Another situation when we need to reduce the dimensions and feature selection does not help much is when we want to visualize data. Then, we need to have at most three dimensions at the end to provide any meaningful graphs.</p><p>Enter feature extraction methods. They restructure the feature space to make it more accessible to the model or simply cut down the dimensions to two or three so that we can show dependencies visually.</p><p>Again, we can distinguish feature extraction methods as being linear or non-linear ones. Also, as seen before in the <span class="emphasis"><em>Selecting features</em></span> section, we will present one method for each type (principal component analysis as a linear and non-linear version of multidimensional scaling). Although, they are widely known and used, they are only representatives for many more interesting and powerful feature extraction methods.</p><div class="section" title="About principal component analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec94"/>About principal component analysis</h2></div></div></div><p>
<span class="strong"><strong>Principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) is<a id="id579" class="indexterm"/> often <a id="id580" class="indexterm"/>the first thing to try out if you want to cut down the number of features and do not know what feature extraction method to use. PCA is limited as it's a linear method, but chances are that it already goes far enough for your model to learn well enough. Add to this the strong mathematical properties it offers and the speed at which it finds the transformed feature space and is later able to transform between original and transformed features; we can almost guarantee that it also will become one of your frequently used machine learning tools.</p><p>Summarizing it, given<a id="id581" class="indexterm"/> the original feature space, PCA finds a linear projection of itself in a lower dimensional space that has the following properties:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The conserved variance is maximized.</li><li class="listitem" style="list-style-type: disc">The final reconstruction error (when trying to go back from transformed features to original ones) is minimized.</li></ul></div><p>As PCA simply transforms the input data, it can be applied both to classification and regression problems. In this section, we will use a classification task to discuss the method.</p><div class="section" title="Sketching PCA"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec18"/>Sketching PCA</h3></div></div></div><p>PCA<a id="id582" class="indexterm"/> involves <a id="id583" class="indexterm"/>a lot of linear algebra, which we do not want to go into. Nevertheless, the basic algorithm can be easily described as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Center the data by subtracting the mean from it.</li><li class="listitem">Calculate the covariance matrix.</li><li class="listitem">Calculate the eigenvectors of the covariance matrix.</li></ol></div><p>If we start with <span class="inlinemediaobject"><img src="images/2772OS_11_28.jpg" alt="Sketching PCA"/></span> features, then the algorithm will return a transformed feature space again with <span class="inlinemediaobject"><img src="images/2772OS_11_28.jpg" alt="Sketching PCA"/></span> dimensions (we gained nothing so far). The nice thing about this algorithm, however, is that the eigenvalues indicate how much of the variance is described by the corresponding eigenvector.</p><p>Let's assume we start with <span class="inlinemediaobject"><img src="images/2772OS_11_29.jpg" alt="Sketching PCA"/></span> features and we know that our model does not work well with more than <span class="inlinemediaobject"><img src="images/2772OS_11_30.jpg" alt="Sketching PCA"/></span> features. Then, we simply pick the <span class="inlinemediaobject"><img src="images/2772OS_11_30.jpg" alt="Sketching PCA"/></span> eigenvectors with the highest eigenvalues.</p></div><div class="section" title="Applying PCA"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec19"/>Applying PCA</h3></div></div></div><p>Let's <a id="id584" class="indexterm"/>consider the<a id="id585" class="indexterm"/> following artificial dataset, which is visualized in the following left plot diagram:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; x1 = np.arange(0, 10, .2)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x2 = x1+np.random.normal(loc=0, scale=1, size=len(x1))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; X = np.c_[(x1, x2)]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; good = (x1&gt;5) | (x2&gt;5) # some arbitrary classes</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; bad = ~good # to make the example look good</strong></span>
</pre></div><div class="mediaobject"><img src="images/2772OS_11_08.jpg" alt="Applying PCA"/></div><p>Scikit-learn <a id="id586" class="indexterm"/>provides the <code class="literal">PCA</code> class in its decomposition<a id="id587" class="indexterm"/> package. In this example, we can clearly see that one dimension should be enough to describe the data. We can specify this using the <code class="literal">n_components</code> parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn import linear_model, decomposition, datasets</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pca = decomposition.PCA(n_components=1)</strong></span>
</pre></div><p>Also, here we can use the <code class="literal">fit()</code> and <code class="literal">transform()</code> methods of <code class="literal">pca</code> (or its <code class="literal">fit_transform()</code> combination) to analyze the data and project it in the transformed feature space:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; Xtrans = pca.fit_transform(X)</strong></span>
</pre></div><p>As we have specified,<code class="literal"> Xtrans</code> contains only one dimension. You can see the result in the preceding right plot diagram. The outcome is even linearly separable in this case. We would not even need a complex classifier to distinguish between both classes.</p><p>To get an understanding of the reconstruction error, we can have a look at the variance of the data that we have retained in the transformation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(pca.explained_variance_ratio_)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; [ 0.96393127]</strong></span>
</pre></div><p>This means that after going from two to one dimension, we are still left with 96 percent of the variance.</p><p>Of course, it's not always this simple. Oftentimes, we don't know what number of dimensions is advisable upfront. In that case, we leave <code class="literal">n_components</code> parameter unspecified when initializing <code class="literal">PCA</code> to let it calculate the full transformation. After fitting the data, <code class="literal">explained_variance_ratio_</code> contains an array of ratios in decreasing order: The first value is the ratio of the basis vector describing the direction of the highest variance, the<a id="id588" class="indexterm"/> second value is the ratio of the direction of the <a id="id589" class="indexterm"/>second highest variance, and so on. After plotting this array, we quickly get a feel of how many components we would need: the number of components immediately before the chart has its elbow is often a good guess.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip22"/>Tip</h3><p>Plots displaying the explained variance over the number of components is called a Scree plot. A nice example of combining a Scree plot with a grid search to find the best setting for the classification problem can be found at <a class="ulink" href="http://scikit-learn.sourceforge.net/stable/auto_examples/plot_digits_pipe.html">http://scikit-learn.sourceforge.net/stable/auto_examples/plot_digits_pipe.html</a>.</p></div></div></div></div><div class="section" title="Limitations of PCA and how LDA can help"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec95"/>Limitations of PCA and how LDA can help</h2></div></div></div><p>Being<a id="id590" class="indexterm"/> a linear method, PCA has, of course, its limitations <a id="id591" class="indexterm"/>when we are faced with data that has non-linear relationships. We won't go into details here, but it's sufficient to say that there are extensions of PCA, for example, Kernel PCA, which introduces a non-linear transformation so that we can still use the PCA approach.</p><p>Another interesting weakness of PCA, which we will cover here, is when it's being applied to special classification problems. Let's replace <code class="literal">good = (x1 &gt; 5) | (x2 &gt; 5)</code> with <code class="literal">good = x1 &gt; x2</code> to simulate such a special case and we quickly see the problem:</p><div class="mediaobject"><img src="images/2772OS_11_09.jpg" alt="Limitations of PCA and how LDA can help"/></div><p>Here, the classes are not distributed according to the axis with the highest variance, but the second highest variance. Clearly, PCA falls flat on its face. As we don't provide PCA with any cues regarding the class labels, it cannot do any better.</p><p>
<span class="strong"><strong>Linear Discriminant Analysis</strong></span> (<span class="strong"><strong>LDA</strong></span>) comes <a id="id592" class="indexterm"/>to the<a id="id593" class="indexterm"/> rescue here. It's a method that tries to maximize the distance of points belonging to different classes, while minimizing the distances of points of the same class. We won't give any more details regarding how in particular the underlying theory works, just a quick tutorial on how to use it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn import lda</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; lda_inst = lda.LDA(n_components=1)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Xtrans = lda_inst.fit_transform(X, good)</strong></span>
</pre></div><p>That's all. Note that in contrast to the previous PCA example, we provide the class labels to the <code class="literal">fit_transform()</code> method. Thus, PCA is an unsupervised feature extraction method, whereas LDA is a supervised one. The result looks as expected:</p><div class="mediaobject"><img src="images/2772OS_11_10.jpg" alt="Limitations of PCA and how LDA can help"/></div><p>So, why then consider PCA at all and not simply use LDA? Well, it's not that simple. With an increasing number of classes and fewer samples per class, LDA does not look that well any more. Also, PCA seems to be not as sensitive to different training sets as LDA. So, when we have to advise which method to use, we can only suggest a clear "it depends".</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Multidimensional scaling"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec65"/>Multidimensional scaling</h1></div></div></div><p>Although, PCA <a id="id594" class="indexterm"/>tries to use optimization for retained<a id="id595" class="indexterm"/> variance, <span class="strong"><strong>multidimensional scaling</strong></span> (<span class="strong"><strong>MDS</strong></span>) tries to retain the relative distances as much as possible when reducing the dimensions. This is useful when we have a high-dimensional dataset and want to get a visual impression.</p><p>MDS does not care about the data points themselves; instead, it's interested in the dissimilarities between pairs of data points and interprets these as distances. The first thing the MDS algorithm is doing is, therefore, taking all the <span class="inlinemediaobject"><img src="images/2772OS_11_28.jpg" alt="Multidimensional scaling"/></span> datapoints of dimension <span class="inlinemediaobject"><img src="images/2772OS_11_31.jpg" alt="Multidimensional scaling"/></span> and calculates a distance matrix using a distance function <span class="inlinemediaobject"><img src="images/2772OS_11_32.jpg" alt="Multidimensional scaling"/></span>, which measures the (most of the time, Euclidean) distance in the original feature space:</p><div class="mediaobject"><img src="images/2772OS_11_33.jpg" alt="Multidimensional scaling"/></div><p>Now, MDS tries to position the individual datapoints in the lower dimensional space such that the new distance there resembles the distances in the original space as much as possible. As MDS is often used for visualization, the choice of the lower dimension is most of the time two or three.</p><p>Let's have a look at the following simple data consisting of three datapoints in five-dimensional space. Two of the datapoints are close by and one is very distinct and we want to visualize this in three and two dimensions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; X = np.c_[np.ones(5), 2 * np.ones(5), 10 * np.ones(5)].T</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(X)</strong></span>
<span class="strong"><strong>[[  1.   1.   1.   1.   1.]</strong></span>
<span class="strong"><strong> [  2.   2.   2.   2.   2.]</strong></span>
<span class="strong"><strong> [ 10.  10.  10.  10.  10.]]</strong></span>
</pre></div><p>Using the <code class="literal">MDS</code> class in scikit-learn's <code class="literal">manifold</code> package, we first specify that we want to transform <code class="literal">X</code> into a three-dimensional Euclidean space:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn import manifold</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; mds = manifold.MDS(n_components=3)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Xtrans = mds.fit_transform(X)</strong></span>
</pre></div><p>To visualize it in two dimensions, we would need to set <code class="literal">n_components</code> accordingly.</p><p>The results can be <a id="id596" class="indexterm"/>seen in the following two graphs. The triangle and circle are both close together, whereas the star is far away:</p><div class="mediaobject"><img src="images/2772OS_11_13.jpg" alt="Multidimensional scaling"/></div><p>Let's have a look<a id="id597" class="indexterm"/> at the slightly more complex Iris dataset. We will use it later to contrast LDA with PCA. The Iris dataset contains four attributes per flower. With the preceding code, we would project it into three-dimensional space while keeping the relative distances between the individual flowers as much as possible. In the previous example, we did not specify any metric, so <code class="literal">MDS</code> will default to Euclidean. This means that flowers that were "different" according to their four attributes should also be far away in the MDS-scaled three-dimensional space and flowers that were similar should be near together now, as shown in the following diagram:</p><div class="mediaobject"><img src="images/2772OS_11_11.jpg" alt="Multidimensional scaling"/></div><p>Reducing the <a id="id598" class="indexterm"/>dimensional reduction to three and two dimensions <a id="id599" class="indexterm"/>with PCA instead, we see the expected bigger spread of the flowers belonging to the same class, as shown in the following diagram:</p><div class="mediaobject"><img src="images/2772OS_11_12.jpg" alt="Multidimensional scaling"/></div><p>Of course, using MDS requires an understanding of the individual feature's units; maybe we are using features that cannot be compared using the Euclidean metric. For instance, a categorical variable, even when encoded as an integer (0= circle, 1= star, 2= triangle, and so on), cannot be compared using Euclidean (is circle closer to star than to triangle?).</p><p>However, once we are aware of this issue, MDS is a useful tool that reveals similarities in our data that otherwise would be difficult to see in the original feature space.</p><p>Looking a bit <a id="id600" class="indexterm"/>deeper into MDS, we realize that it's not a single algorithm, but rather a family of different algorithms, of which we have used just one. The same was true for PCA. Also, in case you realize that neither PCA nor MDS solves your problem, just look at the many other manifold learning algorithms that are available in the scikit-learn toolkit.</p><p>However, before you get overwhelmed by the many different algorithms, it's always best to start with the simplest one and see how far you get with it. Then, take the next more complex one and continue from there.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec66"/>Summary</h1></div></div></div><p>You learned that sometimes you can get rid of complete features using feature selection methods. We also saw that in some cases, this is not enough and we have to employ feature extraction methods that reveal the real and the lower-dimensional structure in our data, hoping that the model has an easier game with it.</p><p>For sure, we only scratched the surface of the huge body of available dimensionality reduction methods. Still, we hope that we got you interested in this whole field, as there are lots of other methods waiting for you to be picked up. At the end, feature selection and extraction is an art, just like choosing the right learning method or training model.</p><p>The next chapter covers the use of Jug, a little Python framework to manage computations in a way that takes advantage of multiple cores or multiple machines. You will also learn about AWS, the Amazon Cloud.</p></div></div>
</body></html>
- en: Chapter 12. Specialized Machine Learning Topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on reaching this point in your machine learning journey! If
    you have not already started work on your own projects, you will do so soon. And
    in doing so, you may find that the task of turning data into action is more difficult
    than it first appeared.
  prefs: []
  type: TYPE_NORMAL
- en: As you gathered data, you may have realized that the information was trapped
    in a proprietary format or spread across pages on the Web. Making matters worse,
    after spending hours reformatting the data, maybe your computer slowed to a crawl
    after running out of memory. Perhaps R even crashed or froze your machine. Hopefully,
    you were undeterred, as these issues can be remedied with a bit more effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers techniques that may not apply to every project, but will
    prove useful for working around such specialized issues. You might find the information
    particularly useful if you tend to work with data that is:'
  prefs: []
  type: TYPE_NORMAL
- en: Stored in unstructured or proprietary formats such as web pages, web APIs, or
    spreadsheets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a specialized domain such as bioinformatics or social network analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Too large to fit in memory or analyses take a very long time to complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You're not alone if you suffer from any of these problems. Although there is
    no panacea—these issues are the bane of the data scientist as well as the reason
    data skills are in high demand—through the dedicated efforts of the R community,
    a number of R packages provide a head start toward solving the problem.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides a cookbook of such solutions. Even if you are an experienced
    R veteran, you may discover a package that simplifies your workflow. Or, perhaps
    one day, you will author a package that makes work easier for everybody else!
  prefs: []
  type: TYPE_NORMAL
- en: Working with proprietary files and databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the examples in this book, real-world data is rarely packaged in a simple
    CSV form that can be downloaded from a website. Instead, significant effort is
    needed to prepare data for analysis. Data must be collected, merged, sorted, filtered,
    or reformatted to meet the requirements of the learning algorithm. This process
    is informally known as **data munging** or **data wrangling**.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation has become even more important, as the size of typical datasets
    has grown from megabytes to gigabytes, and data is gathered from unrelated and
    messy sources, many of which are stored in massive databases. Several packages
    and resources for retrieving and working with proprietary data formats and databases
    are listed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Reading from and writing to Microsoft Excel, SAS, SPSS, and Stata files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A frustrating aspect of data analysis is the large amount of work required to
    pull and combine data from various proprietary formats. Vast troves of data exist
    in files and databases that simply need to be unlocked for use in R. Thankfully,
    packages exist for exactly this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'What used to be a tedious and time-consuming process, requiring knowledge of
    specific tricks and tools across multiple R packages, has been made trivial by
    a relatively new R package called `rio` (an acronym for R input and output). This
    package, by Chung-hong Chan, Geoffrey CH Chan, Thomas J. Leeper, and Christopher
    Gandrud, is described as a "Swiss-army knife for data". It is capable of importing
    and exporting a large variety of file formats, including but not limited to: tab-separated
    (`.tsv`), comma-separated (`.csv`), JSON (`.json`), Stata (`.dta`), SPSS (`.sav`
    and `.por`), Microsoft Excel (`.xls` and `.xlsx`), Weka (`.arff`), and SAS (`.sas7bdat`
    and `.xpt`).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the complete list of file types `rio` can import and export, as well as
    more detailed usage examples, see [http://cran.r-project.org/web/packages/rio/vignettes/rio.html](http://cran.r-project.org/web/packages/rio/vignettes/rio.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rio` package consists of three functions for working with proprietary
    data formats: `import()`, `export()`, and `convert()`. Each does exactly what
    you''d expect, given their name. Consistent with the package''s philosophy of
    keeping things simple, each function uses the filename extension to guess the
    type of file to import, export, or convert.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to import the credit data from previous chapters, which is stored
    in CSV format, simply type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This creates the `credit` data frame as expected; as a bonus, not only did we
    not have to specify the CSV file type, `rio` automatically set `stringsAsFactors
    = FALSE` as well as other reasonable defaults.
  prefs: []
  type: TYPE_NORMAL
- en: 'To export the `credit` data frame to Microsoft Excel (`.xlsx`) format, use
    the `export()` function while specifying the desired filename, as follows. For
    other formats, simply change the file extension to the desired output type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to convert the CSV file to another format directly, without
    an import step, using the `convert()` function. For example, this converts the
    `credit.csv` file to Stata (`.dta`) format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Though the `rio` package covers many common proprietary data formats, it does
    not do everything. The next section covers other ways to get data into R via database
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: Querying data in SQL databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large datasets are often stored in **Database Management Systems** (**DBMSs**)
    such as Oracle, MySQL, PostgreSQL, Microsoft SQL, or SQLite. These systems allow
    the datasets to be accessed using a **Structured Query Language** (**SQL**), a
    programming language designed to pull data from databases. If your DBMS is configured
    to allow **Open Database Connectivity** (**ODBC**), the `RODBC` package by Brian
    Ripley can be used to import this data directly into an R data frame.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have trouble using ODBC to connect to your database, you may try one
    of the DMBS-specific R packages. These include `ROracle`, `RMySQL`, `RPostgresSQL`,
    and `RSQLite`. Though they will function largely similar to the instructions here,
    refer to the package documentation on CRAN for instructions specific to each package.
  prefs: []
  type: TYPE_NORMAL
- en: ODBC is a standard protocol for connecting to databases regardless of operating
    system or DBMS. If you were previously connected to an ODBC database, you most
    likely would have referred to it via its **Data Source Name** (**DSN**). You will
    need the DSN, plus a username and password (if your database requires it) to use
    `RODBC`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The instructions to configure an ODBC connection are highly specific to the
    combination of the OS and DBMS. If you are having trouble setting up an ODBC connection,
    check with your database administrator. Another way to obtain help is via the
    `RODBC` package `vignette`, which can be accessed in R with the `vignette("RODBC")`
    command after the `RODBC` package has been installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open a connection called `my_db` for the database with the `my_dsn` DSN,
    use the `odbcConnect()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if your ODBC connection requires a username and password, they
    should be specified while calling the `odbcConnect()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: With an open database connection, we can use the `sqlQuery()` function to create
    an R data frame from the database rows pulled by an SQL query. This function,
    like the many functions that create data frames, allows us to specify `stringsAsFactors
    = FALSE` to prevent R from automatically converting character data into factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sqlQuery()` function uses typical SQL queries, as shown in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The resulting `results_df` object is a data frame containing all of the rows
    selected using the SQL query stored in `sql_query`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done using the database, the connection can be closed using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Although R will automatically close ODBC connections at the end of an R session,
    it is a better practice to do so explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Working with online data and services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the growing amount of data available from web-based sources, it is increasingly
    important for machine learning projects to be able to access and interact with
    online services. R is able to read data from online sources natively, with some
    caveats. Firstly, by default, R cannot access secure websites (those using the
    `https://` rather than the `http://` protocol). Secondly, it is important to note
    that most web pages do not provide data in a form that R can understand. The data
    would need to be **parsed**, or broken apart and rebuilt into a structured form,
    before it can be useful. We'll discuss the workarounds shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if neither of these caveats applies (that is, if data are already
    online on a nonsecure website and in a tabular form, like CSV, that R can understand
    natively), then R''s `read.csv()` and `read.table()` functions will be able to
    access data from the Web just as if it were on your local machine. Simply supply
    the full URL for the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'R also provides functionality to download other files from the Web, even if
    R cannot use them directly. For a text file, try the `readLines()` function as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For other types of files, the `download.file()` function can be used. To download
    a file to R''s current working directory, simply supply the URL and destination
    filename as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Beyond this base functionality, there are numerous packages that extend R's
    capabilities to work with online data. The most basic of them will be covered
    in the sections that follow. As the Web is massive and ever-changing, these sections
    are far from a comprehensive set of all the ways R can connect to online data.
    There are literally hundreds of packages for everything from niche projects to
    massive ones.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the most complete and up-to-date list of packages, refer to the regularly
    updated CRAN Web Technologies and Services task view at [http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html).
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the complete text of web pages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RCurl` package by Duncan Temple Lang supplies a more robust way of accessing
    web pages by providing an R interface to the **curl** (client for URLs) utility,
    a command-line tool to transfer data over networks. The curl program acts much
    like a programmable web browser; given a set of commands, it can access and download
    the content of nearly anything available on the Web. Unlike R, it can access secure
    websites as well as post data to online forms. It is an incredibly powerful utility.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Precisely because it is so powerful, a complete curl tutorial is outside the
    scope of this chapter. Instead, refer to the online `RCurl` documentation at [http://www.omegahat.org/RCurl/](http://www.omegahat.org/RCurl/).
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the `RCurl` package, downloading a page is as simple as typing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will save the full text of the Packt Publishing homepage (including all
    the web markup) into the R character object named `packt_page`. As shown in the
    following lines, this is not very useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The reason that the first 200 characters of the page look like nonsense is because
    the websites are written using **Hypertext Markup Language** (**HTML**), which
    combines the page text with special tags that tell web browsers how to display
    the text. The `<title>` and `</title>` tags here surround the page title, telling
    the browser that this is the Packt Publishing homepage. Similar tags are used
    to denote other portions of the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though curl is the cross-platform standard to access online content, if you
    work with web data frequently in R, the `httr` package by Hadley Wickham builds
    upon the foundation of `RCurl` to make it more convenient and R-like. We can see
    some of the differences immediately by attempting to download the Packt Publishing
    homepage using the `httr` package''s `GET()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Where the `getURL()` function in `RCurl` downloaded only the HTML, the `GET()`
    function returns a list with site properties in addition to the HTML. To access
    the page content itself, we need to use the `content()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In order to use this data in an R program, it is necessary to process the page
    to transform it into a structured format like a list or data frame. Functions
    to do so are discussed in the sections that follow.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For detailed `httr` documentation and tutorials, visit the project GitHub page
    at [https://github.com/hadley/httr](https://github.com/hadley/httr). The quickstart
    guide is particularly helpful to learn the base functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping data from web pages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because there is a consistent structure of the HTML tags of many web pages,
    it is possible to write programs that look for desired sections of the page and
    extract them in order to compile them into a dataset. This process practice of
    harvesting data from websites and transforming it into a structured form is known
    as **web scraping**.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Though it is frequently used, scraping should be considered a last resort to
    get data from the Web. This is because any changes to the underlying HTML structure
    may break your code, requiring efforts to be fixed. Even worse, it may introduce
    unnoticed errors into your data. Additionally, many websites' terms of use agreements
    explicitly forbid automated data extraction, not to mention the fact that your
    program's traffic may overload their servers. Always check the site's terms before
    you begin your project; you may even find that the site offers its data freely
    via a developer agreement.
  prefs: []
  type: TYPE_NORMAL
- en: The `rvest` package (a pun on the term "harvest") by Hadley Wickham makes web
    scraping a largely effortless process, assuming the data you want can be found
    in a consistent place within HTML.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a simple example using the Packt Publishing homepage. We
    begin by downloading the page as before, using the `html()` function in the `rvest`
    package. Note that this function, when supplied with a URL, simply calls the `GET()`
    function in Hadley Wickham''s `httr` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we''d like to scrape the page title. Looking at the previous HTML code,
    we know that there is only one title per page wrapped within `<title>` and `</title>`
    tags. To pull the title, we supply the tag name to the `html_node()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This keeps the HTML formatting in place, including the `<title>` tags and the
    `&amp;` code, which is the HTML designation for the ampersand symbol. To translate
    this into plain text, we simply run it through the `html_text()` function, as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Notice the use of the `%>%` operator. This is known as a pipe, because it essentially
    "pipes" data from one function to another. The use of pipes allows the creation
    of powerful chains of functions to process HTML data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pipe operator is a part of the `magrittr` package by Stefan Milton Bache
    and Hadley Wickham, installed by default with the `rvest` package. The name is
    a play on René Magritte's famous painting of a pipe (you may recall seeing it
    in [Chapter 1](ch01.html "Chapter 1. Introducing Machine Learning"), *Introducing
    Machine Learning*). For more information on the project, visit its GitHub page
    at [https://github.com/smbache/magrittr](https://github.com/smbache/magrittr).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try a slightly more interesting example. Suppose we''d like to scrape
    a list of all the packages on the CRAN machine learning task view. We begin as
    in the same way we did it earlier, by downloading the HTML page using the `html()`
    function. Since we don''t know how the page is structured, we''ll also peek into
    HTML by typing `cran_ml`, the name of the R object we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking over the output, we find that one section appears to have the data
    we''re interested in. Note that only a subset of the output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `<h3>` tags imply a header of size 3, while the `<ul>` and `<li>` tags refer
    to the creation of an unordered list and list items, respectively. The data elements
    we want are surrounded by `<a>` tags, which are hyperlink anchor tags that link
    to the CRAN page for each package.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because the CRAN page is actively maintained and may be changed at any time,
    do not be surprised if your results differ from those shown here.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this knowledge in hand, we can scrape the links much like we did previously.
    The one exception is that, because we expect to find more than one result, we
    need to use the `html_nodes()` function to return a vector of results rather than
    `html_node()`, which returns only a single item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s peek at the result using the `head()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As we can see on line 6, it looks like the links to some other projects slipped
    in. This is because some packages are hyperlinked to additional websites; in this
    case, the `RWeka` package is linked to both CRAN and its homepage. To exclude
    these results, you might chain this output to another function that could look
    for the `/packages` string in the hyperlink.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, web scraping is always a process of iterate-and-refine as you identify
    more specific criteria to exclude or include specific cases. The most difficult
    cases may even require a human eye to achieve 100 percent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: These are simple examples that merely scratch the surface of what is possible
    with the `rvest` package. Using the pipe functionality, it is possible to look
    for tags nested within tags or specific classes of HTML tags. For these types
    of complex examples, refer to the package documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing XML documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XML is a plaintext, human-readable, structured markup language upon which many
    document formats have been based. It employs a tagging structure in some ways
    similar to HTML, but is far stricter about formatting. For this reason, it is
    a popular online format to store structured datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The `XML` package by Duncan Temple Lang provides a suite of R functionality
    based on the popular C-based `libxml2` parser to read and write XML documents.
    It is the grandfather of XML parsing packages in R and is still widely used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Information on the `XML` package, including simple examples to get you started
    quickly, can be found on the project's website at [http://www.omegahat.org/RSXML/](http://www.omegahat.org/RSXML/).
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the `xml2` package by Hadley Wickham has surfaced as an easier and
    more R-like interface to the `libxml2` library. The `rvest` package, which was
    covered earlier in this chapter, utilizes `xml2` behind the scenes to parse HTML.
    Moreover, `rvest` can be used to parse XML as well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `xml2` GitHub page is found at [https://github.com/hadley/xml2](https://github.com/hadley/xml2).
  prefs: []
  type: TYPE_NORMAL
- en: Because parsing XML is so closely related to parsing HTML, the exact syntax
    is not covered here. Please refer to these packages' documentation for examples.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing JSON from web APIs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Online applications communicate with one another using web-accessible functions
    known as **Application Programming Interfaces** (**APIs**). These interfaces act
    much like a typical website; they receive a request from a client via a particular
    URL and return a response. The difference is that a normal website returns HTML
    meant for display in a web browser, while an API typically returns data in a structured
    form meant for processing by a machine.
  prefs: []
  type: TYPE_NORMAL
- en: Though it is not uncommon to find XML-based APIs, perhaps the most common API
    data structure today is **JavaScript Object Notation** (**JSON**). Like XML, it
    is a standard, plaintext format, most often used for data structures and objects
    on the Web. The format has become popular recently due to its roots in browser-based
    JavaScript applications, but despite the pedigree, its utility is not limited
    to the Web. The ease in which JSON data structures can be understood by humans
    and parsed by machines makes it an appealing data structure for many types of
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'JSON is based on a simple `{key: value}` format. The `{ }` brackets denote
    a JSON object, and the `key` and `value` parameters denote a property of the object
    and the status of the property. An object can have any number of properties and
    the properties themselves may be objects. For example, a JSON object for this
    book might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This example illustrates the data types available to JSON: numeric, character,
    array (surrounded by `[` and `]` characters), and object. Not shown are the `null`
    and Boolean (`true` or `false`) values. The transmission of these types of objects
    from application to application and application to web browser, is what powers
    many of the most popular websites.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For details on the JSON format, go to [http://www.json.org/](http://www.json.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Public-facing APIs allow programs like R to systematically query websites to
    retrieve results in the JSON format, using packages like `RCurl` and `httr`. Though
    a full tutorial on using web APIs is worthy of a separate book, the basic process
    relies on only a couple of steps—it's the details that are tricky.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we wanted to query the Google Maps API to locate the latitude and longitude
    of the Eiffel Tower in France. We first need to review the Google Maps API documentation
    to determine the URL and parameters needed to make this query. We then supply
    this information to the `httr` package''s `GET()` function, adding a list of query
    parameters in order to apply the search address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'By typing the name of the resulting object, we can see some details about the
    request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the resulting JSON, which the `httr` package parsed automatically,
    we use the `content()` function. For brevity, only a handful of lines are shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To access these contents individually, simply refer to them using list syntax.
    The names are based on the JSON objects returned by the Google API. For instance,
    the entire set of results is in an object appropriately named `results` and each
    result is numbered. In this case, we will access the formatted address property
    of the first result, as well as the latitude and longitude:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: These data elements could then be used in an R program as desired.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because the Google Maps API may be updated in the future, if you find that your
    results differ from those shown here, please check the Packt Publishing support
    page for updated code.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you would like to do a conversion to and from the JSON
    format outside the `httr` package, there are a number of packages that add this
    functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rjson` package by Alex Couture-Beil was one of the earliest packages to
    allow R data structures to be converted back and forth from the JSON format. The
    syntax is simple. After installing the `rjson` package, to convert from an R object
    to a JSON string, we use the `toJSON()` function. Notice that the quote characters
    have escaped using the `\"` notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To convert a JSON string into an R object, use the `fromJSON()` function. Quotation
    marks in the string need to be escaped, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in a list structure in a form much like the original JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Recently, two new JSON packages have arrived on the scene. The first, `RJSONIO`,
    by Duncan Temple Lang was intended to be a faster and more extensible version
    of the `rjson` package, though they are now virtually identical. A second package,
    `jsonlite`, by Jeroen Ooms has quickly gained prominence as it creates data structures
    that are more consistent and R-like, especially while using data from web APIs.
    Which of these packages you use is a matter of preference; all three are virtually
    identical in practice as they each implement a `fromJSON()` and `toJSON()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information on the potential benefits of the `jsonlite` package, see:
    Ooms J. The jsonlite package: a practical and consistent mapping between JSON
    data and R objects. 2014\. Available at: [http://arxiv.org/abs/1403.2805](http://arxiv.org/abs/1403.2805)'
  prefs: []
  type: TYPE_NORMAL
- en: Working with domain-specific data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning has undoubtedly been applied to problems across every discipline.
    Although the basic techniques are similar across all domains, some are so specialized
    that communities are formed to develop solutions to the challenges unique to the
    field. This leads to the discovery of new techniques and new terminology that
    is relevant only to domain specific problems.
  prefs: []
  type: TYPE_NORMAL
- en: This section covers a pair of domains that use machine learning techniques extensively,
    but require specialized knowledge to unlock their full potential. Since entire
    books have been written on these topics, it will serve only as the briefest of
    introductions. For more details, seek out the help provided by the resources cited
    in each section.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing bioinformatics data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field of **bioinformatics** is concerned with the application of computers
    and data analysis to the biological domain, particularly with regard to better
    understanding the genome. As genetic data is unique compared to many other types,
    data analysis in the field of bioinformatics offers a number of unique challenges.
    For example, because living creatures have a tremendous number of genes and genetic
    sequencing is still relatively expensive, typical datasets are much wider than
    they are long; that is, they have more features (genes) than examples (creatures
    that have been sequenced). This creates problems while attempting to apply conventional
    visualizations, statistical tests, and machine learning methods to such data.
    Additionally, the increasing use of proprietary **microarray** "lab-on-a-chip"
    techniques requires highly specialized knowledge simply to load the genetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A CRAN task view, which lists some of R's specialized packages for statistical
    genetics and bioinformatics, is available at [http://cran.r-project.org/web/views/Genetics.html](http://cran.r-project.org/web/views/Genetics.html).
  prefs: []
  type: TYPE_NORMAL
- en: The **Bioconductor** project of the Fred Hutchinson Cancer Research Center in
    Seattle, Washington, aims to solve some of these problems by providing a standardized
    set of methods for analyzing genomic data. Using R as its foundation, Bioconductor
    adds bioinformatics-specific packages and documentation on top of the base R software.
  prefs: []
  type: TYPE_NORMAL
- en: Bioconductor provides workflows to analyze DNA and protein microarray data from
    common microarray platforms such as Affymetrix, Illumina, Nimblegen, and Agilent.
    Additional functionality includes sequence annotation, multiple testing procedures,
    specialized visualizations, tutorials, documentation, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the Bioconductor project, visit the project website
    at [http://www.bioconductor.org](http://www.bioconductor.org).
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and visualizing network data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Social network data and graph datasets consist of structures that describe connections,
    or **links** (sometimes also called **edges**), between people or objects known
    as **nodes**. With *N* nodes, a *N* x *N = N[2]* matrix of potential links can
    be created. This creates tremendous computational complexity as the number of
    nodes grows.
  prefs: []
  type: TYPE_NORMAL
- en: The field of **network analysis** is concerned with statistical measures and
    visualizations that identify meaningful patterns of connections. For example,
    the following figure shows three clusters of circular nodes, all connected via
    a square node at the center. A network analysis may reveal the importance of the
    square node, among other key metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing and visualizing network data](img/B03905_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `network` package by Carter T. Butts, David Hunter, and Mark S. Handcock
    offers a specialized data structure to work with networks. This data structure
    is necessary due to the fact that the matrix needed to store *N[2]* potential
    links will quickly exceed available memory; the `network` data structure uses
    a sparse representation to store only existent links, saving a great deal of memory
    if most relationships are nonexistent. A closely related package, `sna` (social
    network analysis), allows the analysis and visualization of the `network` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on `network` and `sna`, including very detailed tutorials
    and documentation, refer to the project website hosted by the University of Washington
    at [http://www.statnet.org/](http://www.statnet.org/).
  prefs: []
  type: TYPE_NORMAL
- en: The `igraph` package by Gábor Csárdi provides another set of tools to visualize
    and analyze network data. It is capable of calculating metrics for very large
    networks. An additional benefit of `igraph` is the fact that it has analogous
    packages for the Python and C programming languages, allowing it to be used to
    perform analyses virtually anywhere. As we will demonstrate shortly, it is very
    easy to use.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the `igraph` package, including demos and tutorials,
    visit the homepage at [http://igraph.org/r/](http://igraph.org/r/).
  prefs: []
  type: TYPE_NORMAL
- en: Using network data in R requires the use of specialized formats, as network
    data are not typically stored in typical tabular data structures like CSV files
    and data frames. As mentioned previously, because there are *N[2]* potential connections
    between *N* network nodes, a tabular structure would quickly grow to be unwieldy
    for all but the smallest *N* values. Instead, graph data are stored in a form
    that lists only the connections that are truly present; absent connections are
    inferred from the absence of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps the simplest of such formats is **edgelist**, which is a text file
    with one line per network connection. Each node must be assigned a unique identifier
    and the links between the nodes are defined by placing the connected nodes'' identifiers
    together on a single line separated by a space. For instance, the following edgelist
    defines three connections between node 0 and nodes 1, 2, and 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To load network data into R, the `igraph` package provides a `read.graph()`
    function that can read edgelist files as well as other more sophisticated formats
    like **Graph Modeling Language** (**GML**). To illustrate this functionality,
    we''ll use a dataset describing friendship among the members of a small karate
    club. To follow along, download the `karate.txt` file from the Packt Publishing
    website and save it in your R working directory. After you''ve installed the `igraph`
    package, the karate network can be read into R as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This will create a sparse matrix object that can be used for graphing and network
    analysis. Note that the `directed = FALSE` parameter forces the network to use
    undirected or bidirectional links between the nodes. Since the karate dataset
    describes friendship, it means that if person 1 is friends with person 2, then
    person 2 must be friends with person 1\. On the other hand, if the dataset described
    fight outcomes, the fact that person 1 defeated person 2 would certainly not imply
    that person 2 defeated person 1\. In this case, the `directed = TRUE` parameter
    should be set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The karate network dataset used here was compiled by *M.E.J. Newman* of the
    University of Michigan. It was first presented in Zachary WW. *An information
    flow model for conflict and fission in small groups*. Journal of Anthropological
    Research. 1977; 33:452-473.
  prefs: []
  type: TYPE_NORMAL
- en: 'To examine the graph, use the `plot()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing and visualizing network data](img/B03905_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Examining the network visualization, it is apparent that there are a few highly
    connected members of the karate club. Nodes 1, 33, and 34 seem to be more central
    than the others, which remain at the club periphery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `igraph` to calculate graph metrics, it is possible to demonstrate our
    hunch analytically. The **degree** of a node measures how many nodes it is linked
    to. The `degree()` function confirms our hunch that nodes 1, 33, and 34 are more
    connected than the others with `16`, `12`, and `17` connections, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Because some connections are more important than others, a variety of network
    measures have been developed to measure node connectivity with this consideration.
    A network metric called **betweenness centrality** is intended to capture the
    number of shortest paths between nodes that pass through each node. Nodes that
    are truly more central to the entire graph will have a higher betweenness centrality
    value, because they act as a bridge between the other nodes. We obtain a vector
    of the centrality measures using the `betweenness()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As nodes 1 and 34 have much greater betweenness values than the others, they
    are more central to the karate club's friendship network. These two individuals,
    with extensive personal friendship networks, may be the "glue" that holds the
    network together.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Betweenness centrality is only one of many metrics intended to capture a node's
    importance, and it isn't even the only measure of centrality. Refer to the `igraph`
    documentation for definitions of other network properties.
  prefs: []
  type: TYPE_NORMAL
- en: The `sna` and `igraph` packages are capable of computing many such graph metrics,
    which may then be used as inputs to machine learning functions. For example, suppose
    we were attempting to build a model predicting who would win an election for the
    club's president. The fact that nodes 1 and 34 are well-connected suggests that
    they may have the social capital needed for such a leadership role. These might
    be the highly valuable predictors of the election's results.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By combining network analysis with machine learning, services like Facebook,
    Twitter, and LinkedIn provide vast stores of network data to make predictions
    about the users' future behavior. A high-profile example is the 2012 U.S. Presidential
    campaign in which chief data scientist Rayid Ghani utilized Facebook data to identify
    people who might be persuaded to vote for Barack Obama.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the performance of R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R has a reputation for being slow and memory-inefficient, a reputation that
    is at least somewhat earned. These faults are largely unnoticed on a modern PC
    for datasets of many thousands of records, but datasets with a million records
    or more can exceed the limits of what is currently possible with consumer-grade
    hardware. The problem worsens if the dataset contains many features or if complex
    learning algorithms are being used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CRAN has a high-performance computing task view that lists packages pushing
    the boundaries of what is possible in R. It can be viewed at [http://cran.r-project.org/web/views/HighPerformanceComputing.html](http://cran.r-project.org/web/views/HighPerformanceComputing.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Packages that extend R past the capabilities of the base software are being
    developed rapidly. This work comes primarily on two fronts: some packages add
    the capability to manage extremely large datasets by making data operations faster
    or allowing the size of the data to exceed the amount of available system memory;
    others allow R to work faster, perhaps by spreading the work over additional computers
    or processors, utilizing specialized computer hardware, or providing machine learning
    algorithms optimized for big data problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Managing very large datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extremely large datasets can cause R to grind to a halt when the system runs
    out of memory to store data. Even if the entire dataset can fit into the available
    memory, additional memory overhead will be needed for data processing. Furthermore,
    very large datasets can take a long amount of time to analyze for no reason other
    than the sheer volume of records; even a quick operation can cause delays when
    performed many millions of times.
  prefs: []
  type: TYPE_NORMAL
- en: Years ago, many would perform data preparation outside R in another programming
    language, or use R but perform analyses on a smaller subset of data. However,
    this is no longer necessary, as several packages have been contributed to R to
    address these problems.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing tabular data structures with dplyr
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `dplyr` package introduced in 2014 by Hadley Wickham and Romain Francois
    is perhaps the most straightforward way to begin working with large datasets in
    R. Though other packages may exceed its capabilities in terms of raw speed or
    the raw size of the data, `dplyr` is still quite capable. More importantly, it
    is virtually transparent after the initial learning curve has passed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on `dplyr`, including some very helpful tutorials, refer
    to the project's GitHub page at [https://github.com/hadley/dplyr](https://github.com/hadley/dplyr).
  prefs: []
  type: TYPE_NORMAL
- en: 'Put simply, the package provides an object called `tbl`, which is an abstraction
    of tabular data. It acts much like a data frame, with several important exceptions:'
  prefs: []
  type: TYPE_NORMAL
- en: Key functionality has been written in C++, which according to the authors results
    in a 20x to 1000x performance increase for many operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R data frames are limited by available memory. The `dplyr` version of a data
    frame can be linked transparently to disk-based databases that can exceed what
    can be stored in memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `dplyr` package makes reasonable assumptions about data frames that optimize
    your effort as well as memory use. It doesn't automatically change data types.
    And, if possible, it avoids making copies of data by pointing to the original
    value instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New operators are introduced that allow common data transformations to be performed
    with much less code while remaining highly readable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Making the transition from data frames to `dplyr` is easy. To convert an existing
    data frame into a `tbl` object, use the `as.tbl()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Typing the name of the table provides information about the object. Even here,
    we see a distinction between `dplyr` and typical R behavior; where as a traditional
    data frame would have displayed many rows of data, `dplyr` objects are more considerate
    of real-world needs. For example, typing the name of the object provides output
    summarized in a form that fits a single screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![Generalizing tabular data structures with dplyr](img/B03905_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Connecting `dplyr` to an external database is straightforward as well. The `dplyr`
    package provides functions to connect to MySQL, PostgreSQL, and SQLite databases.
    These create a connection object that allows `tbl` objects to be pulled from the
    database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the `src_sqlite()` function to create a SQLite database to store
    credit data. SQLite is a simple database that doesn''t require a server. It simply
    connects to a database file, which we''ll call `credit.sqlite3`. Since the file
    doesn''t exist yet, we need to set the `create = TRUE` parameter to create the
    file. Note that for this step to work, you may require to install the `RSQLite`
    package if you have not already done so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the connection, we need to load the data into the database using
    the `copy_to()` function. This uses the `credit_tbl` object to create a database
    table within the database specified by `credit_db_conn`. The `temporary = FALSE`
    parameter forces the table to be created immediately. Since `dplyr` tries to avoid
    copying data unless it must, it will only create the table if it is explicitly
    asked to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the `copy_to()` function will store the data in the `credit.sqlite3`
    file, which can be transported to other systems as needed. To access this file
    later, simply reopen the database connection and create a `tbl` object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In spite of the fact that `dplyr` is routed through a database, the `credit_tbl`
    object here will act exactly like any other `tbl` object and will gain all the
    other benefits of the `dplyr` package.
  prefs: []
  type: TYPE_NORMAL
- en: Making data frames faster with data.table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `data.table` package by Matt Dowle, Tom Short, Steve Lianoglou, and Arun
    Srinivasan provides an enhanced version of a data frame called a **data table**.
    The `data.table` objects are typically much faster than data frames for subsetting,
    joining, and grouping operations. For the largest datasets—those with many millions
    of rows—these objects may be substantially faster than even `dplyr` objects. Yet,
    because it is essentially an improved data frame, the resulting objects can still
    be used by any R function that accepts a data frame.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `data.table` project can be found on GitHub at [https://github.com/Rdatatable/data.table/wiki](https://github.com/Rdatatable/data.table/wiki).
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the `data.table` package, the `fread()` function will read
    tabular files like CSVs into data table objects. For instance, to load the credit
    data used previously, type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The credit data table can then be queried using syntax similar to R''s `[row,
    col]` form, but optimized for speed and some additional useful conveniences. In
    particular, the data table structure allows the `row` portion to select rows using
    an abbreviated subsetting command, and the `col` portion to use a function that
    does something with the selected rows. For example, the following command computes
    the mean requested loan amount for people with a good credit history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: By building larger queries with this simple syntax, very complex operations
    can be performed on data tables. Since the data structure is optimized for speed,
    it can be used with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of the `data.table` structures is that like data frames they
    are limited by the available system memory. The next two sections discuss packages
    that overcome this shortcoming at the expense of breaking compatibility with many
    R functions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `dplyr` and `data.table` packages each have unique strengths. For an in-depth
    comparison, check out the following Stack Overflow discussion at [http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly](http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly).
    It is also possible to have the best of both worlds, as `data.table` structures
    can be loaded into `dplyr` using the `tbl_dt()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Creating disk-based data frames with ff
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ff` package by Daniel Adler, Christian Gläser, Oleg Nenadic, Jens Oehlschlägel,
    and Walter Zucchini provides an alternative to a data frame (`ffdf`) that allows
    datasets of over two billion rows to be created, even if this far exceeds the
    available system memory.
  prefs: []
  type: TYPE_NORMAL
- en: The `ffdf` structure has a physical component that stores the data on a disk
    in a highly efficient form, and a virtual component that acts like a typical R
    data frame, but transparently points to the data stored in the physical component.
    You can imagine the `ffdf` object as a map that points to a location of the data
    on a disk.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ff` project is on the Web at [http://ff.r-forge.r-project.org/](http://ff.r-forge.r-project.org/).
  prefs: []
  type: TYPE_NORMAL
- en: A downside of `ffdf` data structures is that they cannot be used natively by
    most R functions. Instead, the data must be processed in small chunks, and the
    results must be combined later on. The upside of chunking the data is that the
    task can be divided across several processors simultaneously using the parallel
    computing methods presented later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing the `ff` package, to read in a large CSV file, use the `read.csv.ffdf()`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, we cannot work directly with the `ffdf` object, as attempting
    to treat it like a traditional data frame results in an error message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ffbase` package by Edwin de Jonge, Jan Wijffels, and Jan van der Laan
    addresses this issue somewhat by adding capabilities for basic analyses using
    `ff` objects. This makes it possible to use `ff` objects directly for data exploration.
    For instance, after installing the `ffbase` package, the mean function works as
    expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The package also provides other basic functionality such as mathematical operators,
    query functions, summary statistics, and wrappers to work with optimized machine
    learning algorithms like `biglm` (described later in this chapter). Though these
    do not completely eliminate the challenges of working with extremely large datasets,
    they make the process a bit more seamless.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on advanced functionality, visit the `ffbase` project site
    at [http://github.com/edwindj/ffbase](http://github.com/edwindj/ffbase).
  prefs: []
  type: TYPE_NORMAL
- en: Using massive matrices with bigmemory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `bigmemory` package by Michael J. Kane, John W. Emerson, and Peter Haverty
    allows the use of extremely large matrices that exceed the amount of available
    system memory. The matrices can be stored on a disk or in shared memory, allowing
    them to be used by other processes on the same computer or across a network. This
    facilitates parallel computing methods, such as the ones covered later in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Additional documentation on the `bigmemory` package can be found at [http://www.bigmemory.org/](http://www.bigmemory.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Because `bigmemory` matrices are intentionally unlike data frames, they cannot
    be used directly with most of the machine learning methods covered in this book.
    They also can only be used with numeric data. That said, since they are similar
    to a typical R matrix, it is easy to create smaller samples or chunks that can
    be converted into standard R data structures.
  prefs: []
  type: TYPE_NORMAL
- en: The authors also provide the `bigalgebra`, `biganalytics`, and `bigtabulate`
    packages, which allow simple analyses to be performed on the matrices. Of particular
    note is the `bigkmeans()` function in the `biganalytics` package, which performs
    k-means clustering as described in [Chapter 9](ch09.html "Chapter 9. Finding Groups
    of Data – Clustering with k-means"), *Finding Groups of Data – Clustering with
    k-means*. Due to the highly specialized nature of these packages, use cases are
    outside the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Learning faster with parallel computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the early days of computing, processors executed instructions in **serial**
    which meant that they were limited to performing a single task at a time. The
    next instruction could not be started until the previous instruction was complete.
    Although it was widely known that many tasks could be completed more efficiently
    by completing the steps simultaneously, the technology simply did not exist yet.
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning faster with parallel computing](img/B03905_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This was addressed by the development of **parallel computing** methods, which
    use a set of two or more processors or computers to solve a larger problem. Many
    modern computers are designed for parallel computing. Even in the cases in which
    they have a single processor, they often have two or more **cores** that are capable
    of working in parallel. This allows tasks to be accomplished independently of
    one another.
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning faster with parallel computing](img/B03905_12_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Networks of multiple computers called **clusters** can also be used for parallel
    computing. A large cluster may include a variety of hardware and be separated
    over large distances. In this case, the cluster is known as a **grid**. Taken
    to an extreme, a cluster or grid of hundreds or thousands of computers running
    commodity hardware could be a very powerful system.
  prefs: []
  type: TYPE_NORMAL
- en: The catch, however, is that not every problem can be parallelized. Certain problems
    are more conducive to parallel execution than others. One might expect that adding
    100 processors would result in accomplishing 100 times the work in the same amount
    of time (that is, the overall execution time would be 1/100), but this is typically
    not the case. The reason is that it takes effort to manage the workers. Work must
    be divided into equal, nonoverlapping tasks, and each of the workers' results
    must be combined into one final answer.
  prefs: []
  type: TYPE_NORMAL
- en: So-called **embarrassingly parallel** problems are ideal. It is easy to reduce
    these tasks into nonoverlapping blocks of work and recombine the results. An example
    of an embarrassingly parallel machine learning task would be 10-fold cross-validation;
    once the 10 samples are divided, each of the 10 blocks of work is independent,
    meaning that they do not affect the others. As you will soon see, this task can
    be sped up quite dramatically using parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring execution time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Efforts to speed up R will be wasted if it is not possible to systematically
    measure how much time is saved. Although a stopwatch is one option, an easier
    solution would be to wrap the code in a `system.time()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, on my laptop, the `system.time()` function notes that it takes
    about `0.093` seconds to generate a million random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The same function can be used to evaluate the improvement in performance obtained
    by using the methods that were just described or any R function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For what it's worth, when the first edition was published, generating a million
    random numbers took 0.13 seconds. Although I'm now using a slightly more powerful
    computer, this reduction of about 30 percent of the processing time just two years
    later illustrates how quickly computer hardware and software are improving.
  prefs: []
  type: TYPE_NORMAL
- en: Working in parallel with multicore and snow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `parallel` package, now included with R version 2.14.0 and higher, has lowered
    the entry barrier to deploy parallel algorithms by providing a standard framework
    to set up worker processes that can complete tasks simultaneously. It does this
    by including components of the `multicore` and `snow` packages, each taking a
    different approach towards multitasking.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your computer is reasonably recent, you are likely to be able to use parallel
    processing. To determine the number of cores your machine has, use the `detectCores()`
    function as follows. Note that your output will differ depending on your hardware
    specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `multicore` package was developed by Simon Urbanek and allows parallel processing
    on a single machine that has multiple processors or processor cores. It utilizes
    the multitasking capabilities of a computer's operating system to **fork** additional
    R sessions that share the same memory. It is perhaps the simplest way to get started
    with parallel processing in R. Unfortunately, because Windows does not support
    forking, this solution will not work everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to get started with the `multicore` functionality is to use the
    `mclapply()` function, which is a parallel version of `lapply()`. For instance,
    the following blocks of code illustrate how the task of generating a million random
    numbers can be divided across 1, 2, 4, and 8 cores. The `unlist()` function is
    used to combine the parallel results (a list) into a single vector after each
    core has completed its chunk of work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Notice how as the number of cores increases, the elapsed time decreases, and
    the benefit tapers off. Though this is a simple example, it can be adapted easily
    to many other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `snow` package (simple networking of workstations) by Luke Tierney, A.
    J. Rossini, Na Li, and H. Sevcikova allows parallel computing on multicore or
    multiprocessor machines as well as on a network of multiple machines. It is slightly
    more difficult to use, but offers much more power and flexibility. After installing
    `snow`, to set up a cluster on a single machine, use the `makeCluster()` function
    with the number of cores to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Because `snow` communicates via network traffic, depending on your operating
    system, you may receive a message to approve access through your firewall.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm whether the cluster is operational, we can ask each node to report
    back its hostname. The `clusterCall()` function executes a function on each machine
    in the cluster. In this case, we''ll define a function that simply calls the `Sys.info()`
    function and returns the `nodename` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Unsurprisingly, since all four nodes are running on a single machine, they
    report back the same hostname. To have the four nodes run a different command,
    supply them with a unique parameter via the `clusterApply()` function. Here, we''ll
    supply each node with a different letter. Each node will then perform a simple
    function on its letter in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we''re done with the cluster, it''s important to terminate the processes
    it spawned. This will free up the resources each node is using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Using these simple commands, it is possible to speed up many machine learning
    tasks. For larger big data problems, much more complex `snow` configurations are
    possible. For instance, you may attempt to configure a **Beowulf cluster**—a network
    of many consumer-grade machines. In academic and industry research settings with
    dedicated computing clusters, `snow` can use the `Rmpi` package to access these
    high-performance **message-passing interface** (**MPI**) servers. Working with
    such clusters requires the knowledge of network configurations and computing hardware,
    which is outside the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a much more detailed introduction to `snow`, including some information
    on how to configure parallel computing on several computers over a network, see
    [http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf](http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Taking advantage of parallel with foreach and doParallel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `foreach` package by Steve Weston of Revolution Analytics provides perhaps
    the easiest way to get started with parallel computing, particularly if you are
    running R on Windows, as some of the other packages are platform-specific.
  prefs: []
  type: TYPE_NORMAL
- en: The core of the package is a new `foreach` looping construct. If you have worked
    with other programming languages, you may be familiar with it. Essentially, it
    allows looping over a number of items in a set without explicitly counting the
    number of items; in other words, *for each* item in the set, *do* something.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the `foreach` package, Revolution Analytics (recently acquired
    by Microsoft) has developed high-performance, enterprise-ready R builds. Free
    versions are available for trial and academic use. For more information, see their
    website at [http://www.revolutionanalytics.com/](http://www.revolutionanalytics.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re thinking that R already provides a set of apply functions to loop
    over the sets of items (for example, `apply()`, `lapply()`, `sapply()`, and so
    on), you are correct. However, the `foreach` loop has an additional benefit: iterations
    of the loop can be completed in parallel using a very simple syntax. Let''s see
    how this works.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the command we''ve been using to generate a million random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'After the `foreach` package has been installed, it can be expressed by a loop
    that generates four sets of 250,000 random numbers in parallel. The `.combine`
    parameter is an optional setting that tells `foreach` which function it should
    use to combine the final set of results from each loop iteration. In this case,
    since each iteration generates a set of random numbers, we simply use the `c()`
    concatenate function to create a single, combined vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'If you noticed that this function didn''t result in a speed improvement, good
    catch! The reason is that by default, the `foreach` package runs each loop iteration
    in serial. The `doParallel` sister package provides a parallel backend for `foreach`
    that utilizes the `parallel` package included with R, which was described earlier
    in this chapter. After installing the `doParallel` package, simply register the
    number of cores and swap the `%do%` command with `%dopar%`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the output, this code results in the expected performance improvement,
    nearly cutting the execution time in half.
  prefs: []
  type: TYPE_NORMAL
- en: 'To close the `doParallel` cluster, simply type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Though the cluster will be closed automatically at the conclusion of the R session,
    it is better form to do so explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel cloud computing with MapReduce and Hadoop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **MapReduce** programming model was developed at Google as a way to process
    their data on a large cluster of networked computers. MapReduce defined parallel
    programming as a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: A **map** step in which a problem is divided into smaller tasks that are distributed
    across the computers in the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **reduce** step in which the results of the small chunks of work are collected
    and synthesized into a final solution to the original problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A popular open source alternative to the proprietary MapReduce framework is
    **Apache Hadoop**. The Hadoop software comprises of the MapReduce concept, plus
    a distributed filesystem capable of storing large amounts of data across a cluster
    of computers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Packt Publishing has published a large number of books on Hadoop. To search
    current offerings, visit [https://www.packtpub.com/all/?search=hadoop](https://www.packtpub.com/all/?search=hadoop).
  prefs: []
  type: TYPE_NORMAL
- en: Several R projects that provide an R interface to Hadoop are in development.
    The RHadoop project by Revolution Analytics provides an R interface to Hadoop.
    The project provides a package, `rmr`, intended to be an easy way for R developers
    to write MapReduce programs. Another companion package, `plyrmr`, provides functionality
    similar to the `dplyr` package to process large datasets. Additional RHadoop packages
    provide R functions to access Hadoop's distributed data stores.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the RHadoop project, see [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki).
  prefs: []
  type: TYPE_NORMAL
- en: Another similar project is RHIPE by Saptarshi Guha, which attempts to bring
    Hadoop's divide and recombine philosophy into R by managing the communication
    between R and Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `RHIPE` package is not yet available at CRAN, but it can be built from the
    source available on the Web at [http://www.datadr.org](http://www.datadr.org).
  prefs: []
  type: TYPE_NORMAL
- en: GPU computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to parallel processing uses a computer's **Graphics Processing
    Unit** (**GPU**) to increase the speed of mathematical calculations. A GPU is
    a specialized processor that is optimized to rapidly display images on a computer
    screen. Because a computer often needs to display complex 3D graphics (particularly
    for video games), many GPUs use hardware designed for parallel processing and
    extremely efficient matrix and vector calculations. A side benefit is that they
    can be used to efficiently solve certain types of mathematical problems. Where
    a computer processor may have 16 cores, a GPU may have thousands.
  prefs: []
  type: TYPE_NORMAL
- en: '![GPU computing](img/B03905_12_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The downside of GPU computing is that it requires specific hardware that is
    not included in many computers. In most cases, a GPU from the manufacturer Nvidia
    is required, as they provide a proprietary framework called **Complete Unified
    Device Architecture** (**CUDA**) that makes the GPU programmable using common
    languages such as C++.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on Nvidia's role in GPU computing, go to [http://www.nvidia.com/object/what-is-gpu-computing.html](http://www.nvidia.com/object/what-is-gpu-computing.html).
  prefs: []
  type: TYPE_NORMAL
- en: The `gputools` package by Josh Buckner, Mark Seligman, and Justin Wilson implements
    several R functions, such as matrix operations, clustering, and regression modeling
    using the Nvidia CUDA toolkit. The package requires a CUDA 1.3 or higher GPU and
    the installation of the Nvidia CUDA toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying optimized learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the machine learning algorithms covered in this book are able to work
    on extremely large datasets with relatively minor modifications. For instance,
    it would be fairly straightforward to implement Naive Bayes or the Apriori algorithm
    using one of the data structures for big datasets described in the previous sections.
    Some types of learners, such as ensembles, lend themselves well to parallelization,
    because the work of each model can be distributed across processors or computers
    in a cluster. On the other hand, some require larger changes to the data or algorithm,
    or need to be rethought altogether, before they can be used with massive datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections examine packages that provide optimized versions of the
    learning algorithms we've worked with so far.
  prefs: []
  type: TYPE_NORMAL
- en: Building bigger regression models with biglm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `biglm` package by Thomas Lumley provides functions to train regression
    models on datasets that may be too large to fit into memory. It works by using
    an iterative process in which the model is updated little by little using small
    chunks of data. In spite of it being a different approach, the results will be
    nearly identical to what would be obtained by running the conventional `lm()`
    function on the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For convenience while working with the largest datasets, the `biglm()` function
    allows the use of a SQL database in place of a data frame. The model can also
    be trained with chunks obtained from data objects created by the `ff` package
    described previously.
  prefs: []
  type: TYPE_NORMAL
- en: Growing bigger and faster random forests with bigrf
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `bigrf` package by Aloysius Lim implements the training of random forests
    for classification and regression on datasets that are too large to fit into memory.
    It uses the `bigmemory` objects as described earlier in this chapter. For speedier
    forest growth, the package can be used with the `foreach` and `doParallel` packages
    described previously to grow trees in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information, including examples and Windows installation instructions,
    see the package's wiki, which is hosted on GitHub at [https://github.com/aloysius-lim/bigrf](https://github.com/aloysius-lim/bigrf).
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating models in parallel with caret
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `caret` package by Max Kuhn (covered extensively in [Chapter 10](ch10.html
    "Chapter 10. Evaluating Model Performance"), *Evaluating Model Performance* and
    [Chapter 11](ch11.html "Chapter 11. Improving Model Performance"), *Improving
    Model Performance*) will transparently utilize a parallel backend if one has been
    registered with R using the `foreach` package described previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a simple example in which we attempt to train a random
    forest model on the credit dataset. Without parallelization, the model takes about
    109 seconds to be trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, if we use the `doParallel` package to register the four
    cores to be used in parallel, the model takes under 32 seconds to build—less than
    a third of the time—and we didn''t need to change even a single line of the `caret`
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Many of the tasks involved in training and evaluating models, such as creating
    random samples and repeatedly testing predictions for 10-fold cross-validation
    are embarrassingly parallel and ripe for performance improvements. With this in
    mind, it is wise to always register multiple cores before beginning a `caret`
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Configuration instructions and a case study of the performance improvements
    needed to enable parallel processing in `caret` are available on the project's
    website at [http://topepo.github.io/caret/parallel.html](http://topepo.github.io/caret/parallel.html).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is certainly an exciting time to be studying machine learning. Ongoing work
    on the relatively uncharted frontiers of parallel and distributed computing offers
    great potential for tapping the knowledge found in the deluge of big data. The
    burgeoning data science community is facilitated by the free and open source R
    programming language, which provides a very low barrier for entry—you simply need
    to be willing to learn.
  prefs: []
  type: TYPE_NORMAL
- en: The topics you have learned, both in this chapter and in the previous chapters,
    provide the foundation to understand more advanced machine learning methods. It
    is now your responsibility to keep learning and adding tools to your arsenal.
    Along the way, be sure to keep in mind the *No Free Lunch* theorem—no learning
    algorithm can rule them all, and they all have varying strengths and weaknesses.
    For this reason, there will always be a human element to machine learning, adding
    subject-specific knowledge and the ability to match the appropriate algorithm
    to the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming years, it will be interesting to see how the human side changes
    as the line between machine learning and human learning is blurred. Services such
    as Amazon's Mechanical Turk provide crowd-sourced intelligence, offering a cluster
    of human minds ready to perform simple tasks at a moment's notice. Perhaps one
    day, just as we have used computers to perform tasks that human beings cannot
    do easily, computers will employ human beings to do the reverse. What interesting
    food for thought!
  prefs: []
  type: TYPE_NORMAL

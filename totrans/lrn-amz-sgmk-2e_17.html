<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer206">
			<h1 id="_idParaDest-291"><a id="_idTextAnchor290"/>Chapter 13: Optimizing Prediction Cost and Performance</h1>
			<p>In the previous chapter, you learned how to automate training and deployment workflows.</p>
			<p>In this final chapter, we'll focus on optimizing cost and performance for prediction infrastructure, which typically accounts for 90% of the machine learning spend by AWS customers. This number may come as a surprise, until we realize that a model built by a single training job may end on multiple endpoints running 24/7 on a large scale.</p>
			<p>Hence, great care must be taken to optimize your prediction infrastructure to ensure that you get the most bang for your buck!</p>
			<p>This chapter features the following topics:</p>
			<ul>
				<li>Autoscaling an endpoint</li>
				<li>Deploying a multi-model endpoint</li>
				<li>Deploying a model with Amazon Elastic Inference</li>
				<li>Compiling models with Amazon SageMaker Neo</li>
			</ul>
			<h1 id="_idParaDest-292"><a id="_idTextAnchor291"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you haven't got one already, please point your browser at <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create it. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS <strong class="bold">Command Line Interface</strong> (<strong class="bold">CLI</strong>) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>).  </p>
			<p>You will need a working Python 3.x environment. ­Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory but strongly encouraged, as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>Code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-293"><a id="_idTextAnchor292"/>Autoscaling an endpoint</h1>
			<p>Autoscaling has long been the<a id="_idIndexMarker1496"/> most important technique in adjusting infrastructure size for incoming traffic, and it's available for SageMaker endpoints. However, it's based on <strong class="bold">Application Auto Scaling</strong> and not on <strong class="bold">EC2 Auto Scaling</strong> (<a href="https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html">https://docs.aws.amazon.com/autoscaling/application/userguide/what-is-application-auto-scaling.html</a>), although the <a id="_idIndexMarker1497"/>concepts are extremely similar.</p>
			<p>Let's set up <a id="_idIndexMarker1498"/>autoscaling for the <strong class="bold">XGBoost</strong> model we trained on the Boston Housing dataset:</p>
			<ol>
				<li>We first create an <strong class="bold">endpoint configuration</strong>, and we use it to build the endpoint. Here, we use the m5 instance family; t2 and t3 are not recommended for autoscaling as their burstable behavior makes it harder to measure their real load:<p class="source-code">model_name = 'sagemaker-xgboost-2020-06-09-08-33-24-782'</p><p class="source-code">endpoint_config_name = 'xgboost-one-model-epc'</p><p class="source-code">endpoint_name = 'xgboost-one-model-ep'</p><p class="source-code">production_variants = [{</p><p class="source-code">    'VariantName': 'variant-1',</p><p class="source-code">    'ModelName': model_name,</p><p class="source-code">    'InitialInstanceCount': 2,</p><p class="source-code">    'InitialVariantWeight': 1,</p><p class="source-code">    'InstanceType': 'ml.m5.large'}]</p><p class="source-code">sm.create_endpoint_config(</p><p class="source-code">    EndpointConfigName=endpoint_config_name,</p><p class="source-code">    ProductionVariants=production_variants)</p><p class="source-code">sm.create_endpoint(</p><p class="source-code">    EndpointName=endpoint_name,</p><p class="source-code">    EndpointConfigName=endpoint_config_name)</p></li>
				<li>Once the endpoint is in <a id="_idIndexMarker1499"/>service, we define the target value that we<a id="_idIndexMarker1500"/> want to scale on, namely the number of instances backing the endpoint: <p class="source-code">app = boto3.client('application-autoscaling')</p><p class="source-code">app.register_scalable_target(</p><p class="source-code"> ServiceNamespace='sagemaker',</p><p class="source-code"> ResourceId=</p><p class="source-code">     'endpoint/xgboost-one-model-ep/variant/variant-1',</p><p class="source-code"> ScalableDimension=</p><p class="source-code">    'sagemaker:variant:DesiredInstanceCount',</p><p class="source-code"> MinCapacity=2,</p><p class="source-code"> MaxCapacity=10)</p></li>
				<li>Then, we apply a scaling policy for this target value:<p class="source-code">policy_name = 'xgboost-scaling-policy'</p><p class="source-code">app.put_scaling_policy(</p><p class="source-code"> PolicyName=policy_name,</p><p class="source-code"> ServiceNamespace='sagemaker',</p><p class="source-code"> ResourceId=</p><p class="source-code">   'endpoint/xgboost-one-model-ep/variant/variant-1',</p><p class="source-code"> ScalableDimension=</p><p class="source-code">   'sagemaker:variant:DesiredInstanceCount',</p><p class="source-code"> PolicyType='TargetTrackingScaling',</p></li>
				<li>We use the only built-in metric available in SageMaker, <strong class="source-inline">SageMakerVariantInvocationsPerInstance</strong>. We could also define a custom metric if we wanted to. We set the metric threshold at 1,000 invocations per minute. This is a bit of an arbitrary value. In real life, we would run a load test on a single instance and monitor model latency in order to find the actual value that<a id="_idIndexMarker1501"/> ought to trigger<a id="_idIndexMarker1502"/> autoscaling. You can find more information at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html</a>. We also define a 60-second cooldown for scaling in and out, a good practice for smoothing out transient traffic drops and peaks:<p class="source-code"> TargetTrackingScalingPolicyConfiguration={</p><p class="source-code">   'TargetValue': 1000.0,</p><p class="source-code">   'PredefinedMetricSpecification': {</p><p class="source-code">       'PredefinedMetricType': </p><p class="source-code">       'SageMakerVariantInvocationsPerInstance'</p><p class="source-code">    },</p><p class="source-code">   'ScaleInCooldown': 60,</p><p class="source-code">   'ScaleOutCooldown': 60</p><p class="source-code"> }</p><p class="source-code">)</p></li>
				<li>As shown in the following screenshot, autoscaling is now configured on the endpoint:<div id="_idContainer198" class="IMG---Figure"><img src="Images/B17705_13_1.jpg" alt="Figure 13.1 – Viewing autoscaling&#13;&#10;" width="1650" height="265"/></div><p class="figure-caption">Figure 13.1 – Viewing autoscaling</p></li>
				<li>Using an infinite loop, we<a id="_idIndexMarker1503"/> send some traffic to the endpoint:<p class="source-code">test_sample = '0.00632, 18.00, 2.310, 0, 0.5380, 6.5750, 65.20, 4.0900, 1, 296.0, 15.30, 396.90, 4.98'</p><p class="source-code">smrt=boto3.Session().client(service_name='runtime.sagemaker') </p><p class="source-code">while True:</p><p class="source-code">    smrt.invoke_endpoint(EndpointName=endpoint_name,</p><p class="source-code">                         ContentType='text/csv',</p><p class="source-code">                         Body=test_sample)</p></li>
				<li>Looking at the <strong class="bold">CloudWatch</strong> metrics for the endpoints, as shown in the following screenshot, we<a id="_idIndexMarker1504"/> see that invocations per instance exceed the threshold we defined: 1.42k versus 1k:<div id="_idContainer199" class="IMG---Figure"><img src="Images/B17705_13_2.jpg" alt="Figure 13.2 – Viewing CloudWatch metrics&#13;&#10;" width="1650" height="663"/></div><p class="figure-caption">Figure 13.2 – Viewing CloudWatch metrics</p></li>
				<li>Autoscaling quickly kicks in <a id="_idIndexMarker1505"/>and decides to add<a id="_idIndexMarker1506"/> another instance, as visible in the following screenshot. If the load was even higher, it could decide to add several instances at once:<div id="_idContainer200" class="IMG---Figure"><img src="Images/B17705_13_3.jpg" alt="Figure 13.3 – Viewing autoscaling&#13;&#10;" width="1650" height="281"/></div><p class="figure-caption">Figure 13.3 – Viewing autoscaling</p></li>
				<li>A few minutes later, the extra instance is in service, and invocations per instance are now below the threshold (935 versus 1,000):<div id="_idContainer201" class="IMG---Figure"><img src="Images/B17705_13_4.jpg" alt="Figure 13.4 – Viewing CloudWatch metrics&#13;&#10;" width="1634" height="643"/></div><p class="figure-caption">Figure 13.4 – Viewing CloudWatch metrics</p><p>A similar process takes place<a id="_idIndexMarker1507"/> when traffic decreases.</p></li>
				<li>Once we're finished, we <a id="_idIndexMarker1508"/>delete everything:<p class="source-code">app.delete_scaling_policy(</p><p class="source-code"> PolicyName=policy_name,</p><p class="source-code"> ServiceNamespace='sagemaker',</p><p class="source-code"> ScalableDimension='sagemaker:variant :DesiredInstanceCount',</p><p class="source-code"> ResourceId='endpoint/xgboost-one-model-ep/variant/variant-1')</p><p class="source-code">sm.delete_endpoint(EndpointName=endpoint_name)</p><p class="source-code">sm.delete_endpoint_config(</p><p class="source-code">  EndpointConfigName=endpoint_config_name)</p></li>
			</ol>
			<p>Setting up autoscaling is easy. It helps you automatically adapt your prediction infrastructure and the<a id="_idIndexMarker1509"/> associated costs to changing business conditions.</p>
			<p>Now, let's study another<a id="_idIndexMarker1510"/> technique that you'll find extremely useful when<a id="_idIndexMarker1511"/> you're dealing with a very large number of models: <strong class="bold">multi-model endpoints</strong>.</p>
			<h1 id="_idParaDest-294"><a id="_idTextAnchor293"/>Deploying a multi-model endpoint</h1>
			<p>Multi-model endpoints are useful<a id="_idIndexMarker1512"/> when you're dealing with a large number of models where it wouldn't make sense to deploy to individual endpoints. For example, imagine a SaaS company building a regression model for each one of their 10,000 customers. Surely, they wouldn't want to manage (and pay for) 10,000 endpoints!</p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor294"/>Understanding multi-model endpoints</h2>
			<p>A multi-model endpoint can serve CPU-based<a id="_idIndexMarker1513"/> predictions from an arbitrary number of models stored in S3 (GPUs are not supported at the time of writing). The path of the model artifact to use is passed in each prediction request. Models are loaded and unloaded dynamically, according to usage and the amount of memory available on the endpoint. Models can also be added to, or removed from, the endpoint by simply copying or deleting artifacts in S3.</p>
			<p>In order to serve multiple models, your inference container must implement a specific set of APIs that the endpoint will invoke: LOAD MODEL, LIST MODEL, GET MODEL, UNLOAD MODEL, and INVOKE MODEL. You can find the details at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/mms-container-apis.html">https://docs.aws.amazon.com/sagemaker/latest/dg/mms-container-apis.html</a>. </p>
			<p>At the time of writing, the latest built-in containers for <strong class="bold">scikit-learn</strong>, <strong class="bold">TensorFlow</strong>, <strong class="bold">Apache MXNet</strong>, and <strong class="bold">PyTorch</strong> natively support these APIs. The <strong class="bold">XGBoost</strong>, <strong class="bold">kNN</strong>, <strong class="bold">Linear Learner</strong>, and <strong class="bold">Random Cut Forest</strong> built-in algorithms also support them.</p>
			<p>For other algorithms and frameworks, your best option is to build a custom container that includes the <strong class="bold">SageMaker Inference Toolkit</strong>, as it already<a id="_idIndexMarker1514"/> implements the required APIs (<a href="https://github.com/aws/sagemaker-inference-toolkit">https://github.com/aws/sagemaker-inference-toolkit</a>). </p>
			<p>This toolkit is based on the multi-model server (<a href="https://github.com/awslabs/multi-model-server">https://github.com/awslabs/multi-model-server</a>), which you<a id="_idIndexMarker1515"/> could also use directly from the CLI to serve predictions from multiple models. You can find more information at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html">https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html</a>. </p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor295"/>Building a multi-model endpoint with scikit-learn</h2>
			<p>Let's build a multi-model<a id="_idIndexMarker1516"/> endpoint with <strong class="bold">scikit-learn</strong>, hosting models trained <a id="_idIndexMarker1517"/>on the Boston Housing dataset. This is only supported on scikit-learn 0.23-1 and above:</p>
			<ol>
				<li value="1">We upload the dataset to S3:<p class="source-code">import sagemaker, boto3</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()</p><p class="source-code">prefix = 'sklearn-boston-housing-mme'</p><p class="source-code">training = sess.upload_data(path='housing.csv', </p><p class="source-code">                            key_prefix=prefix + </p><p class="source-code">                            '/training')</p><p class="source-code">output = 's3://{}/{}/output/'.format(bucket,prefix)</p></li>
				<li>We train three models with a different test size, storing their names in a dictionary. Here, we use the latest version of scikit-learn, the first one to support multi-model endpoints:<p class="source-code">from sagemaker.sklearn import SKLearn</p><p class="source-code">jobs = {}</p><p class="source-code">for test_size in [0.2, 0.1, 0.05]:</p><p class="source-code">    sk = SKLearn(entry_point=</p><p class="source-code">                'sklearn-boston-housing.py',</p><p class="source-code">        role=sagemaker.get_execution_role(),</p><p class="source-code">        framework_version='0.23-1',</p><p class="source-code">        instance_count=1,</p><p class="source-code">        instance_type='ml.m5.large',</p><p class="source-code">        output_path=output,</p><p class="source-code">        hyperparameters={ 'normalize': True,</p><p class="source-code">                          'test-size': test_size }</p><p class="source-code">    )</p><p class="source-code">    sk.fit({'training':training}, wait=False)</p><p class="source-code">    jobs[sk.latest_training_job.name] = {}</p><p class="source-code">    jobs[sk.latest_training_job.name]['test-size'] =   </p><p class="source-code">        test_size</p></li>
				<li>We find the S3 URI of the <a id="_idIndexMarker1518"/>model artifact along with<a id="_idIndexMarker1519"/> its prefix:<p class="source-code">import boto3</p><p class="source-code">sm = boto3.client('sagemaker')</p><p class="source-code">for j in jobs.keys():</p><p class="source-code">    job = sm.describe_training_job(TrainingJobName=j)</p><p class="source-code">    jobs[j]['artifact'] =</p><p class="source-code">        job['ModelArtifacts']['S3ModelArtifacts']</p><p class="source-code">    jobs[j]['key'] = '/'.join(</p><p class="source-code">        job['ModelArtifacts']['S3ModelArtifacts']</p><p class="source-code">        .split('/')[3:])</p></li>
				<li>We delete any previous model stored in S3:<p class="source-code">%%sh -s "$bucket" "$prefix"</p><p class="source-code">aws s3 rm --recursive s3://$1/$2/models</p></li>
				<li>We copy the three<a id="_idIndexMarker1520"/> model artifacts to this location:<p class="source-code">s3 = boto3.client('s3')</p><p class="source-code">for j in jobs.keys():</p><p class="source-code">    copy_source = { 'Bucket': bucket, </p><p class="source-code">                    'Key': jobs[j]['key'] }</p><p class="source-code">    s3.copy_object(CopySource=copy_source,  </p><p class="source-code">                   Bucket=bucket, </p><p class="source-code">                   Key=prefix+'/models/'+j+'.tar.gz')</p><p class="source-code">response = s3.list_objects(Bucket=bucket, </p><p class="source-code">                           Prefix=prefix+'/models/')</p><p class="source-code">for o in response['Contents']:</p><p class="source-code">    print(o['Key'])</p><p>This lists the model <a id="_idIndexMarker1521"/>artifacts:</p><p class="source-code"><strong class="bold">sklearn-boston-housing-mme/models/sagemaker-scikit-learn-2021-09-01-07-52-22-679</strong></p><p class="source-code"><strong class="bold">sklearn-boston-housing-mme/models/sagemaker-scikit-learn-2021-09-01-07-52-26-399</strong></p><p class="source-code"><strong class="bold">sklearn-boston-housing-mme/models/sagemaker-scikit-learn-2021-09-01-08-05-33-229</strong></p></li>
				<li>We define the name of the script and the S3 location where we'll upload the code archive. Here, I'm passing the training script, which includes a <strong class="source-inline">model_fn()</strong> function to load the model. This is the only function that will be used to serve predictions:<p class="source-code">script = 'sklearn-boston-housing.py'</p><p class="source-code">script_archive = 's3://{}/{}/source/source.tar.gz'.</p><p class="source-code">                 format(bucket, prefix)</p></li>
				<li>We create the code archive and we upload it to S3:<p class="source-code">%%sh -s "$script" "$script_archive"</p><p class="source-code">tar cvfz source.tar.gz $1</p><p class="source-code">aws s3 cp source.tar.gz $2</p></li>
				<li>We create the <a id="_idIndexMarker1522"/>multi-model endpoint with the <strong class="source-inline">create_model()</strong> API and we set the <strong class="source-inline">Mode</strong> parameter<a id="_idIndexMarker1523"/> accordingly:<p class="source-code">import time</p><p class="source-code">model_name = prefix+'-'+time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">response = sm.create_model(</p><p class="source-code">  ModelName = model_name,</p><p class="source-code">  ExecutionRoleArn = role,</p><p class="source-code">  Containers = [{</p><p class="source-code">    'Image': sk.image_uri,</p><p class="source-code">    'ModelDataUrl':'s3://{}/{}/models/'.format(bucket, </p><p class="source-code">                    prefix),</p><p class="source-code">    'Mode': 'MultiModel',</p><p class="source-code">    'Environment': {</p><p class="source-code">        'SAGEMAKER_PROGRAM' : script,</p><p class="source-code">        'SAGEMAKER_SUBMIT_DIRECTORY' : script_archive</p><p class="source-code">    }</p><p class="source-code">  }]</p><p class="source-code">)</p></li>
				<li>We create the endpoint configuration as usual:<p class="source-code">epc_name = prefix+'-epc'+time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">response = sm.create_endpoint_config(</p><p class="source-code">    EndpointConfigName = epc_name,</p><p class="source-code">    ProductionVariants=[{</p><p class="source-code">        'InstanceType': 'ml.m5.large',</p><p class="source-code">        'InitialInstanceCount': 1,</p><p class="source-code">        'InitialVariantWeight': 1,</p><p class="source-code">        'ModelName': model_name,</p><p class="source-code">        'VariantName': 'variant-1'}]</p><p class="source-code">)</p></li>
				<li>We create the endpoint as usual:<p class="source-code">ep_name = prefix+'-ep'+time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">response = sm.create_endpoint(</p><p class="source-code">    EndpointName=ep_name,</p><p class="source-code">    EndpointConfigName=epc_name)</p></li>
				<li>Once the<a id="_idIndexMarker1524"/> endpoint is in service, we load samples from the dataset <a id="_idIndexMarker1525"/>and convert them to a <strong class="source-inline">numpy</strong> array:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from io import BytesIO</p><p class="source-code">data = pd.read_csv('housing.csv')</p><p class="source-code">payload = data[:10].drop(['medv'], axis=1)</p><p class="source-code">buffer = BytesIO()</p><p class="source-code">np.save(buffer, payload.values)</p></li>
				<li>We predict these samples with all three models, passing the name of the model to use for each prediction request, such as <strong class="bold">sagemaker-scikit-learn-2021-09-01-08-05-33-229</strong>:<p class="source-code">smrt = boto3.client('runtime.sagemaker')</p><p class="source-code">for j in jobs.keys():</p><p class="source-code">    model_name=j+'.tar.gz'</p><p class="source-code">    response = smrt.invoke_endpoint(</p><p class="source-code">        EndpointName=ep_name,</p><p class="source-code">        TargetModel=model_name,</p><p class="source-code">        Body=buffer.getvalue(),</p><p class="source-code">        ContentType='application/x-npy')</p><p class="source-code">    print(response['Body'].read())</p></li>
				<li>We could train more models, copy their artifacts to the same S3 location, and use them directly without recreating the endpoint. We could also delete those models we don't need.</li>
				<li>Once we're finished, we delete the endpoint:<p class="source-code">sm.delete_endpoint(EndpointName=ep_name)</p><p class="source-code">sm.delete_endpoint_config(EndpointConfigName=epc_name)</p></li>
			</ol>
			<p>As you can see, multi-model <a id="_idIndexMarker1526"/>endpoints are a great way to serve as many models as you'd like from a single endpoint, and setting them up isn't difficult. </p>
			<p>In the next section, we're <a id="_idIndexMarker1527"/>going to study another cost optimization technique that can help you save a lot of money on GPU prediction: <strong class="bold">Amazon Elastic Inference</strong>.</p>
			<h1 id="_idParaDest-297"><a id="_idTextAnchor296"/>Deploying a model with Amazon Elastic Inference</h1>
			<p>When deploying a model, you<a id="_idIndexMarker1528"/> have to <a id="_idIndexMarker1529"/>decide whether it should run on a CPU instance or on a GPU instance. In some cases, there isn't much of a debate. For example, some algorithms simply don't benefit from GPU acceleration, so they should be deployed to CPU instances. At the other end of the spectrum, complex deep learning models for computer vision or natural language processing run best on GPUs.</p>
			<p>In many cases, the situation is not that clear-cut. First, you should know what the maximum predicted latency is for your application. If you're predicting a click-through rate for a real-time ad tech application, every millisecond counts; if you're predicting customer churn in a back-office application, not so much.</p>
			<p>In addition, even models that could benefit from GPU acceleration may not be large and complex enough to fully utilize the thousands of cores available on a modern GPU. In such scenarios, you're stuck between a rock and a hard place: deploying on CPU would be a little slow for your needs, and deploying on GPU wouldn't be cost-effective.</p>
			<p>This is the problem that Amazon Elastic Inference<a id="_idIndexMarker1530"/> aims to solve (<a href="https://aws.amazon.com/machine-learning/elastic-inference/">https://aws.amazon.com/machine-learning/elastic-inference/</a>). It lets you attach fractional GPU acceleration to any EC2 instance, including notebook instances and endpoint instances. <strong class="bold">Accelerators</strong> come in three different sizes (medium, large, and extra large), which let you find the best <a id="_idIndexMarker1531"/>cost-performance ratio for your application.</p>
			<p>Elastic Inference is available for <strong class="bold">TensorFlow</strong>, <strong class="bold">PyTorch</strong>, and <strong class="bold">Apache MXNet</strong>. You can use it in your own code running on<a id="_idIndexMarker1532"/> EC2 instances, thanks to AWS extensions available in the <strong class="bold">Deep Learning AMI</strong>. You can also <a id="_idIndexMarker1533"/>use it with <strong class="bold">Deep Learning Containers</strong>. More information is available at <a href="https://docs.aws.amazon.com/elastic-inference/latest/developerguide/working-with-ei.html">https://docs.aws.amazon.com/elastic-inference/latest/developerguide/working-with-ei.html</a>. </p>
			<p>Of course, <strong class="bold">Elastic Inference</strong> is available on SageMaker. You can attach an accelerator to a <strong class="bold">Notebook Instance</strong> at creation time and work with the built-in <strong class="bold">conda</strong> environments. You can also attach an <a id="_idIndexMarker1534"/>accelerator to an endpoint, and we'll show you how to do this in the next example.</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor297"/>Deploying a model with Amazon Elastic Inference</h2>
			<p>Let's reuse<a id="_idIndexMarker1535"/> the <strong class="bold">Image Classification</strong> model we trained <a id="_idIndexMarker1536"/>on dog and cat images in <a href="B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 5</em></a>, <em class="italic">Training Computer Vision Models</em>. This is based on an 18-layer <strong class="bold">ResNet</strong> model, which is pretty small as far as convolution neural networks are concerned:</p>
			<ol>
				<li value="1">Once the model has been trained, we deploy it as usual on two endpoints: one backed by an <strong class="source-inline">ml.c5.large</strong> instance and another one backed by an <strong class="source-inline">ml.g4dn.xlarge</strong> instance, the most cost-effective GPU instance available on SageMaker:<p class="source-code">import time</p><p class="source-code">endpoint_name = 'c5-'+time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">c5_predictor = ic.deploy(initial_instance_count=1,</p><p class="source-code">                         instance_type='ml.c5.large',</p><p class="source-code">                         endpoint_name=endpoint_name,</p><p class="source-code">                         wait=False)</p><p class="source-code">endpoint_name = 'g4-'+time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">g4_predictor = ic.deploy(</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.g4dn.xlarge',</p><p class="source-code">    endpoint_name=endpoint_name,</p><p class="source-code">    wait=False)</p></li>
				<li>We then download a test image, predict it 1,000 times, and measure the total time it<a id="_idIndexMarker1537"/> takes:<p class="source-code">with open(file_name, 'rb') as f:</p><p class="source-code">    payload = f.read()</p><p class="source-code">    payload = bytearray(payload)</p><p class="source-code">def predict_images(predictor, iterations=1000):</p><p class="source-code">    total = 0</p><p class="source-code">    for i in range(0, iterations):</p><p class="source-code">        tick = time.time()</p><p class="source-code">        response = runtime.invoke_endpoint(</p><p class="source-code">            EndpointName=predictor.endpoint_name,                                 </p><p class="source-code">            ContentType='application/x-image',</p><p class="source-code">            Body=payload)</p><p class="source-code">        tock = time.time()</p><p class="source-code">        total += tock-tick</p><p class="source-code">    return total/iterations</p><p class="source-code">predict_images(c5_predictor)</p><p class="source-code">predict_images(g4_predictor)</p></li>
				<li>The results<a id="_idIndexMarker1538"/> are shown in the next<a id="_idIndexMarker1539"/> table (us-east-1 prices):<div id="_idContainer202" class="IMG---Figure"><img src="Images/B17705_13_Table_1.jpg" alt="" width="1472" height="212"/></div><p>Unsurprisingly, the GPU instance is about twice as fast. Yet, the CPU instance is more cost-effective, as it's over four times less expensive. Putting it another way, you could run your endpoint with four CPU instances instead of one GPU instance and get more throughput for the same cost. This shows why it's so important to understand the latency requirement of your application. "Fast" and "slow" are very relative concepts!</p></li>
				<li>We then deploy the same model <a id="_idIndexMarker1540"/>on <a id="_idIndexMarker1541"/>three more endpoints backed by <strong class="source-inline">ml.c5.large</strong> instances, accelerated by a medium, large, and extra-large <strong class="bold">Elastic Inference accelerator</strong>. All it takes is an<a id="_idIndexMarker1542"/> extra parameter in the <strong class="source-inline">deploy()</strong> API. Here's the code for the medium endpoint:<p class="source-code">endpoint_name = 'c5-medium-'</p><p class="source-code">   +time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">c5_medium_predictor = ic.deploy(</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.c5.large',</p><p class="source-code">    accelerator_type='ml.eia2.medium',</p><p class="source-code">    endpoint_name=endpoint_name,</p><p class="source-code">    wait=False)</p><p class="source-code">predict_images(c5_medium_predictor)</p><p>You can see the results in the following table:</p><div id="_idContainer203" class="IMG---Figure"><img src="Images/B17705_13_Table_2.jpg" alt="" width="1589" height="721"/></div><p>We get up to 20% speed-up compared to the naked CPU endpoint, and the cost is lower than if we used a GPU instance. Let's keep tweaking:</p></li>
				<li>Attentive readers will <a id="_idIndexMarker1543"/>have noticed <a id="_idIndexMarker1544"/>that the previous tables include teraFLOP values for both 32-bit and 16-bit floating-point values. Indeed, either one of these data types may be used to store model parameters. Looking at the documentation for the image classification algorithm, we see that we can actually select a data type with the <strong class="source-inline">precision_dtype</strong> parameter and that the default value is <strong class="source-inline">float32</strong>. This begs the question: would the results differ if we trained our model in <strong class="source-inline">float16</strong> mode? There's only one way to know, isn't there?<p class="source-code">ic.set_hyperparameters(</p><p class="source-code">    num_layers=18,                       </p><p class="source-code">    use_pretrained_model=0,</p><p class="source-code">    num_classes=2</p><p class="source-code">    num_training_samples=22500,</p><p class="source-code">    mini_batch_size=128,</p><p class="source-code">    precision_dtype='float16',</p><p class="source-code">    epochs=10)                   </p></li>
				<li>Training again, we hit the same model accuracy as in <strong class="source-inline">float32</strong> mode. Deploying benchmarking again, we get the following results:</li>
			</ol>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="Images/B17705_13_Table_3.jpg" alt="" width="1535" height="737"/>
				</div>
			</div>
			<p>No meaningful difference is visible on the naked instances. Predicting with an <strong class="bold">FP-16</strong> model on the large and extra-large accelerators helps us speed up predictions by about 10% compared to the <strong class="bold">FP-32</strong> model. Pretty good! This performance level is definitely a nice upgrade compared to a naked CPU instance, and it's cost-effective compared to a GPU instance.</p>
			<p>In fact, switching a single endpoint instance from <strong class="source-inline">ml.g4dn.xlarge</strong> to <strong class="source-inline">ml.c5.large+ml.eia2.large</strong> would save you ($0.736–$0.438) x 24 x 30 = $214 dollars per month. That's serious money!</p>
			<p>As you can see, Amazon Elastic Inference is extremely easy to use, and it gives you additional <a id="_idIndexMarker1545"/>deployment options. Once <a id="_idIndexMarker1546"/>you've defined the prediction latency requirement for your application, you can quickly experiment and find the best cost-performance ratio. </p>
			<p>Now, let's talk about another SageMaker capability that lets you compile models for a specific hardware architecture: <strong class="bold">Amazon Neo</strong>.</p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor298"/>Compiling models with Amazon SageMaker Neo</h1>
			<p>Embedded <a id="_idIndexMarker1547"/>software<a id="_idIndexMarker1548"/> developers have long learned how to write highly optimized code that both runs fast and uses hardware resources frugally. In theory, the same techniques could also be applied to optimize machine learning predictions. In practice, this is a daunting task given the complexity of machine learning libraries<a id="_idIndexMarker1549"/> and <a id="_idIndexMarker1550"/>models.</p>
			<p>This is the problem that Amazon SageMaker Neo aims to solve.</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor299"/>Understanding Amazon SageMaker Neo</h2>
			<p>Amazon Neo has two <a id="_idIndexMarker1551"/>components: a model compiler that optimizes models for the underlying hardware, and a <a id="_idIndexMarker1552"/>small runtime named <strong class="bold">Deep Learning Runtime</strong> (<strong class="bold">DLR</strong>), used to load optimized models<a id="_idIndexMarker1553"/> and run predictions (<a href="https://aws.amazon.com/sagemaker/neo">https://aws.amazon.com/sagemaker/neo</a>). </p>
			<p>Amazon SageMaker Neo can compile models trained with the following:</p>
			<ul>
				<li><strong class="bold">Two built-in algorithms</strong>: XGBoost and Image Classification.</li>
				<li><strong class="bold">Built-in frameworks</strong>: TensorFlow, PyTorch, and Apache MXNet, as well as models in <strong class="bold">ONNX</strong> format. Many <a id="_idIndexMarker1554"/>operators are supported, and you can find the full list at <a href="https://aws.amazon.com/releasenotes/sagemaker-neo-supported-frameworks-and-operators">https://aws.amazon.com/releasenotes/sagemaker-neo-supported-frameworks-and-operators</a>.</li>
			</ul>
			<p>Training takes place as usual, using your estimator of choice. Then, using the <strong class="source-inline">compile_model()</strong> API, we can <a id="_idIndexMarker1555"/>easily compile the model for one of these hardware targets:</p>
			<ul>
				<li>Amazon EC2 instances of the following families: <strong class="source-inline">c4</strong>, <strong class="source-inline">c5</strong>, <strong class="source-inline">m4</strong>, <strong class="source-inline">m5</strong>, <strong class="source-inline">p2</strong>, <strong class="source-inline">p3</strong>, and <strong class="source-inline">inf1</strong> (which we'll discuss later in this chapter), as well as Lambda</li>
				<li>AI-powered cameras: AWS DeepLens and Acer aiSage</li>
				<li>NVIDIA Jetson platforms: TX1, TX2, Nano, and Xavier</li>
				<li>Raspberry Pi</li>
				<li>System-on-chip platforms from Rockchip, Qualcomm, Ambarella, and more</li>
			</ul>
			<p>Model compilation performs both architecture optimizations (such as fusing layers) and code optimizations (replacing machine learning operators with hardware-optimized versions). The resulting artifact is stored in S3 and contains both the original model and its optimized form. </p>
			<p>The DLR is then used to load the model and predict with it. Of course, it can be used in a standalone fashion, such as on a Raspberry Pi. You can find installation instructions at <a href="https://neo-ai-dlr.readthedocs.io">https://neo-ai-dlr.readthedocs.io</a>. As the DLR is open source (<a href="https://github.com/neo-ai/neo-ai-dlr">https://github.com/neo-ai/neo-ai-dlr</a>), you can also build it from source and – why not? – customize it for your own hardware platform!</p>
			<p>When it comes to using the DLR with SageMaker, things are much simpler. SageMaker provides built-in containers with Neo support, and these are the ones you should use to deploy models compiled with Neo (as already mentioned, the training container remains unchanged). You can find<a id="_idIndexMarker1556"/> a list of Neo-enabled containers at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-cli.html">https://docs.aws.amazon.com/sagemaker/latest/dg/neo-deployment-hosting-services-cli.html</a>.</p>
			<p>Last, but not least, one<a id="_idIndexMarker1557"/> of the benefits of the DLR is its small size. For example, the Python package for p2 and p3 instances is only 5.4 MB in size, orders of magnitude smaller than your typical deep learning library and its dependencies. This is obviously critical for embedded environments, and it's also welcome on SageMaker as containers will be smaller too.</p>
			<p>Let's reuse our image classification example and see whether Neo can speed it up. </p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor300"/>Compiling and deploying an image classification model on SageMaker</h2>
			<p>In order to give Neo <a id="_idIndexMarker1558"/>a little more work, we<a id="_idIndexMarker1559"/> train a 50-layer ResNet this time. Then, we'll compile it, deploy it to an endpoint, and compare it <a id="_idIndexMarker1560"/>with the vanilla<a id="_idIndexMarker1561"/> model:</p>
			<ol>
				<li value="1">Setting <strong class="source-inline">num_layers</strong> to <strong class="source-inline">50</strong>, we train the model for 30 epochs. Then, we deploy it to an <strong class="source-inline">ml.c5.4xlarge</strong> instance as usual:<p class="source-code">ic_predictor = ic.deploy(initial_instance_count=1,</p><p class="source-code">    instance_type='ml.c5.4xlarge',                         </p><p class="source-code">    endpoint_name=ic_endpoint_name)</p></li>
				<li>We compile the model with Neo, targeting the EC2 c5 instance family. We also define the input shape of the model: one image, three channels (red, green, blue), and 224 x 224 pixels (the default value for the image classification algorithm). As built-in algorithms are implemented with Apache MXNet, we set the framework accordingly:<p class="source-code">output_path = 's3://{}/{}/output-neo/'</p><p class="source-code">              .format(bucket, prefix)</p><p class="source-code">ic_neo_model = ic.compile_model(</p><p class="source-code">    target_instance_family='ml_c5',</p><p class="source-code">    input_shape={'data':[1, 3, 224, 224]},</p><p class="source-code">    role=role,</p><p class="source-code">    framework='mxnet',</p><p class="source-code">    framework_version='1.5.1',</p><p class="source-code">    output_path=output_path)</p></li>
				<li>We then <a id="_idIndexMarker1562"/>deploy the compiled model as usual, explicitly setting the prediction container to<a id="_idIndexMarker1563"/> the<a id="_idIndexMarker1564"/> Neo-enabled<a id="_idIndexMarker1565"/> version of image classification:<p class="source-code">ic_neo_model.image = get_image_uri(</p><p class="source-code">    session.boto_region_name, </p><p class="source-code">    'image-classification-neo', </p><p class="source-code">    repo_version='latest')</p><p class="source-code">ic_neo_predictor = ic_neo_model.deploy(</p><p class="source-code">    endpoint_name=ic_neo_endpoint_name,</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.c5.4xlarge')</p></li>
				<li>Downloading a test image, and using the same benchmarking function that we used for Amazon Elastic Inference, we measure the time required to predict 1,000 images:<p class="source-code">predict_images(ic_predictor)</p><p class="source-code">predict_images(ic_neo_predictor)</p><p>Prediction with the vanilla model takes 87 seconds. Prediction with the Neo-optimized model takes 28.5 seconds, three times faster! That compilation step sure paid<a id="_idIndexMarker1566"/> off. You'll also be <a id="_idIndexMarker1567"/>happy to learn that compiling Neo models is free of charge, so there's really no reason not to try it.</p></li>
			</ol>
			<p>Let's take a look at these compiled models.</p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor301"/>Exploring models compiled with Neo</h2>
			<p>Looking at the output<a id="_idIndexMarker1568"/> location passed to the <strong class="source-inline">compile_model()</strong> API, we see the model artifact generated by Neo:</p>
			<p class="source-code">$ aws s3 ls s3://sagemaker-eu-west-1-123456789012/dogscats/output-neo/</p>
			<p class="source-code">model-ml_c5.tar.gz</p>
			<p>Copying it locally and extracting it, we see that it contains both the original model and its compiled version:</p>
			<p class="source-code">$ aws s3 cp s3://sagemaker-eu-west-1-123456789012/dogscats/output-neo/model-ml_c5.tar.gz .</p>
			<p class="source-code">$ tar xvfz model-ml_c5.tar.gz</p>
			<p class="source-code">compiled.meta</p>
			<p class="source-code">model-shapes.json</p>
			<p class="source-code">compiled.params</p>
			<p class="source-code">compiled_model.json</p>
			<p class="source-code">compiled.so</p>
			<p>In particular, the <strong class="source-inline">compiled.so</strong> file is a<a id="_idIndexMarker1569"/> native file containing hardware-optimized versions of the model operators:</p>
			<p class="source-code">$ file compiled.so</p>
			<p class="source-code">compiled.so: ELF 64-bit LSB shared object, x86-64</p>
			<p class="source-code">$ nm compiled.so | grep conv | head -3</p>
			<p class="source-code">0000000000005880 T fused_nn_contrib_conv2d_NCHWc</p>
			<p class="source-code">00000000000347a0 T fused_nn_contrib_conv2d_NCHWc_1</p>
			<p class="source-code">0000000000032630 T fused_nn_contrib_conv2d_NCHWc_2</p>
			<p>We could look at the assembly code for these, but something tells me that most of you wouldn't particularly enjoy it. Joking aside, this is completely unnecessary. All we need to know is how to compile and deploy models with Neo.</p>
			<p>Now, how about we deploy our model on a <strong class="bold">Raspberry Pi</strong>?</p>
			<h2 id="_idParaDest-303"><a id="_idTextAnchor302"/>Deploying an image classification model on a Raspberry Pi</h2>
			<p>The Raspberry Pi is a<a id="_idIndexMarker1570"/> fantastic device, and <a id="_idIndexMarker1571"/>despite its limited compute and memory capabilities, it's well capable of predicting images with complex deep learning models. Here, I'm using a Raspberry Pi 3 Model B, with a 1.2 GHz quad-core ARM processor and 1 GB of memory. That's definitely not much, yet it could run a vanilla Apache MXNet model. </p>
			<p>Inexplicably, there is no pre-packaged version of MXNet for Raspberry Pi, and building it from source is a painstakingly long and unpredictable process. (I'm looking at you, OOM errors!) Fortunately, thanks to the DLR, we can do away with all of it!</p>
			<ol>
				<li value="1">In our SageMaker <a id="_idIndexMarker1572"/>notebook, we compile the model for the Raspberry Pi:<p class="source-code">output_path = 's3://{}/{}/output-neo/'</p><p class="source-code">              .format(bucket, prefix)</p><p class="source-code">ic_neo_model = ic.compile_model(</p><p class="source-code">    target_instance_family='rasp3b',</p><p class="source-code">    input_shape={'data':[1, 3, 224, 224]},</p><p class="source-code">    role=role,</p><p class="source-code">    framework='mxnet',</p><p class="source-code">    framework_version='1.5.1',</p><p class="source-code">    output_path=output_path)</p></li>
				<li>On our local <a id="_idIndexMarker1573"/>machine, we fetch the compiled model artifact from S3 and copy it to the Raspberry Pi:<p class="source-code"><strong class="bold">$ aws s3 cp s3://sagemaker-eu-west-1-123456789012/dogscats/output-neo/model-rasp3b.tar.gz .</strong></p><p class="source-code"><strong class="bold">$ scp model-rasp3b.tar.gz pi@raspberrypi:~</strong></p></li>
				<li>Moving to the Raspberry Pi, we extract the compiled model to the <strong class="source-inline">resnet50</strong> directory:<p class="source-code"><strong class="bold">$ mkdir resnet50</strong></p><p class="source-code"><strong class="bold">$ tar xvfz model-rasp3b.tar.gz -C resnet50</strong></p></li>
				<li>Installing the DLR is very easy. We locate the appropriate package at <a href="https://github.com/neo-ai/neo-ai-dlr/releases">https://github.com/neo-ai/neo-ai-dlr/releases</a>, download it, and use <strong class="source-inline">pip</strong> to install it:<p class="source-code"><strong class="bold">$ wget https://neo-ai-dlr-release.s3-us-west-2.amazonaws.com/v1.9.0/rasp3b/dlr-1.9.0-py3-none-any.whl </strong></p><p class="source-code"><strong class="bold">$ pip3 install dlr-1.9.0-py3-none-any.whl</strong></p></li>
				<li>We first write a function that loads an image from a file, resizes it to 224 x 224 pixels, and shapes it as a (1, 3, 224, 224) <strong class="source-inline">numpy</strong> array, the correct input shape of our model:<p class="source-code">import numpy as np</p><p class="source-code">from PIL import Image</p><p class="source-code">def process_image(filename):</p><p class="source-code">    image = Image.open(filename)</p><p class="source-code">    image = image.resize((224,224))   </p><p class="source-code">    image = np.asarray(image)         # (224,224,3)</p><p class="source-code">    image = np.moveaxis(image, 2, 0). # (3,224,224)</p><p class="source-code">    image = image[np.newaxis, :].     # (1,3,224,224)</p><p class="source-code">    return image</p></li>
				<li>Then, we<a id="_idIndexMarker1574"/> import the DLR and load the <a id="_idIndexMarker1575"/>compiled model from the <strong class="source-inline">resnet50</strong> directory:<p class="source-code">from dlr import DLRModel</p><p class="source-code">model = DLRModel('resnet50')</p></li>
				<li>Then, we load a dog image… or an image of a cat. Your choice!<p class="source-code">image = process_image('dog.jpg')</p><p class="source-code">#image = process_image('cat.png')</p><p class="source-code">input_data = {'data': image}</p></li>
				<li>Finally, we predict the image 100 times, printing the prediction to defeat any lazy evaluation that MXNet could implement:<p class="source-code">import time</p><p class="source-code">total = 0</p><p class="source-code">for i in range(0,100):</p><p class="source-code">    tick = time.time()</p><p class="source-code">    out = model.run(input_data)</p><p class="source-code">    print(out[0])</p><p class="source-code">    tock = time.time()</p><p class="source-code">    total+= tock-tick</p><p class="source-code">print(total)</p></li>
			</ol>
			<p>The following dog and cat images are respectively predicted as [2.554065e-09 1.000000e+00] and [9.9967313e-01 3.2689856e-04], which is very nice given the validation accuracy of our model (about 84%):</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="Images/B17705_13_5.jpg" alt="Figure 13.5 – Test images (source: Wikimedia)&#13;&#10;" width="883" height="455"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.5 – Test images (source: Wikimedia)</p>
			<p>Prediction time is about 1.2 <a id="_idIndexMarker1576"/>seconds per image, which is<a id="_idIndexMarker1577"/> slow but certainly good enough for plenty of embedded applications. Predicting with the vanilla model takes about 6–7 seconds, so the speed-up is very significant.</p>
			<p>As you can see, compiling models is a very effective technique. In the next section, we're going to focus on one of Neo's targets, <strong class="bold">AWS Inferentia</strong>. </p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor303"/>Deploying models on AWS Inferentia</h2>
			<p>AWS Inferentia is a custom chip designed<a id="_idIndexMarker1578"/> specifically for high-throughput and low-cost prediction (<a href="https://aws.amazon.com/machine-learning/inferentia">https://aws.amazon.com/machine-learning/inferentia</a>). Inferentia chips are hosted on <strong class="bold">EC2 inf1</strong> instances. These come in different sizes, with 1, 4, or 16 chips. Each chip contains four <strong class="bold">NeuronCores</strong>, implementing<a id="_idIndexMarker1579"/> high-performance matrix multiply engines that speed up typical deep learning operations such as convolution. NeuronCores also contain large caches that save external memory accesses.</p>
			<p>In order to run on Inferentia, models<a id="_idIndexMarker1580"/> need to be compiled and deployed with the Neuron SDK (<a href="https://github.com/aws/aws-neuron-sdk">https://github.com/aws/aws-neuron-sdk</a>). This SDK lets you work with TensorFlow, PyTorch, and Apache MXNet models.</p>
			<p>You can work with the Neuron SDK on EC2 instances, compiling and deploying models yourself. Once again, SageMaker simplifies the whole process, as inf1 instances are part of the target architectures that Neo can compile models for.</p>
			<p>You can find an example at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_neo_compilation_jobs/deploy_tensorflow_model_on_Inf1_instance">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker_neo_compilation_jobs/deploy_tensorflow_model_on_Inf1_instance</a>. </p>
			<p>To close this chapter, let's sum<a id="_idIndexMarker1581"/> up all the cost optimization techniques we discussed throughout the book.</p>
			<h1 id="_idParaDest-305"><a id="_idTextAnchor304"/>Building a cost optimization checklist</h1>
			<p>You should <a id="_idIndexMarker1582"/>constantly pay attention to cost, even in the early stages of your machine learning project. Even if you're not paying the AWS bill, someone is, and I'm sure you'll quite quickly find out who that person is if you spend too much.</p>
			<p>Regularly going through the following checklist will help you spend as little as possible, get the most machine learning-happy bang for your buck, and hopefully keep the finance team off your back!</p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor305"/>Optimizing costs for data preparation</h2>
			<p>With so much focus on<a id="_idIndexMarker1583"/> optimizing training and deployment, it's easy to overlook data preparation. Yet, this critical piece of the machine learning workflow can incur very significant costs.</p>
			<p class="callout-heading">Tip #1</p>
			<p class="callout">Resist the urge to build ad hoc ETL tools running on instance-based services.</p>
			<p>Obviously, your workflows will require data to be processed in a custom fashion, such as applying domain-specific feature engineering. Working with a managed service such<a id="_idIndexMarker1584"/> as <strong class="bold">Amazon Glue</strong>, <strong class="bold">Amazon Athena</strong>, or <strong class="bold">Amazon SageMaker Data Wrangler</strong>, you will never have<a id="_idIndexMarker1585"/> to provision any infrastructure, and you will only pay for what<a id="_idIndexMarker1586"/> you use. </p>
			<p>As a second choice, <strong class="bold">Amazon EMR</strong> is a fine service, provided that<a id="_idIndexMarker1587"/> you understand how to optimize its cost. As much as <a id="_idIndexMarker1588"/>possible, you should avoid running long-lived, low-usage clusters. Instead, you should run transient clusters and rely massively on <strong class="bold">Spot Instances</strong> for task nodes. You can find more information at the following sites:</p>
			<ul>
				<li><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-longrunning-transient.html</a> </li>
				<li><a href="https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html</a></li>
			</ul>
			<p>The same advice applies to <strong class="bold">Amazon EC2</strong> instances.</p>
			<p class="callout-heading">Tip #2</p>
			<p class="callout">Use SageMaker Ground Truth and automatic labeling to cut down on data labeling costs.</p>
			<p>If you need to label large unstructured datasets, enabling automatic labeling in <strong class="bold">SageMaker Ground Truth</strong> can save you a significant amount of time and money compared to labeling everything<a id="_idIndexMarker1589"/> manually. You can read about it at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html</a>. </p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor306"/>Optimizing costs for experimentation</h2>
			<p>Experimentation is another<a id="_idIndexMarker1590"/> area that is often overlooked, and you should apply the following tips to minimize the related spend.</p>
			<p class="callout-heading">Tip #3</p>
			<p class="callout">You don't have to use SageMaker Studio.</p>
			<p>As explained in <a href="B17705_01_Final_JM_ePub.xhtml#_idTextAnchor013"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing Amazon SageMaker</em>, you can easily work with SageMaker Python SDK on your local machine or on a local development server.</p>
			<p class="callout-heading">Tip #4</p>
			<p class="callout">Stop Studio instances when you don't need them.</p>
			<p>This sounds like an obvious one, but <a id="_idIndexMarker1591"/>are you really doing it? There's really no reason to run idle instances; commit your work, stop them, and then restart them when you need them again. Storage is persisted.</p>
			<p class="callout-heading">Tip #5</p>
			<p class="callout">Experiment on a small scale and with instances of the correct size.</p>
			<p>Do you really need the full dataset to start visualizing data and evaluating algorithms? Probably not. By working on a small fraction of your dataset, you'll be able to use smaller notebook instances. Here's an example: imagine 5 developers working 10 hours a day on their own <strong class="source-inline">ml.c5.2xlarge</strong> notebook instance. The daily cost is 5 x 10 x $0.557 = $27.85. </p>
			<p>Right-sizing to <strong class="source-inline">ml.t3.xlarge</strong> (less RAM, burstable behavior), the daily cost would be reduced to 5 x 10 x $0.233 = $11.65. You would save $486 per month, which you could certainly spend on more<a id="_idIndexMarker1592"/> experimentation, more training, and more <strong class="bold">automatic model tuning</strong>.</p>
			<p>If you need to perform large-scale cleaning and processing, please take the time to migrate that work to a managed service (see Tip #1) instead of working all day long with a humongous instance. Don't say, "Me? Never!" I know you're doing it!</p>
			<p class="callout-heading">Tip #6</p>
			<p class="callout">Use local mode.</p>
			<p>We saw in <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services with Built-In Frameworks</em>, how to use <strong class="bold">local mode</strong> to avoid firing up managed infrastructure in the AWS cloud. This is a great<a id="_idIndexMarker1593"/> technique to quickly iterate at no cost in the experimentation phase!</p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor307"/>Optimizing costs for model training</h2>
			<p>There are many techniques you<a id="_idIndexMarker1594"/> can use, and we've already discussed most of them.</p>
			<p class="callout-heading">Tip #7</p>
			<p class="callout">Don't train on Studio instances.</p>
			<p>I'm going to repeat myself here, but it's an important point. Unfortunately, this antipattern seems to be pretty common. People pick a large instance (such as <strong class="source-inline">ml.p3.2xlarge</strong>), fire up a large job in their notebook, leave it running, forget about it, and end up paying good money for an instance sitting idle for hours once the job is complete. </p>
			<p>Instead, please run your training jobs on <strong class="bold">managed instances</strong>. Thanks to <strong class="bold">distributed </strong><strong class="bold">training</strong>, you'll get your results<a id="_idIndexMarker1595"/> much quicker, and as instances are terminated as soon as training is complete, you will never overpay for training.</p>
			<p>As a bonus, you won't be at the mercy of a clean-up script (or an overzealous admin) killing all notebook instances in the middle of the night ("because they're doing nothing, right?").</p>
			<p class="callout-heading">Tip #8</p>
			<p class="callout">Pack your dataset in RecordIO/TFRecord files.</p>
			<p>This makes it easier and faster to move your dataset around and distribute it to training instances. We discussed this at length in <a href="B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 5</em></a>, <em class="italic">Training Computer Vision Models</em>, and <a href="B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Natural Language Processing Models</em>.</p>
			<p class="callout-heading">Tip #9</p>
			<p class="callout">Use pipe mode.</p>
			<p><strong class="bold">Pipe mode</strong> streams your dataset<a id="_idIndexMarker1596"/> directly from Amazon S3 to your training instances. No copying is involved, which saves on start-up time. We discussed this feature in detail in <a href="B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168"><em class="italic">Chapter 9</em></a>, <em class="italic">Scaling Your Training Jobs</em>.</p>
			<p class="callout-heading">Tip #10</p>
			<p class="callout">Right-size training instances.</p>
			<p>We saw how to do this in <a href="B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168"><em class="italic">Chapter 9</em></a>, <em class="italic">Scaling Your Training Jobs</em>. One word: <strong class="bold">CloudWatch</strong> metrics.</p>
			<p class="callout-heading">Tip #11</p>
			<p class="callout">Use Managed Spot Training.</p>
			<p>We covered this in great detail in <a href="B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Training Techniques</em>. If that didn't convince you, nothing will! Seriously, there <a id="_idIndexMarker1597"/>are very few instances when <strong class="bold">Managed Spot Training</strong> should not be used, and it should be a default setting in your notebooks.</p>
			<p class="callout-heading">Tip #12</p>
			<p class="callout">Use AWS-provided versions of TensorFlow, Apache MXNet, and so on.</p>
			<p>We have entire teams dedicated to extracting the last bit of performance from deep learning libraries on AWS. No offense, but if you think you can <strong class="source-inline">pip install</strong> and go faster, your time is probably better invested elsewhere. You can find more information at the<a id="_idIndexMarker1598"/> following links:</p>
			<ul>
				<li><a href="https://aws.amazon.com/blogs/machine-learning/faster-training-with-optimized-tensorflow-1-6-on-amazon-ec2-c5-and-p3-instances/">https://aws.amazon.com/blogs/machine-learning/faster-training-with-optimized-tensorflow-1-6-on-amazon-ec2-c5-and-p3-instances/</a>, </li>
				<li><a href="https://aws.amazon.com/about-aws/whats-new/2018/11/tensorflow-scalability-to-256-gpus/">https://aws.amazon.com/about-aws/whats-new/2018/11/tensorflow-scalability-to-256-gpus/</a></li>
				<li><a href="https://aws.amazon.com/blogs/machine-learning/amazon-web-services-aSchieves-fastest-training-times-for-bert-and-mask-r-cnn/">https://aws.amazon.com/blogs/machine-learning/amazon-web-services-aSchieves-fastest-training-times-for-bert-and-mask-r-cnn/</a> </li>
			</ul>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor308"/>Optimizing costs for model deployment</h2>
			<p>This very chapter was dedicated to<a id="_idIndexMarker1599"/> several of these techniques. I'll add a few more ideas to cut costs even further.</p>
			<p class="callout-heading">Tip #13</p>
			<p class="callout">Use batch transform if you don't need online predictions.</p>
			<p>Some applications don't require a live endpoint. They are perfectly fine with <strong class="bold">batch transform</strong>, which we studied in <a href="B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Deploying Machine Learning Models</em>. The extra benefit is that the underlying instances are terminated automatically when the batch job is done, meaning that you will never overpay for prediction because you left an endpoint running for a week for no good reason.</p>
			<p class="callout-heading">Tip #14</p>
			<p class="callout">Delete unnecessary endpoints.</p>
			<p>This requires no explanation, and I have written "Delete the endpoint when you're done" tens of times in this book already. Yet, this is still a common mistake. </p>
			<p class="callout-heading">Tip #15</p>
			<p class="callout">Right-size endpoints and use autoscaling.</p>
			<p class="callout-heading">Tip #16</p>
			<p class="callout">Use a multi-model endpoint to consolidate models.</p>
			<p class="callout-heading">Tip #17</p>
			<p class="callout">Compile models with Amazon Neo to use fewer hardware resources.</p>
			<p class="callout-heading">Tip #18</p>
			<p class="callout">At large scale, use<a id="_idIndexMarker1600"/> AWS Inferentia instead of GPU instances.</p>
			<p>And, of course, the mother of all tips for all things AWS, which is why we dedicated a full chapter to it (<a href="B17705_12_Final_JM_ePub.xhtml#_idTextAnchor260"><em class="italic">Chapter 12</em></a>, <em class="italic">Automating Machine Learning Workflows</em>).</p>
			<p class="callout-heading">Tip #19</p>
			<p class="callout">Automate, automate, automate!</p>
			<p class="callout-heading">Tip #20</p>
			<p class="callout">Purchase Savings Plans for Amazon SageMaker.</p>
			<p><strong class="bold">Savings Plans</strong> is a flexible pricing model that offers low prices on AWS usage, in exchange for a commitment<a id="_idIndexMarker1601"/> to a consistent amount of usage for a one-year or three-year term (<a href="https://aws.amazon.com/savingsplans/">https://aws.amazon.com/savingsplans/</a>).</p>
			<p>Savings Plans is now available for SageMaker, and you'll find it in the console at <a href="https://console.aws.amazon.com/cost-management/home?/savings-plans/">https://console.aws.amazon.com/cost-management/home?/savings-plans/</a>.</p>
			<p>Built-in recommendations help you pick the right commitment and purchase a plan in minutes. Depending on the term and the commitment, you could save up to 72% (!) on all instance-based SageMaker costs. You can find a demo at <a href="https://aws.amazon.com/blogs/aws/slash-your-machine-learning-costs-with-instance-price-reductions-and-savings-plans-for-amazon-sagemaker/">https://aws.amazon.com/blogs/aws/slash-your-machine-learning-costs-with-instance-price-reductions-and-savings-plans-for-amazon-sagemaker/</a>.</p>
			<p>Equipped with this checklist, not only<a id="_idIndexMarker1602"/> will you slash your machine learning budget but you will also build more robust and more agile workflows. Rome wasn't built in a day, so please take your time, use common sense, apply the techniques that matter most right now, and iterate.</p>
			<h1 id="_idParaDest-310"><a id="_idTextAnchor309"/>Summary</h1>
			<p>In this final chapter, you learned different techniques that help to reduce prediction costs with SageMaker. First, you saw how to use autoscaling to scale prediction infrastructure according to incoming traffic. Then, you learned how to deploy an arbitrary number of models on the same endpoint, thanks to multi-model endpoints. </p>
			<p>We also worked with Amazon Elastic Inference, which allows you to add fractional GPU acceleration to a CPU-based instance and find the right cost-performance ratio for your application. We then moved on to Amazon SageMaker Neo, an innovative capability that compiles models for a given hardware architecture, both for EC2 instances and embedded devices. Finally, we built a cost optimization checklist that will come in handy for your upcoming SageMaker projects.</p>
			<p>You've made it to the end. Congratulations! You now know a lot about SageMaker. Now, go grab a dataset, build cool stuff, and let me know about it! </p>
		</div>
	</div></body></html>
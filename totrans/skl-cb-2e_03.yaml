- en: Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing dimensionality with PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using factor analysis for decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using kernel PCA for nonlinear dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using truncated SVD to reduce dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using decomposition to classify with DictionaryLearning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing dimensionality reduction with manifolds – t-SNE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing methods to reduce dimensionality with pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will reduce the number of features or inputs into the machine
    learning models. This is a very important operation because sometimes datasets
    have a lot of input columns, and reducing the number of columns creates simpler
    models that take less computing power to predict.
  prefs: []
  type: TYPE_NORMAL
- en: The main model used in this section is **principal component analysis** (**PCA**).
    You do not have to know how many features you can reduce the dataset to, thanks
    to PCA's explained variance. A similar model in performance is **truncated singular
    value decomposition** (**truncated SVD**). It is always best to first choose a
    linear model that allows you to know how many columns you can reduce the set to,
    such as PCA or truncated SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Later in the chapter, check out the modern method of **t-distributed stochastic
    neighbor embedding** (**t-SNE**), which makes features easier to visualize in
    lower dimensions. In the final recipe, you can examine a complex pipeline and
    grid search that finds the best composite estimator consisting of dimensionality
    reductions joined with several support vector machines.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing dimensionality with PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it's time to take the math up a level! PCA is the first somewhat advanced
    technique discussed in this book. While everything else thus far has been simple
    statistics, PCA will combine statistics and linear algebra to produce a preprocessing
    step that can help to reduce dimensionality, which can be the enemy of a simple
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PCA is a member of the decomposition module of scikit-learn. There are several
    other decomposition methods available, which will be covered later in this recipe.
    Let''s use the iris dataset, but it''s better if you use your own data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import the `decomposition` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a default PCA object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to other objects in scikit-learn, the PCA object takes relatively
    few arguments. Now that the PCA object (an instance PCA) has been created, simply
    transform the data by calling the `fit_transform` method, with `iris_X` as the
    argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the PCA object has been fitted, we can see how well it has done at
    explaining the variance (explained in the following *How it works...* section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA has a general mathematical definition and a specific use case in data analysis.
    PCA finds the set of orthogonal directions that represent the original data matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, PCA works by mapping the original dataset into a new space where
    each of the new column vectors of the matrix are orthogonal. From a data analysis
    perspective, PCA transforms the covariance matrix of the data into column vectors
    that can explain certain percentages of the variance. For example, with the iris
    dataset, 92.5 percent of the variance of the overall dataset can be explained
    by the first component.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is extremely useful because dimensionality is problematic in data analysis.
    Quite often, algorithms applied to high-dimensional datasets will overfit on the
    initial training, and thus lose generality to the test set. If most of the underlying
    structure of the data can be faithfully represented by fewer dimensions, then
    it''s generally considered a worthwhile trade-off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data matrix is now 150 x 2, instead of 150 x 4\. The separability of the
    classes remains even after reducing the dimensionality by two. We can see how
    much of the variance is represented by the two components that remain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize what PCA has done, let''s plot the first two dimensions of the
    iris dataset with before-after pictures of the PCA transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/de1daad3-cf27-468d-95aa-895177d2e8e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `PCA` object can also be created with the amount of explained variance
    in mind from the start. For example, if we want to be able to explain at least
    98 percent of the variance, the `PCA` object will be created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Since we wanted to explain variance slightly more than the two component examples,
    a third was included.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the final dimensions of the data are two or three, these two or
    three columns contain information from all four original columns.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is recommended that PCA is scaled beforehand. Do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f1597dbd-7243-49f4-a582-d7d3c68c63ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This looks a bit worse. Regardless, you should always consider the scaled PCA
    if you consider PCA. Preferably, you can scale with a pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Using pipelines prevents errors and reduces the amount of debugging of complex
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Using factor analysis for decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Factor analysis is another technique that we can use to reduce dimensionality.
    However, factor analysis makes assumptions and PCA does not. The basic assumption
    is that there are implicit features responsible for the features of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will boil down to the explicit features from our samples in an attempt
    to understand the independent variables as much as the dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To compare PCA and factor analysis, let''s use the iris dataset again, but
    we''ll first need to load the `FactorAnalysis` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From a programming perspective, factor analysis isn''t much different from
    PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare the following plot to the plot in the last section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46688b30-dcfe-4db6-8ce7-df0ecc70da1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Since factor analysis is a probabilistic transform, we can examine different
    aspects, such as the log likelihood of the observations under the model, and better
    still, compare the log likelihoods across models.
  prefs: []
  type: TYPE_NORMAL
- en: Factor analysis is not without flaws. The reason is that you're not fitting
    a model to predict an outcome, you're fitting a model as a preparation step. This
    isn't a bad thing, but errors here are compounded when training the actual model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Factor analysis is similar to PCA, which was covered previously. However, there
    is an important distinction to be made. PCA is a linear transformation of the
    data to a different space where the first component explains the variance of the
    data, and each subsequent component is orthogonal to the first component.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can think of PCA as taking a dataset of *N* dimensions and
    going down to some space of *M* dimensions, where *M* < *N*.
  prefs: []
  type: TYPE_NORMAL
- en: Factor analysis, on the other hand, works under the assumption that there are
    only *M* important features and a linear combination of these features (plus noise)
    creates the dataset in *N* dimensions. To put it another way, you don't do regression
    on an outcome variable, you do regression on the features to determine the latent
    factors of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, a big drawback is that you do not know how many columns you can
    reduce the data to. PCA gives you the explained variance metric to guide you through
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: Using kernel PCA for nonlinear dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the techniques in statistics are linear by nature, so in order to capture
    nonlinearity, we might need to apply some transformation. PCA is, of course, a
    linear transformation. In this recipe, we'll look at applying nonlinear transformations,
    and then apply PCA for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Life would be so easy if data was always linearly separable, but unfortunately,
    it's not. Kernel PCA can help to circumvent this issue. Data is first run through
    the kernel function that projects the data onto a different space; then, PCA is
    performed.
  prefs: []
  type: TYPE_NORMAL
- en: To familiarize yourself with the kernel functions, it will be a good exercise
    to think of how to generate data that is separable by the kernel functions available
    in the kernel PCA. Here, we'll do that with the cosine kernel. This recipe will
    have a bit more theory than the previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting, load the iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cosine kernel works by comparing the angle between two samples represented
    in the feature space. It is useful when the magnitude of the vector perturbs the
    typical distance measure used to compare samples. As a reminder, the cosine between
    two vectors is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/232fac96-295d-456a-b747-3919da10a26d.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that the cosine between *A* and *B* is the dot product of the two
    vectors normalized by the product of the individual norms. The magnitude of vectors
    *A* and *B* have no influence on this calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s go back to the iris dataset to use it for visual comparisons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, visualize the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fbf706e-bfd2-42eb-89bb-f052348e25b8.png)'
  prefs: []
  type: TYPE_IMG
- en: The result looks slightly better, although we would have to measure it to know
    for sure.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several different kernels available besides the cosine kernel. You
    can even write your own kernel function. The available kernels are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Poly (polynomial)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RBF (radial basis function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-computed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also options that are contingent on the kernel choice. For example,
    the degree argument will specify the degree for the poly, RBF, and sigmoid kernels;
    also, gamma will affect the RBF or poly kernels.
  prefs: []
  type: TYPE_NORMAL
- en: The recipe on SVM will cover the RBF kernel function in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel methods are great to create separability, but they can also cause overfitting
    if used without care. Make sure to train-test them properly.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the available kernels are smooth, continuous, and differentiable functions.
    They do not create the jagged edges of regression trees.
  prefs: []
  type: TYPE_NORMAL
- en: Using truncated SVD to reduce dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Truncated SVD is a matrix factorization technique that factors a matrix *M*
    into the three matrices *U*, Σ, and *V*. This is very similar to PCA, except that
    the factorization for SVD is done on the data matrix, whereas for PCA, the factorization
    is done on the covariance matrix. Typically, SVD is used under the hood to find
    the principle components of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Truncated SVD is different from regular SVDs in that it produces a factorization
    where the number of columns is equal to the specified truncation. For example,
    given an *n* x *n* matrix, SVD will produce matrices with *n* columns, whereas
    truncated SVD will produce matrices with the specified number of columns. This
    is how the dimensionality is reduced. Here, we''ll again use the iris dataset
    so that you can compare this outcome against the PCA outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This object follows the same form as the other objects we've used.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll import the required object, then we''ll fit the model and examine
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72fd3041-d160-41b6-b57b-97cee44d0df9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The results look pretty good. Like PCA, there is explained variance with `explained_variance_ratio_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've walked through how, is performed in scikit-learn, let's look
    at how we can use only SciPy, and learn a bit in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to use SciPy''s `linalg` to perform SVD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can reconstruct the original matrix `D` to confirm `U`, `S`, and `V` as
    a decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix that is actually returned by truncated SVD is the dot product of
    the `U` and `S` matrices. If we want to simulate the truncation, we will drop
    the smallest singular values and the corresponding column vectors of `U`. So,
    if we want a single component here, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In general, if we want to truncate to some dimensionality, for example, *t*,
    we drop *N - t* singular values.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Truncated SVD has a few miscellaneous things that are worth noting with respect
    to the method.
  prefs: []
  type: TYPE_NORMAL
- en: Sign flipping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's a gotcha with truncated SVDs. Depending on the state of the random number
    generator, successive fittings of truncated SVD can flip the signs of the output.
    In order to avoid this, it's advisable to fit truncated SVD once, and then use
    transforms from then on. This is another good reason for pipelines!
  prefs: []
  type: TYPE_NORMAL
- en: 'To carry this out, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Sparse matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One advantage of truncated SVD over PCA is that truncated SVD can operate on
    sparse matrices, while PCA cannot. This is due to the fact that the covariance
    matrix must be computed for PCA, which requires operating on the entire matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Using decomposition to classify with DictionaryLearning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll show how a decomposition method can actually be used for
    classification. `DictionaryLearning` attempts to take a dataset and transform
    it into a sparse representation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With `DictionaryLearning`, the idea is that the features are the basis for
    the resulting datasets. Load the iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, create a training set by taking every other element of `iris_X`
    and `y`. Take the remaining elements for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Import `DictionaryLearning`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Use three components to represent the three species of iris:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform every other data point so that we can test the classifier on the
    resulting data points after the learner is trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now test the transform simply by typing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the output. Notice how each value is sited on the *x*, *y*,
    or *z* axis, along with the other values and zero; this is called sparseness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c4c5530f-20de-4344-aa5b-67921fe012e5.png)'
  prefs: []
  type: TYPE_IMG
- en: If you look closely, you can see there was a training error. One of the classes
    was misclassified. Only being wrong once isn't a big deal, though. There was also
    an error in the classification. If you remember some of the other visualizations,
    the red and green classes were the two classes that often appeared close together.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`DictionaryLearning` has a background in signal processing and neurology. The
    idea is that only few features can be active at any given time. Therefore, `DictionaryLearning`
    attempts to find a suitable representation of the underlying data, given the constraint
    that most of the features should be zero.'
  prefs: []
  type: TYPE_NORMAL
- en: Doing dimensionality reduction with manifolds – t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a short and practical recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you read the rest of the chapter, we have been doing a lot of dimensionality
    reduction with the iris dataset. Let''s continue the pattern for additional easy
    comparisons. Load the iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Load `PCA` and some classes from the `manifold` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Run all the transforms on `iris_X`. One of the transforms is t-SNE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cf664e0a-cb29-454d-88ae-c57c6156f69c.png)'
  prefs: []
  type: TYPE_IMG
- en: The t-SNE algorithm has been popular recently, yet it takes a lot of computing
    time and power. ISO produces an interesting graphic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, in cases where the dimensionality of the data is very high (more
    than 50 columns) the scikit-learn documentation suggests doing PCA or truncated
    SVD before t-SNE. The iris dataset is small, but we can write the syntax to perform
    t-SNE after PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e1289eae-8719-49d6-a074-408e9ad9bc21.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In mathematics, a manifold is a space that is locally Euclidean at every point,
    yet is embedded in a higher-dimensional space. For example, the outer surface
    of a sphere is a two-dimensional manifold in three dimensions. When we walk around
    on the surface of the sphere of the Earth, we tend to perceive the 2D plane of
    the ground rather than all of 3D space. We navigate using 2D maps, not higher-dimensional
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: The `manifold` module in scikit-learn is useful for understanding high-dimensional
    spaces in two or three dimensions. The algorithms in the module gather information
    about the local structure around a point and seek to preserve it. What are the
    neighbors of a point? How far away are the neighbors of a point?
  prefs: []
  type: TYPE_NORMAL
- en: For example, the Isomap algorithm attempts to preserve geodesic distances between
    all of the points in an algorithm, starting with a nearest neighbor search, followed
    by a graph search, and then a partial eigenvalue decomposition. The point of the
    algorithm is to preserve distances and a manifold's local geometric structure.
    The **multi-dimensional scaling** (**MDS**) algorithm also respects distances.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE converts Euclidean distances between pairs of points in the dataset into
    probabilities. Around each point there is a Gaussian centered at that point, and
    the probability distribution represents the chance of any other point being a
    neighbor. Points very far away from each other have a low chance of being neighbors.
    Here, we have turned point locations into distances and then probabilities. t-SNE
    maintains the local structure very well by utilizing the probabilities of two
    points being neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: In a very general sense manifold methods start by examining the neighbors of
    every point, which represent the local structure of a manifold, and attempt to
    preserve that local structure in different ways. It is similar to you walking
    around your neighborhood or block constructing a 2D map of the local structure
    around you and focusing on two dimensions rather than three.
  prefs: []
  type: TYPE_NORMAL
- en: Testing methods to reduce dimensionality with pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we will see how different estimators composed of dimensionality reduction
    and a support vector machine perform.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the iris dataset and some dimensionality reduction libraries. This is
    a big step for this particular recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instantiate a pipeline object with two main parts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An object to reduce dimensionality
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An estimator with a predict method
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Note in the following code that Isomap comes from the `manifold` module and
    that the **n****on-negative matrix factorization** (**NMF**) algorithm utilizes
    SVDs to break up a matrix into non-negative factors, its main purpose in this
    section is to compare its performance with other algorithms, but it is useful
    in **natural language processing** (**NLP**) where matrix factorizations cannot
    be negative. Now type the following parameter grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This parameter grid will allow scikit-learn to cycle through a few dimensionality
    reduction techniques coupled with two SVM types: linear SVC and SVC for classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now run a grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now look at the best parameters to determine the best model. A PCA with SVC
    was the best model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'If you would like to create a dataframe of results, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you can predict on an unseen instance with the `grid.predict(X_test)` method for
    a testing set `X_test`. We will do several grid searches in later chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grid search does cross-validation to determine the best score. In this case,
    all the data was used for three-fold cross-validation. For the rest of the book,
    we will save some data for testing to make sure the models do not run into anomalous
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'A final note on the pipeline you just saw: the `sklearn.decomposition` methods
    will work for the first step of reducing dimensionality within the pipeline, but
    not all of the manifold methods were designed for pipelines.'
  prefs: []
  type: TYPE_NORMAL

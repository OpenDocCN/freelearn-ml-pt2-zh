- en: Building Models with Distance Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using k-means to cluster data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing the number of centroids
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessing cluster correctness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MiniBatch k-means to handle more data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantizing an image with k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the closest objects in the feature space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic clustering with Gaussian Mixture Models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using k-means for outlier detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using KNN for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll cover clustering. Clustering is often grouped with unsupervised
    techniques. These techniques assume that we do not know the outcome variable.
    This leads to ambiguity in outcomes and objectives in practice, but nevertheless,
    clustering can be useful. As we'll see, we can use clustering to localize our
    estimates in a supervised setting. This is perhaps why clustering is so effective;
    it can handle a wide range of situations, and often the results are, for the lack
    of a better term, sane.
  prefs: []
  type: TYPE_NORMAL
- en: We'll walk through a wide variety of applications in this chapter, from image
    processing to regression and outlier detection. Clustering is related to classification
    of categories. You have a finite set of blobs or categories. Unlike classification,
    you do not know the categories in advance. Additionally, clustering can often
    be viewed through a continuous and probabilistic or optimization lens.
  prefs: []
  type: TYPE_NORMAL
- en: Different interpretations lead to various trade-offs. We'll walk through how
    to fit the models here so that you'll have the tools to try out many models when
    faced with a clustering problem.
  prefs: []
  type: TYPE_NORMAL
- en: Using k-means to cluster data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a dataset, we observe sets of points gathered together. With k-means, we
    will categorize all the points into groups, or clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s walk through some simple clustering; then we''ll talk about how
    k-means works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, since we''ll be doing some plotting, import `matplotlib` as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to walk through a simple example that clusters blobs of fake data.
    Then we''ll talk a little bit about how k-means works to find the optimal number
    of blobs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at our blobs, we can see that there are three distinct clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c3a55fba-8576-4348-98e5-b471123d27d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we can use k-means to find the centers of these clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example, we''ll pretend we know that there are three centers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/887c0b1a-c85f-42f3-bb92-ac9ab324c15d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Other attributes are useful too. For instance, the `labels_` attribute will
    produce the expected label for each point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check whether `kmean.labels_` is the same as the classes, but because
    k-means has no knowledge of the classes going in, it cannot assign the sample
    index values to both classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Feel free to swap `1` and `0` in classes to check whether it matches up with
    `labels_`. The transform function is quite useful in the sense that it will output
    the distance between each point and the centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: k-means is actually a very simple algorithm that works to minimize the within-cluster
    sum of squares of distances from the mean. We'll be minimizing the sum of squares
    yet again!
  prefs: []
  type: TYPE_NORMAL
- en: 'It does this by first setting a prespecified number of clusters, K, and then
    alternating between the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Assigning each observation to the nearest cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating each centroid by calculating the mean of each observation assigned
    to this cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This happens until some specified criterion is met. Centroids are difficult
    to interpret, and it can also be very difficult to determine whether we have the
    correct number of centroids. It's important to understand whether your data is
    unlabeled or not as this will directly influence the evaluation measures you can
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the number of centroids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When doing k-means clustering, we really do not know the right number of clusters
    in advance, so finding this out is an important step. Once we know (or estimate)
    the number of centroids, the problem will start to look more like a classification
    one as our knowledge to work with will have increased substantially.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating the model performance for unsupervised techniques is a challenge.
    Consequently, `sklearn` has several methods for evaluating clustering when a ground
    truth is known, and very few for when it isn't.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with a single cluster model and evaluate its similarity. This is
    more for the purpose of mechanics as measuring the similarity of one cluster count
    is clearly not useful in finding the ground truth number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get started, we''ll create several blobs that can be used to simulate clusters
    of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we''ll look at the silhouette distance. Silhouette distance is the ratio
    of the difference between the in-cluster dissimilarity and the closest out-of-cluster
    dissimilarity, and the maximum of these two values. It can be thought of as a
    measure of how separate the clusters are. Let''s look at the distribution of distances
    from the points to the cluster centers; it''s useful to understand silhouette
    distances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is part of the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b217e764-86a9-4ccc-adf8-e51a507c9209.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that generally, the higher the number of coefficients close to 1 (which
    is good), the better the score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The average of the silhouette coefficients is often used to describe the entire
    model''s fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s very common; in fact, the metrics module exposes a function to arrive
    at the value we just got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c24ff17-ff8d-4555-ad8c-89dd97bac11f.png)'
  prefs: []
  type: TYPE_IMG
- en: This plot shows that the silhouette averages as the number of centroids increase.
    We can see that the optimum number, according to the data generating process,
    is 3; but here it looks like it's around 7 or 8\. This is the reality of clustering;
    quite often, we won't get the correct number of clusters. We can only really hope
    to estimate the number of clusters to some approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing cluster correctness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We talked a little bit about assessing clusters when the ground truth is not
    known. However, we have not yet talked about assessing k-means when the cluster
    is known. In a lot of cases, this isn't knowable; however, if there is outside
    annotation, we will know the ground truth or at least the proxy sometimes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, let's assume a world where we have an outside agent supplying us with the
    ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create a simple dataset, evaluate the measures of correctness against
    the ground truth in several ways, and then discuss them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we walk through the metrics, let''s take a look at the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fc26c78b-07e5-41d5-a23c-15fea61431ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to fit a k-means model, we''ll create a `KMeans` object from the cluster
    module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve fit the model, let''s have a look at the cluster centroids:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a43c8911-edf8-4152-a51c-17a20fd4c660.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we can view the clustering performance as a classification exercise,
    the metrics that are useful in its context are also useful here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Clearly, we have some backward clusters. So, let''s get this straightened out
    first, and then we''ll look at the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we''re roughly correct 90% of the time. The second measure of similarity
    we''ll look at is the mutual information score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As the score tends to be 0, the label assignments are probably not generated
    through similar processes; however, a score closer to 1 means that there is a
    large amount of agreement between the two labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s look at what happens when the mutual information score
    itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the name, we can tell that there is probably an unnormalized `mutual_info_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: These are very close; however, normalized mutual information is the mutual information
    divided by the root of the product of the entropy of each set truth and assigned
    label.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One cluster metric we haven't talked about yet, and one that is not reliant
    on the ground truth, is inertia. It is not very well documented as a metric at
    the moment. However, it is a metric that k-means minimizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inertia is the sum of the squared difference between each point and its assigned
    cluster. We can use a bite of NumPy to determine this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Using MiniBatch k-means to handle more data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-means is a nice method to use; however, it is not ideal for a lot of data.
    This is due to the complexity of k-means. This said, we can get approximate solutions
    with much better algorithmic complexity using MiniBatch k-means.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MiniBatch k-means is a faster implementation of k-means. K-means is computationally
    very expensive; the problem is NP-hard.
  prefs: []
  type: TYPE_NORMAL
- en: However, using MiniBatch k-means, we can speed up k-means by orders of magnitude.
    This is achieved by taking many subsamples that are called MiniBatches. Given
    the convergence properties of subsampling, a close approximation to regular k-means
    is achieved provided there are good initial conditions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s do some very high-level profiling of MiniBatch clustering. First, we''ll
    look at the overall speed difference, and then we''ll look at the errors in the
    estimates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Understand that these metrics are meant to expose the issue. Therefore, great
    care is taken to ensure the highest accuracy of the benchmarks. There is a lot
    of information available on this topic; if you really want to get to the heart
    of why MiniBatch k-means is better at scaling, it will be a good idea to review
    what's available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the setup is complete, we can measure the time difference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s a large difference in CPU times. The difference in the clustering
    performance is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the two arrays; the located centers are ordered the same. This is random—the
    clusters do not have to be ordered the same. Look at the distance between the
    first cluster centers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This seems to be very close. The diagonals will contain the cluster center
    differences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The batches here are key. Batches are iterated through to find the batch mean;
    for the next iteration, the prior batch mean is updated in relation to the current
    iteration. There are several options that dictate the general k-means behavior
    and parameters that determine how MiniBatch k-means gets updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `batch_size` parameter determines how large the batches should be. Just
    for fun, let''s run MiniBatch; however, this time we set the batch size to be
    the same as the dataset size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Clearly, this is against the spirit of the problem, but it does illustrate an
    important point. Choosing poor initial conditions can affect how well models,
    particularly clustering models, converge. With MiniBatch k-means, there is no
    guarantee that the global optimum will be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: There are many powerful lessons in MiniBatch k-means. It uses the power of many
    random samples, similar to bootstrapping. When creating an algorithm for big data,
    you can use many random samples on many machines processing in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Quantizing an image with k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image processing is an important topic in which clustering has some application.
    It's worth pointing out that there are several very good image processing libraries
    in Python. `scikit-image` is a sister project of scikit-learn. It's worth taking
    a look at if you want to do anything complicated.
  prefs: []
  type: TYPE_NORMAL
- en: A big point of this chapter is that images are data as well and clustering can
    be used to try to guess where some objects in an image are. Clustering can be
    part of an image processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will have some fun in this recipe. The goal is to use a cluster to blur an
    image. First, we'll make use of SciPy to read the image. The image is translated
    in a three-dimensional array; the `x` and `y` coordinates describe the height
    and width, and the third dimension represents the RGB values for each image.
  prefs: []
  type: TYPE_NORMAL
- en: Begin by downloading or moving an `.jpg` image to the folder where your IPython
    notebook is located. You can use a picture of yours. I use a picture of myself
    named `headshot.jpg`.
  prefs: []
  type: TYPE_NORMAL
- en: How do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s read the image in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is seen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2fd4a5a-4121-49f4-9ccc-15c8cb68a495.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That''s me! Now that we have the image, let''s check its dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: To actually quantize the image, we need to convert it into a two-dimensional
    array, with the length being 379 x 337 and the width being the RGB values. A better
    way to think about this is to have a bunch of data points in three-dimensional
    space and cluster the points to reduce the number of distant colors in the image—a
    simple way to do quantization.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s reshape our array; it is a NumPy array, and thus simple to work
    with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can start the clustering process. First, let''s import the cluster module
    and create a k-means object. We''ll pass `n_clusters=5` so that we have five clusters,
    or really, five distinct colors. This will be a good recipe to practice using
    silhouette distance, which we reviewed in the *Optimizing the number of centroids*
    recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the centers, the next thing we need is the labels. This will
    tell us which points should be associated with which clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we require the simplest of NumPy array manipulation followed
    by a bit of reshaping, and we''ll have the new image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the resultant image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb03efc6-0bd1-4c20-9351-d62bdde53ca8.png)'
  prefs: []
  type: TYPE_IMG
- en: The clustering separated the image into a few regions.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the closest object in the feature space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, the easiest thing to do is to find the distance between two objects.
    We just need to find some distance metric, compute the pairwise distances, and
    compare the outcomes with what is expected.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lower level utility in scikit-learn is `sklearn.metrics.pairwise`. It contains
    server functions used to compute distances between vectors in a matrix X or between
    vectors in X and Y easily. This can be useful for information retrieval. For example,
    given a set of customers with attributes of X, we might want to take a reference
    customer and find the closest customers to this customer.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we might want to rank customers by the notion of similarity measured
    by a distance function. The quality of similarity depends upon the feature space
    selection as well as any transformation we might do on the space. We'll walk through
    several different scenarios of measuring distance.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the `pairwise_distances` function to determine the closeness of
    objects. Remember that the closeness is just similarity, which we grade using
    our distance function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the pairwise distance function from the metrics module
    and create a dataset to play with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The simplest way to check the distances is `pairwise_distances`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`distances` is an N x N matrix with 0s along the diagonals. In the simplest
    case, let''s see the distances between each point and the first point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Ranking the points by closeness is very easy with np.argsort:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The great thing about `argsort` is that now we can sort our `points` matrix
    to get the actual points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: It's useful to see what the closest points look like as follows. The chosen
    point, points [0], is colored green. The closest points are colored red (except
    for the chosen point).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that other than some assurances, this works as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/99cb8345-f0b7-419b-be50-5e78e4f0da9d.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given some distance function, each point is measured in a pairwise function.
    Consider two points represented as vectors in N-dimensional space with components
    *p[i]* and *q[i]*; the default is the Euclidean distance, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/015397b7-7363-4a0b-b2f5-b324b967fe94.png)'
  prefs: []
  type: TYPE_IMG
- en: Verbally, this takes the difference between each component of the two vectors,
    squares these differences, sums them all, and then finds the square root. This
    looks very familiar as we used something very similar when looking at the mean
    squared error. If we take the square root, we have the same thing. In fact, a
    metric used often is root mean square deviation (RMSE), which is just the applied
    distance function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, this looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'There are several other functions available in scikit-learn, but scikit-learn
    will also use distance functions of SciPy. At the time of writing this book, the
    scikit-learn distance functions support sparse matrixes. Check out the SciPy documentation
    for more information on the distance functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cityblock`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cosine`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`euclidean`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`manhattan`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now solve problems. For example, if we were standing on a grid at the
    origin and the lines were the streets, how far would we have to travel to get
    to point *(5, 5)*?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using pairwise distances, we can find the similarity between bit vectors. For
    N-dimensional vectors *p* and *q*, it''s a matter of finding the hamming distance,
    which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afd3edd0-ade2-460c-9b74-0916c44bb4ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that scikit-learn's `hamming` metric returns the hamming distance divided
    by the length of the vectors, `4` in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic clustering with Gaussian mixture models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In k-means, we assume that the variance of the clusters is equal. This leads
    to a subdivision of space that determines how the clusters are assigned; but what
    about a situation where the variances are not equal and each cluster point has
    some probabilistic association with it?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's a more probabilistic way of looking at k-means clustering. Hard k-means
    clustering is the same as applying a Gaussian mixture model with a covariance
    matrix, `S`, which can be factored to the error times of the identity matrix.
    This is the same covariance structure for each cluster. It leads to spherical
    clusters. However, if we allow S to vary, a GMM can be estimated and used for
    prediction. We'll look at how this works in a univariate sense and then expand
    to more dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to create some data. For example, let''s simulate heights of
    both women and men. We''ll use this example throughout this recipe. It''s a simple
    example, but hopefully it will illustrate what we''re trying to accomplish in
    an N-dimensional space, which is a little easier to visualize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b0ba254-044d-48b7-9e75-3e3e53127334.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we might be interested in subsampling the group, fitting the distribution,
    and then predicting the remaining groups:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to get the empirical distribution of the heights of both men and
    women based on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: For the test set, we will calculate based on the likelihood that the data point
    was generated from either distribution, and the most likely distribution will
    get the appropriate label assigned.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will, of course, look at how accurate we were:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the difference in likelihoods. Assume that we guess situations when
    the men''s probability is higher, but we overwrite them if the women''s probability
    is higher:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, the question is how accurate we are. Since `guesses_m` will be *1*
    if we are correct and *0* if we aren''t, we take the mean of the vector and get
    the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Not too bad! Now, to see how well we did with the women''s group, we use the
    following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s allow the variance to differ between groups. First, create some new
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2ae342ce-0c92-4060-a6de-dd66b8de5179.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can create the same PDFs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6f93504d-a8a5-4578-8ba0-0f545236ba50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can imagine this in a multidimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/550cf069-4940-4f13-9035-935e2ae9d0bb.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Okay, so now that we''ve looked at how we can classify points based on distribution,
    let''s look at how we can do this in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''re conscientious data scientists, we''ll create a training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Fitting and predicting is done in the same way as fitting is done for many
    of the other objects in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: There are other methods worth looking at now that the model has been fit. For
    example, using `score_samples`, we can actually get the per-sample likelihood
    for each label.
  prefs: []
  type: TYPE_NORMAL
- en: Using k-means for outlier detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll look at both the debate and mechanics of k-means for outlier
    detection. It can be useful to isolate some types of errors, but care should be
    taken when using it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use k-means to do outlier detection on a cluster of points. It's important
    to note that there are many camps when it comes to outliers and outlier detection.
    On one hand, we're potentially removing points that were generated by the data-generating
    process by removing outliers. On the other hand, outliers can be due to a measurement
    error or some other outside factor.
  prefs: []
  type: TYPE_NORMAL
- en: This is the most credence we'll give to the debate. The rest of this recipe
    is about finding outliers; we'll work under the assumption that our choice to
    remove outliers is justified. The act of outlier detection is a matter of finding
    the centroids of the clusters and then identifying points that are potential outliers
    by their distances from the centroid.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we''ll generate a single blob of 100 points, and then we''ll identify
    the five points that are furthest from the centroid. These are the potential outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s important that the k-means cluster has a single center. This idea is
    similar to a one-class SVM that is used for outlier detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the plot. Those playing along at home, try to guess which
    points will be identified as one of the five outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/327feae1-f9d0-414f-9203-c236df182253.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s identify the five closest points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see which plots are the farthest away:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2912168d-0c4d-4153-8eeb-df2038428a22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s easy to remove these points if we like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, the centroid clearly changes with the removal of these points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize the difference between the old and new centroids:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/497f4845-bb5c-42b0-9a6a-67cc2ae16000.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, the centroid hasn't moved much, which is to be expected only when removing
    the five most extreme values. This process can be repeated until we're satisfied
    that the data is representative of the process.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we''ve already seen, there is a fundamental connection between the Gaussian
    distribution and the k-means clustering. Let''s create an empirical Gaussian based
    on the centroid and sample covariance matrix and look at the probability of each
    point—theoretically, the five points we removed. This just shows that we have,
    in fact, removed the values with the least likelihood. This idea between distances
    and likelihoods is very important and will come around quite often in your machine
    learning training. Use the following command to create an empirical Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Using KNN for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is covered elsewhere in the book, but we might also want to run a
    regression on pockets of the feature space. We can think that our dataset is subject
    to several data processes. If this is true, only training on similar data points
    is a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our old friend, regression, can be used in the context of clustering. Regression
    is obviously a supervised technique, so we'll use **K-Nearest Neighbors** (**KNN**)
    clustering rather than k-means. For KNN regression, we'll use the K closest points
    in the feature space to build the regression rather than using the entire space
    as in regular regression.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this recipe, we''ll use the `iris` dataset. If we want to predict something
    such as the petal width for each flower, clustering by iris species can potentially
    give us better results. The KNN regression won''t cluster by the species, but
    we''ll work under the assumption that the Xs will be close for the same species,
    in this case, the petal length:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use the `iris` dataset for this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll try to predict the petal length based on the sepal length and width.
    We''ll also fit a regular linear regression to see how well the KNN regression
    does in comparison:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, for the KNN regression, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at what the KNN regression does when we tell it to use the 10 closest
    points for regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8fd74776-d305-48be-bb6d-549028459772.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It might be completely clear that the predictions are close for the most part,
    but let''s look at the predictions for the Setosa species as compared to the actuals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the plots again, we see that the setosa species (upper-left cluster)
    is largely overestimated by linear regression, and KNN is fairly close to the
    actual values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works..
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'KNN regression is very simple to calculate by taking the average of the *K*
    closest points to the point being tested. Let''s manually predict a single point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to get the 10 closest points to our `example_point`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We can see that this is very close to what was expected.
  prefs: []
  type: TYPE_NORMAL

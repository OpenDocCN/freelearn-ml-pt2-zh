["```py\nod = sagemaker.estimator.Estimator(\n     container,\n     role,\n     instance_count=2,                                 \n     instance_type='ml.p3.2xlarge',                                 \n     use_spot_instances=True,\n     max_run=7200,                     # 2 hour\n     max_wait=36000,                   # +8 hours\n     output_path=s3_output_location)\n```", "```py\nUpdating the best model with validation-mAP=1.615789635726003e-05\nSaved checkpoint to \"/opt/ml/model/model_algo_1-0000.params\"\n```", "```py\nTraining seconds: 7794\nBillable seconds: 2338\nManaged Spot Training savings: 70.0%\n```", "```py\n    model.compile(\n        loss=tf.keras.losses.categorical_crossentropy,\n        optimizer=tf.keras.optimizers.Adam(),\n        metrics=['accuracy'])\n    ```", "```py\n    from tensorflow.keras.callbacks import ModelCheckpoint\n    chk_dir = '/opt/ml/checkpoints'\n    chk_name = 'fmnist-cnn-{epoch:04d}'\n    checkpointer = ModelCheckpoint(\n        filepath=os.path.join(chk_dir, chk_name),\n        monitor='val_accuracy')\n    ```", "```py\n    model.fit(x=x_train, y=y_train, \n              validation_data=(x_val, y_val),\n              batch_size=batch_size, epochs=epochs,\n              callbacks=[checkpointer],\n              verbose=1)\n    ```", "```py\n    from tensorflow.keras.models import save_model\n    save_model(model, os.path.join(model_dir, '1'),  \n               save_format='tf')\n    ```", "```py\n    from sagemaker.tensorflow import TensorFlow\n    tf_estimator = TensorFlow(\n        entry_point='fmnist-1.py',\n        role=sagemaker.get_execution_role(),\n        instance_count=1,\n        instance_type='ml.g4dn.xlarge',     \n        framework_version='2.1.0',\n        py_version='py3',\n        hyperparameters={'epochs': 20},\n        output_path=output_path,\n        use_spot_instances=True,\n        max_run=3600,\n        max_wait=7200,\n        checkpoint_s3_uri=chk_path)\n    ```", "```py\n    INFO:tensorflow:Assets written to /opt/ml/checkpoints/fmnist-cnn-0001/assets\n    ```", "```py\n    $ aws s3 ls s3://sagemaker-eu-west-1-123456789012/keras2\n    fashion-mnist/checkpoints/\n    PRE fmnist-cnn-0001/\n    PRE fmnist-cnn-0002/\n    PRE fmnist-cnn-0003/\n    PRE fmnist-cnn-0006/\n    . . .\n    ```", "```py\n    import glob\n    checkpoints = sorted(\n        glob.glob(os.path.join(chk_dir,'fmnist-cnn-*')))\n    ```", "```py\n    from tensorflow.keras.models import load_model\n    if checkpoints :\n        last_checkpoint = checkpoints[-1]\n        last_epoch = int(last_checkpoint.split('-')[-1])\n        model = load_model(last_checkpoint)\n        print('Loaded checkpoint for epoch ', last_epoch)\n    ```", "```py\n    else:\n        last_epoch = 0\n        model = Sequential()\n        . . .\n    ```", "```py\n    model.fit(x=x_train, y=y_train, \n              validation_data=(x_val, y_val), \n              batch_size=batch_size,\n              epochs=epochs,\n              initial_epoch=last_epoch,\n              callbacks=[checkpointer],\n              verbose=1)\n    ```", "```py\nLoaded checkpoint for epoch 20\n. . .\nEpoch 21/25\n```", "```py\nINFO:tensorflow:Assets written to: /opt/ml/checkpoints/fmnist-cnn-0021/assets\n```", "```py\n    od = sagemaker.estimator.Estimator(\n         container,\n         role,                                        \n         instance_count=1,                                        \n         instance_type='ml.p3.2xlarge',                                       \n         output_path=s3_output_location,                                        \n         use_spot_instances=True,\n         max_run=7200,\n         max_wait=36000,\n         volume_size=1)       \n    ```", "```py\n    od.set_hyperparameters(base_network='resnet-50',\n                           use_pretrained_model=1,\n                           num_classes=20,\n                           epochs=30,\n                           num_training_samples=16551,\n                           mini_batch_size=90)\n    ```", "```py\n    from sagemaker.tuner import ContinuousParameter,\n    hyperparameter_ranges = {\n        'learning_rate': ContinuousParameter(0.001, 0.1, \n                         scaling_type='Logarithmic'), \n        'momentum': ContinuousParameter(0.8, 0.999), \n        'weight_decay': ContinuousParameter(0.0001, 0.001)\n    }\n    ```", "```py\n    objective_metric_name = 'validation:mAP'\n    objective_type = 'Maximize'\n    ```", "```py\n    from sagemaker.tuner import HyperparameterTuner\n    tuner = HyperparameterTuner(od,\n                objective_metric_name,\n                hyperparameter_ranges,\n                objective_type=objective_type,\n                max_jobs=30,\n                max_parallel_jobs=2,\n                early_stopping_type='Auto')\n    ```", "```py\n    tuner.fit(inputs=data_channels, wait=False)\n    ```", "```py\nimport subprocess, sys\ndef install(package):\n    subprocess.call([sys.executable, \"-m\", \"pip\",\n                     \"install\", package])\ninstall('keras-metrics')\nimport keras_metrics\n. . . \nmodel.compile(\n    loss=tf.keras.losses.categorical_crossentropy,\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy',\n              keras_metrics.precision(),\n              keras_metrics.recall(),\n              keras_metrics.f1_score()])\n```", "```py\nloss: 0.0869 - accuracy: 0.9678 - precision: 0.9072 - recall: 0.8908 - f1_score: 0.8989 - val_loss: 0.2301 - val_accuracy: 0.9310 - val_precision: 0.9078 - val_recall: 0.8915 - val_f1_score: 0.8996\n```", "```py\nobjective_metric_name = 'val_f1'\nobjective_type = 'Maximize'\nmetric_definitions = [\n    {'Name': 'val_f1',\n     'Regex': 'val_f1_score: ([0-9\\\\.]+)'\n    }]\n```", "```py\n    objective_metric_name = 'val_acc'\n    objective_type = 'Maximize'\n    metric_definitions = [\n        {'Name': 'val_f1', \n         'Regex': 'val_f1_score: ([0-9\\\\.]+)'},\n        {'Name': 'val_acc', \n         'Regex': 'val_accuracy: ([0-9\\\\.]+)'}\n    ]\n    ```", "```py\n    from sagemaker.tuner import ContinuousParameter, IntegerParameter\n    hyperparameter_ranges = {\n        'learning_rate': ContinuousParameter(0.001, 0.2, \n                         scaling_type='Logarithmic'), \n        'batch-size': IntegerParameter(32,512)\n    }\n    ```", "```py\n    tuner = HyperparameterTuner(\n        tf_estimator,\n        objective_metric_name,\n        hyperparameter_ranges,                          \n        metric_definitions=metric_definitions,\n        objective_type=objective_type,\n        max_jobs=20,\n        max_parallel_jobs=2,\n        early_stopping_type='Auto')\n    ```", "```py\n    from sagemaker.analytics import HyperparameterTuningJobAnalytics\n    exp = HyperparameterTuningJobAnalytics(\n       hyperparameter_tuning_job_name=\n       tuner.latest_tuning_job.name)\n    jobs = exp.dataframe()\n    jobs.sort_values('FinalObjectiveValue', ascending=0)\n    ```", "```py\n    from tensorflow.keras.callbacks import EarlyStopping\n    early_stopping = EarlyStopping(\n        monitor='val_accuracy',\n        min_delta=0.0001,\n        patience=10,\n        verbose=1,\n        mode='auto')\n    ```", "```py\n    from tensorflow.keras.callbacks import Callback\n    class LogBestMetric(Callback):\n        def on_train_begin(self, logs={}):\n            self.val_accuracy = []\n        def on_train_end(self, logs={}):\n            print(\"Best val_accuracy:\",\n                  max(self.val_accuracy))\n        def on_epoch_end(self, batch, logs={}):\n            self.val_accuracy.append(\n                logs.get('val_accuracy'))\n            best_val_metric = LogBestMetric()\n    ```", "```py\n    model.fit(. . . \n        callbacks=[checkpointer, early_stopping, \n                   best_val_metric])\n    ```", "```py\n    Epoch 00048: early stopping\n    Best val_accuracy: 0.9259\n    ```", "```py\n    objective_metric_name = 'val_acc'\n    objective_type = 'Maximize'\n    metric_definitions = [\n        {'Name': 'val_acc', \n         'Regex': 'Best val_accuracy: ([0-9\\\\.]+)'}\n    ]\n    ```", "```py\n    parser.add_argument(\n        '--filters1', type=int, default=64)\n    parser.add_argument(\n        '--filters2', type=int, default=64)\n    parser.add_argument(\n        '--dropout-conv', type=float, default=0.2)\n    parser.add_argument(\n        '--dropout-fc', type=float, default=0.2)\n    ```", "```py\n    from sagemaker.tuner import ContinuousParameter, \n                                IntegerParameter\n    hyperparameter_ranges = {\n        learning-rate': ContinuousParameter(0.01, 0.14), \n        'batch-size': IntegerParameter(130,160),\n        'filters1': IntegerParameter(16,256),\n        'filters2': IntegerParameter(16,256),\n        'dropout-conv': ContinuousParameter(0.001,0.5, \n                        scaling_type='Logarithmic'),\n        'dropout-fc': ContinuousParameter(0.001,0.5, \n                      scaling_type='Logarithmic')\n    }\n    ```", "```py\n    from sagemaker.debugger import rule_configs, Rule\n    xgb_estimator = Estimator(container,\n      role=sagemaker.get_execution_role(),\n      instance_count=1,\n      instance_type='ml.m5.large',\n      output_path='s3://{}/{}/output'.format(bucket, prefix),\n      rules=[\n        Rule.sagemaker(rule_configs.overtraining()),\n        Rule.sagemaker(rule_configs.overfit())\n      ]\n    )\n    ```", "```py\n    xgb_estimator.set_hyperparameters(\n      objective='reg:linear', num_round=100)\n    xgb_estimator.fit(xgb_data, wait=False)\n    ```", "```py\n    description = xgb_estimator.latest_training_job.rule_job_summary()\n    for rule in description:\n      rule.pop('LastModifiedTime')\n      rule.pop('RuleEvaluationJobArn')\n      print(rule)\n    ```", "```py\n    {'RuleConfigurationName': 'Overtraining',  \n     'RuleEvaluationStatus': 'InProgress'}\n    {'RuleConfigurationName': 'Overfit', \n     'RuleEvaluationStatus': 'InProgress'}\n    ```", "```py\n    {'RuleConfigurationName': 'Overtraining',\n     'RuleEvaluationStatus': 'NoIssuesFound'}\n    {'RuleConfigurationName': 'Overfit', \n     'RuleEvaluationStatus': 'NoIssuesFound'}\n    ```", "```py\n    from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n    save_interval = '1'\n    xgb_estimator = Estimator(container,\n        role=role,\n        instance_count=1,\n        instance_type='ml.m5.large',\n        output_path='s3://{}/{}/output'.format(bucket,  \n                                               prefix),\n\n        debugger_hook_config=DebuggerHookConfig(                \n            s3_output_path=\n            's3://{}/{}/debug'.format(bucket,prefix),\n          collection_configs=[\n            CollectionConfig(name='metrics',\n              parameters={\"save_interval\": \n                          save_interval}),\n            CollectionConfig(name='average_shap',  \n              parameters={\"save_interval\": \n                          save_interval}),\n            CollectionConfig(name='feature_importance', \n              parameters={\"save_interval\": save_interval})\n          ]\n        )\n    )\n    ```", "```py\n    from smdebug.trials import create_trial\n    s3_output_path = xgb_estimator.latest_job_debugger_artifacts_path()\n    trial = create_trial(s3_output_path)\n    ```", "```py\n    trial.tensor_names()\n    ['average_shap/f0','average_shap/f1','average_shap/f10', … \n     'feature_importance/cover/f0','feature_importance/cover/f1',…\n     'train-rmse','validation-rmse']\n    ```", "```py\n    trial.tensor_names(collection=\"metrics\")\n    ['train-rmse', 'validation-rmse']\n    ```", "```py\n    def plot_features(tensor_prefix):\n        num_features = len(dataset.columns)-1\n        for i in range(0,num_features):\n        feature = tensor_prefix+'/f'+str(i)\n        steps = trial.tensor(feature).steps()\n        v = [trial.tensor(feature).value(s) for s in steps]\n        plt.plot(steps, v, label=dataset.columns[i+1])\n        plt.autoscale()\n        plt.title(tensor_prefix)\n        plt.legend(loc='upper left')\n        plt.show()\n    ```", "```py\n    plot_features('average_shap')\n    ```", "```py\n    plot_features('feature_importance/weight')\n    ```", "```py\n    tf.compat.v1.disable_eager_execution()\n    ```", "```py\n    from sagemaker.tensorflow import TensorFlow\n    from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n    save_interval = '100'\n    tf_estimator = TensorFlow(entry_point='fmnist-5.py',\n        role=role,\n        instance_count=1,\n        instance_type='ml.p3.2xlarge',\n        framework_version='2.1.0', \n        py_version='py3',\n        hyperparameters={'epochs': 30},\n        output_path=output_path,\n        use_spot_instances=True,\n        max_run=3600,\n        max_wait=7200,\n    ```", "```py\n        rules=[      \n    Rule.sagemaker(\n        rule_configs.poor_weight_initialization()), \n    Rule.sagemaker(\n        rule_configs.dead_relu()),\n    Rule.sagemaker(\n        rule_configs.check_input_images(), \n        rule_parameters={\"channel\": '3'})\n        ],\n    ```", "```py\n        debugger_hook_config=DebuggerHookConfig(                \n            s3_output_path='s3://{}/{}/debug'\n                   .format(bucket, prefix),\n            collection_configs=[\n                CollectionConfig(name='metrics',  \n                    parameters={\"save_interval\": \n                                save_interval}),\n                CollectionConfig(name='losses', \n                    parameters={\"save_interval\": \n                                save_interval}),\n                CollectionConfig(name='outputs', \n                    parameters={\"save_interval\": \n                                save_interval}),\n                CollectionConfig(name='weights', \n                    parameters={\"save_interval\": \n                                save_interval}),\n                CollectionConfig(name='gradients', \n                    parameters={\"save_interval\": \n                                save_interval})\n            ],\n        )\n    )\n    ```", "```py\n    ********* Debugger Rule Status *********\n    *\n    * PoorWeightInitialization: InProgress        \n    * DeadRelu: InProgress        \n    * CheckInputImages: InProgress        \n    *\n    ****************************************\n    ```", "```py\n    description = tf_estimator.latest_training_job.rule_job_summary()\n    for rule in description:\n        rule.pop('LastModifiedTime')\n        rule.pop('RuleEvaluationJobArn')\n        print(rule)\n    {'RuleConfigurationName': 'PoorWeightInitialization', \n     'RuleEvaluationStatus': 'NoIssuesFound'}\n    {'RuleConfigurationName': 'DeadRelu',\n     'RuleEvaluationStatus': 'NoIssuesFound'}\n    {'RuleConfigurationName': 'CheckInputImages', \n     'RuleEvaluationStatus': 'NoIssuesFound'}\n    ```", "```py\n    from smdebug.trials import create_trial\n    s3_output_path = tf_estimator.latest_job_debugger_artifacts_path()\n    trial = create_trial(s3_output_path)\n    ```", "```py\n    w = trial.tensor('conv2d/weights/conv2d/kernel:0')\n    g = trial.tensor(\n    'training/Adam/gradients/gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter:0')\n    print(w.value(0).shape)\n    print(g.value(0).shape)\n    (3, 3, 1, 64)\n    (3, 3, 1, 64)\n    ```", "```py\n    plot_conv_filter('conv2d/weights/conv2d/kernel:0', 63)\n    ```", "```py\n    fs_output_dir = '/opt/ml/processing/output/fs/'\n    os.makedirs(fs_output_dir, exist_ok=True)\n    fs_output_path = os.path.join(fs_output_dir, 'fs_data.tsv')  \n    data.to_csv(fs_output_path, index=False,header=True, sep='\\t')\n    ```", "```py\n    s3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2021-07-05-07-54-15-145/output/bt_data\n    s3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2021-07-05-07-54-15-145/output/fs_data\n    ```", "```py\n    fs_training_output_path = 's3://sagemaker-us-east-1-123456789012/sagemaker-scikit-learn-2021-07-05-07-54-15-145/output/fs_data/fs_data.tsv'\n    data = pd.read_csv(fs_training_output_path, sep='\\t',\n                       error_bad_lines=False, dtype='str')\n    data.head()\n    ```", "```py\n    from sagemaker.feature_store.feature_group import FeatureGroup\n    feature_group_name = 'amazon-reviews-feature-group-' + strftime('%d-%H-%M-%S', gmtime())\n    feature_group = FeatureGroup(\n        name=feature_group_name,    \n        sagemaker_session=feature_store_session)\n    ```", "```py\n    record_identifier_feature_name = 'review_id'\n    ```", "```py\n    event_time_feature_name = 'event_time'\n    current_time_sec = int(round(time.time()))\n    data = data.assign(event_time=current_time_sec)\n    ```", "```py\n    data['review_id'] = data['review_id']\n        .astype('str').astype('string')\n    data['product_id'] = data['product_id']\n        .astype('str').astype('string')\n    data['review_body'] = data['review_body']\n        .astype('str').astype('string')\n    data['label'] = data['label']\n        .astype('str').astype('string')\n    data['star_rating'] = data['star_rating']\n        .astype('int64')\n    data['event_time'] = data['event_time']\n        .astype('float64')\n    ```", "```py\n    feature_group.load_feature_definitions(\n        data_frame=data)\n    ```", "```py\n    feature_group.create(\n      role_arn=role,\n      s3_uri='s3://{}/{}'.format(default_bucket, prefix),\n      enable_online_store=True,\n      record_identifier_name=\n          record_identifier_feature_name,\n      event_time_feature_name=\n          event_time_feature_name,\n      description=\"1.8M+ tokenized camera reviews from the   \n                   Amazon Customer Reviews dataset\",\n      tags=[\n          { 'Key': 'Dataset', \n            'Value': 'amazon customer reviews' },\n          { 'Key': 'Subset',\n            'Value': 'cameras' },\n          { 'Key': 'Owner',\n            'Value': 'Julien Simon' }\n      ])\n    ```", "```py\nfeature_group.ingest(data_frame=data, max_workers=10, \n                     wait=True)\n```", "```py\n    query_string = \n    'SELECT label,review_body FROM \n    \"'+ feature_group_table +'\"'\n    + ' INNER JOIN (\n          SELECT product_id FROM (\n              SELECT product_id, avg(star_rating) as  \n                     avg_rating, count(*) as review_count\n              FROM \"'+ feature_group_table+ '\"' + '\n              GROUP BY product_id) \n          WHERE review_count > 1000) tmp \n    ON \"'+feature_group_table+'\"'\n    + '.product_id=tmp.product_id;'\n    ```", "```py\n    dataset = pd.DataFrame()\n    feature_group_query.run(query_string=query_string, output_location='s3://'+default_bucket+'/query_results/')\n    feature_group_query.wait()dataset = feature_group_query.as_dataframe()\n    dataset.head()\n    ```", "```py\n    from sagemaker import clarify\n    clarify_processor = clarify.SageMakerClarifyProcessor(\n        role=role,\n        instance_count=1,\n        instance_type='ml.m5.large',\n        sagemaker_session=session)\n    ```", "```py\n    bias_report_output_path = 's3://{}/{}/clarify-bias'.format(bucket, prefix)\n    data_config = clarify.DataConfig(\n        s3_data_input_path=train_uri,\n        s3_output_path=bias_report_output_path,\n        label='Label',\n        headers=train_data.columns.to_list(),\n        dataset_type='text/csv')\n    ```", "```py\n    model_config = clarify.ModelConfig(\n        model_name=xgb_predictor.endpoint_name,\n        instance_type='ml.t2.medium',\n        instance_count=1,\n        accept_type='text/csv')\n    ```", "```py\n    bias_config = clarify.BiasConfig(\n        label_values_or_threshold=[1],\n        facet_name='Sex_',\n        facet_values_or_threshold=[1])\n    ```", "```py\nclarify_processor.run_bias(\n    data_config=data_config,\n    model_config=model_config,\n    bias_config=bias_config)\n```", "```py\n    explainability_output_path = 's3://{}/{}/clarify-explainability.format(bucket, prefix)\n    data_config = clarify.DataConfig(\n        s3_data_input_path=train_uri,\n        s3_output_path= explainability_output_path,\n        label='Label',\n        headers=train_data.columns.to_list(),\n        dataset_type='text/csv')\n    ```", "```py\n    shap_config = clarify.SHAPConfig(\n        baseline=test_no_labels_uri,\n        num_samples=2*86+2048,\n        agg_method='mean_abs',\n        save_local_shap_values=True\n    )\n    ```", "```py\n    clarify_processor.run_explainability(\n        data_config=explainability_data_config,\n        model_config=model_config,\n        explainability_config=shap_config\n    )\n    ```", "```py\n    female_male_not_50k_count = train_data['Sex_'].where(\n        train_data['Label']==0).value_counts()\n    female_male_50k_count = train_data['Sex_'].where(\n        train_data['Label']==1).value_counts()\n    ratios = female_male_50k_count / \n             female_male_not_50k_count\n    print(ratios)\n    ```", "```py\n    0.0    0.457002\n    1.0    0.128281\n    ```", "```py\n    from imblearn.over_sampling import SMOTE\n    female_instances = train_data[train_data['Sex_']==1]\n    female_X = female_instances.drop(['Label'], axis=1)\n    female_Y = female_instances['Label']\n    oversample = SMOTE(sampling_strategy=ratios[0])\n    balanced_female_X, balanced_female_Y = oversample.fit_resample(female_X, female_Y)\n    balanced_female=pd.concat([balanced_female_X, balanced_female_Y], axis=1)\n    ```", "```py\n    male_instances = train_data[train_data['Sex_']==0]\n    balanced_train_data=pd.concat(\n        [male_instances, balanced_female], axis=0)\n    ```", "```py\n    from imblearn.under_sampling import RandomUnderSampler\n    X = balanced_train_data.drop(['Sex_'], axis=1)\n    Y = balanced_train_data['Sex_']\n    undersample = RandomUnderSampler(\n        sampling_strategy='not minority')\n    X,Y = undersample.fit_resample(X, Y)\n    balanced_train_data=pd.concat([X, Y], axis=1)\n    ```", "```py\n    female_male_count= balanced_train_data['Sex_']    \n        .value_counts()\n    female_male_50k_count = balanced_train_data['Sex_']\n        .where(balanced_train_data['Label']==1)\n        .value_counts()\n    ratios = female_male_50k_count/female_male_count\n    print(female_male_count)\n    print(female_male_50k_count)\n    print(ratios)\n    ```", "```py\n    1.0    0.313620\n    0.0    0.312039\n    ```"]
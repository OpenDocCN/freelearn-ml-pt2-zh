<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Getting Started with Ensemble Learning</h1>
                </header>
            
            <article>
                
<p> Ensemble learning involves a combination of techniques that allows multiple machine learning models, called base learners (or, sometimes, weak learners), to consolidate their predictions and output a single, optimal prediction, given their respective inputs and outputs.</p>
<p>In this chapter, we will give an overview of the main problems that ensembles try to solve, namely, bias and variance, as well as the relationship between them. This will help us understand the motivation behind identifying the root cause of an under-performing model and using an ensemble to address it. Furthermore, we will go over the basic categories of the methodologies available, as well as the difficulties we can expect to encounter when implementing ensembles.</p>
<p>The main topics covered in this chapter are the following:</p>
<ul>
<li>Bias, variance, and the trade-off between the two</li>
<li>The motivation behind using ensemble learning</li>
<li>Identifying the root cause of an under-performing model</li>
<li>Ensemble learning methods</li>
<li>Difficulties in applying ensemble learning successfully</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter02</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2JKkWYS">http://bit.ly/2JKkWYS</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bias, variance, and the trade-off</h1>
                </header>
            
            <article>
                
<p>Machine learning models are not perfect; they are prone to a number of errors. The two most common sources of errors are bias and variance. Although two distinct problems, they are interconnected and relate to a model's available degree of freedom or complexity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is bias?</h1>
                </header>
            
            <article>
                
<p>Bias refers to the inability of a method to correctly estimate the target. This does not only apply to machine learning. For example, in statistics, if we want to measure a population's average and do not sample carefully, the estimated average will be biased. In simple terms, the method's (sampling) estimation will not closely match the actual target (average).</p>
<p>In machine learning, bias refers to the difference between the expected prediction and its target. Biased models cannot properly fit the training data, resulting in poor in-sample performance and out-of-sample performance. A good example of a biased model arises when we try to fit a sine function with a simple linear regression. The model cannot fit the sine function, as it lacks the required complexity to do so. Thus, it will not be able to perform well in-sample or out-of-sample. This problem is called underfitting. A graphical example is illustrated in the following f<span>igure</span> :</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-578 image-border" src="assets/7a3d6234-f46c-4cad-8796-7ab8628778b7.png" style="width:30.67em;height:22.92em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"> A biased linear regression model for sine function data</div>
<p>The mathematical formula for bias is the difference between the target value and the expected prediction:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/65ccbf7c-cef5-44d5-9493-84f1cfd3b861.png" style="width:7.58em;height:1.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is variance?</h1>
                </header>
            
            <article>
                
<p>Variance refers to how much individuals vary within a group. Again, variance is a concept from statistics. Taking a sample from a population, variance indicates how much each individual's value differs from the mean.</p>
<p>In machine learning, variance refers to the model's variability or sensitivity to data changes. This means that high-variance models can generally fit the training data well and so achieve high in-sample performance, but perform poorly out-of-sample. This is due to the model's complexity. For example, a decision tree can have high variance if it creates a rule for every single instance in the training dataset. This is called <strong>overfitting</strong>. The following figure depicts a decision tree trained on the preceding dataset. Blue dots represent the training data and orange dots represent the test data.</p>
<p>As is evident, the model fits the training data perfectly but does not perform on the test data so well:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-579 image-border" src="assets/30604a53-1e14-4353-ad9b-39214f6d22b4.png" style="width:36.00em;height:27.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">A high-variance decision tree model on the sine function</div>
<p>The mathematical formula for variance is depicted as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/257b5bbd-0482-441b-a392-ba62d837f00c.png" style="width:13.58em;height:2.25em;"/></p>
<p>Essentially, this is the standard formula for population variance, assuming that our population is comprised of our models, as they have been produced by the machine learning algorithm. For example, as we saw earlier in <span class="cdp-organizer-chapter-number"><a href="57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml"/><a href="57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml"/><a href="57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml">Chapter 1</a>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">A Machine Learning Refresher</span></span></em>, neural networks can have different training outcomes, depending on their initial weights. If we consider all the neural networks with the same architecture, but different initial weights, by training them, we will have a population of different models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Trade-off</h1>
                </header>
            
            <article>
                
<p>Bias and variance are two of the three major components that comprise a model's error. The third is called the irreducible error and can be attributed to inherent randomness or variability in the data. The total error of a model can be decomposed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/78620b7b-b43f-4b03-a677-6c991c9c441f.png" style="width:23.08em;height:1.17em;"/></p>
<p>As we saw earlier, bias and variance stem from the same source: model complexity. While bias arises from too little complexity and freedom, variance thrives in complex models. Thus, it is not possible to reduce bias without increasing variance and vice versa. Nevertheless, there is an optimal point of complexity, where the error is minimized as bias and variance are at an optimal trade-off point. When the model's complexity is at this optimal point (the red dotted line in the next <span>f</span><span>igure</span>), then the model performs best both in-sample and out-of-sample. As is evident in the next figure, the error can never be reduced to zero.</p>
<p>Furthermore, although some may think that it is better to reduce the bias, even at the cost of increased variance, it is clear that the model would not perform better, even if it was unbiased, due to the error that variance inevitably induces:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b59162a8-d7de-44fe-8b33-b599478bb9d8.png" style="width:35.50em;height:26.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Bias-variance trade-off and its effect on the error</div>
<p>The following <span>figure</span> depicts the perfect model, with a minimum amount of combined bias and variance, or reducible error. Although the model does not fit the data perfectly, this is due to noise that is inherent in the dataset. If we try to fit the training data better, we will induce overfitting (variance). If we try to simplify the model further, we will induce underfitting (bias):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-581 image-border" src="assets/30ecba78-5dd9-4246-83f7-365dc7db0a82.png" style="width:28.58em;height:21.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Perfect model for our data, a sine function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble learning</h1>
                </header>
            
            <article>
                
<p>Ensemble learning involves a collection of machine learning methods aimed at improving the predictive performance of algorithms by combining many models. We will analyze the motivation behind using such methods to solve problems that arise from high bias and variance. Furthermore, we will present methods that allow the identification of bias and variance in machine learning models, as well as basic classes of ensemble learning methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p>Ensemble learning aims to solve the problems of bias and variance. By combining many models, we can reduce the ensemble's error, while retaining the individual models' complexities. As we saw earlier, there is a certain lower limit imposed on each model error, which is related to the model complexity.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Furthermore, we mentioned that the same algorithm can produce different models, due to the initial conditions, hyperparameters, and other factors. By combining different, diverse models, we can reduce the expected error of the group, while each individual model remains unchanged. This is due to statistics, rather than pure learning.</p>
<p>In order to better demonstrate this, let's consider an ensemble of 11 base learners for a classification, each with a probability of misclassification (error) equal to <em>err</em>=0.15 or 15%. Now, we want to create a simple ensemble. We always assume that the output of most base learners is the correct answer. Assuming that they are diverse (in statistics, uncorrelated), the probability that the majority of them is wrong is 0.26%:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/83545e56-347d-450e-82b7-46f736d3c1f0.png" style="width:20.33em;height:3.75em;"/></p>
<p>As is evident, the more base learners we add to the ensemble, the more accurate the ensemble will be, under the condition that each learner is uncorrelated to the others. Of course, this is increasingly difficult to achieve. Furthermore, the law of diminishing returns applies. Each new uncorrelated base learner contributes less to the overall error reduction than the previously added base learner. The following <span>f</span><span>igure</span><span> </span>shows the ensemble error percentage for a number of uncorrelated base learners. As is evident, the greatest reduction is applied when we add two uncorrelated base learners:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-582 image-border" src="assets/4da2f13b-6bd6-4d1d-915f-6a2600b83d12.png" style="width:31.17em;height:23.25em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">The relation between the number of base learners and the ensemble error</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying bias and variance</h1>
                </header>
            
            <article>
                
<p>Although bias and variance have theoretical formulas, it is difficult to calculate their actual values. A simple way to estimate them empirically is with learning and validation curves.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Validation curves</h1>
                </header>
            
            <article>
                
<p>Validation curves refer to an algorithm's achieved performance, given different hyperparameters. For each hyperparameter value, we perform k-fold cross validations and store the in-sample performance and out-of-sample performance. We then calculate and plot the mean and standard deviation of in-sample and out-of-sample performance for each hyperparameter value. By examining the relative and absolute performance, we can gauge the level of bias and variance in our model.</p>
<p>Borrowing the <kbd>KNeighborsClassifier</kbd> example from <span class="cdp-organizer-chapter-number"><a href="https://cdp.packtpub.com/hands_on_ensemble_learning_with_python/wp-admin/post.php?post=25&amp;action=edit#post_24">Chapter 1</a>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">A Machine Learning Refresher</span></span></em>, we modify it in order to experiment with different neighbor numbers. We start by loading the required libraries and data. Notice that we import <kbd>validation_curve</kbd> from <kbd>sklearn.model_selection</kbd>. This is scikit-learn's own implementation of validation curves:</p>
<pre class="mce-root"># --- SECTION 1 ---<br/># Libraries and data loading<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import validation_curve<br/>from sklearn.neighbors import KNeighborsClassifier<br/>bc = load_breast_cancer()</pre>
<p>Next, we define our features and targets (<kbd>x</kbd> and <kbd>y</kbd>), as well as our base learner. Furthermore, we define our parameter search space with <kbd>param_range = [2,3,4,5]</kbd> and use <kbd>validation_curve</kbd>. In order to use it, we must define our base learner, our features, targets, the parameter's name that we wish to test, as well as the parameter's values to test. Furthermore, we define the cross-validation's K folds with <kbd>cv=10</kbd>, as well as the metric that we wish to calculate, with <kbd>scoring="accuracy"</kbd>:</p>
<pre># --- SECTION 2 ---<br/># Create in-sample and out-of-sample scores<br/>x, y = bc.data, bc.target<br/>learner = KNeighborsClassifier()<br/>param_range = [2,3,4,5]<br/>train_scores, test_scores = validation_curve(learner, x, y,<br/>                                             param_name='n_neighbors',<br/>                                             param_range=param_range,<br/>                                             cv=10,<br/>                                             scoring="accuracy")</pre>
<p>Afterward,we calculate the mean and standard deviation for both in-sample performance (<kbd>train_scores</kbd>) as well as out-of-sample performance (<kbd>test_scores</kbd>):</p>
<pre># --- SECTION 3 ---<br/># Calculate the average and standard deviation for each hyperparameter<br/>train_scores_mean = np.mean(train_scores, axis=1)<br/>train_scores_std = np.std(train_scores, axis=1)<br/>test_scores_mean = np.mean(test_scores, axis=1)<br/>test_scores_std = np.std(test_scores, axis=1)</pre>
<p>Finally, we plot the means and deviations. We plot the means as curves, using <kbd>plt.plot</kbd>. In order to plot the standard deviations, we create a transparent rectangle surrounding the curves, with a width equal to the standard deviation at each hyperparameter value point. This is achieved with the use of <kbd>plt.fill_between</kbd>, by passing the value points as the first parameter, the lowest rectangle's point as the second parameter, and the highest point as the third. Furthermore, <kbd>alpha=0.1</kbd> instructs <kbd>matplotlib</kbd> to make the rectangle transparent (combining the rectangle's color with the background in a 10%-90% ratio, respectively): </p>
<div class="packt_infobox">Sections 3 and 4 are adapted from the scikit-learn examples found <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html">https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html</a>.</div>
<pre># --- SECTION 4 ---<br/># Plot the scores<br/>plt.figure()<br/>plt.title('Validation curves')<br/># Plot the standard deviations<br/>plt.fill_between(param_range, train_scores_mean - train_scores_std,<br/>                 train_scores_mean + train_scores_std, alpha=0.1,<br/>                 color="C1")<br/>plt.fill_between(param_range, test_scores_mean - test_scores_std,<br/>                 test_scores_mean + test_scores_std, alpha=0.1, color="C0")<br/><br/># Plot the means<br/>plt.plot(param_range, train_scores_mean, 'o-', color="C1",<br/>         label="Training score")<br/>plt.plot(param_range, test_scores_mean, 'o-', color="C0",<br/>         label="Cross-validation score")<br/>plt.xticks(param_range)<br/>plt.xlabel('Number of neighbors')<br/>plt.ylabel('Accuracy')<br/>plt.legend(loc="best")<br/>plt.show()<br/></pre>
<p>The script finally outputs the following. As the curves close the distance between them, the variance generally reduces. The further away they both are from the desired accuracy (taking into account the irreducible error), the bias increases.</p>
<p>Furthermore, the relative standard deviations are also an indicator of variance:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-583 image-border" src="assets/2029909d-0236-464d-b286-61b25feaed3b.png" style="width:30.67em;height:22.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Validation curves for K-Nearest-Neighbors, 2 to 5 neighbor</div>
<p>The following table presents the bias and variance identification based on validation curves:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p> </p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Great</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Small</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Distance between curves</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>High Variance</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Low Variance</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Distance from desired accuracy</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>High Bias</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Low Bias</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Relative rectangle area ratio</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>High Variance</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Low Variance</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref">Bias and variance identification based on validation curves</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning curves</h1>
                </header>
            
            <article>
                
<p>Another way to identify bias and variance is to generate learning curves. Like validation curves, we generate a number of in-sample and out-of-sample performance statistics with cross-validation. Instead of experimenting with different hyperparameter values, we utilize different amounts of training data. Again, by examining the means and standard deviations of in-sample and out-of-sample performance, we can get an idea about the amount of bias and variance inherent in our models.</p>
<p>Scikit-learn implements learning curves in the <kbd>sklearn.model_selection</kbd> module as <kbd>learning_curve</kbd>. Once again, we will use the <kbd>KNeighborsClassifier</kbd> example from <span class="cdp-organizer-chapter-number"><a href="57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml">Chapter 1</a>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">A Machine Learning Refresher</span></span></em>. First, we import the required libraries and load the breast cancer dataset:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import learning_curve<br/>bc = load_breast_cancer()</pre>
<p>Following that, we define the amount of training instances that will be used at each cross-validation set with <kbd>train_sizes = [50, 100, 150, 200, 250, 300]</kbd>, instantiate the base learner, and call <kbd>learning_curve</kbd>. The function returns a tuple of the train set sizes, the in-sample performance scores, and out-of-sample performance scores. The function accepts the base learner, the dataset features and targets, and the train set sizes <span>as parameters</span><span> </span>in a list with <kbd>train_sizes=train_sizes</kbd> and the number of cross-validation folds with <kbd>cv=10</kbd>:</p>
<pre># --- SECTION 2 ---<br/># Create in-sample and out-of-sample scores<br/>x, y = bc.data, bc.target<br/>learner = KNeighborsClassifier()<br/>train_sizes = [50, 100, 150, 200, 250, 300]<br/>train_sizes, train_scores, test_scores = learning_curve(learner, x,                                 y,  train_sizes=train_sizes, cv=10)</pre>
<p>Again, we calculate the mean and standard deviation of in-sample and out-of-sample performance:</p>
<pre># --- SECTION 3 ---<br/># Calculate the average and standard deviation for each hyperparameter<br/>train_scores_mean = np.mean(train_scores, axis=1)<br/>train_scores_std = np.std(train_scores, axis=1)<br/>test_scores_mean = np.mean(test_scores, axis=1)<br/>test_scores_std = np.std(test_scores, axis=1)</pre>
<p>Finally, we plot the means and standard deviations as curves and rectangles, as we did before:</p>
<pre># --- SECTION 4 ---<br/># Plot the scores<br/>plt.figure()<br/>plt.title('Learning curves')<br/># Plot the standard deviations<br/>plt.fill_between(train_sizes, train_scores_mean - train_scores_std,<br/> train_scores_mean + train_scores_std, alpha=0.1,<br/> color="C1")<br/>plt.fill_between(train_sizes, test_scores_mean - test_scores_std,<br/> test_scores_mean + test_scores_std, alpha=0.1, color="C0")<br/><br/># Plot the means<br/>plt.plot(train_sizes, train_scores_mean, 'o-', color="C1",<br/> label="Training score")<br/>plt.plot(train_sizes, test_scores_mean, 'o-', color="C0",<br/> label="Cross-validation score")<br/><br/>plt.xticks(train_sizes)<br/>plt.xlabel('Size of training set (instances)')<br/>plt.ylabel('Accuracy')<br/>plt.legend(loc="best")<br/>plt.show()<br/></pre>
<p>The final output is depicted as follows. The model seems to reduce its variance for the first 200 training samples. After that, it seems that the means diverge, as well as the standard deviation of the cross-validation score increasing, thus indicating an increase in variance.</p>
<p>Note that, although both curves have above 90% accuracy for training sets with at least 150 instances, this does not imply low bias. Datasets that are highly separable (good quality data with low noise) tend to produce such curves—no matter what combination of algorithms and hyperparameters we choose. Moreover, noisy datasets (for example, instances with the same features that have different targets) will not be able to produce high accuracy models—no matter what techniques we use.</p>
<p class="mce-root"/>
<p>Thus, bias must be measured by comparing the learning and validation curves to a desired accuracy (one that is considered achievable, given the dataset quality), rather than its absolute value:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-584 image-border" src="assets/d1e5331a-a5fe-4ce5-bc55-42fedbddc08e.png" style="width:34.58em;height:25.83em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Learning curves for K-Nearest-Neighbors, 50 to 300 training instances</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble methods</h1>
                </header>
            
            <article>
                
<p>Ensemble methods are divided into two major classes or taxonomies: generative and non-generative methods. Non-generative methods are focused on combining the predictions of a set of pretrained models. These models are usually trained independently of one another, and the ensemble algorithm dictates how their predictions will be combined. Base classifiers are not affected by the fact that they exist in an ensemble.</p>
<p>In this book, we will cover two main non-generative methods: voting and stacking. Voting, as the name implies(see <span class="cdp-organizer-chapter-number"><a href="ad9aa66b-7b30-4779-8914-0ff58140b3e8.xhtml">Chapter 3</a>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Voting</span></span></em>), refers to techniques that allow models to vote in order to produce a single answer, similar to how individuals vote in national elections. The most popular (most voted for) answer is selected as the winner. <span class="cdp-organizer-chapter-number"><a href="49a05219-d6cb-4893-aaac-49280842b647.xhtml">Chapter 4</a>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Stacking</span></span></em>, on the other hand, refers to methods that utilize a model (the meta-learner) that learns how to best combine the base learner's predictions. Although stacking entails the generation of a new model, it does not affect the base learners, thus it is a non-generative method.</p>
<p class="mce-root"/>
<p>Generative methods, on the other hand, are able to generate and affect the base learners that they use. They can either tune their learning algorithm or the dataset used to train them, in order to ensure diversity and high model performance. Furthermore, some algorithms can induce randomness in models, in order to further enforce diversity.</p>
<p>The main generative methods that we will cover in this book are bagging, boosting, and random forests. Boosting is a technique mainly targeting biased models. Its main idea is to sequentially generate models, such that each new model addresses biases inherent in the previous models. Thus, by iteratively correcting previous errors, the final ensemble has a significantly lower bias. Bagging aims to reduce variance. The bagging algorithm resamples instances of the training dataset, creating many individual and diverse datasets, originating from the same dataset. Afterward, a separate model is trained on each sampled dataset, forcing diversity between the ensemble models. Finally, <span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Random Forests</span></span>, is similar to bagging, in that it resamples from the training dataset. Instead of sampling instances, it samples features, thus creating even more diverse trees, as features strongly correlated to the target may be absent in many trees.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Difficulties in ensemble learning</h1>
                </header>
            
            <article>
                
<p>Although ensemble learning can greatly increase the performance of machine learning models, it comes at a cost. There are difficulties and drawbacks in correctly implementing it. Some of these difficulties and drawbacks will now be discussed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weak or noisy data</h1>
                </header>
            
            <article>
                
<p>The most important ingredient of a successful model is the dataset. If the data contains noise or incomplete information, there is not a single machine learning technique that will generate a highly performant model.</p>
<p>Let's illustrate this with a simple example. Suppose we study populations (in the statistical sense) <span>of cars</span> and we gather data about the color, shape, and manufacturer. It is difficult to generate a very accurate model for either variable, as a lot of cars are the same color and shape but are made by a different manufacturer. The following table depicts this sample dataset.</p>
<p>The best any model can do is achieve 33% classification accuracy, as there are three viable choices for any given feature combination. Adding more features to the dataset can greatly improve the model's performance. Adding more models to an ensemble cannot improve performance:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Color</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Shape</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Manufacturer</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Black</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Sedan</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>BMW</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Black</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Sedan</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Audi</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Black</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Sedan</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Alfa Romeo</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Blue</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Hatchback</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Ford</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Blue</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Hatchback</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Opel</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Blue</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Hatchback</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Fiat</p>
</td>
</tr>
</tbody>
</table>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Car dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding interpretability</h1>
                </header>
            
            <article>
                
<p>By employing a large number of models, interpretability is greatly reduced. For example, a single decision tree can easily explain how it produced a prediction, by simply following the decisions made at each node. On the other hand, it is difficult to interpret why an ensemble of 1,000 trees predicted a single value. Moreover, depending on the ensemble method, there may be more to explain than the prediction process itself. How and why did the ensemble choose to train these specific models. Why did it not choose to train other models? Why did it not choose to train more models?</p>
<p>When the model's results are to be presented to an audience, especially a not-so-highly-technical audience, simpler but more easily explainable models may be a better solution.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Furthermore, when the prediction must also include a probability (or confidence level), some ensemble methods (such as boosting) tend to deliver poor probability estimates:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-585 image-border" src="assets/ccb77e87-ebd1-47a0-b835-3e054f2bb8f3.png" style="width:42.25em;height:33.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Interpretability of a single tree versus a 1000</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computational cost</h1>
                </header>
            
            <article>
                
<p>Another drawback of ensembles is the computational cost they impose. Training a single neural network is computationally expensive. Training a 1000 of them requires a 1000 times more computational resources. Furthermore, some methods are sequential by nature. This means that it is not possible to harness the power of distributed computing. Instead, each new model must be trained when the previous model is completed. This imposes time penalties on the model's development process, on top of the increased computational cost.</p>
<p>Computational costs do not only hinder the development process; when the ensemble is put into production, the inference time will suffer as well. If the ensemble consists of 1,000 models, then all of those models must be fed with new data, produce predictions, and then those predictions must be combined in order to produce the ensemble output. In latency-sensitive settings (financial exchanges, real-time systems, and so on), sub-millisecond execution times are expected, thus a few microseconds of added latency can make a huge difference.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing the right models</h1>
                </header>
            
            <article>
                
<p>Finally, the models that comprise the ensemble must possess certain characteristics. There is no point in creating any ensemble from a number of identical models. Generative methods may produce their own models, but the algorithm used as well as its initial hyperparameters are usually selected by the analyst. Furthermore, the model's achievable diversity depends on a number of factors, such as the size and quality of the dataset, and the learning algorithm itself.</p>
<p>A single model that is similar in behavior to the data-generating process will usually outperform any ensemble, both in terms of accuracy as well as latency. In our bias-variance example, the simple sine function will always outperform any ensemble, as the data is generated from the same function <span>with some added noise</span>. An ensemble of many linear regressions may be able to approximate the sine function, but it will always require more time to train and execute. Furthermore, it will not be able to generalize (predict out-of-sample) as well as the sine function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented the concepts of bias and variance, as well as the trade-off between them. They are essential in understanding how and why a model may under-perform, either in-sample or out-of-sample. We then introduced the concept and motivation of ensemble learning, how to identify bias and variance in models, as well as basic categories of ensemble learning methods. We presented ways to measure and plot bias and variance, using scikit-learn and matplotlib. Finally, we talked about the difficulties and drawbacks of implementing ensemble learning methods. Some key points to remember are the following.</p>
<p>High-bias models usually have difficulty performing well in-sample. This is also called <strong>underfitting</strong>. It is due to the model's simplicity (or lack of complexity). <span>High-variance models usually have difficulty generalizing or performing well out-of-sample, while they perform reasonably well in-sample. This is called</span> <strong>overfitting</strong><span>. It is usually due to the model's unnecessary complexity. The <strong>b</strong></span><span><strong>ias-variance trade-off</strong> refers to the fact that as the model's complexity increases, its bias decreases, while its variance increases. </span><span>Ensemble learning aims to address high bias or variance, by combining the predictions of many diverse models. These models are usually called</span> <strong>base-learners</strong><span>. For model selection, <strong>v</strong></span><span><strong>alidation curves</strong> indicate how a model performs in-sample and out-of-sample for a given set of hyperparameters. </span><span><strong>Learning curves</strong> are the same as validation curves but instead of a set of hyperparameters, they use</span> different train set sizes. <span>Substantial distance between the train and test curves indicates high variance.</span> A big rectangle area <span>around the test curve also indicates high variance.</span> A substantial distance between both curves from the target accuracy indicates high bias<span>. </span><span>Generative methods have control over the generation and training of their base learners; non-generative methods do not. </span><span>Ensemble learning can have a negligible or negative impact on performance when data is poor or models are correlated. It can impact negatively on the interpretability of models and the computational resources required.</span></p>
<p><span> </span>In the next chapter, we will present the Voting ensemble, as well as how to use it for both regression and classification problems.</p>


            </article>

            
        </section>
    </body></html>
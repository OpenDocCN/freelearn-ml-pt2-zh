<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Feature Extraction and Preprocessing"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Feature Extraction and Preprocessing</h1></div></div></div><p>The examples discussed in the previous chapter used simple numeric explanatory variables, such as the diameter of a pizza. Many machine learning problems require learning from observations of categorical variables, text, or images. In this chapter, you will learn basic techniques for preprocessing data and creating feature representations of these observations. These techniques can be used with the regression models discussed in <a class="link" href="ch02.html" title="Chapter 2. Linear Regression">Chapter 2</a>, <span class="emphasis"><em>Linear Regression</em></span>, as well as the models we will discuss in subsequent chapters.</p><div class="section" title="Extracting features from categorical variables"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Extracting features from categorical variables</h1></div></div></div><p>Many <a class="indexterm" id="id141"/>machine learning problems have <span class="strong"><strong>categorical</strong></span>, or <span class="strong"><strong>nominal</strong></span>, rather than continuous features. For example, an application <a class="indexterm" id="id142"/>that predicts a job's salary based on its description might use categorical features such as the job's location. Categorical variables are commonly encoded using <span class="strong"><strong>one-of-K</strong></span> or <span class="strong"><strong>one-hot</strong></span> encoding, in which the explanatory variable is encoded using one binary feature for each of the variable's possible values.</p><p>For example, let's assume that our model has a <code class="literal">city</code> explanatory variable that can take one of three values: <code class="literal">New York</code>, <code class="literal">San Francisco</code>, or <code class="literal">Chapel Hill</code>. One-hot encoding represents this explanatory variable using one binary feature for each of the three possible cities.</p><p>In scikit-learn, the <code class="literal">DictVectorizer</code> class <a class="indexterm" id="id143"/>can be used to one-hot encode categorical features:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.feature_extraction import DictVectorizer
&gt;&gt;&gt; onehot_encoder = DictVectorizer()
&gt;&gt;&gt; instances = [
&gt;&gt;&gt;     {'city': 'New York'},
&gt;&gt;&gt;     {'city': 'San Francisco'},
&gt;&gt;&gt;     {'city': 'Chapel Hill'}&gt;&gt;&gt; ]
&gt;&gt;&gt; print onehot_encoder.fit_transform(instances).toarray()
[[ 0.  1.  0.] [ 0.  0.  1.][ 1.  0.  0.]]</pre></div><p>Note that resulting features will not necessarily be ordered in the feature vector as they were encountered. In the first training example, the <code class="literal">city</code> feature's value is <code class="literal">New York</code>. The second element in the feature vectors corresponds to the <code class="literal">New York</code> value and is set to <code class="literal">1</code> for the first instance. It may seem intuitive to represent the values of a categorical explanatory variable with a single integer feature, but this would encode artificial information. For <a class="indexterm" id="id144"/>example, the feature vectors for the previous example would have only one dimension. <code class="literal">New York</code> could be represented by <code class="literal">0</code>, <code class="literal">San Francisco</code> by <code class="literal">1</code>, and <code class="literal">Chapel Hill</code> by <code class="literal">2</code>. This representation <a class="indexterm" id="id145"/>would encode an order for the values of the variable that does not exist in the real world; there is no natural order of cities.</p></div></div>
<div class="section" title="Extracting features from text"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec30"/>Extracting features from text</h1></div></div></div><p>Many <a class="indexterm" id="id146"/>machine learning problems use text as an explanatory variable. Text must be transformed to a different representation that encodes as much of its <a class="indexterm" id="id147"/>meaning as possible in a feature vector. In the following sections we will review variations of the most common representation of text that is used in machine learning: the bag-of-words model.</p><div class="section" title="The bag-of-words representation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec11"/>The bag-of-words representation</h2></div></div></div><p>The most <a class="indexterm" id="id148"/>common representation of text is the <span class="strong"><strong>bag-of-words</strong></span> model. This representation uses a multiset, or bag, that encodes the words that appear in a text; the bag-of-words does not encode any of the text's syntax, ignores the order of words, and disregards all grammar. Bag-of-words can be thought of as an extension to one-hot encoding. It creates one feature for each word of interest in the text. The bag-of-words model is motivated by the intuition that documents containing similar words often have similar meanings. The bag-of-words model can be used effectively for document classification and retrieval despite the limited information that it encodes.</p><p>A collection of documents is <a class="indexterm" id="id149"/>called a <span class="strong"><strong>corpus</strong></span>. Let's use a corpus with the following two documents to examine the bag-of-words model:</p><div class="informalexample"><pre class="programlisting">corpus = [
    'UNC played Duke in basketball',
    'Duke lost the basketball game'
]</pre></div><p>This corpus contains eight unique words: <code class="literal">UNC</code>, <code class="literal">played</code>, <code class="literal">Duke</code>, <code class="literal">in</code>, <code class="literal">basketball</code>, <code class="literal">lost</code>, <code class="literal">the</code>, and <code class="literal">game</code>. The corpus's unique words comprise its <span class="strong"><strong>vocabulary</strong></span>. The bag-of-words model uses a feature vector with an element for each of the words in the corpus's vocabulary to represent each document. Our corpus has eight unique words, so each document <a class="indexterm" id="id150"/>will be represented by a vector with eight elements. The number of <a class="indexterm" id="id151"/>elements that comprise a feature vector is called the vector's <span class="strong"><strong>dimension</strong></span>. A <span class="strong"><strong>dictionary </strong></span>
<a class="indexterm" id="id152"/>maps the vocabulary to indices in the feature vector.</p><p>In the most basic bag-of-words representation, each element in the feature vector is a binary value that represents whether or not the corresponding word appeared in the document. For example, the first word in the first document is <code class="literal">UNC</code>. The first word in the dictionary is <code class="literal">UNC</code>, so the first element in the vector is equal to one. The last word in the dictionary is <code class="literal">game</code>. The first document does not contain the word <code class="literal">game</code>, so the eighth element in its vector is <a class="indexterm" id="id153"/>set to <code class="literal">0</code>. The <code class="literal">CountVectorizer</code> class can produce a bag-of-words representation from a string or file. By default, <code class="literal">CountVectorizer</code> converts the characters in the documents to lowercase, and <span class="strong"><strong>tokenizes</strong></span> the documents. Tokenization is the process of splitting a <a class="indexterm" id="id154"/>string into <span class="strong"><strong>tokens</strong></span>,<span class="strong"><strong> </strong></span>or meaningful sequences of characters. Tokens frequently are words, but they may also be shorter sequences including punctuation characters and affixes. The <code class="literal">CountVectorizer</code> class tokenizes using a regular expression that splits strings on whitespace and extracts sequences of characters that are two or more characters in length.</p><p>The documents in our corpus are represented by the following feature vectors:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer
&gt;&gt;&gt; corpus = [
&gt;&gt;&gt;     'UNC played Duke in basketball',
&gt;&gt;&gt;     'Duke lost the basketball game'
&gt;&gt;&gt; ]
&gt;&gt;&gt; vectorizer = CountVectorizer()
&gt;&gt;&gt; print vectorizer.fit_transform(corpus).todense()
&gt;&gt;&gt; print vectorizer.vocabulary_
[[1 1 0 1 0 1 0 1]
 [1 1 1 0 1 0 1 0]]
{u'duke': 1, u'basketball': 0, u'lost': 4, u'played': 5, u'game': 2, u'unc': 7, u'in': 3, u'the': 6}</pre></div><p>Now let's add a third document to our corpus:</p><div class="informalexample"><pre class="programlisting">corpus = [
    'UNC played Duke in basketball',
    'Duke lost the basketball game',
    'I ate a sandwich'
]</pre></div><p>Our corpus's dictionary now contains the following ten unique words. Note that <code class="literal">I</code> and <code class="literal">a</code> were not extracted as they do not match the default regular expression that CountVectorizer uses to tokenize strings:</p><div class="informalexample"><pre class="programlisting">{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8} </pre></div><p>Now, our feature vectors are as follows:</p><div class="informalexample"><pre class="programlisting">UNC played Duke in basketball = [[0 1 1 0 1 0 1 0 0 1]]
Duke lost the basketball game = [[0 1 1 1 0 1 0 0 1 0]]
I ate a sandwich = [[1 0 0 0 0 0 0 1 0 0]]</pre></div><p>The meanings of the first two documents are more similar to each other than they are to the third document, and their corresponding feature vectors are more similar to each other than they are to the third document's feature vector when using a metric such as <span class="strong"><strong>Euclidean distance</strong></span>. The Euclidean <a class="indexterm" id="id155"/>distance between two vectors is equal to the <span class="strong"><strong>Euclidean norm</strong></span>, or L2 <a class="indexterm" id="id156"/>norm, of the difference between the two vectors:</p><div class="mediaobject"><img alt="The bag-of-words representation" src="graphics/8365OS_03_01.jpg"/></div><p>Recall that the Euclidean norm of a vector is equal to the vector's magnitude, which is given by the following equation:</p><div class="mediaobject"><img alt="The bag-of-words representation" src="graphics/8365OS_03_02.jpg"/></div><p>scikit-learn's <code class="literal">euclidean_distances</code> function can be used to calculate the distance between two or more vectors, and it confirms that the most semantically similar documents are also the closest to each other in space. In the following example, we will use the <code class="literal">euclidean_distances</code> function to compare the feature vectors for our documents:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.metrics.pairwise import euclidean_distances
&gt;&gt;&gt; counts = [
&gt;&gt;&gt;     [0, 1, 1, 0, 0, 1, 0, 1],
&gt;&gt;&gt;     [0, 1, 1, 1, 1, 0, 0, 0],
&gt;&gt;&gt;     [1, 0, 0, 0, 0, 0, 1, 0]
&gt;&gt;&gt; ]
&gt;&gt;&gt; print 'Distance between 1st and 2nd documents:', euclidean_distances(counts[0], counts[1])
&gt;&gt;&gt; print 'Distance between 1st and 3rd documents:', euclidean_distances(counts[0], counts[2])
&gt;&gt;&gt; print 'Distance between 2nd and 3rd documents:', euclidean_distances(counts[1], counts[2])
Distance between 1st and 2nd documents: [[ 2.]]
Distance between 1st and 3rd documents: [[ 2.44948974]]
Distance between 2nd and 3rd documents: [[ 2.44948974]]</pre></div><p>Now let's assume that we are using a corpus of news articles instead of our toy corpus. Our dictionary may now have hundreds of thousands of unique words instead of only twelve. The feature vectors representing the articles will each have hundreds of thousands of elements, and many of the elements will be zero. Most sports articles will not have any of the words particular to finance articles and most culture articles will not have any of the words particular to articles about <a class="indexterm" id="id157"/>finance. High-dimensional feature vectors that have many zero-valued elements are called <span class="strong"><strong>sparse vectors</strong></span>.</p><p>Using high-dimensional data creates several problems for all machine learning tasks, including those that do not involve text. The first problem is that high-dimensional vectors require more memory than smaller vectors. NumPy provides some data types that mitigate this problem by efficiently representing only the nonzero elements of sparse vectors.</p><p>The second problem is <a class="indexterm" id="id158"/>known as the <span class="strong"><strong>curse of dimensionality</strong></span>, or the <span class="strong"><strong>Hughes effect</strong></span>. As the feature space's dimensionality increases, more training data is required to <a class="indexterm" id="id159"/>ensure that there are enough training instances with each combination of the feature's values. If there are insufficient training instances for a feature, the algorithm may overfit noise in the training data and fail to generalize. In the following sections, we will review several strategies to reduce the dimensionality of text features. In <a class="link" href="ch07.html" title="Chapter 7. Dimensionality Reduction with PCA">Chapter 7</a>, <span class="emphasis"><em>Dimensionality Reduction with PCA</em></span>, we will review techniques for numerical dimensionality reduction.</p></div><div class="section" title="Stop-word filtering"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec12"/>Stop-word filtering</h2></div></div></div><p>A basic strategy to reduce the dimensions of the feature space is to convert all of the text to lowercase. This is <a class="indexterm" id="id160"/>motivated by the insight that the letter case does not contribute to the meanings of most words; <code class="literal">sandwich</code> and <code class="literal">Sandwich</code> have the same meaning in most contexts. Capitalization may indicate that a word is at the beginning of a sentence, but the bag-of-words model has already discarded all information from word order and grammar.</p><p>A second strategy is to remove words that are common to most of the documents in the corpus. These words, called <span class="strong"><strong>stop </strong></span>
<a class="indexterm" id="id161"/>
<span class="strong"><strong>words</strong></span>, include determiners such as <code class="literal">the</code>, <code class="literal">a</code>, and <code class="literal">an</code>; auxiliary verbs such as <code class="literal">do</code>, <code class="literal">be</code>, and <code class="literal">will</code>; and prepositions such as <code class="literal">on</code>, <code class="literal">around</code>, and <code class="literal">beneath</code>. Stop words are often functional words that contribute to the document's meaning through grammar rather than their denotations. The <code class="literal">CountVectorizer</code> class <a class="indexterm" id="id162"/>can filter stop words provided as the <code class="literal">stop_words</code> keyword argument and also includes a basic English stop list. Let's recreate the feature vectors for our documents using stop filtering:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer
&gt;&gt;&gt; corpus = [
&gt;&gt;&gt;     'UNC played Duke in basketball',
&gt;&gt;&gt;     'Duke lost the basketball game',
&gt;&gt;&gt;     'I ate a sandwich'
&gt;&gt;&gt; ]
&gt;&gt;&gt; vectorizer = CountVectorizer(stop_words='english')
&gt;&gt;&gt; print vectorizer.fit_transform(corpus).todense()
&gt;&gt;&gt; print vectorizer.vocabulary_
[[0 1 1 0 0 1 0 1]
 [0 1 1 1 1 0 0 0]
 [1 0 0 0 0 0 1 0]]
{u'duke': 2, u'basketball': 1, u'lost': 4, u'played': 5, u'game': 3, u'sandwich': 6, u'unc': 7, u'ate': 0}</pre></div><p>The feature vectors have <a class="indexterm" id="id163"/>now fewer dimensions, and the first two document vectors are still more similar to each other than they are to the third document.</p></div><div class="section" title="Stemming and lemmatization"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec13"/>Stemming and lemmatization</h2></div></div></div><p>While stop filtering is an easy strategy for dimensionality reduction, most stop lists contain only a few <a class="indexterm" id="id164"/>hundred words. A large corpus may still have hundreds of thousands of unique words after filtering. Two similar strategies for further reducing dimensionality are <a class="indexterm" id="id165"/>called <span class="strong"><strong>stemming</strong></span> and <span class="strong"><strong>lemmatization</strong></span>.</p><p>A high-dimensional document vector may separately encode several derived or inflected forms of the same word. For example, <code class="literal">jumping</code> and <code class="literal">jumps</code> are both forms of the word <code class="literal">jump</code>; a document vector in a corpus of long-jumping articles may encode each inflected form with a separate element in the feature vector. Stemming and lemmatization are two strategies to condense inflected and derived forms of a word into a single feature.</p><p>Let's consider another toy corpus with two documents:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer
&gt;&gt;&gt; corpus = [
&gt;&gt;&gt;     'He ate the sandwiches',
&gt;&gt;&gt;     'Every sandwich was eaten by him'
&gt;&gt;&gt; ]
&gt;&gt;&gt; vectorizer = CountVectorizer(binary=True, stop_words='english')
&gt;&gt;&gt; print vectorizer.fit_transform(corpus).todense()
&gt;&gt;&gt; print vectorizer.vocabulary_
[[1 0 0 1]
 [0 1 1 0]]
{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}</pre></div><p>The documents have similar meanings, but their feature vectors have no elements in common. Both documents contain a conjugation of <code class="literal">ate</code> and an inflected form of <code class="literal">sandwich</code>. Ideally, these similarities should be reflected in the feature vectors. Lemmatization is the process of determining <a class="indexterm" id="id166"/>the <span class="strong"><strong>lemma</strong></span>, or the morphological root, of an inflected word based on its context. Lemmas are the base forms of words that are used to key the word in a dictionary. <span class="strong"><strong>Stemming</strong></span> has a similar goal to lemmatization, but it does not <a class="indexterm" id="id167"/>attempt to produce the morphological roots of words. Instead, stemming removes all patterns of characters that appear to be affixes, resulting in a token <a class="indexterm" id="id168"/>that is not necessarily a valid word. Lemmatization frequently requires a lexical resource, like WordNet, and the word's part of speech. Stemming algorithms frequently use rules instead of lexical resources to produce stems and can operate on any token, even without its context.</p><p>Let's consider lemmatization of the word <code class="literal">gathering</code> in two documents:</p><div class="informalexample"><pre class="programlisting">corpus = [
    'I am gathering ingredients for the sandwich.',
    'There were many wizards at the gathering.'
]</pre></div><p>In the first sentence <code class="literal">gathering</code> is a verb, and its lemma is <code class="literal">gather</code>. In the second sentence <code class="literal">gathering</code> is <a class="indexterm" id="id169"/>a noun, and its lemma is <code class="literal">gathering</code>. We will use the <span class="strong"><strong>Natural Language Tool Kit</strong></span> (<span class="strong"><strong>NTLK</strong></span>) to stem and lemmatize the corpus. NLTK can be installed using the instructions at <a class="ulink" href="http://www.nltk.org/install.html">http://www.nltk.org/install.html</a>. After installation, execute the following code:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download()</pre></div><p>Then follow the instructions to download the corpora for NLTK. </p><p>Using the parts of speech of <code class="literal">gathering</code>, NLTK's <code class="literal">WordNetLemmatizer</code> correctly lemmatizes the words in both documents as shown in the following example:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem.wordnet import WordNetLemmatizer
&gt;&gt;&gt; lemmatizer = WordNetLemmatizer()
&gt;&gt;&gt; print lemmatizer.lemmatize('gathering', 'v')
&gt;&gt;&gt; print lemmatizer.lemmatize('gathering', 'n')
gather
gathering</pre></div><p>Let's compare lemmatization with stemming. The Porter stemmer cannot consider the inflected form's part of speech and returns <code class="literal">gather</code> for both documents:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; stemmer = PorterStemmer()
&gt;&gt;&gt; print stemmer.stem('gathering')
gather</pre></div><p>Now let's <a class="indexterm" id="id170"/>lemmatize our toy corpus:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk import word_tokenize
&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; from nltk.stem.wordnet import WordNetLemmatizer
&gt;&gt;&gt; from nltk import pos_tag
&gt;&gt;&gt; wordnet_tags = ['n', 'v']
&gt;&gt;&gt; corpus = [
&gt;&gt;&gt;     'He ate the sandwiches',
&gt;&gt;&gt;     'Every sandwich was eaten by him'
&gt;&gt;&gt; ]
&gt;&gt;&gt; stemmer = PorterStemmer()
&gt;&gt;&gt; print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus]
&gt;&gt;&gt; def lemmatize(token, tag):
&gt;&gt;&gt;     if tag[0].lower() in ['n', 'v']:
&gt;&gt;&gt;         return lemmatizer.lemmatize(token, tag[0].lower())
&gt;&gt;&gt;     return token
&gt;&gt;&gt; lemmatizer = WordNetLemmatizer()
&gt;&gt;&gt; tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]
&gt;&gt;&gt; print 'Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus]
Stemmed: [['He', 'ate', 'the', 'sandwich'], ['Everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]
Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]</pre></div><p>Through stemming and lemmatization, we reduced the dimensionality of our feature space. We produced feature representations that more effectively encode the meanings of the documents despite the fact that the words in the corpus's dictionary are inflected differently in the sentences.</p></div><div class="section" title="Extending bag-of-words with TF-IDF weights"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec14"/>Extending bag-of-words with TF-IDF weights</h2></div></div></div><p>In the <a class="indexterm" id="id171"/>previous section we used the bag-of-words representation to create feature vectors that encode whether or not a word from the corpus's dictionary appears in a document. These feature vectors do not encode grammar, word order, or the frequencies of words. It is intuitive that the frequency with <a class="indexterm" id="id172"/>which a word appears in a document could indicate the extent to which a document pertains to that word. A long document that contains one occurrence of a word may discuss an entirely different topic than a document that contains many occurrences of the same word. In this section, we will create feature vectors that encode the frequencies of words, and discuss strategies to mitigate two problems caused by encoding term frequencies.</p><p>Instead of using a binary value for each element in the feature vector, we will now use an integer that represents the number of times that the words appeared in the document.</p><p>We will use the following corpus. With stop word filtering, the corpus is represented by the following feature vector:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer
&gt;&gt;&gt; corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich']
&gt;&gt;&gt; vectorizer = CountVectorizer(stop_words='english')
&gt;&gt;&gt; print vectorizer.fit_transform(corpus).todense()
[[2 1 3 1 1]]
{u'sandwich': 2, u'wizard': 4, u'dog': 1, u'transfigured': 3, u'ate': 0}</pre></div><p>The element for <code class="literal">dog</code> is now set to <code class="literal">1</code> and the element for <code class="literal">sandwich</code> is set to <code class="literal">2</code> to indicate that the corresponding words occurred once and twice, respectively. Note that the <code class="literal">binary</code> keyword argument of <code class="literal">CountVectorizer</code> is omitted; its default value is <code class="literal">False</code>, which causes it to return raw term frequencies rather than binary frequencies. Encoding the terms' raw frequencies in the feature vector provides additional information about the meanings of the documents, but assumes that all of the documents are of similar lengths. </p><p>Many words might appear with the same frequency in two documents, but the documents could still be dissimilar if one document is many times larger than the other. scikit-learn's <code class="literal">TfdfTransformer</code> object can mitigate this problem by transforming a matrix of term frequency vectors into a matrix of normalized term frequency weights. By default, <code class="literal">TfdfTransformer</code> smoothes the raw counts and applies L2 normalization. The smoothed, normalized term frequencies are given by the following equation:</p><div class="mediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_03.jpg"/></div><p><span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_10.jpg"/></span>
 is the frequency of term <span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_16.jpg"/></span> in document <span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_13.jpg"/></span> and <span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_11.jpg"/></span> is the L2 norm of the count vector. In addition to <a class="indexterm" id="id173"/>normalizing raw term counts, we can improve our feature vectors by calculating <span class="strong"><strong>logarithmically scaled term frequencies</strong></span>, which scale the counts to a more limited range,<span class="strong"><strong> </strong></span>or <span class="strong"><strong>augmented term frequencies</strong></span>, which further mitigates the bias for longer documents. Logarithmically scaled term <a class="indexterm" id="id174"/>frequencies are given by the following equation:</p><div class="mediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_04.jpg"/></div><p>The<code class="literal"> TfdfTransformer</code> object calculates logarithmically scaled term frequencies when its <code class="literal">sublinear_tf</code> keyword argument is set to <code class="literal">True</code>. Augmented frequencies are given by the following equation:</p><div class="mediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_05.jpg"/></div><p><span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_12.jpg"/></span>
 is the greatest frequency of all of the words in document <span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_13.jpg"/></span>. scikit-learn 0.15.2 does not implement augmented term frequencies, but the output of <code class="literal">CountVectorizer</code> can be easily transformed.</p><p>Normalization, logarithmically scaled term frequencies, and augmented term frequencies can represent the frequencies of terms in a document while mitigating the effects of different document sizes. However, another problem remains with these representations. The feature vectors <a class="indexterm" id="id175"/>contain large weights for terms that occur frequently in a document, even if those terms occur frequently in most documents in the corpus. These terms do not help to represent the meaning of a <a class="indexterm" id="id176"/>particular document relative to the rest of the corpus. For example, most of the documents in a corpus of articles about Duke's basketball team could include the words <code class="literal">basketball</code>, <code class="literal">Coach K</code>, and <code class="literal">flop</code>. These words can be thought of as corpus-specific stop words and may not be useful to <a class="indexterm" id="id177"/>calculate the similarity of documents. The <span class="strong"><strong>inverse document frequency</strong></span> (<span class="strong"><strong>IDF</strong></span>) is a measure of how rare or common a word is in a corpus. The inverse document frequency is given by the following equation:</p><div class="mediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_06.jpg"/></div><p>Here, <span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_14.jpg"/></span> is the total number of documents in the corpus and <span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_15.jpg"/></span> is the number of documents in the corpus that contain the term <span class="inlinemediaobject"><img alt="Extending bag-of-words with TF-IDF weights" src="graphics/8365OS_03_16.jpg"/></span>. A term's <span class="strong"><strong>TF-IDF</strong></span> value is the <a class="indexterm" id="id178"/>product of its term frequency and inverse document frequency. <code class="literal">TfidfTransformer</code> returns TF-IDF's weight when its <code class="literal">use_idf</code> keyword argument is set to its default value, <code class="literal">True</code>. Since TF-IDF weighted feature vectors are commonly used to represent text, scikit-learn provides a <code class="literal">TfidfVectorizer</code> class that wraps <code class="literal">CountVectorizer</code> and <code class="literal">TfidfTransformer</code>. Let's use <code class="literal">TfidfVectorizer</code> to create TF-IDF weighted feature vectors for our corpus:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer
&gt;&gt;&gt; corpus = [
&gt;&gt;&gt;     'The dog ate a sandwich and I ate a sandwich',
&gt;&gt;&gt;     'The wizard transfigured a sandwich'
&gt;&gt;&gt; ]
&gt;&gt;&gt; vectorizer = TfidfVectorizer(stop_words='english')
&gt;&gt;&gt; print vectorizer.fit_transform(corpus).todense()
[[ 0.75458397  0.37729199  0.53689271  0.          0.        ]
 [ 0.          0.          0.44943642  0.6316672   0.6316672 ]]</pre></div><p>By comparing the TF-IDF weights to the raw term frequencies, we can see that words that are common to <a class="indexterm" id="id179"/>many of the documents in the <a class="indexterm" id="id180"/>corpus, such as <code class="literal">sandwich</code>, have been penalized.</p></div><div class="section" title="Space-efficient feature vectorizing with the hashing trick"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec15"/>Space-efficient feature vectorizing with the hashing trick</h2></div></div></div><p>In this <a class="indexterm" id="id181"/>chapter's previous examples, a dictionary containing all of the corpus's unique tokens is used to map a document's tokens to the elements of a feature vector. Creating this dictionary has two drawbacks. First, two passes are required over the corpus: the first pass is used to create the dictionary <a class="indexterm" id="id182"/>and the second pass is used to create feature vectors for the documents. Second, the dictionary must be stored in memory, which could be prohibitive for large corpora. It is possible to avoid creating this dictionary through applying a hash function to the token to determine its index in the feature vector directly. This shortcut is called the <span class="strong"><strong>hashing trick</strong></span>. The following example uses <code class="literal">HashingVectorizer</code> to demonstrate the hashing trick:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn.feature_extraction.text import HashingVectorizer
&gt;&gt;&gt; corpus = ['the', 'ate', 'bacon', 'cat']
&gt;&gt;&gt; vectorizer = HashingVectorizer(n_features=6)
&gt;&gt;&gt; print vectorizer.transform(corpus).todense()
[[-1.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  1.  0.  0.]
 [ 0.  0.  0.  0. -1.  0.]
 [ 0.  1.  0.  0.  0.  0.]]</pre></div><p>The hashing trick is stateless. It can be used to create feature vectors in both parallel and online, or streaming, applications because it does not require an initial pass over the corpus.. Note that <code class="literal">n_features</code> is an optional keyword argument. Its default value, <span class="inlinemediaobject"><img alt="Space-efficient feature vectorizing with the hashing trick" src="graphics/8365OS_03_17.jpg"/></span>, is adequate for most problems; it is set to <code class="literal">6</code> here so that the matrix will be small enough to print and still display all of the nonzero features. Also, note that some of the term frequencies are negative. Since hash collisions are possible, <code class="literal">HashingVectorizer</code> uses a signed hash function. The value of a feature takes the same sign as its token's hash; if the term <code class="literal">cats</code> appears twice in a document and is hashed to <code class="literal">-3</code>, the fourth element of the document's feature vector will be decremented by two. If the term <code class="literal">dogs</code> <a class="indexterm" id="id183"/>also appears twice and is hashed to <code class="literal">3</code>, the fourth element of the feature vector will be incremented by two. Using a signed <a class="indexterm" id="id184"/>hash function creates the possibility that errors from hash collisions will cancel each other out rather than accumulate; a loss of information is preferable to a loss of information and the addition of spurious information. Another disadvantage of the hashing trick is that the resulting model is more difficult to inspect, as the hashing function cannot recall what input token is mapped to each element of the feature vector.</p></div></div>
<div class="section" title="Extracting features from images"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec31"/>Extracting features from images</h1></div></div></div><p>Computer <a class="indexterm" id="id185"/>vision is the study and design of computational artifacts that process and understand images. These artifacts sometimes employ machine learning. An overview of computer vision is far beyond the scope of this book, but in this section we will review some basic techniques used in computer vision to represent images in machine learning problems.</p><div class="section" title="Extracting features from pixel intensities"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec16"/>Extracting features from pixel intensities</h2></div></div></div><p>A digital image <a class="indexterm" id="id186"/>is usually a raster, or pixmap, that <a class="indexterm" id="id187"/>maps colors to coordinates on a grid. An image can be viewed as a matrix in which each element represents a color. A basic feature representation for an image can be constructed by reshaping <a class="indexterm" id="id188"/>the matrix into a vector by concatenating its rows together. <span class="strong"><strong>Optical character recognition</strong></span> (<span class="strong"><strong>OCR</strong></span>) is a canonical machine learning problem. Let's use this technique to create basic feature representations that could be used in an <a class="indexterm" id="id189"/>OCR application for recognizing hand-written digits in character-delimited forms.</p><p>The <code class="literal">digits</code> dataset included with scikit-learn contains grayscale images of more than 1,700 hand-written digits between zero and nine. Each image has eight pixels on a side. Each pixel is represented by an intensity value between zero and 16; white is the most intense and is indicated by zero, and black is the least intense and is indicated by 16. The following figure is an image of a hand-written digit taken from the dataset:</p><div class="mediaobject"><img alt="Extracting features from pixel intensities" src="graphics/8365OS_03_07.jpg"/></div><p>Let's create a <a class="indexterm" id="id190"/>feature vector for the image by <a class="indexterm" id="id191"/>reshaping its 8 x 8 matrix into a 64-dimensional vector:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; digits = datasets.load_digits()
&gt;&gt;&gt; print 'Digit:', digits.target[0]
&gt;&gt;&gt; print digits.images[0]
&gt;&gt;&gt; print 'Feature vector:\n', digits.images[0].reshape(-1, 64)
Digit: 0
[[  0.   0.   5.  13.   9.   1.   0.   0.]
 [  0.   0.  13.  15.  10.  15.   5.   0.]
 [  0.   3.  15.   2.   0.  11.   8.   0.]
 [  0.   4.  12.   0.   0.   8.   8.   0.]
 [  0.   5.   8.   0.   0.   9.   8.   0.]
 [  0.   4.  11.   0.   1.  12.   7.   0.]
 [  0.   2.  14.   5.  10.  12.   0.   0.]
 [  0.   0.   6.  13.  10.   0.   0.   0.]]
Feature vector:
[[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.  13.  15.  10.  15.
    5.   0.   0.   3.  15.   2.   0.  11.   8.   0.   0.   4.  12.   0.
    0.   8.   8.   0.   0.   5.   8.   0.   0.   9.   8.   0.   0.   4.
   11.   0.   1.  12.   7.   0.   0.   2.  14.   5.  10.  12.   0.   0.
    0.   0.   6.  13.  10.   0.   0.   0.]]</pre></div><p>This representation can be effective for some basic tasks, like recognizing printed characters. However, recording the intensity of every pixel in the image produces prohibitively large feature vectors. A tiny 100 x 100 grayscale image would require a 10,000-dimensional vector, and a 1920 x 1080 grayscale image would require a 2,073,600-dimensional vector. Unlike the TF-IDF feature vectors we created, in most problems these vectors are not sparse. Space-complexity is not the only disadvantage of this representation; learning from the intensities of pixels at particular locations results in models that are sensitive to changes in the scale, rotation, and translation of images. A model trained on our <a class="indexterm" id="id192"/>basic feature representations might not be able to recognize the same zero if it were shifted a few pixels in any direction, enlarged, or rotated a few degrees. Furthermore, learning from pixel intensities is itself problematic, as the model can become sensitive to changes in illumination. For these reasons, this representation is ineffective for tasks that involve photographs or <a class="indexterm" id="id193"/>other natural images. Modern computer vision applications frequently use either hand-engineered feature extraction methods that are applicable to many different problems, or automatically learn features without supervision problem using techniques such as deep learning. We will focus on the former in the next section.</p></div><div class="section" title="Extracting points of interest as features"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec17"/>Extracting points of interest as features</h2></div></div></div><p>The feature <a class="indexterm" id="id194"/>vector we created previously represents every pixel in the image; all of the informative attributes of the image are represented <a class="indexterm" id="id195"/>and all of the noisy attributes are represented too. After inspecting the training data, we can see that all of the images have a perimeter of white pixels; these pixels are not useful features. Humans can quickly recognize many objects without observing every attribute of the object. We can recognize a car from the contours of the hood without observing the rear-view mirrors, and we can recognize an image of a human face from a nose or mouth. This intuition is motivation to create representations of only the most informative attributes of an image. These informative attributes, or <span class="strong"><strong>points of interest</strong></span>, are points that are surrounded by rich textures and can be <a class="indexterm" id="id196"/>reproduced despite perturbing the image. <span class="strong"><strong>Edges</strong></span> and <span class="strong"><strong>corners </strong></span>
<a class="indexterm" id="id197"/>are two common types of points of interest. An <a class="indexterm" id="id198"/>edge is a boundary at which pixel intensity rapidly changes, and a corner <a class="indexterm" id="id199"/>is an intersection of two edges. Let's use scikit-image to extract points of <a class="indexterm" id="id200"/>interest from the following figure:</p><div class="mediaobject"><img alt="Extracting points of interest as features" src="graphics/8365OS_03_08.jpg"/></div><p>The code to <a class="indexterm" id="id201"/>extract the points of interest is as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import numpy as nps
&gt;&gt;&gt; from skimage.feature import corner_harris, corner_peaks
&gt;&gt;&gt; from skimage.color import rgb2gray
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; import skimage.io as io
&gt;&gt;&gt; from skimage.exposure import equalize_hist

&gt;&gt;&gt; def show_corners(corners, image):
&gt;&gt;&gt;     fig = plt.figure()
&gt;&gt;&gt;     plt.gray()
&gt;&gt;&gt;     plt.imshow(image)
&gt;&gt;&gt;     y_corner, x_corner = zip(*corners)
&gt;&gt;&gt;     plt.plot(x_corner, y_corner, 'or')
&gt;&gt;&gt;     plt.xlim(0, image.shape[1])
&gt;&gt;&gt;     plt.ylim(image.shape[0], 0)
&gt;&gt;&gt;     fig.set_size_inches(np.array(fig.get_size_inches()) * 1.5)
&gt;&gt;&gt;     plt.show()

&gt;&gt;&gt; mandrill = io.imread('/home/gavin/PycharmProjects/mastering-machine-learning/ch4/img/mandrill.png')
&gt;&gt;&gt; mandrill = equalize_hist(rgb2gray(mandrill))
&gt;&gt;&gt; corners = corner_peaks(corner_harris(mandrill), min_distance=2)
&gt;&gt;&gt; show_corners(corners, mandrill)</pre></div><p>The following figure plots the extracted points of interest. Of the image's 230400 pixels, 466 were extracted as points of interest. This representation is much more compact; ideally, there is enough <a class="indexterm" id="id202"/>variation proximal to the <a class="indexterm" id="id203"/>points of interest to reproduce them despite changes in the image's illumination.</p><div class="mediaobject"><img alt="Extracting points of interest as features" src="graphics/8365OS_03_09.jpg"/></div></div><div class="section" title="SIFT and SURF"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec18"/>SIFT and SURF</h2></div></div></div><p>
<span class="strong"><strong>Scale-Invariant Feature Transform</strong></span> (<span class="strong"><strong>SIFT</strong></span>) is a method for extracting features from an image that is less <a class="indexterm" id="id204"/>sensitive to the scale, rotation, and illumination of the image than the extraction methods we have previously discussed. Each SIFT feature, or descriptor, is a vector that describes edges and <a class="indexterm" id="id205"/>corners in a region of an image. Unlike the points of interest in our previous example, SIFT also captures information about the composition of each point of interest and its surroundings. <span class="strong"><strong>Speeded-Up Robust Features</strong></span> (<span class="strong"><strong>SURF</strong></span>) is another method of extracting interesting points of an image and creating descriptions that are invariant of the image's scale, orientation, and illumination.<span class="strong"><strong> </strong></span>SURF can be computed more quickly than SIFT, and it is more effective at recognizing features across images that have been transformed in certain ways.</p><p>Explaining how SIFT and SURF extraction are implemented is beyond the scope of this book. However, with an intuition for how they work, we can still effectively use libraries that implement them.</p><p>In this example, we will extract SURF from the following image using the <code class="literal">mahotas</code> library.</p><div class="mediaobject"><img alt="SIFT and SURF" src="graphics/8365OS_03_18.jpg"/></div><p>Like the extracted points of interest, the extracted SURF are only the first step in creating a feature representation that could be used in a machine learning task. Different SURF will be extracted for each instance in the training set. In <a class="link" href="ch06.html" title="Chapter 6. Clustering with K-Means">Chapter 6</a>, <span class="emphasis"><em>Clustering with K-Means</em></span>, we will cluster extracted SURF to learn features that can be used by an image classifier. In the following example we will use <code class="literal">mahotas</code> to extract SURF descriptors:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import mahotas as mh
&gt;&gt;&gt; from mahotas.features import surf

&gt;&gt;&gt; image = mh.imread('zipper.jpg', as_grey=True)
&gt;&gt;&gt; print 'The first SURF descriptor:\n', surf.surf(image)[0]
&gt;&gt;&gt; print 'Extracted %s SURF descriptors' % len(surf.surf(image))
The first SURF descriptor:
[  6.73839947e+02   2.24033945e+03   3.18074483e+00   2.76324459e+03
  -1.00000000e+00   1.61191475e+00   4.44035121e-05   3.28041690e-04
   2.44845817e-04   3.86297608e-04  -1.16723672e-03  -8.81290243e-04
   1.65414959e-03   1.28393061e-03  -7.45077384e-04   7.77655540e-04
   1.16078772e-03   1.81434398e-03   1.81736394e-04  -3.13096961e-04
    3.06559785e-04   3.43443699e-04   2.66200498e-04  -5.79522387e-04
   1.17893036e-03   1.99547411e-03  -2.25938217e-01  -1.85563853e-01
   2.27973631e-01   1.91510135e-01  -2.49315698e-01   1.95451021e-01
   2.59719480e-01   1.98613061e-01  -7.82458546e-04   1.40287015e-03
   2.86712113e-03   3.15971628e-03   4.98444730e-04  -6.93986983e-04
   1.87531652e-03   2.19041521e-03   1.80681053e-01  -2.70528820e-01
   2.32414943e-01   2.72932870e-01   2.65725332e-01   3.28050743e-01
   2.98609869e-01   3.41623138e-01   1.58078002e-03  -4.67968721e-04
   2.35704122e-03   2.26279888e-03   6.43115065e-06   1.22501486e-04
   1.20064616e-04   1.76564805e-04   2.14148537e-03   8.36243899e-05
   2.93382280e-03   3.10877776e-03   4.53469215e-03  -3.15254535e-04
   6.92437341e-03   3.56880279e-03  -1.95228401e-04   3.73674995e-05
   7.02700555e-04   5.45156362e-04]
Extracted 994 SURF descriptors</pre></div></div></div>
<div class="section" title="Data standardization"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec32"/>Data standardization</h1></div></div></div><p>Many estimators <a class="indexterm" id="id206"/>perform better when they are trained on standardized data sets. Standardized data has <a class="indexterm" id="id207"/>
<span class="strong"><strong>zero mean</strong></span> and <span class="strong"><strong>unit variance</strong></span>. An explanatory variable with <a class="indexterm" id="id208"/>zero mean is centered about the origin; its average value is zero. A feature vector has unit variance when the variances of its features are all of the same order of magnitude. For example, assume that a feature vector encodes two explanatory variables. The first values of the first variable range from zero to one. The values of the second explanatory variable range from zero to 100,000. The second feature must be scaled to a range closer to {0,1} for the data to have unit variance. If a feature's variance is orders of magnitude greater than the variances of the other features, that feature may dominate the learning algorithm and prevent it from learning from the other variables. Some learning algorithms also converge to the optimal parameter values more slowly when data is not standardized. The value of an explanatory variable can be standardized by subtracting the variable's mean and dividing the difference by the <a class="indexterm" id="id209"/>variable's standard deviation. Data can be easily standardized using scikit-learn's <code class="literal">scale</code> function:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn import preprocessing
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([
&gt;&gt;&gt;     [0., 0., 5., 13., 9., 1.],
&gt;&gt;&gt;     [0., 0., 13., 15., 10., 15.],
&gt;&gt;&gt;     [0., 3., 15., 2., 0., 11.]
&gt;&gt;&gt; ])
&gt;&gt;&gt; print preprocessing.scale(X)
[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]
 [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]
 [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]</pre></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec33"/>Summary</h1></div></div></div><p>In this chapter, we discussed feature extraction and developed an understanding about the basic techniques for transforming arbitrary data into feature representations that can be used by machine learning algorithms. First, we created features from categorical explanatory variables using one-hot encoding and scikit-learn's <code class="literal">DictVectorizer</code>.Then, we discussed the creation of feature vectors for one of the most common types of data used in machine learning problems: text. We worked through several variations of the bag-of-words model, which discards all syntax and encodes only the frequencies of the tokens in a document. We began by creating basic binary term frequencies with <code class="literal">CountVectorizer</code>. You learned to preprocess text by filtering stop words and stemming tokens, and you also replaced the term counts in our feature vectors with TF-IDF weights that penalize common words and normalize for documents of different lengths. Next, we created feature vectors for images. We began with an optical character recognition problem in which we represented images of hand-written digits with flattened matrices of pixel intensities. This is a computationally costly approach. We improved our representations of images by extracting only their most interesting points as SURF descriptors.</p><p>Finally, you learned to standardize data to ensure that our estimators can learn from all of the explanatory variables and can converge as quickly as possible. We will use these feature extraction techniques in the subsequent chapters' examples. In the next chapter, we will combine the bag-of-words representation with a generalization of multiple linear regressions to classify documents.</p></div></body></html>
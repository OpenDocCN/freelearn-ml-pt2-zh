- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mining the 20 Newsgroups Dataset with Text Analysis Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we went through a bunch of fundamental machine learning
    concepts and supervised learning algorithms. Starting from this chapter, as the
    second step of our learning journey, we will be covering in detail several important
    unsupervised learning algorithms and techniques related to text analysis. To make
    our journey more interesting, we will start with a **Natural Language Processing**
    (**NLP**) problem—exploring the 20 newsgroups data. You will gain hands-on experience
    and learn how to work with text data, especially how to convert words and phrases
    into machine-readable values and how to clean up words with little meaning. We
    will also visualize text data by mapping it into a two-dimensional space in an
    unsupervised learning manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go into detail on each of the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How computers understand language – NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Touring popular NLP libraries and picking up NLP basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the newsgroups data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the newsgroups data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking about features for text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the newsgroups data with t-SNE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing words with dense vectors – word embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How computers understand language – NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Chapter 1*, *Getting Started with Machine Learning and Python*, I mentioned
    that machine learning-driven programs or computers are good at discovering event
    patterns by processing and working with data. When the data is well structured
    or well defined, such as in a Microsoft Excel spreadsheet table or a relational
    database table, it is intuitively obvious why machine learning is better at dealing
    with it than humans. Computers read such data the same way as humans—for example,
    `revenue: 5,000,000` as the revenue being 5 million, and `age: 30` as the age
    being 30; then computers crunch assorted data and generate insights in a faster
    way than humans. However, when the data is unstructured, such as words with which
    humans communicate, news articles, or someone’s speech in another language, it
    seems that computers cannot understand words as well as humans do (yet). While
    computers have made significant progress in understanding words and natural language,
    they still fall short of human-level understanding in many aspects.'
  prefs: []
  type: TYPE_NORMAL
- en: What is NLP?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a lot of information in the world about words, raw text, or, broadly
    speaking, **natural language**. This refers to any language that humans use to
    communicate with each other. Natural language can take various forms, including,
    but not limited to, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Text, such as a web page, SMS, emails, and menus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio, such as speech and commands to Siri
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signs and gestures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many other forms, such as songs, sheet music, and Morse code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list is endless, and we are all surrounded by natural language all of the
    time (that’s right, right now as you are reading this book). Given the importance
    of this type of unstructured data (natural language data), we must have methods
    to get computers to understand and reason with natural language and to extract
    data from it. Programs equipped with NLP techniques can already do a lot in certain
    areas, which already seems magical!
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP is a significant subfield of machine learning that deals with the interactions
    between machines (computers) and human (natural) languages. The data for NLP tasks
    can be in different forms, for example, text from social media posts, web pages,
    or even medical prescriptions, or audio from voice mails, commands to control
    systems, or even a favorite song or movie. Nowadays, NLP is broadly involved in
    our daily lives: we cannot live without machine translation, weather forecast
    scripts are automatically generated, we find voice search convenient, we get the
    answer to a question (such as “What is the population of Canada?”) quickly thanks
    to intelligent question-answering systems, speech-to-text technology helps people
    with special needs, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI and its applications like ChatGPT are pushing the boundaries of
    NLP even further. Imagine a world where you can have a conversation with a virtual
    assistant that can not only answer your questions in a comprehensive way but also
    generate different creative text formats, like poems, code, scripts, musical pieces,
    emails, letters, and so on. By analyzing massive amounts of text data, it can
    learn the underlying patterns and structures of language, allowing it to generate
    human-quality text content. For instance, you could ask ChatGPT to write a funny
    birthday poem for your friend, craft a compelling marketing email for your business,
    or even brainstorm ideas for a new blog post.
  prefs: []
  type: TYPE_NORMAL
- en: The history of NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If machines are able to understand language like humans do, we consider them
    intelligent. In 1950, the famous mathematician Alan Turing proposed in an article,
    *Computing Machinery and Intelligence*, a test as a criterion of machine intelligence.
    It’s now called the **Turing test** ([https://plato.stanford.edu/entries/turing-test/](https://plato.stanford.edu/entries/turing-test/)),
    and its goal is to examine whether a computer is able to adequately understand
    languages so as to fool humans into thinking that the machine is another human.
    It is probably no surprise to you that no computer has passed the Turing test
    yet, but the 1950s is considered to be when the history of NLP started.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding language might be difficult, but would it be easier to automatically
    translate texts from one language to another? On my first ever programming course,
    the lab booklet had the algorithm for coarse-grained machine translation. This
    type of translation involved looking up words in dictionaries and generating text
    in a new language. A more practically feasible approach would be to gather texts
    that are already translated by humans and train a computer program on these texts.
    In 1954, in the Georgetown–IBM experiment ([https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment](https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment)),
    scientists claimed that machine translation would be solved in three to five years.
    Unfortunately, a machine translation system that can beat human expert translators
    does not exist yet. But machine translation has been greatly evolving since the
    introduction of deep learning and has seen incredible achievements in certain
    areas, for example, social media (Facebook open sourced a neural machine translation
    system, [https://ai.facebook.com/tools/translate/](https://ai.facebook.com/tools/translate/)),
    real-time conversation (Microsoft Translator, SwiftKey Keyboard, and Google Pixel
    Buds), and image-based translation, such as Google Translate.
  prefs: []
  type: TYPE_NORMAL
- en: Conversational agents, or chatbots, are another hot topic in NLP. The fact that
    computers are able to have a conversation with us has reshaped the way businesses
    are run. In 2016, Microsoft’s AI chatbot, Tay ([https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/)),
    was unleashed to mimic a teenage girl and converse with users on Twitter (now
    X) in real time. She learned how to speak from all the things users posted and
    commented on Twitter. However, she was overwhelmed by tweets from trolls and automatically
    learned their bad behaviors and started to output inappropriate things on her
    feeds. She ended up being terminated within 24 hours. Generative AI models like
    ChatGPT are another area of active research, pushing the boundaries of what’s
    possible. They can be helpful for creative text formats or specific tasks, but
    achieving true human-level understanding in conversation remains an ongoing pursuit.
  prefs: []
  type: TYPE_NORMAL
- en: NLP applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are also several text analysis tasks that attempt to organize knowledge
    and concepts in such a way that they become easier for computer programs to manipulate.
  prefs: []
  type: TYPE_NORMAL
- en: The way we organize and represent concepts is called **ontology**. An ontology
    defines concepts and relationships between concepts. For instance, we can have
    a so-called triple, such as (`"python"`, `"language"`, `"is-a"`) representing
    the relationship between two concepts, such as *Python is a language*.
  prefs: []
  type: TYPE_NORMAL
- en: An important use case for NLP at a much lower level, compared to the previous
    cases, is **part-of-speech** (**PoS**) **tagging**. A PoS is a grammatical word
    category such as a noun or verb. PoS tagging tries to determine the appropriate
    tag for each word in a sentence or a larger document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table gives examples of English PoSs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Part of speech** | **Examples** |'
  prefs: []
  type: TYPE_TB
- en: '| Noun | David, machine |'
  prefs: []
  type: TYPE_TB
- en: '| Pronoun | They, her |'
  prefs: []
  type: TYPE_TB
- en: '| Adjective | Awesome, amazing |'
  prefs: []
  type: TYPE_TB
- en: '| Verb | Read, write |'
  prefs: []
  type: TYPE_TB
- en: '| Adverb | Very, quite |'
  prefs: []
  type: TYPE_TB
- en: '| Preposition | Out, at |'
  prefs: []
  type: TYPE_TB
- en: '| Conjunction | And, but |'
  prefs: []
  type: TYPE_TB
- en: '| Interjection | Phew, oops |'
  prefs: []
  type: TYPE_TB
- en: '| Article | A, the |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: PoS examples'
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of real-world NLP applications involving supervised learning,
    such as PoS tagging, mentioned earlier, and **sentiment analysis**. A typical
    example is identifying news sentiment, which could be positive or negative in
    the binary case, or positive, neutral, or negative in multiclass classification.
    News sentiment analysis provides a significant signal to trading in the stock
    market.
  prefs: []
  type: TYPE_NORMAL
- en: Another example we can easily think of is news topic classification, where classes
    may or may not be mutually exclusive. In the newsgroup example that we just discussed,
    classes are mutually exclusive (despite slight overlapping), such as technology,
    sports, and religion. It is, however, good to realize that a news article can
    be occasionally assigned multiple categories (multi-label classification). For
    example, an article about the Olympic Games may be labeled sports and politics
    if there is unexpected political involvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, an interesting application that is perhaps unexpected is **Named Entity
    Recognition** (**NER**). Named entities are phrases of definitive categories,
    such as names of persons, companies, geographic locations, dates and times, quantities,
    and monetary values. NER is an important subtask of information extraction to
    seek and identify such entities. For example, we can conduct NER on the following
    sentence: `SpaceX[Organization]`, a `California[Location]`-based company founded
    by a famous tech entrepreneur `Elon Musk[Person]`, announced that it would manufacture
    the next-generation, `9[Quantity]`-meter-diameter launch vehicle and spaceship
    for the first orbital flight in `2020[Date]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other key NLP applications include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Language translation**: NLP powers machine translation systems, enabling
    automatic translation of text or speech from one language to another. Platforms
    like Google Translate and Microsoft Translator utilize NLP to provide real-time
    translation services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: NLP is essential in speech recognition systems, converting
    spoken language into written text. Virtual assistants like Siri, Alexa, and Google
    Assistant rely on NLP to understand user commands and respond appropriately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text summarization**: NLP can automatically generate concise summaries of
    lengthy texts, providing a quick overview of the content. Text summarization is
    useful for information retrieval and content curation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language generation**: NLP models, such as **Generative Pre-trained Transformers**
    (**GPTs**), can generate human-like text, including creative writing, poetry,
    and dialogue generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information retrieval**: NLP assists in information retrieval from large
    volumes of unstructured data, such as web pages, documents, and news articles.
    Search engines use NLP techniques to understand user queries and retrieve relevant
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chatbots, question answering, and virtual assistants**: NLP powers chatbots
    and virtual assistants to provide interactive and conversational experiences.
    These systems can answer queries, assist with tasks, and guide users through various
    processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how unsupervised learning, including clustering
    and topic modeling, is applied to text data. We will begin by covering NLP basics
    in the upcoming sections of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Touring popular NLP libraries and picking up NLP basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered a short list of real-world applications of NLP, we
    will be touring the essential stack of Python NLP libraries. These packages handle
    a wide range of NLP tasks, as mentioned previously, including sentiment analysis,
    text classification, and NER.
  prefs: []
  type: TYPE_NORMAL
- en: Installing famous NLP libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most famous NLP libraries in Python include the **Natural Language Toolkit**
    (**NLTK**), **spaCy**, **Gensim**, and **TextBlob.** The scikit-learn library
    also has impressive NLP-related features. Let’s take a look at them in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NLTK**: This library ([http://www.nltk.org/](http://www.nltk.org/)) was originally
    developed for educational purposes and is now widely used in industry as well.
    It is said that you can’t talk about NLP without mentioning NLTK. It is one of
    the most famous and leading platforms for building Python-based NLP applications.
    You can install it simply by running the following command line in the terminal:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re using `conda`, execute the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**spaCy**: This library (`https://spacy.io/`) is a more powerful toolkit in
    the industry than NLTK. This is mainly for two reasons: first, `spaCy` is written
    in Cython, which is much more memory-optimized (now you can see where the `Cy`
    in `spaCy` comes from) and excels in NLP tasks; second, `spaCy` uses state-of-the-art
    algorithms for core NLP problems, such as **convolutional neural network** (**CNN**)
    models for tagging and NER. However, it could seem advanced for beginners. In
    case you’re interested, here are the installation instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run the following command line in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For `conda`, execute the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Gensim**: This library ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)),
    developed by Radim Rehurek, has been gaining popularity over recent years. It
    was initially designed in 2008 to generate a list of similar articles given an
    article, hence the name of this library (`generate similar`—> `Gensim`). It was
    later drastically improved by Radim Rehurek in terms of its efficiency and scalability.
    Again, you can easily install it via `pip` by running the following command line:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of `conda`, you can execute the following command line in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should make sure that the dependencies, NumPy and SciPy, are already installed
    before Gensim.
  prefs: []
  type: TYPE_NORMAL
- en: '**TextBlob**: This library ([https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/))
    is a relatively new one built on top of NLTK. It simplifies NLP and text analysis
    with easy-to-use built-in functions and methods, as well as wrappers around common
    tasks. We can install `TextBlob` by running the following command line in the
    terminal:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, for conda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`TextBlob` has some useful features that are not available in NLTK (currently),
    such as spell checking and correction, language detection, and translation.'
  prefs: []
  type: TYPE_NORMAL
- en: Corpora
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NLTK comes with over 100 collections of large and well-structured text datasets,
    which are called **corpora** in NLP. Here are some of the main corpora that NLTK
    provides:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gutenberg Corpus**: A collection of literary works from Project Gutenberg,
    containing thousands of books in various languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reuters Corpus**: A collection of news articles from the Reuters newswire
    service, widely used for text classification and topic modeling tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Web and Chat Text**: A collection of web text and chat conversations, providing
    a glimpse into informal language and internet slang.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Movie Reviews Corpus**: A collection of movie reviews, often used for sentiment
    analysis and text classification tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Treebank Corpus**: A collection of parsed and tagged sentences from the Penn
    Treebank, used for training and evaluating syntactic parsers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WordNet**: A lexical database of English words, containing synsets (groups
    of synonymous words) and hypernyms (is-a relationships).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corpora can be used as dictionaries for checking word occurrences and as training
    pools for model learning and validating. Some more useful and interesting corpora
    include the Web Text corpus, Twitter (X) samples, the Shakespeare corpus, Sentiment
    Polarity, the Names corpus (this contains lists of popular names, which we will
    be exploring very shortly), WordNet, and the Reuters benchmark corpus. The full
    list can be found at [http://www.nltk.org/nltk_data](http://www.nltk.org/nltk_data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before using any of these corpus resources, we need to first download them
    by running the following code in the Python interpreter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A new window will pop up and ask you which collections (the **Collections**
    tab in the following screenshot) or corpus (the **Corpora** tab in the following
    screenshot) to download, and where to keep the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21047_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Collections tab in the NLTK installation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing the whole popular package is the quickest solution since it contains
    all the important corpora needed for your current study and future research. Installing
    a particular corpus, as shown in the following screenshot, is also fine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21047_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Corpora tab in the NLTK installation'
  prefs: []
  type: TYPE_NORMAL
- en: Once the package or corpus you want to explore is installed, you can take a
    look at the **Names** corpus (make sure the names corpus is installed for this
    example).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the names corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check out the first `10` names in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There are, in total, `7944` names, as shown in the following output derived
    by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Other corpora are also fun to explore.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the easy-to-use and abundant corpora pool, more importantly, NLTK is
    also good at many NLP and text analysis tasks, including tokenization, PoS tagging,
    NER, word stemming, and lemmatization. We’ll look at these tasks next.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a text sequence, **tokenization** is the task of breaking it into fragments,
    which can be words, characters, or sentences. Certain characters are usually removed,
    such as punctuation marks, digits, and emoticons. The remaining fragments are
    the so-called **tokens** used for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokens composed of one word are also called **unigrams** in computational linguistics;
    **bigrams** are composed of two consecutive words; **trigrams** of three consecutive
    words; and **n-grams** of *n* consecutive words. Here is an example of tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, font, line, number  Description automatically
    generated](img/B21047_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Tokenization example'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement word-based tokenization using the `word_tokenize` function
    in NLTK. We will use the input text `''''''I am reading a book.`, and on the next
    line, `It is Python Machine Learning By Example,`, then `4th edition.''''''`,
    as an example, as shown in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Word tokens are obtained.
  prefs: []
  type: TYPE_NORMAL
- en: The `word_tokenize` function keeps punctuation marks and digits, and only discards
    whitespaces and newlines.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might think word tokenization is simply splitting a sentence by space and
    punctuation. Here’s an interesting example showing that tokenization is more complex
    than you think:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The tokenizer accurately recognizes the words `'U.K.'` and `'U.S.A'` as tokens
    instead of `'U'` and `'.'` followed by `'K'`, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy also has an outstanding tokenization feature. It uses an accurately trained
    model that is constantly updated. To install it, we can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load the `en_core_web_sm` model (if you have not downloaded the model,
    you can run `python -m spacy download en_core_web_sm` to do so) and parse the
    sentence using this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also segment text based on sentences. For example, in the same input
    text, using the `sent_tokenize` function from NLTK, we have the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Two sentence-based tokens are returned, as there are two sentences in the input
    text.
  prefs: []
  type: TYPE_NORMAL
- en: PoS tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can apply an off-the-shelf tagger from NLTK or combine multiple taggers to
    customize the tagging process. It is easy to directly use the built-in tagging
    function, `pos_tag`, as in `pos_tag(input_tokens)`, for instance, but behind the
    scenes, it is actually a prediction from a pre-built supervised learning model.
    The model is trained based on a large corpus composed of words that are correctly
    tagged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reusing an earlier example, we can perform PoS tagging as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The PoS tag following each token is returned. We can check the meaning of a
    tag using the `help` function. Looking up `PRP` and `VBP`, for example, gives
    us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In spaCy, getting a PoS tag is also easy. The `token` object parsed from an
    input sentence has an attribute called `pos_`, which is the tag we are looking
    for. Let’s print `pos_` for each token, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We have just played around with PoS tagging with NLP packages. What about NER?
    Let’s see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: NER
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a text sequence, the NER task is to locate and identify words or phrases
    that are of definitive categories, such as names of persons, companies, locations,
    and dates. Let’s take a peep at an example of using spaCy for NER.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, tokenize an input sentence, `The book written by Hayden Liu in 2024
    was sold at $30 in America`, as usual, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The resultant `token` object contains an attribute called `ents`, which are
    the named entities. We can extract the tagging for each recognized named entity
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the results that `Hayden Liu` is `PERSON`, `2024` is `DATE`,
    `30` is `MONEY`, and `America` is `GPE` (country). Please refer to [https://spacy.io/api/annotation#section-named-entities](https://spacy.io/api/annotation#section-named-entities)
    for a full list of named entity tags.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word **stemming** is a process of reverting an inflected or derived word to
    its root form. For instance, *machine* is the stem of *machines*, and *learning*
    and *learned* are generated from *learn* as their stem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The word **lemmatization** is a cautious version of stemming. It considers
    the PoS of a word when conducting stemming. Also, it traces back to the lemma
    (base or canonical form) of the word. We will discuss these two text preprocessing
    techniques, stemming and lemmatization, in further detail shortly. For now, let’s
    take a quick look at how they’re implemented respectively in NLTK by performing
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `porter` as one of the three built-in stemming algorithms (`LancasterStemmer`
    and `SnowballStemmer` are the other two) and initialize the stemmer as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Stem `machines` and `learning`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Stemming sometimes involves the chopping of letters if necessary, as you can
    see in `machin` in the preceding command output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, import a lemmatization algorithm based on the built-in WordNet corpus
    and initialize a lemmatizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to stemming, we lemmatize `machines` and `learning`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Why is `learning` unchanged? The algorithm defaults to finding the lemma for
    nouns unless you specify otherwise. If you want to treat `learning` as a verb,
    you can specify it in `lemmatizer.lemmatize('learning', nltk.corpus.wordnet.VERB)`,
    which will return `learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Semantics and topic modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gensim is famous for its powerful semantic and topic-modeling algorithms. Topic
    modeling is a typical text-mining task of discovering the hidden semantic structures
    in a document. A semantic structure in plain English is the distribution of word
    occurrences. It is obviously an unsupervised learning task. What we need to do
    is to feed in plain text and let the model figure out the abstract topics. For
    example, we can use topic modeling to group product reviews on an e-commerce site
    based on the common themes expressed in the reviews. We will study topic modeling
    in detail in *Chapter 8*, *Discovering Underlying Topics in the Newsgroups Dataset
    with Clustering and Topic Modeling*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to robust semantic modeling methods, gensim also provides the following
    functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word embedding**: Also known as **word vectorization**, this is an innovative
    way to represent words while preserving words’ co-occurrence features. Later in
    this chapter, we will delve into a comprehensive exploration of word embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Similarity querying**: This functionality retrieves objects that are similar
    to the given query object. It’s a feature built on top of word embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed computing**: This functionality makes it possible to efficiently
    learn from millions of documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last but not least, as mentioned in the first chapter, *Getting Started with
    Machine Learning and Python*, scikit-learn is the main package we have used throughout
    this entire book. Luckily, it provides all the text processing features we need,
    such as tokenization, along with comprehensive machine learning functionalities.
    Plus, it comes with a built-in loader for the 20 newsgroups dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the tools are available and properly installed, what about the data?
  prefs: []
  type: TYPE_NORMAL
- en: Getting the newsgroups data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The project in this chapter is about the 20 newsgroups dataset. It’s composed
    of text taken from newsgroup articles, as its name implies. It was originally
    collected by Ken Lang and now has been widely used for experiments in text applications
    of machine learning techniques, specifically NLP techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The data contains approximately 20,000 documents across 20 online newsgroups.
    A newsgroup is a place on the internet where people can ask and answer questions
    about a certain topic. The data is already cleaned to a certain degree and already
    split into training and testing sets. The cutoff point is at a certain date.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original data comes from [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/),
    with 20 different topics listed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`comp.graphics`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.os.ms-windows.misc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.sys.ibm.pc.hardware`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.sys.mac.hardware`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`comp.windows.x`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.autos`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.motorcycles`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.sport.baseball`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rec.sport.hockey`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.crypt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.electronics`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.med`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sci.space`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`misc.forsale`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.politics.misc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.politics.guns`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.politics.mideast`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`talk.religion.misc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alt.atheism`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`soc.religion.christian`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the documents in the dataset are in English. And we can easily deduce
    the topics from the newsgroups’ names.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is labeled and each document is composed of text data and a group
    label. This also makes it a perfect fit for supervised learning, such as text
    classification. At the end of the chapter, feel free to practice classification
    on this dataset using what you’ve learned so far in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the newsgroups are closely related or even overlapping – for instance,
    those five computer groups (`comp.graphics`, `comp.os.ms-windows.misc`, `comp.sys.ibm.pc.hardware`,
    `comp.sys.mac.hardware`, and `comp.windows.x`). Some are not closely related to
    each other, such as Christian (`soc.religion.christian`) and baseball (`rec.sport.baseball`).
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it’s a perfect use case for unsupervised learning such as clustering,
    with which we can see whether similar topics are grouped together and unrelated
    ones are far apart. Moreover, we can even discover abstract topics beyond the
    original 20 labels using topic modeling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s focus on exploring and analyzing the text data. We will get started
    with acquiring the data.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to download the dataset manually from the original website or
    many other online repositories. However, there are also many versions of the dataset—some
    are cleaned in a certain way and some are in raw form. To avoid confusion, it
    is best to use a consistent acquisition method. The scikit-learn library provides
    a utility function that loads the dataset. Once the dataset is downloaded, it’s
    automatically cached. We don’t need to download the same dataset twice.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, caching the dataset, especially for a relatively small one, is
    considered a good practice. Other Python libraries also provide data download
    utilities, but not all of them implement automatic caching. This is another reason
    why we love scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, we first import the loader function for the 20 newsgroups data,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we download the dataset with all the default parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also specify one or more certain topic groups and particular sections
    (training, testing, or both) and just load such a subset of data in the program.
    The full list of parameters and options for the loader function is summarized
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Default value** | **Example values** | **Description**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `subset` | `''train''` | `''train'',''test'',''all''` | The dataset to load:
    the training set, the testing set, or both. |'
  prefs: []
  type: TYPE_TB
- en: '| `data_home` | `~/scikit_learn_data` | `~/myfolder` | Directory where the
    files are stored and cached. |'
  prefs: []
  type: TYPE_TB
- en: '| `categories` | `None` | `[''sci.space",alt.atheism'']` | List of newsgroups
    to load. If `None`, all newsgroups will be loaded. |'
  prefs: []
  type: TYPE_TB
- en: '| `shuffle` | `True` | `True, False` | Boolean indicating whether to shuffle
    the data. |'
  prefs: []
  type: TYPE_TB
- en: '| `random_state` | `42` | `7, 43` | Random seed integer used to shuffle the
    data. |'
  prefs: []
  type: TYPE_TB
- en: '| `remove` | `0` | `(''headers'',''footers'',''quotes'')` | Tuple indicating
    the part(s) among “header, footer, and quote” of each newsgroup post to omit.
    Nothing is removed by default. |'
  prefs: []
  type: TYPE_TB
- en: '| `download_if_missing` | `True` | `True, False` | Boolean indicating whether
    to download the data if it is not found locally. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.2: List of parameters of the fetch_20newsgroups() function'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that `random_state` is useful for the purpose of reproducibility. You
    are able to get the same dataset every time you run the script. Otherwise, working
    on datasets shuffled under different orders might bring in unnecessary variations.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we loaded the newsgroups data. Let’s explore it next.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the newsgroups data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After we download the 20 newsgroups dataset by whatever means we prefer, the
    `data` object of `groups` is cached in memory. The `data` object is in the form
    of a key-value dictionary. Its keys are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The `target_names` key gives the 20 newsgroups names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `target` key corresponds to a newsgroup, but is encoded as an integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, what are the distinct values for these integers? We can use the `unique`
    function from NumPy to figure it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: They range from `0` to `19`, representing the 1st, 2nd, 3rd, …, and 20th newsgroup
    topics in `groups['target_names']`.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of multiple topics or categories, it is important to know what
    the distribution of topics is. A balanced class distribution is the easiest to
    deal with because there are no under-represented or over-represented categories.
    However, frequently, we have a skewed distribution with one or more categories
    dominating.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `seaborn` package ([https://seaborn.pydata.org/](https://seaborn.pydata.org/))
    to compute the histogram of categories and plot it utilizing the `matplotlib`
    package ([https://matplotlib.org/](https://matplotlib.org/)). We can install both
    packages via `pip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of `conda`, you can execute the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Remember to install `matplotlib` before `seaborn` as `matplotlib` is one of
    the dependencies of the `seaborn` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s display the distribution of the classes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of a number  Description automatically generated](img/B21047_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Distribution of newsgroup classes'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the distribution is approximately uniform so that’s one less
    thing to worry about.
  prefs: []
  type: TYPE_NORMAL
- en: It’s good to visualize data to get a general idea of how the data is structured,
    what possible issues may arise, and whether there are any irregularities that
    we have to take care of.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other keys are quite self-explanatory: `data` contains all newsgroup documents
    and `filenames` stores the path where each document is located in your filesystem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s have a look at the first document and its topic number and name
    by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: If `random_state` isn’t fixed (`42` by default), you may get different results
    running the preceding scripts.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the first document is from the `rec.autos` newsgroup, which
    was assigned the number `7`. Reading this post, we can easily figure out that
    it’s about cars. The word `car` actually occurs a number of times in the document.
    Words such as `bumper` also seem very car-oriented. However, words such as `doors`
    may not necessarily be car-related, as they may also be associated with home improvement
    or another topic.
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, it makes sense to not distinguish between `doors` and `door`,
    or the same word with different capitalization, such as `Doors`. There are some
    rare cases where capitalization does matter – for instance, if we’re trying to
    find out whether a document is about the band called `The Doors` or the more common
    concept, `the doors` (made of wood or another material).
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about features for text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the preceding analysis, we can safely conclude that if we want to figure
    out whether a document was from the `rec.autos` newsgroup, the presence or absence
    of words such as `car`, `doors`, and `bumper` can be very useful features. The
    presence or not of a word is a Boolean variable, and we can also look at the count
    of certain words. For instance, `car` occurs multiple times in the document. Maybe
    the more times such a word is found in a text, the more likely it is that the
    document has something to do with cars.
  prefs: []
  type: TYPE_NORMAL
- en: Counting the occurrence of each word token
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It seems that we are only interested in the occurrence of certain words, their
    count, or a related measure, and not in the order of the words. We can therefore
    view a text as a collection of words. This is called the **Bag of Words** (**BoW**)
    model. This is a very basic model but it works pretty well in practice. We can
    optionally define a more complex model that takes into account the order of words
    and PoS tags. However, such a model is going to be more computationally expensive
    and more difficult to program. In reality, the basic BoW model, in most cases,
    suffices. We can give it a shot and see whether the BoW model makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by converting documents into a matrix where each row represents each
    newsgroup document and each column represents a word token, or specifically, a
    unigram to begin with. The value of each element in the matrix is the number of
    times the word (column) occurs in the document (row). We are utilizing the `CountVectorizer`
    class from scikit-learn to do the work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The important parameters and options for the count conversion function are
    summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ngram_range` | `(1,1)` | `(1,2), (2,2)` | Lower and upper bound of the n-grams
    to be extracted in the input text, for example `(1,1)` means unigram, `(1,2)`
    means unigram and bigram. |'
  prefs: []
  type: TYPE_TB
- en: '| `stop_words` | `None` | `''english'' or list [''a'',''the'', ''of''] or None`
    | Which stop word list to use: can be `''english''` referring to the built-in
    list, or a customized input list. If `None`, no words will be removed. |'
  prefs: []
  type: TYPE_TB
- en: '| `lowercase` | `True` | `True, False` | Whether or not to convert all characters
    to lowercase. |'
  prefs: []
  type: TYPE_TB
- en: '| `max_features` | `None` | `None, 200, 500` | The number of top (most frequent)
    tokens to consider, or all tokens if `None`. |'
  prefs: []
  type: TYPE_TB
- en: '| `binary` | `False` | `True, False` | If true, all non-zero counts become
    1s. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.3: List of parameters of the CountVectorizer() function'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first initialize the count vectorizer with the `500` top features (500 most
    frequent tokens):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Use it to fit on the raw text data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the count vectorizer captures the top 500 features and generates a token
    count matrix out of the original text input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The resulting count matrix is a sparse matrix where each row only stores non-zero
    elements (hence, only 798,221 elements instead of `11314 * 500 = 5,657,000`).
    For example, the first document is converted into a sparse vector composed of
    `53` non-zero elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in seeing the whole matrix, feel free to run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If you just want the first row, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at the following output derived from the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A number pattern with numbers  Description automatically generated](img/B21047_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Output of count vectorization'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what are those 500 top features? They can be found in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Our first trial doesn’t look perfect. Obviously, the most popular tokens are
    numbers, or letters with numbers such as `a86`, which do not convey important
    information. Moreover, there are many words that have no actual meaning, such
    as `you`, `the`, `them`, and `then`. Also, some words contain identical information,
    for example, `tell` and `told`, `use` and `used`, and `time` and `times`. Let’s
    tackle these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Text preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by retaining letter-only words so that numbers such as `00` and `000`
    and combinations of letters and numbers such as `b8f` will be removed. The filter
    function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This will generate a cleaned version of the newsgroups data.
  prefs: []
  type: TYPE_NORMAL
- en: Dropping stop words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We didn’t talk about `stop_words` as an important parameter in `CountVectorizer`.
    **Stop words** are those common words that provide little value in helping to
    differentiate documents. In general, stop words add noise to the BoW model and
    can be removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s no universal list of stop words. Hence, depending on the tools or packages
    you are using, you will remove different sets of stop words. Take scikit-learn
    as an example—you can check the list that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'To drop stop words from the newsgroups data, we simply just need to specify
    the `stop_words` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Besides stop words, you may notice that names are included in the top features,
    such as `andrew`. We can filter names with the `Names` corpus from NLTK we just
    worked with.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing inflectional and derivational forms of words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, we have two basic strategies to deal with words from the
    same root—stemming and lemmatization. Stemming is a quicker approach that involves,
    if necessary, chopping off letters; for example, *words* becomes *word* after
    stemming. The result of stemming doesn’t have to be a valid word. For instance,
    *trying* and *try* become *tri*. Lemmatizing, on the other hand, is slower but
    more accurate. It performs a dictionary lookup and guarantees to return a valid
    word. Recall that we implemented both stemming and lemmatization using NLTK previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting all of these (preprocessing, dropping stop words, lemmatizing, and
    count vectorizing) together, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the features are much more meaningful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We have just converted text from each raw newsgroup document into a sparse vector
    of size `500`. For a vector from a document, each element represents the number
    of times a word token occurs in this document. Also, these 500 word tokens are
    selected based on their overall occurrences after text preprocessing, the removal
    of stop words, and lemmatization. Now, you may ask questions such as, “Is such
    an occurrence vector representative enough, or does such an occurrence vector
    convey enough information that can be used to differentiate the document from
    documents on other topics?” You will see the answer in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the newsgroups data with t-SNE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can answer these questions easily by visualizing those representation vectors.
    If we can see that the document vectors from the same topic form a cluster, we
    did a good job mapping the documents into vectors. But how? They are of 500 dimensions,
    while we can visualize data of, **at most**, three dimensions. We can resort to
    t-SNE for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: What is dimensionality reduction?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Dimensionality reduction** is an important machine learning technique that
    reduces the number of features and, at the same time, retains as much information
    as possible. It is usually performed by obtaining a set of new principal features.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, it is difficult to visualize data of high dimensions. Given
    a three-dimensional plot, we sometimes don’t find it straightforward to observe
    any findings, not to mention 10, 100, or 1,000 dimensions. Moreover, some of the
    features in high-dimensional data may be correlated and, as a result, bring in
    redundancy. This is why we need dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is not simply taking out a pair of two features from
    the original feature space. It is transforming the original feature space into
    a new space of fewer dimensions. The data transformation can be linear, such as
    the famous one, **Principal Component Analysis** (**PCA**), which maps the data
    in a higher dimensional space to a lower dimensional space where the variance
    of the data is maximized, which we will talk about in *Chapter 9*, *Recognizing
    Faces with Support Vector Machine*, or nonlinear, such as neural networks and
    t-SNE, which is coming up shortly. **Non-negative Matrix Factorization** (**NMF**)
    is another powerful algorithm, which we will study in detail in *Chapter 8*, *Discovering
    Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling*.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the day, most dimensionality reduction algorithms are in the family
    of **unsupervised learning** as the target or label information (if available)
    is not used in data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE for dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**t-SNE** stands for **t-distributed Stochastic Neighbor Embedding**. It is
    a popular nonlinear dimensionality reduction technique developed by Laurens van
    der Maaten and Geoffrey Hinton ([https://www.cs.toronto.edu/~hinton/absps/tsne.pdf](https://www.cs.toronto.edu/~hinton/absps/tsne.pdf)).
    t-SNE has been widely used for data visualization in various domains, including
    computer vision, NLP, bioinformatics, and computational genomics.'
  prefs: []
  type: TYPE_NORMAL
- en: As its name implies, t-SNE embeds high-dimensional data into a low-dimensional
    (usually two-dimensional or three-dimensional) space while preserving the local
    structure and pairwise similarities of the data as much as possible. It first
    models a probability distribution over neighbors around data points by assigning
    a high probability to similar data points and an extremely small probability to
    dissimilar ones. Note that similarity and neighbor distances are measured by Euclidean
    distance or other metrics. Then, t-SNE constructs a projection onto a low-dimensional
    space where the divergence between the input distribution and output distribution
    is minimized. The original high-dimensional space is modeled as a Gaussian distribution,
    while the output low-dimensional space is modeled as a t-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll herein implement t-SNE using the `TSNE` class from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s use t-SNE to verify our count vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: We pick three distinct topics, `talk.religion.misc`, `comp.graphics`, and `sci.space`,
    and visualize document vectors from these three topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, just load documents of these three labels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: We go through the same process and generate a count matrix, `data_cleaned_count_3`,
    with 500 features from the input, `groups_3`. You can refer to the steps in previous
    sections as you just need to repeat the same code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we apply t-SNE to reduce the 500-dimensional matrix to a two-dimensional
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters we specify in the `TSNE` object are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components`: The output dimension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perplexity`: The number of nearest data points considered neighbors in the
    algorithm with a typical value of between 5 and 50'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state`: The random seed for program reproducibility'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: The factor affecting the process of finding the optimal mapping
    space with a typical value of between 10 and 1,000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the `TSNE` object only takes in a dense matrix, hence we convert the
    sparse matrix, `data_cleaned_count_3`, into a dense one using `toarray()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We just successfully reduced the input dimension from 500 to 2\. Finally, we
    can easily visualize it in a two-dimensional scatter plot where the *x* axis is
    the first dimension, the *y* axis is the second dimension, and the color, `c`,
    is based on the topic label of each original document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following screenshot for the end result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A colorful dots on a white background  Description automatically generated](img/B21047_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Applying t-SNE to data from three different topics'
  prefs: []
  type: TYPE_NORMAL
- en: Data points from the three topics are in different colors – green, purple, and
    yellow. We can observe three clear clusters. Data points from the same topic are
    close to each other, while those from different topics are far away. Obviously,
    count vectors are great representations of original text data as they preserve
    the distinction between three different topics.
  prefs: []
  type: TYPE_NORMAL
- en: You can also play around with the parameters and see whether you can obtain
    a nicer plot where the three clusters are better separated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Count vectorization does well in keeping document disparity. How about maintaining
    similarity? We can also check that using documents from overlapping topics, such
    as these five topics—`comp.graphics`, `comp.os.ms-windows.misc`, `comp.sys.ibm.pc.hardware`,
    `comp.sys.mac.hardware`, and `comp.windows.x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar processes (including text clean-up, count vectorization, and t-SNE)
    are repeated and the resulting plot is displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A colorful dots on a white background  Description automatically generated](img/B21047_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Applying t-SNE to data from five similar topics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data points from those five computer-related topics are all over the place,
    which means they are contextually similar. To conclude, count vectors are simple
    yet great representations of original text data as they are also good at preserving
    similarity among related topics. The question now arises: can we improve upon
    word (term) count representations? Let’s progress to the next section, where we
    will explore dense vector representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Representing words with dense vectors – word embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word count representation results in a high-dimensional, sparse vector where
    each element represents the frequency of a specific word. Recall that we only
    looked at the `500` most frequent words previously to avoid this issue. Otherwise,
    we would have to represent each document with a vector of more than 1 million
    dimensions (depending on the size of the vocabulary). Also, word count representation
    lacks the ability to capture the semantics or context of words. It only considers
    the frequency of words in a document or corpus. On the contrary, **word embedding**
    represents words in a **dense** (**continuous**) vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Building embedding models using shallow neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Word embedding maps each word to a dense vector of fixed dimensions. Its dimensionality
    is a lot lower than the size of the vocabulary and is usually several hundred
    only. For example, the word *machine* can be represented as a vector `[1.4, 2.1,
    10.3, 0.2, 6.81]`.
  prefs: []
  type: TYPE_NORMAL
- en: So, how can we embed a word into a vector? One solution is **word2vec** (see
    *Efficient Estimation of Word Representations in Vector Space*, by Tomas Mikolov,
    Kai Chen, Greg Corrado, and Jeff Dean, [https://arxiv.org/pdf/1301.3781](https://arxiv.org/pdf/1301.3781));
    this trains a **shallow neural network** to predict a word given the other words
    around it, which is called **Continuous Bag of Words** (**CBOW**), or to predict
    the other words around a word, which is called the **skip-gram** approach. The
    **weights** (**coefficients**) of the trained neural network are the embedding
    vectors for the corresponding words. Let’s look at a concrete example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the sentence *I love reading python machine learning by example* in a
    corpus and `5` as the size of the **word window**, we can have the following training
    sets for the CBOW neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input of neural network** | **Output of neural network** |'
  prefs: []
  type: TYPE_TB
- en: '| `(I, love, python, machine)` | `(reading)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(love, reading, machine, learning)` | `(python)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(reading, python, learning, by)` | `(machine)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(python, machine, by, example)` | `(learning)` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.4: Input and output of the neural network for CBOW'
  prefs: []
  type: TYPE_NORMAL
- en: During training, the inputs and outputs of the neural network are one-hot encoding
    vectors, where values are either 1 for present words or 0 for absent words. And
    we can have millions of training samples constructed from a corpus, sentence by
    sentence. After the network is trained, the weights that connect the input layer
    and hidden layer embed individual input words.
  prefs: []
  type: TYPE_NORMAL
- en: 'A skip-gram-based neural network embeds words in a similar way. But its input
    and output are an inverse version of CBOW. Given the same sentence, *I love reading
    python machine learning by example*, and `5` as the size of the word window, we
    can have the following training sets for the skip-gram neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input of neural network** | **Output of neural network** |'
  prefs: []
  type: TYPE_TB
- en: '| `(reading)` | `(i)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(reading)` | `(love)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(reading)` | `(python)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(reading)` | `(machine)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(python)` | `(love)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(python)` | `(reading)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(python)` | `(machine)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(python)` | `(learning)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(machine)` | `(reading)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(machine)` | `(python)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(machine)` | `(learning)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(machine)` | `(by)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(learning)` | `(python)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(learning)` | `(machine)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(learning)` | `(by)` |'
  prefs: []
  type: TYPE_TB
- en: '| `(learning)` | `(example)` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.5: Input and output of the neural network for skip-gram'
  prefs: []
  type: TYPE_NORMAL
- en: The embedding vectors are of real values, where each dimension encodes an aspect
    of meaning for the words in the vocabulary. This helps preserve the semantic information
    of the words, as opposed to discarding it, as in the dummy one-hot encoding approach
    using the word count approach. An interesting phenomenon is that vectors from
    semantically similar words are proximate to each other in geometric space. For
    example, both the words *clustering* and *grouping* refer to unsupervised clustering
    in the context of machine learning, hence their embedding vectors are close together.
    Word embedding is able to capture the meanings of words and their **context**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualizing word embeddings can be a helpful tool to explore patterns, identify
    relationships between words, and assess the effectiveness of your embedding model.
    Here are some best practices for visualizing word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: Word embeddings typically have high-dimensional
    vectors. To visualize them, reduce their dimensionality. We can use techniques
    like PCA or t-SNE to project the high-dimensional data into 2D or 3D space while
    preserving distances between data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Cluster similar word embeddings together to identify groups
    of words with similar meanings or contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing pre-trained embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training a word embedding neural network can be time-consuming and computationally
    expensive. Fortunately, many organizations and research institutions (such as
    Google, Meta AI Research, OpenAI, Stanford NLP Group, and Hugging Face) have developed
    pre-trained word embedding models based on different kinds of corpora and made
    them readily available for developers and researchers to use in various NLP tasks.
    We can simply use these **pre-trained** models to map words to vectors. Some popular
    pre-trained word embedding models are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Configurations of popular pre-trained word embedding models'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have embedding vectors for individual words, we can represent a document
    sample by averaging all of the vectors of words present in this document.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best practice**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have the word embeddings for all words in the document, aggregate
    them into a single vector representation for the entire document. Common aggregation
    techniques include averaging and summation. More sophisticated methods include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Weighted average, where the weights are based on word importance, such as TF-IDF
    score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max/min pooling, where the maximum or minimum value for each dimension across
    all word embeddings is taken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resulting vectors of document samples are then consumed by downstream predictive
    tasks, such as classification, similarity ranking in search engines, and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s play around with `gensim`, a popular NLP package with powerful word
    embedding modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the package and load a pre-trained model, `glove-twitter-25`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: You will see the process bar if you run this line of code. The `glove-twitter-25`
    model is one of the smallest ones so the download will not take very long.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can obtain the embedding vector for a word (`computer`, for example), as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The result is a 25-dimension float vector, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also get the top 10 words that are most contextually relevant to `computer`
    using the `most_similar` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The result looks promising.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we demonstrate how to generate embedding vectors for a document with
    a simple example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The resulting vector is the **average** of embedding vectors of eight input
    words.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional NLP applications, such as text classification and information
    retrieval tasks, where word frequency plays a significant role, word count representation
    is still an outstanding solution. In more complicated areas requiring understanding
    and semantic relationships between words, such as text summarization, machine
    translation, and question answering, word embedding is used extensively and extracts
    far better features than the traditional approach.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the fundamental concepts of NLP as an important
    subfield in machine learning, including tokenization, stemming and lemmatization,
    and PoS tagging. We also explored three powerful NLP packages and worked on some
    common tasks using NLTK and spaCy. Then we continued with the main project, exploring
    the 20 newsgroups data. We began by extracting features with tokenization techniques
    and went through text preprocessing, stop word removal, and lemmatization. We
    then performed dimensionality reduction and visualization with t-SNE and proved
    that count vectorization is a good representation of text data. We proceeded with
    a more modern representation technique, word embedding, and illustrated how to
    utilize a pre-trained embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: We had some fun mining the 20 newsgroups data using dimensionality reduction
    as an unsupervised approach. Moving forward, in the next chapter, we’ll be continuing
    our unsupervised learning journey, specifically looking at topic modeling and
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do you think all of the top 500 word tokens contain valuable information? If
    not, can you impose another list of stop words?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you use stemming instead of lemmatization to process the 20 newsgroups data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you increase `max_features` in `CountVectorizer` from `500` to `5000` and
    see how the t-SNE visualization will be affected?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with representing the data for the three topics discussed in the
    chapter using the `word2vec-google-news-300` model in Gensim and visualize them
    with t-SNE. Assess whether the visualization appears more improved compared to
    the result shown in *Figure 7.6* using word count representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1878468721786989681.png)'
  prefs: []
  type: TYPE_IMG

["```py\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> cancer_data = load_breast_cancer()\n    >>> X = cancer_data.data\n    >>> Y = cancer_data.target\n    >>> print('Input data size :', X.shape)\n    Input data size : (569, 30)\n    >>> print('Output data size :', Y.shape)\n    Output data size : (569,)\n    >>> print('Label names:', cancer_data.target_names)\n    Label names: ['malignant' 'benign']\n    >>> n_pos = (Y == 1).sum()\n    >>> n_neg = (Y == 0).sum()\n    >>> print(f'{n_pos} positive samples and {n_neg} negative samples.')\n    357 positive samples and 212 negative samples. \n    ```", "```py\n    >>> from sklearn.model_selection import train_test_split\n    >>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42) \n    ```", "```py\n    >>> from sklearn.svm import SVC\n    >>> clf = SVC(kernel='linear', C=1.0, random_state=42) \n    ```", "```py\n    >>> clf.fit(X_train, Y_train) \n    ```", "```py\n    >>> accuracy = clf.score(X_test, Y_test)\n    >>> print(f'The accuracy is: {accuracy*100:.1f}%')\n    The accuracy is: 95.8% \n    ```", "```py\n    >>> from sklearn.datasets import load_wine\n    >>> wine_data = load_wine()\n    >>> X = wine_data.data\n    >>> Y = wine_data.target\n    >>> print('Input data size :', X.shape)\n    Input data size : (178, 13)\n    >>> print('Output data size :', Y.shape)\n    Output data size : (178,)\n    >>> print('Label names:', wine_data.target_names)\n    Label names: ['class_0' 'class_1' 'class_2']\n    >>> n_class0 = (Y == 0).sum()\n    >>> n_class1 = (Y == 1).sum()\n    >>> n_class2 = (Y == 2).sum()\n    >>> print(f'{n_class0} class0 samples,\\n{n_class1} class1 samples,\\n{n_class2} class2 samples.')\n    59 class0 samples,\n    71 class1 samples,\n    48 class2 samples. \n    ```", "```py\n    >>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42) \n    ```", "```py\n    >>> clf = SVC(kernel='linear', C=1.0, random_state=42)\n    >>> clf.fit(X_train, Y_train) \n    ```", "```py\n    >>> accuracy = clf.score(X_test, Y_test)\n    >>> print(f'The accuracy is: {accuracy*100:.1f}%')\n    The accuracy is: 97.8% \n    ```", "```py\n    >>> from sklearn.metrics import classification_report\n    >>> pred = clf.predict(X_test)\n    >>> print(classification_report(Y_test, pred))\n                  precision    recall  f1-score   support\n               0       1.00      1.00      1.00        15\n               1       1.00      0.94      0.97        18\n               2       0.92      1.00      0.96        12\n        accuracy                           0.98        45\n       macro avg       0.97      0.98      0.98        45\n    weighted avg       0.98      0.98      0.98        45 \n    ```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> X = np.c_[# negative class\n...           (.3, -.8),\n...           (-1.5, -1),\n...           (-1.3, -.8),\n...           (-1.1, -1.3),\n...           (-1.2, -.3),\n...           (-1.3, -.5),\n...           (-.6, 1.1),\n...           (-1.4, 2.2),\n...           (1, 1),\n...           # positive class\n...           (1.3, .8),\n...           (1.2, .5),\n...           (.2, -2),\n...           (.5, -2.4),\n...           (.2, -2.3),\n...           (0, -2.7),\n...           (1.3, 2.1)].T\n>>> Y = [-1] * 8 + [1] * 8 \n```", "```py\n>>> gamma_option = [1, 2, 4] \n```", "```py\n>>> for i, gamma in enumerate(gamma_option, 1):\n...     svm = SVC(kernel='rbf', gamma=gamma)\n...     svm.fit(X, Y)\n...     plt.scatter(X[:, 0], X[:, 1], c=['b']*8+['r']*8, zorder=10)\n...     plt.axis('tight')\n...     XX, YY = np.mgrid[-3:3:200j, -3:3:200j]\n...     Z = svm.decision_function(np.c_[XX.ravel(), YY.ravel()])\n...     Z = Z.reshape(XX.shape)\n...     plt.pcolormesh(XX, YY, Z > 0 , cmap=plt.cm.Paired)\n...     plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n...            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n...     plt.title('gamma = %d' % gamma)\n...     plt.show() \n```", "```py\n>>> from sklearn.datasets import fetch_lfw_people\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976012\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976009\nDownloading LFW metadata: https://ndownloader.figshare.com/files/5976006\nDownloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015\n>>> face_data = fetch_lfw_people(min_faces_per_person=80) \n```", "```py\npip install pillow \n```", "```py\n>>> face_data = fetch_lfw_people(data_home='./',\n                                 min_faces_per_person=80,   \n                                 download_if_missing=False ) \n```", "```py\n>>> X = face_data.data\n>>> Y = face_data.target\n>>> print('Input data size :', X.shape)\nInput data size : (1140, 2914)\n>>> print('Output data size :', Y.shape)\nOutput data size : (1140,)\n>>> print('Label names:', face_data.target_names)\nLabel names: ['Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Tony Blair'] \n```", "```py\n>>> for i in range(5):\n...     print(f'Class {i} has {(Y == i).sum()} samples.')\nClass 0 has 236 samples.\nClass 1 has 121 samples.\nClass 2 has 530 samples.\nClass 3 has 109 samples.\nClass 4 has 144 samples. \n```", "```py\n>>> fig, ax = plt.subplots(3, 4)\n>>> for i, axi in enumerate(ax.flat):\n...     axi.imshow(face_data.images[i], cmap='bone')\n...     axi.set(xticks=[], yticks=[],\n...             xlabel=face_data.target_names[face_data.target[i]])\n...\n>>> plt.show() \n```", "```py\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n                                                        random_state=42) \n```", "```py\n>>> clf = SVC(class_weight='balanced', random_state=42) \n```", "```py\n>>> parameters = {'C': [10, 100, 300],\n...               'gamma': [0.0001,  0.0003, 0.001],\n...               'kernel' : ['rbf', 'linear'] }\n>>> from sklearn.model_selection import GridSearchCV\n>>> grid_search = GridSearchCV(clf, parameters, n_jobs=-1, cv=5) \n```", "```py\n>>> grid_search.fit(X_train, Y_train) \n```", "```py\n>>> print('The best model:\\n', grid_search.best_params_)\nThe best model:\n {'C': 300, 'gamma': 0.001, 'kernel': 'rbf'} \n```", "```py\n>>> print('The best averaged performance:', grid_search.best_score_)\n The best averaged performance: 0.8456140350877192 \n```", "```py\n>>> clf_best = grid_search.best_estimator_\n>>> pred = clf_best.predict(X_test) \n```", "```py\n>>> print(f'The accuracy is: {clf_best.score(X_test,\n...       Y_test)*100:.1f}%')\nThe accuracy is: 89.8%\n>>> from sklearn.metrics import classification_report\n>>> print(classification_report(Y_test, pred,\n...           target_names=face_data.target_names))\n                   precision    recall  f1-score   support\n     Colin Powell       0.90      0.88      0.89        64\n  Donald Rumsfeld       0.90      0.84      0.87        32\n    George W Bush       0.89      0.94      0.92       127\nGerhard Schroeder       0.90      0.90      0.90        29\n       Tony Blair       0.90      0.85      0.88        33\n        Accuracy                            0.90       285\n        macro avg       0.90      0.88      0.89       285\n     weighted avg       0.90      0.90      0.90       285 \n```", "```py\n>>> from sklearn.decomposition import PCA\n>>> pca = PCA(n_components=100, whiten=True, random_state=42)\n>>> svc = SVC(class_weight='balanced', kernel='rbf',\n...           random_state=42)\n>>> from sklearn.pipeline import Pipeline\n>>> model = Pipeline([('pca', pca),\n...                  ('svc', svc)]) \n```", "```py\n>>> parameters_pipeline = {'svc__C': [1, 3, 10],\n...                       'svc__gamma': [0.01,  0.03, 0.003]}\n>>> grid_search = GridSearchCV(model, parameters_pipeline ,\n                               n_jobs=-1, cv=5)\n>>> grid_search.fit(X_train, Y_train) \n```", "```py\n>>> print('The best model:\\n', grid_search.best_params_)\nThe best model:\n {'svc__C': 1, 'svc__gamma': 0.01}\n>>> print('The best averaged performance:', grid_search.best_score_)\nThe best averaged performance: 0.8619883040935671\n>>> model_best = grid_search.best_estimator_\n>>> print(f'The accuracy is: {model_best.score(X_test, Y_test)*100:.1f}%')\nThe accuracy is: 92.3%\n>>> pred = model_best.predict(X_test)\n>>> print(classification_report(Y_test, pred,\n                                target_names=face_data.target_names))\n                   precision    recall  f1-score   support\n     Colin Powell       0.94      0.94      0.94        64\n  Donald Rumsfeld       0.93      0.84      0.89        32\n    George W Bush       0.91      0.97      0.94       127\nGerhard Schroeder       0.92      0.79      0.85        29\n       Tony Blair       0.94      0.91      0.92        33\n        accuracy                            0.92       285\n        macro avg       0.93      0.89      0.91       285\n     weighted avg       0.92      0.92      0.92       285 \n```", "```py\n    >>> from sklearn import datasets\n    >>> diabetes = datasets.load_diabetes()\n    >>> X = diabetes.data\n    >>> Y = diabetes.target\n    >>> print('Input data size :', X.shape)\n    Input data size : (442, 10)\n    >>> print('Output data size :', Y.shape)\n    Output data size : (442,) \n    ```", "```py\n    >>> num_test = 30   \n    >>> X_train = diabetes.data[:-num_test, :]\n    >>> y_train = diabetes.target[:-num_test]\n    >>> X_test = diabetes.data[-num_test:, :]\n    >>> y_test = diabetes.target[-num_test:] \n    ```", "```py\n    >>> from sklearn.svm import SVR\n    >>> regressor = SVR(C=100, kernel='linear')\n    >>> regressor.fit(X_train, y_train) \n    ```", "```py\n    >>> from sklearn.metrics import r2_score\n    >>> predictions = regressor.predict(X_test)\n    >>> print(r2_score(y_test, predictions))\n    0.5868189735154503 \n    ```", "```py\n    >>> parameters = {'C': [300, 500, 700],\n                      'gamma': [0.3, 0.6, 1],\n                      'kernel' : ['rbf', 'linear']}\n    >>> regressor = SVR()\n    >>> grid_search = GridSearchCV(regressor, parameters, n_jobs=-1,\n                                   cv=5)\n    >>> grid_search.fit(X_train, y_train) \n    ```", "```py\n    >>> print('The best model:\\n', grid_search.best_params_)\n    The best model: {'C': 300, 'gamma': 1.5, 'kernel': 'rbf'} \n    ```", "```py\n    >>> model_best = grid_search.best_estimator_\n    >>> predictions = model_best.predict(X_test)\n    >>> print(r2_score(Y_test, predictions)) \n    ```"]
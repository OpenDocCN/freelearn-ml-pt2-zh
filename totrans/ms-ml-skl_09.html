<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;From the Perceptron to Support Vector Machines"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. From the Perceptron to Support Vector Machines</h1></div></div></div><p>In the previous chapter we discussed the perceptron. As a binary classifier, the perceptron cannot be used to effectively classify linearly inseparable feature representations. We encountered a similar problem to this in our discussion of multiple linear regression in <a class="link" href="ch02.html" title="Chapter 2. Linear Regression">Chapter 2</a>, <span class="emphasis"><em>Linear Regression</em></span>; we examined a dataset in which the response variable was not linearly related to the explanatory variables. To improve the accuracy of the model, we introduced a special case of multiple linear regression called polynomial regression. We created synthetic combinations of features, and were able to model a linear relationship between the response variable and the features in the higher-dimensional feature space.</p><p>While this method of increasing the dimensions of the feature space may seem like a promising technique to use when approximating nonlinear functions with linear models, it suffers from two related problems. The first is a computational problem; computing the mapped features and working with larger vectors requires more computing power. The second problem pertains to generalization; increasing the dimensions of the feature representation introduces the curse of dimensionality. Learning from high-dimensional feature representations requires exponentially more training data to avoid overfitting.</p><p>In this chapter, we will discuss a powerful model for classification and regression called the <span class="strong"><strong>support vector </strong></span>
<a class="indexterm" id="id484"/>
<span class="strong"><strong>machine</strong></span> (<span class="strong"><strong>SVM</strong></span>). First, we will revisit mapping features to higher-dimensional spaces. Then, we will discuss how support vector machines mitigate the computation and generalization problems encountered when learning from the data mapped to higher-dimensional spaces. Entire books are devoted to describing support vector machines, and describing the optimization algorithms used to train SVMs requires more advanced math than we have used in previous chapters. Instead of working through toy examples in detail as we have done in previous chapters, we will try to develop an intuition for how support vector machines work in order to apply them effectively with scikit-learn.</p><div class="section" title="Kernels and the kernel trick"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec61"/>Kernels and the kernel trick</h1></div></div></div><p>Recall that <a class="indexterm" id="id485"/>the perceptron separates the instances of the positive class from the instances of the negative class using a hyperplane as a decision boundary. The decision boundary is given by the following equation:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_01.jpg"/></div><p>Predictions are made using the following function:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_02.jpg"/></div><p>Note that previously we expressed the inner product <span class="inlinemediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_40.jpg"/></span> as <span class="inlinemediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_41.jpg"/></span>. To be consistent with the notational conventions used for support vector machines, we will adopt the former notation in this chapter.</p><p>While the proof is beyond the scope of this chapter, we can write the model differently. The following expression of the model is called <a class="indexterm" id="id486"/>the <span class="strong"><strong>dual</strong></span> form. The expression we used previously is the <span class="strong"><strong>primal</strong></span> <a class="indexterm" id="id487"/>form:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_05.jpg"/></div><p>The most important difference between the primal and dual forms is that the primal form computes the inner product of the <span class="emphasis"><em>model parameters</em></span> and the test instance's feature vector, while the dual form computes the inner product of the <span class="emphasis"><em>training instances</em></span> and the test instance's feature vector. Shortly, we will exploit this property of the dual form to work with linearly inseparable classes. First, we must formalize our definition of mapping features to higher-dimensional spaces.</p><p>In the section on polynomial regression in <a class="link" href="ch02.html" title="Chapter 2. Linear Regression">Chapter 2</a>, <span class="emphasis"><em>Linear Regression</em></span>, we mapped features to a higher-dimensional space in which they were linearly related to the response variable. The mapping increased the number of features by creating quadratic terms from combinations of the original features. These synthetic features allowed us to express a nonlinear function with a linear model. In general, a mapping is given by the following expression:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_07.jpg"/></div><p>The plot on the left in the following figure shows the original feature space of a linearly inseparable data set. The plot on the right shows that the data is linearly separable after mapping to a higher-dimensional space:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_08.jpg"/></div><p>Let's return to the dual form of our decision boundary, and the observation that the feature vectors appear <a class="indexterm" id="id488"/>only inside of a dot product. We could map the data to a higher-dimensional space by applying the mapping to the feature vectors as follows:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_09.jpg"/></div><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_10.jpg"/></div><p>As noted, this mapping allows us to express more complex models, but it introduces computation and generalization problems. Mapping the feature vectors and computing their dot products can require a prohibitively large amount of processing power.</p><p>Observe in the second equation that while we have mapped the feature vectors to a higher-dimensional space, the feature vectors still only appear as a dot product. The dot product is scalar; we do not require the mapped feature vectors once this scalar has been computed. If we can use a different method to produce the same scalar as the dot product of the mapped vectors, we can avoid the costly work of explicitly computing the dot product and mapping the feature vectors.</p><p>Fortunately, there is such a method called the <a class="indexterm" id="id489"/>
<span class="strong"><strong>kernel trick</strong></span>. A <span class="strong"><strong>kernel</strong></span> is a function that, given the original feature vectors, returns the same value as the dot product of its corresponding mapped feature vectors. Kernels do not explicitly map the feature vectors to a higher-dimensional <a class="indexterm" id="id490"/>space, or calculate the dot product of the mapped vectors. Kernels produce the same value through a different series of operations that can often be computed more efficiently. Kernels are defined more formally in the following equation:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_11.jpg"/></div><p>Let's demonstrate how kernels work. Suppose that we have two feature vectors, <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>z</em></span>:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_12.jpg"/></div><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_13.jpg"/></div><p>In our model, we wish to map the feature vectors to a higher-dimensional space using the following transformation:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_14.jpg"/></div><p>The dot product of the mapped, normalized feature vectors is equivalent to:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_15.jpg"/></div><p>The kernel given by the following equation produces the same value as the dot product of the mapped feature vectors:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_16.jpg"/></div><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_11.jpg"/></div><p>Let's plug in values for the feature vectors to make this example more concrete:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_17.jpg"/></div><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_18.jpg"/></div><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_19.jpg"/></div><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_20.jpg"/></div><p>The kernel <span class="inlinemediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_43.jpg"/></span> produced the same value as the dot product <span class="inlinemediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_44.jpg"/></span> of the mapped feature vectors, but never explicitly mapped the feature vectors to the higher-dimensional space and required fewer arithmetic operations. This example used only two dimensional feature <a class="indexterm" id="id491"/>vectors. Data sets with even a modest number of features can <a class="indexterm" id="id492"/>result in mapped feature spaces with massive <a class="indexterm" id="id493"/>dimensions. scikit-learn provides several commonly used kernels, including the polynomial, sigmoid, Gaussian, and linear kernels. Polynomial kernels are given by the following equation:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_21.jpg"/></div><p>Quadratic <a class="indexterm" id="id494"/>kernels, or polynomial kernels where <span class="emphasis"><em>k</em></span> is equal to 2, are <a class="indexterm" id="id495"/>commonly used in natural language processing.</p><p>The sigmoid kernel <a class="indexterm" id="id496"/>is given by the following equation. <span class="inlinemediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_45.jpg"/></span> and <span class="inlinemediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_46.jpg"/></span> are <a class="indexterm" id="id497"/>hyperparameters that can be tuned through cross-validation:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_22.jpg"/></div><p>The Gaussian kernel <a class="indexterm" id="id498"/>is a good first choice for problems requiring nonlinear models. The <a class="indexterm" id="id499"/>Gaussian kernel is a <span class="strong"><strong>radial basis function</strong></span>. A decision boundary that is a hyperplane in the mapped feature space is similar to a <a class="indexterm" id="id500"/>decision boundary that is a hypersphere in the original space.  The feature space produced by the Gaussian kernel can have an infinite number of dimensions, a feat that would be impossible otherwise. The Gaussian kernel is given by the following equation:</p><div class="mediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_23.jpg"/></div><p><span class="inlinemediaobject"><img alt="Kernels and the kernel trick" src="graphics/8365OS_09_45.jpg"/></span> is a hyperparameter. It is always important to scale the features when using support vector machines, but feature scaling is especially important when using the Gaussian kernel.</p><p>Choosing a kernel can be challenging. Ideally, a kernel will measure the similarity between instances in a way that is useful to the task. While kernels are commonly used with support vector machines, they can also be used with any model that can be expressed in terms of the dot product of two feature vectors, including logistic regression, perceptrons, and principal component analysis. In the next section, we will address the second problem caused by mapping to high-dimensional feature spaces: generalization.</p></div></div>
<div class="section" title="Maximum margin classification and support vectors"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec62"/>Maximum margin classification and support vectors</h1></div></div></div><p>The following <a class="indexterm" id="id501"/>figure depicts instances from two linearly separable classes and three possible decision boundaries. All of the decision boundaries separate the training instances of the positive class from the training instances of the negative class, and a perceptron could learn any of them. Which of these decision boundaries is most likely to perform best on test data?</p><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_24.jpg"/></div><p>From this visualization, it is intuitive that the dotted decision boundary is the best. The solid decision boundary is near many of the positive instances. The test set could contain a positive instance that has a slightly smaller value for the first explanatory variable, <span class="inlinemediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_47.jpg"/></span>; this instance would be <a class="indexterm" id="id502"/>classified incorrectly. The dashed decision boundary is farther away from most of the training instances; however, it is near one of the positive instances and one of the negative instances. The following figure provides a different perspective on evaluating decision boundaries:</p><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_25.jpg"/></div><p>Assume that the line plotted is the decision boundary for a logistic regression classifier. The instance labeled <span class="strong"><strong>A</strong></span> is far from the decision boundary; it would be predicted to belong to the positive class with a high probability. The instance labeled <span class="strong"><strong>B</strong></span> would still be predicted to belong to the positive class, but the probability would be lower as the instance is closer to the decision boundary. Finally, the instance labeled <span class="strong"><strong>C</strong></span> would be predicted to belong to the positive class with a low probability; even a small change to the training data could <a class="indexterm" id="id503"/>change the class that is predicted. The most confident predictions are for the instances that are farthest from the decision boundary. We can <a class="indexterm" id="id504"/>estimate the confidence of the prediction using its <span class="strong"><strong>functional margin</strong></span>. The functional margin of the training set is given by the following equations:</p><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_26.jpg"/></div><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_01.jpg"/></div><p>In the preceding formulae <span class="inlinemediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_48.jpg"/></span> is the true class of the instance. The functional margin is large for instance <span class="strong"><strong>A</strong></span> and small for instance <span class="strong"><strong>C</strong></span>. If <span class="strong"><strong>C</strong></span> were misclassified, the functional margin would be negative. The instances for which the functional margin is equal to one are called <span class="strong"><strong>support vectors</strong></span>. These <a class="indexterm" id="id505"/>instances alone are sufficient to define the decision boundary; the other instances are not required to predict the class of a test instance. Related to the functional margin is the <span class="strong"><strong>geometric margin</strong></span>, or the maximum width of the band that separates the support vectors. The geometric margin is equal to the <a class="indexterm" id="id506"/>normalized functional margin. It is necessary to normalize the functional margins as they can be scaled by using <span class="inlinemediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_49.jpg"/></span>, which is problematic for training. When <span class="inlinemediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_49.jpg"/></span> is a unit vector, the geometric margin is equal to the functional vector. We can now formalize our definition of the best decision boundary as having the greatest geometric margin. The model parameters that maximize the geometric margin can be solved through the following constrained optimization problem:</p><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_27.jpg"/></div><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_28.jpg"/></div><p>A useful property of support vector machines is that this optimization problem is convex; it has a single local minimum that is also the global minimum. While the proof is beyond the scope of this chapter, the previous optimization problem can be written using the dual form of the model to accommodate kernels as follows:</p><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_29.jpg"/></div><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_34.jpg"/></div><div class="mediaobject"><img alt="Maximum margin classification and support vectors" src="graphics/8365OS_09_30.jpg"/></div><p>Finding the parameters that maximize the geometric margin subject to the constraints that all of the positive instances have functional margins of at least 1 and all of the negative instances have functional margins of at most -1 is a quadratic programming problem. This problem is <a class="indexterm" id="id507"/>commonly solved using an algorithm called <span class="strong"><strong>Sequential Minimal Optimization</strong></span> (<span class="strong"><strong>SMO</strong></span>). The SMO algorithm breaks the optimization problem down into a series of the smallest possible subproblems, which are then solved analytically.</p></div>
<div class="section" title="Classifying characters in scikit-learn"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec63"/>Classifying characters in scikit-learn</h1></div></div></div><p>Let's apply support <a class="indexterm" id="id508"/>vector machines to a classification problem. In recent years, support vector machines have been used successfully in the task of character recognition. Given an image, the classifier must predict the character that is depicted. Character recognition is a component of many optical character-recognition systems. Even <a class="indexterm" id="id509"/>small images require high-dimensional representations when raw pixel intensities are used as features. If the classes are linearly inseparable and must be mapped to a higher-dimensional feature space, the dimensions of the feature space can become even larger. Fortunately, SVMs are suited to working with such data efficiently. First, we will use scikit-learn to train a support vector machine to recognize handwritten digits. Then, we will work on a more challenging problem: recognizing alphanumeric characters in photographs.</p><div class="section" title="Classifying handwritten digits"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec35"/>Classifying handwritten digits</h2></div></div></div><p>The Mixed National <a class="indexterm" id="id510"/>Institute of Standards and Technology database is a collection of 70,000 images of handwritten digits. The digits were sampled <a class="indexterm" id="id511"/>from documents written by employees of the US Census Bureau and American high school students. The images are grayscale and 28 x 28 pixels in dimension. Let's inspect some of the images using the following script:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; from sklearn.datasets import fetch_mldata
&gt;&gt;&gt; import matplotlib.cm as cm

&gt;&gt;&gt; digits = fetch_mldata('MNIST original', data_home='data/mnist').data
&gt;&gt;&gt; counter = 1
&gt;&gt;&gt; for i in range(1, 4):
&gt;&gt;&gt;     for j in range(1, 6):
&gt;&gt;&gt;         plt.subplot(3, 5, counter)
&gt;&gt;&gt;         plt.imshow(digits[(i - 1) * 8000 + j].reshape((28, 28)), cmap=cm.Greys_r)
&gt;&gt;&gt;         plt.axis('off')
&gt;&gt;&gt;         counter += 1
&gt;&gt;&gt; plt.show()</pre></div><p>First, we load the data. scikit-learn provides the <code class="literal">fetch_mldata</code> convenience function to download the data set if it is not found on disk, and read it into an object. Then, we create a subplot for five instances for the digits zero, one, and two. The script produces the following figure:</p><div class="mediaobject"><img alt="Classifying handwritten digits" src="graphics/8365OS_09_32.jpg"/></div><p>The MNIST data set is partitioned into a training set of 60,000 images and test set of 10,000 images. The dataset is commonly used to evaluate a variety of machine learning models; it is popular <a class="indexterm" id="id512"/>because little preprocessing is required. Let's <a class="indexterm" id="id513"/>use scikit-learn to build a classifier that can predict the digit depicted in an image.</p><p>First, we import the necessary classes:</p><div class="informalexample"><pre class="programlisting">from sklearn.datasets import fetch_mldata
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import scale
from sklearn.cross_validation import train_test_split
from sklearn.svm import SVC
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import classification_report</pre></div><p>The script will fork additional processes during grid search, which requires execution from a<code class="literal"> __main_</code>_ block.</p><div class="informalexample"><pre class="programlisting">if __name__ == '__main__':
    data = fetch_mldata('MNIST original', data_home='data/mnist')
    X, y = data.data, data.target
    X = X/255.0*2 – 1</pre></div><p>Next, we load the data using the <code class="literal">fetch_mldata</code> convenience function. We scale the features and center each feature around the origin. We then split the preprocessed data into training and test sets using the following line of code:</p><div class="informalexample"><pre class="programlisting">    X_train, X_test, y_train, y_test = train_test_split(X, y)</pre></div><p>Next, we instantiate an <code class="literal">SVC</code>, or support vector classifier, object. This object exposes an API like that of scikit-learn's other estimators; the classifier is trained using the <code class="literal">fit</code> method, and predictions are made using the <code class="literal">predict</code> method. If you consult the documentation for <code class="literal">SVC</code>, you will find that the estimator requires more hyperparameters than most of the other estimators we discussed. It is common for more powerful estimators to require more hyperparameters. The most interesting hyperparameters for <code class="literal">SVC</code> are <a class="indexterm" id="id514"/>set by the <code class="literal">kernel</code>, <code class="literal">gamma</code>, and <code class="literal">C</code> keyword arguments. The <code class="literal">kernel</code> keyword argument specifies the kernel to be used. scikit-learn provides <a class="indexterm" id="id515"/>implementations of the linear, polynomial, sigmoid, and radial basis function kernels. The <code class="literal">degree</code> keyword argument should also be set when the polynomial kernel is used. <code class="literal">C</code> controls regularization; it is similar to the lambda hyperparameter we used for logistic regression. The keyword argument <code class="literal">gamma</code> is the kernel coefficient for the sigmoid, polynomial, and RBF kernels. Setting these hyperparameters can be challenging, so we tune them by grid searching with the <a class="indexterm" id="id516"/>following code.</p><div class="informalexample"><pre class="programlisting">    pipeline = Pipeline([
        ('clf', SVC(kernel='rbf', gamma=0.01, C=100))
    ])
    print X_train.shape
    parameters = {
        'clf__gamma': (0.01, 0.03, 0.1, 0.3, 1),
        'clf__C': (0.1, 0.3, 1, 3, 10, 30),
    }
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=2, verbose=1, scoring='accuracy')
    grid_search.fit(X_train[:10000], y_train[:10000])
    print 'Best score: %0.3f' % grid_search.best_score_
    print 'Best parameters set:'
    best_parameters = grid_search.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print '\t%s: %r' % (param_name, best_parameters[param_name])
    predictions = grid_search.predict(X_test)
    print classification_report(y_test, predictions)</pre></div><p>The following is the output of the preceding script:</p><div class="informalexample"><pre class="programlisting">Fitting 3 folds for each of 30 candidates, totalling 90 fits
[Parallel(n_jobs=2)]: Done   1 jobs       | elapsed:  7.7min
[Parallel(n_jobs=2)]: Done  50 jobs       | elapsed: 201.2min
[Parallel(n_jobs=2)]: Done  88 out of  90 | elapsed: 304.8min remaining:  6.9min
[Parallel(n_jobs=2)]: Done  90 out of  90 | elapsed: 309.2min finished
Best score: 0.966
Best parameters set:
	clf__C: 3
	clf__gamma: 0.01
             precision    recall  f1-score   support

        0.0       0.98      0.99      0.99      1758
        1.0       0.98      0.99      0.98      1968
        2.0       0.95      0.97      0.96      1727
        3.0       0.97      0.95      0.96      1803
        4.0       0.97      0.98      0.97      1714
        5.0       0.96      0.96      0.96      1535
        6.0       0.98      0.98      0.98      1758
        7.0       0.97      0.96      0.97      1840
        8.0       0.95      0.96      0.96      1668
        9.0       0.96      0.95      0.96      1729

avg / total       0.97      0.97      0.97     17500</pre></div><p>The best model <a class="indexterm" id="id517"/>has an average F1 score of 0.97; this score can be increased <a class="indexterm" id="id518"/>further by training on more than the first ten thousand instances.</p></div><div class="section" title="Classifying characters in natural images"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec36"/>Classifying characters in natural images</h2></div></div></div><p>Now let's try <a class="indexterm" id="id519"/>a more challenging problem. We <a class="indexterm" id="id520"/>will classify alphanumeric characters in natural images. The Chars74K dataset, collected by T. E. de Campos, B. R. Babu, and M. Varma for <span class="emphasis"><em>Character Recognition in Natural Images</em></span>, contains more than 74,000 images of the digits zero through to nine and <a class="indexterm" id="id521"/>the characters for both cases of the English alphabet. The following are three examples of images of the lowercase letter <code class="literal">z</code>. Chars74K can be downloaded from <a class="ulink" href="http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/">http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/</a>.</p><div class="mediaobject"><img alt="Classifying characters in natural images" src="graphics/8365OS_09_33.jpg"/></div><p>Several types of images comprise the collection. We will use 7,705 images of characters that were extracted from photographs of street scenes taken in Bangalore, India. In contrast to MNIST, the images in this portion of Chars74K depict the characters in a variety of fonts, colors, and perturbations. After expanding the archive, we will use the files in the <code class="literal">English/Img/GoodImg/Bmp/</code> directory. First we will import the necessary classes.</p><div class="informalexample"><pre class="programlisting">import os
import numpy as np
from sklearn.svm import SVC
from sklearn.cross_validation import train_test_split
from sklearn.metrics import classification_report
import Image</pre></div><p>Next we will define a function that resizes images using the Python Image Library:</p><div class="informalexample"><pre class="programlisting">def resize_and_crop(image, size):
    img_ratio = image.size[0] / float(image.size[1])
    ratio = size[0] / float(size[1])
    if ratio &gt; img_ratio:
        image = image.resize((size[0], size[0] * image.size[1] / image.size[0]), Image.ANTIALIAS)
        image = image.crop((0, 0, 30, 30))
    elif ratio &lt; img_ratio:
        image = image.resize((size[1] * image.size[0] / image.size[1], size[1]), Image.ANTIALIAS)
        image = image.crop((0, 0, 30, 30))
    else:
        image = image.resize((size[0], size[1]), Image.ANTIALIAS)
    return image</pre></div><p>Then we load will the images for each of the 62 classes and convert them to grayscale. Unlike MNIST, the images of Chars74K do not have consistent dimensions, so we will resize them to 30 pixels on a side using the resize_and_crop function we defined. Finally, we will convert the processed images to a NumPy array:</p><div class="informalexample"><pre class="programlisting">X = []
y = []

for path, subdirs, files in os.walk('data/English/Img/GoodImg/Bmp/'):
    for filename in files:
        f = os.path.join(path, filename)
        img = Image.open(f).convert('L') # convert to grayscale
        img_resized = resize_and_crop(img, (30, 30))
        img_resized = np.asarray(img_resized.getdata(), dtype=np.float64) \
            .reshape((img_resized.size[1] * img_resized.size[0], 1))
        target = filename[3:filename.index('-')]
        X.append(img_resized)
        y.append(target)

X = np.array(X)
X = X.reshape(X.shape[:2])

We will then train a support vector classifier with a polynomial kernel.classifier = SVC(verbose=0, kernel='poly', degree=3)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)
classifier.fit(X_train, y_train)
predictions = classifier.predict(X_test)
print classification_report(y_test, predictions)</pre></div><p>The preceding script produces the following output:</p><div class="informalexample"><pre class="programlisting">             precision    recall  f1-score   support

        001       0.24      0.22      0.23        23
        002       0.24      0.45      0.32        20
       ...
        061       0.33      0.15      0.21        13
        062       0.08      0.25      0.12         8

avg / total       0.41      0.34      0.36      1927</pre></div><p>It is apparent that this is a more challenging task than classifying digits in MNIST. The appearances of the characters vary more widely, the characters are perturbed more since the images were sampled from photographs rather than scanned documents. Furthermore, there are far fewer training instances for each class in Chars74K than there are in MNIST. The performance of the classifier could be improved by adding training data, preprocessing the images differently, or using more sophisticated feature representations.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Summary</h1></div></div></div><p>In this chapter, we discussed the support vector machine—a powerful model that can mitigate some of the limitations of perceptrons. The perceptron can be used effectively for linearly separable classification problems, but it cannot express more complex decision boundaries without expanding the feature space to higher dimensions. Unfortunately, this expansion is prone to computation and generalization problems. Support vector machines redress the first problem using kernels, which avoid explicitly computing the feature mapping. They redress the second problem by maximizing the margin between the decision boundary and the nearest instances. In the next chapter, we will discuss models called artificial neural networks, which, like support vector machines, extend the perceptron to overcome its limitations.</p></div></body></html>
<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer100">
			<h1 id="_idParaDest-71"><a id="_idTextAnchor069"/>Chapter 4: Training Machine Learning Models</h1>
			<p>In the previous chapter, you learned how Amazon SageMaker Autopilot makes it easy to build, train, and optimize models automatically, without writing a line of machine learning code.</p>
			<p>For problem types that are not supported by SageMaker Autopilot, the next best option is to use one of the algorithms already implemented in SageMaker and to train it on your dataset. These algorithms are referred to as <strong class="bold">built-in algorithms</strong>, and they cover many typical machine learning problems, from classification to time series to anomaly detection.</p>
			<p>In this chapter, you will learn about built-in algorithms for supervised and unsupervised learning, what type of problems you can solve with them, and how to use them with the SageMaker SDK:</p>
			<ul>
				<li>Discovering the built-in algorithms in Amazon SageMaker</li>
				<li>Training and deploying models with built-in algorithms</li>
				<li>Using the SageMaker SDK with built-in algorithms</li>
				<li>Working with more built-in algorithms</li>
			</ul>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor070"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you don't already have one, please point your browser to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> to create one. You should also familiarize yourself with the AWS Free Tier (<a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>), which lets you use many AWS services for free within certain usage limits.</p>
			<p>You will need to install and configure the AWS Command-Line Interface (CLI) for your account (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>). </p>
			<p>You will need a working Python 3.x environment. Installing the Anaconda distribution (<a href="https://www.anaconda.com/">https://www.anaconda.com/</a>) is not mandatory, but strongly encouraged as it includes many projects that we will need (Jupyter, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and more).</p>
			<p>Code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor071"/>Discovering the built-in algorithms in Amazon SageMaker</h1>
			<p>Built-in algorithms are machine learning algorithms implemented, and in some cases invented, by Amazon (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a>). They let you quickly train <a id="_idIndexMarker263"/>and deploy your own <a id="_idIndexMarker264"/>models without writing a line of machine learning code. Indeed, since the training and prediction code is readily available, you don't have to worry about implementing it, and you can focus on the machine learning problem at hand. As usual with SageMaker, infrastructure is fully managed, saving you even more time.</p>
			<p>In this section, you'll learn about the built-in algorithms for traditional machine learning problems. Algorithms for computer vision and natural language processing will be covered in the next two chapters.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor072"/>Supervised learning</h2>
			<p>Supervised learning focuses on <a id="_idIndexMarker265"/>problems that require <a id="_idIndexMarker266"/>a labeled dataset, such as regression or classification:</p>
			<ul>
				<li><strong class="bold">Linear Learner</strong> builds linear models to solve regression problems, as <a id="_idIndexMarker267"/>well as classification problems (binary or multi-class).</li>
				<li><strong class="bold">Factorization Machines</strong> builds linear models to solve <a id="_idIndexMarker268"/>regression problems, as well as classification problems (binary or multi-class). Factorization machines are a generalization of linear models, and they're a good fit for high-dimension, sparse <a id="_idIndexMarker269"/>datasets, such as <a id="_idIndexMarker270"/>user-item interaction matrices in recommendation problems.</li>
				<li><strong class="bold">K-nearest neighbors</strong> (<strong class="bold">KNN</strong>) builds non-parametric models <a id="_idIndexMarker271"/>for regression and classification problems.</li>
				<li><strong class="bold">XGBoost</strong> builds models for regression, classification, and ranking <a id="_idIndexMarker272"/>problems. XGBoost is possibly the most widely used machine learning algorithm used today, and SageMaker uses the open source implementation <a id="_idIndexMarker273"/>available at <a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a>.</li>
				<li><strong class="bold">DeepAR</strong> builds forecasting models for multivariate time series. DeepAR <a id="_idIndexMarker274"/>is an Amazon-invented <a id="_idIndexMarker275"/>algorithm based on <strong class="bold">Recurrent Neural Networks</strong>, and you can read more about it at <a href="https://arxiv.org/abs/1704.04110">https://arxiv.org/abs/1704.04110</a>.</li>
				<li><strong class="bold">Object2Vec</strong> learns low-dimension <a id="_idIndexMarker276"/>embeddings from general-purpose high-dimensional objects. Object2Vec is an algorithm invented by Amazon.</li>
				<li><strong class="bold">BlazingText</strong> builds text classification models. This algorithm was invented by Amazon, and <a id="_idIndexMarker277"/>you can read more about it at <a href="https://dl.acm.org/doi/10.1145/3146347.3146354">https://dl.acm.org/doi/10.1145/3146347.3146354</a>.</li>
			</ul>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor073"/>Unsupervised learning</h2>
			<p>Unsupervised <a id="_idIndexMarker278"/>learning <a id="_idIndexMarker279"/>doesn't require a labeled dataset, and includes problems such as clustering or anomaly detection:</p>
			<ul>
				<li><strong class="bold">K-means</strong> builds clustering models. SageMaker uses a modified version <a id="_idIndexMarker280"/>of the web-scale k-means clustering algorithm (<a href="https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf</a>). </li>
				<li><strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) builds dimensionality <a id="_idIndexMarker281"/>reduction models.</li>
				<li><strong class="bold">Random Cut Forest</strong> builds <a id="_idIndexMarker282"/>anomaly detection models.</li>
				<li><strong class="bold">IP Insights</strong> builds models to identify usage patterns for IPv4 addresses. This comes <a id="_idIndexMarker283"/>in handy for monitoring, cybersecurity, and so on.</li>
				<li><strong class="bold">BlazingText </strong>computes word vectors, a very useful representation for natural language <a id="_idIndexMarker284"/>processing tasks.</li>
			</ul>
			<p>We'll cover some of these algorithms in detail in the rest of this chapter.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor074"/>A word about scalability</h2>
			<p>Before we dive into training and deploying models with the algorithms, you <a id="_idIndexMarker285"/>may wonder why you should use them instead of their counterparts in well-known libraries such as <strong class="source-inline">scikit-learn</strong> and <strong class="source-inline">R</strong>.</p>
			<p>First, these algorithms have been implemented and tuned by Amazon teams, who are not exactly newcomers to machine learning! A lot of effort has been put into making sure that these algorithms run as fast as possible on AWS infrastructure, no matter what type of instance you <a id="_idIndexMarker286"/>use. In addition, many of these algorithms support <strong class="bold">distributed training</strong> out of the box, letting you split model training across a cluster of fully managed instances. </p>
			<p>Thanks to this, benchmarks indicate that these algorithms are generally 10 times better than competing implementations. In many cases, they are also much more cost-effective. You can learn more about this at the following URLs: </p>
			<ul>
				<li>AWS Tel Aviv Summit 2018: <em class="italic">Speed Up Your Machine Learning Workflows with Built-In Algorithms</em>: <a href="https://www.youtube.com/watch?v=IeIUr78OrE0">https://www.youtube.com/watch?v=IeIUr78OrE0</a></li>
				<li><em class="italic">Elastic Machine Learning Algorithms in Amazon</em>, Liberty et al., SIGMOD'20: SageMaker: <a href="https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker">https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker</a></li>
			</ul>
			<p>Of course, these algorithms benefit from all the features present in SageMaker, as you will find out by the end of the book.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor075"/>Training and deploying models with built-in algorithms</h1>
			<p>Amazon SageMaker lets you train and deploy models in <a id="_idIndexMarker287"/>many different configurations. Although it encourages best practices, it is a modular service that lets you do <a id="_idIndexMarker288"/>things your own way.</p>
			<p>In this section, we'll first look at a <a id="_idIndexMarker289"/>typical end-to-end workflow, where we use SageMaker <a id="_idIndexMarker290"/>from data upload all the way to model deployment. Then, we'll discuss alternative workflows, and how you can cherry-pick the features that you need. Finally, we will take a look under the hood, and see what happens from an infrastructure perspective when we train and deploy.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor076"/>Understanding the end-to-end workflow</h2>
			<p>Let's look at a typical SageMaker workflow. You'll see it again <a id="_idIndexMarker291"/>and again in our examples, as well as in the AWS notebooks available on GitHub (<a href="https://github.com/awslabs/amazon-sagemaker-examples/">https://github.com/awslabs/amazon-sagemaker-examples/</a>):</p>
			<ol>
				<li><strong class="bold">Make your dataset available in Amazon S3</strong>: In most examples, we'll download a dataset from the internet, or load a local copy. However, in real life, your raw dataset would probably already be in S3, and you would prepare it using one of the services discussed in <a href="B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Handling Data Preparation Techniques</em>: splitting it for training and validation, engineering features, and so on. In any case, the dataset must be in a format that the algorithm understands, such as CSV and <strong class="source-inline">protobuf</strong> (<a href="https://developers.google.com/protocol-buffers">https://developers.google.com/protocol-buffers</a>).</li>
				<li><strong class="bold">Configure the training job</strong>: This is where you select the algorithm that you want to train with, set hyperparameters, and define infrastructure requirements for the training job.</li>
				<li><strong class="bold">Launch the training job</strong>: This is where we pass the location of your dataset in S3. Training takes place on managed infrastructure, created and provisioned automatically according <a id="_idIndexMarker292"/>to your requirements. Once training is complete, the <strong class="bold">model artifact</strong> is saved in S3. The training infrastructure is terminated automatically, and you only pay for what you used.</li>
				<li><strong class="bold">Deploy the model</strong>: You can deploy a model either on a <strong class="bold">real-time HTTPS endpoint</strong> for live <a id="_idIndexMarker293"/>prediction or for <strong class="bold">batch transform</strong>. Again, you simply need to define infrastructure requirements.</li>
				<li><strong class="bold">Predict data</strong>: Either invoking a real-time endpoint or a batch transformer. As you would expect, infrastructure is managed here too. For production, you would also monitor the quality of data and predictions.</li>
				<li><strong class="bold">Clean up!</strong>: This involves taking the endpoint down, to avoid unnecessary charges.</li>
			</ol>
			<p>Understanding this workflow is critical in being <a id="_idIndexMarker294"/>productive with Amazon SageMaker. Fortunately, the SageMaker SDK has simple APIs that closely match these steps, so you shouldn't be confused about which one to use, or when to use it.</p>
			<p>Before we start looking at the SDK, let's consider alternative workflows that could make sense in your business and technical environments.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor077"/>Using alternative workflows</h2>
			<p>Amazon SageMaker is a modular service <a id="_idIndexMarker295"/>that lets you work your way. Let's first consider a workflow where you would train on SageMaker and deploy on your own server, whatever the reasons may be.</p>
			<h3>Exporting a model</h3>
			<p>Steps 1-3 would be the same as in the <a id="_idIndexMarker296"/>previous example, and then you would do the following:</p>
			<ol>
				<li value="1">Download the training artifact from S3, which is materialized as a <strong class="source-inline">model.tar.gz</strong> file.</li>
				<li>Extract the model stored in the artifact.</li>
				<li>On your own server, load <a id="_idIndexMarker297"/>the model with the appropriate machine learning library:<ul><li><strong class="bold">For XGBoost models</strong>: Use one of the implementations available at <a href="https://xgboost.ai/">https://xgboost.ai/</a>. </li><li><strong class="bold">For BlazingText models</strong>: Use the <strong class="source-inline">fastText</strong> implementation available at <a href="https://fasttext.cc/">https://fasttext.cc/</a>.</li><li><strong class="bold">For all other models</strong>: Use <strong class="bold">Apache MXNet</strong> (<a href="https://mxnet.apache.org/">https://mxnet.apache.org/</a>).</li></ul></li>
			</ol>
			<p>Now, let's see how you could import an existing model and deploy it on SageMaker.</p>
			<h3>Importing a model</h3>
			<p>The steps are equally simple:</p>
			<ol>
				<li value="1">Package your <a id="_idIndexMarker298"/>model in a model artifact (<strong class="source-inline">model.tar.gz</strong>).</li>
				<li>Upload the artifact to an S3 bucket.</li>
				<li>Register the artifact as a SageMaker model.</li>
				<li>Deploy the model and predict.</li>
			</ol>
			<p>This is just a quick look. We'll run full examples for both workflows in <a href="B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Deploying Machine Learning Models</em>.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor078"/>Using fully managed infrastructure</h2>
			<p>All SageMaker jobs run on managed <a id="_idIndexMarker299"/>infrastructure. Let's take a look under the hood and see what happens when we train and deploy models.</p>
			<h3>Packaging algorithms in Docker containers</h3>
			<p>All SageMaker algorithms must be packaged in <strong class="bold">Docker</strong> containers. Don't worry, you <a id="_idIndexMarker300"/>don't need to know much about Docker in order to use SageMaker. If you're not <a id="_idIndexMarker301"/>familiar with it, I would recommend going through this tutorial to understand key concepts and tools: <a href="https://docs.docker.com/get-started/">https://docs.docker.com/get-started/</a>. It's always good to know a little more than actually required!</p>
			<p>As you would expect, built-in algorithms are pre-packaged, and containers are readily available for training <a id="_idIndexMarker302"/>and deployment. They are hosted in <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">ECR</strong>), AWS' Docker <a id="_idIndexMarker303"/>registry service (<a href="https://aws.amazon.com/ecr/">https://aws.amazon.com/ecr/</a>). As ECR is a region-based service, you will find a collection of containers in each region where SageMaker is available. </p>
			<p>You can <a id="_idIndexMarker304"/>find the list of built-in algorithm containers at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html</a>. For instance, the name of the container for the Linear Learner algorithm in the eu-west-1 region is <strong class="source-inline">438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest</strong>. These containers can only be pulled to SageMaker managed instances, so you won't be able to run them on your local machine.</p>
			<p>Now let's look at the underlying infrastructure.</p>
			<h3>Creating the training infrastructure</h3>
			<p>When you launch a training job, SageMaker fires up infrastructure according to your <a id="_idIndexMarker305"/>requirements (instance type and instance count).</p>
			<p>Once a training instance is in service, it pulls the appropriate training container from ECR. Hyperparameters are applied to the algorithm, which also receives the location of your dataset. By default, the algorithm then copies the full dataset from S3 and starts training. If distributed training is configured, SageMaker automatically distributes dataset batches to the different instances in the cluster.</p>
			<p>Once training is complete, the model is packaged in a model artifact saved in S3. Then, the training infrastructure is <a id="_idIndexMarker306"/>shut down automatically. Logs are available in <strong class="bold">Amazon CloudWatch Logs</strong>. Last but not least, you're only charged for the exact amount of training time.</p>
			<h3>Creating the prediction infrastructure</h3>
			<p>When you launch a deployment job, SageMaker once again creates infrastructure <a id="_idIndexMarker307"/>according to your requirements.</p>
			<p>Let's focus on real-time endpoints for now, and not on batch transform. </p>
			<p>Once an endpoint instance is in service, it pulls the appropriate prediction container from ECR and loads your model from S3. Then, the HTTPS endpoint is provisioned and is ready for prediction within minutes.</p>
			<p>If you configured the endpoint with several instances, load balancing and high availability are <a id="_idIndexMarker308"/>set up automatically. If you configured <strong class="bold">Auto Scaling</strong>, this is applied as well.</p>
			<p>As you would expect, an endpoint stays up until it's deleted explicitly, either in the AWS Console or with a SageMaker API call. In the meantime, you will be charged for the endpoint, so <strong class="bold">please make sure to delete endpoints that you don't need!</strong> </p>
			<p>Now that we understand the big picture, let's start looking at the SageMaker SDK, and how we can use it to train and deploy models.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor079"/>Using the SageMaker SDK with built-in algorithms</h1>
			<p>Being<a id="_idIndexMarker309"/> familiar with the SageMaker SDK is important <a id="_idIndexMarker310"/>to making the most of SageMaker. You can find its documentation at <a href="https://sagemaker.readthedocs.io">https://sagemaker.readthedocs.io</a>. </p>
			<p>Walking through a simple example is the best way to get started. In this section, we'll use the Linear Learner algorithm to train a regression model on the Boston Housing dataset (<a href="https://www.kaggle.com/c/boston-housing">https://www.kaggle.com/c/boston-housing</a>). We'll proceed very slowly, leaving no stone unturned. Once again, these concepts are essential, so please take your time, and make sure you understand every step fully.</p>
			<p class="callout-heading">Reminder</p>
			<p class="callout">I recommend that you follow along and run the code available in the companion GitHub repository. Every effort has been made to check all code samples present in the text. However, for those of you who have an electronic version, copying and pasting may have unpredictable results: formatting issues, weird quotes, and so on. </p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor080"/>Preparing data</h2>
			<p>Built-in algorithms expect the dataset to be in a certain format, such as <strong class="bold">CSV</strong>, <strong class="bold">protobuf</strong>, or <strong class="bold">libsvm</strong>. Supported formats are listed in the algorithm documentation. For instance, Linear Learner supports CSV and RecordIO-wrapped protobuf (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output">https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output</a>). </p>
			<p>Our input dataset is already in the repository in CSV format, so let's use that. The dataset preparation will be extremely simple, and we'll run it manually:</p>
			<ol>
				<li value="1">Using <strong class="source-inline">pandas</strong>, we load the<a id="_idIndexMarker311"/> CSV dataset with pandas:<p class="source-code">import pandas as pd</p><p class="source-code">dataset = pd.read_csv('housing.csv')</p></li>
				<li>Then, we print the shape of the dataset:<p class="source-code">print(dataset.shape)</p><p>It contains 506 samples and 13 columns:</p><p class="source-code"><strong class="bold">(506, 13)</strong></p></li>
				<li>Now, we display the first 5 lines of the dataset:<p class="source-code">dataset[:5]</p><p>This prints out the table visible in the following figure. For each house, we see 12 features, and a target attribute (<strong class="source-inline">medv</strong>) set to the median value of the house in thousands of dollars:</p><div id="_idContainer091" class="IMG---Figure"><img src="Images/B17705_04_1.jpg" alt="Figure 4.1 – Viewing the dataset&#13;&#10;" width="618" height="163"/></div><p class="figure-caption">Figure 4.1 – Viewing the dataset</p></li>
				<li>Reading the<a id="_idIndexMarker312"/> algorithm documentation (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html">https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html</a>), we see that <em class="italic">Amazon SageMaker requires that a CSV file doesn't have a header record and that the target variable is in the first column</em>. Accordingly, we move the <strong class="source-inline">medv</strong> column to the front of the dataframe:<p class="source-code">dataset = pd.concat([dataset['medv'],</p><p class="source-code">                     dataset.drop(['medv'], axis=1)], </p><p class="source-code">                     axis=1)</p></li>
				<li>A bit of <strong class="source-inline">scikit-learn</strong> magic helps split the dataframe up into two parts – 90% for training, and 10% for validation:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">training_dataset, validation_dataset =  </p><p class="source-code">    train_test_split(dataset, test_size=0.1)</p></li>
				<li>We save these two splits to individual CSV files, without either an index or a header:<p class="source-code">training_dataset.to_csv('training_dataset.csv', </p><p class="source-code">                        index=False, header=False)</p><p class="source-code">validation_dataset.to_csv('validation_dataset.csv', </p><p class="source-code">                          index=False, header=False)</p></li>
				<li>We now <a id="_idIndexMarker313"/>need to upload these two files to S3. We could use any bucket, and here we'll use the default bucket conveniently created by SageMaker in the region we're running in. We can find its name with the <strong class="source-inline">sagemaker.Session.default_bucket()</strong> API:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()</p></li>
				<li>Finally, we use the <strong class="source-inline">sagemaker.Session.upload_data()</strong> API to upload the two <strong class="bold">CSV</strong> files to the default bucket. Here, the training and validation datasets are made of a single file each, but we could upload multiple files if needed. For this reason, <strong class="bold">we must upload the datasets under different S3 prefixes</strong>, so that their files won't be mixed up:<p class="source-code">prefix = 'boston-housing'</p><p class="source-code">training_data_path = sess.upload_data(</p><p class="source-code">    path='training_dataset.csv', </p><p class="source-code">    key_prefix=prefix + '/input/training')</p><p class="source-code">validation_data_path = sess.upload_data(</p><p class="source-code">    path='validation_dataset.csv', </p><p class="source-code">    key_prefix=prefix + '/input/validation')</p><p class="source-code">print(training_data_path)</p><p class="source-code">print(validation_data_path)</p><p>The two S3 paths look like this. Of course, the account number in the default bucket name will be different:</p><p class="source-code"><strong class="bold">s3://sagemaker-eu-west-1-123456789012/boston-housing/input/training/training_dataset.csv</strong></p><p class="source-code"><strong class="bold">s3://sagemaker-eu-west-1-123456789012/boston-housing/input/validation/validation_dataset.csv</strong></p></li>
			</ol>
			<p>Now that data is ready in S3, we can configure the training job.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor081"/>Configuring a training job</h2>
			<p>The <strong class="source-inline">Estimator</strong> object (<strong class="source-inline">sagemaker.estimator.Estimator</strong>) is the cornerstone of model <a id="_idIndexMarker314"/>training. It lets you select the appropriate algorithm, define your training infrastructure requirements, and more. </p>
			<p>The SageMaker SDK also includes algorithm-specific estimators, such as <strong class="source-inline">sagemaker.LinearLearner</strong> or <strong class="source-inline">sagemaker.PCA</strong>. I generally find them less flexible than the generic estimator (no CSV support, for one thing), and I don't recommend using them. Using the <strong class="source-inline">Estimator</strong> object also lets you reuse your code across examples, as we will see in the next sections:</p>
			<ol>
				<li value="1">Earlier in this chapter, we learned that SageMaker algorithms are packaged in Docker containers. Using <strong class="source-inline">boto3</strong> and the <strong class="source-inline">image_uris.retrieve()</strong> API, we can easily find the name of the Linear Learner algorithm in the region we're running:<p class="source-code">from sagemaker import get_execution_role</p><p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region = sess.boto_session.region_name</p><p class="source-code">container = retrieve('linear-learner', region)</p></li>
				<li>Now that we know the name of the container, we can configure our training job with the <strong class="source-inline">Estimator</strong> object. In addition to the container name, we also pass the IAM role that SageMaker instances will use, the instance type and instance count to use for training, as well as the output location for the model. <strong class="source-inline">Estimator</strong> will generate a training job automatically, and we could also set our own prefix with the <strong class="source-inline">base_job_name</strong> parameter:<p class="source-code">from sagemaker.estimator import Estimator</p><p class="source-code">ll_estimator = Estimator(</p><p class="source-code">    container,</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    output_path='s3://{}/{}/output'.format(bucket, </p><p class="source-code">                                           prefix))</p><p>SageMaker <a id="_idIndexMarker315"/>supports plenty of different instance types, with some differences across AWS regions. You can find the full list at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html">https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html</a>. </p><p>Which one should we use here? Looking at the Linear Learner documentation (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances">https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances</a>), we see that <em class="italic">you can train the Linear Learner algorithm on single- or multi-machine CPU and GPU instances</em>. Here, we're working with a tiny dataset, so let's select the smallest training instance available in our region: <strong class="source-inline">ml.m5.large</strong>. </p><p>Checking the pricing page (<a href="https://aws.amazon.com/sagemaker/pricing/">https://aws.amazon.com/sagemaker/pricing/</a>), we see that this instance costs $0.128 per hour in the eu-west-1 region (the one I'm using for this job).</p></li>
				<li>Next, we have to set <strong class="bold">hyperparameters</strong>. This step is possibly one of the most obscure and <a id="_idIndexMarker316"/>most difficult parts of any machine learning project. Here's my tried and tested advice: read the algorithm documentation, stick to mandatory parameters only unless you really know what you're doing, and quickly check optional parameters for default values that could clash <a id="_idIndexMarker317"/>with your dataset. In <a href="B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Training Techniques</em>, we'll see how to solve hyperparameter selection with <strong class="bold">Automatic Model Tuning</strong>. <p>Let's look at the documentation and see which hyperparameters are mandatory (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html">https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html</a>). As it turns out, there is only one: <strong class="source-inline">predictor_type</strong>. It defines the type of problem that Linear Learner is training on (regression, binary classification, or multiclass classification). </p><p>Taking a deeper look, we see that the default value for <strong class="source-inline">mini_batch_size</strong> is 1000: this <a id="_idIndexMarker318"/>isn't going to work well with our 506-sample dataset, so let's set it to 32. We also learn that the <strong class="source-inline">normalize_data</strong> parameter is set to true by default, which makes it unnecessary to normalize data ourselves:</p><p class="source-code">ll_estimator.set_hyperparameters(</p><p class="source-code">    predictor_type='regressor', </p><p class="source-code">    mini_batch_size=32)</p></li>
				<li>Now, let's define the data channels: a channel is a named source of data passed to a SageMaker estimator. All built-in algorithms need at least a training channel, and many also accept additional channels for validation and testing. Here, we have two channels, which both provide data in CSV format. The <strong class="source-inline">TrainingInput()</strong> API lets us define their location, their format, whether they are compressed, and so on:<p class="source-code">from sagemaker import TrainingInput</p><p class="source-code">training_data_channel = TrainingInput(</p><p class="source-code">    s3_data=training_data_path, </p><p class="source-code">    content_type='text/csv')</p><p class="source-code">validation_data_channel = TrainingInput(</p><p class="source-code">    s3_data=validation_data_path,  </p><p class="source-code">    content_type='text/csv')</p><p>By default, data <a id="_idIndexMarker319"/>served by a channel will be fully copied to each training instance, which is fine for small datasets. We'll study alternatives in <a href="B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Training Techniques</em>.</p></li>
			</ol>
			<p>Everything is now ready for training, so let's launch our job.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor082"/>Launching a training job</h2>
			<p>All it takes is one<a id="_idIndexMarker320"/> line of code:</p>
			<ol>
				<li value="1">We simply pass a Python dictionary containing the two channels to the <strong class="source-inline">fit()</strong> API:<p class="source-code">ll_estimator.fit(</p><p class="source-code">    {'train': training_data_channel, </p><p class="source-code">     'validation': validation_data_channel})</p><p>Immediately, the training job starts:</p><p class="source-code"><strong class="bold">Starting - Starting the training job.</strong></p></li>
				<li>As soon as the job is launched, it appears in the <strong class="bold">SageMaker components and registries</strong> | <strong class="bold">Experiments and trials</strong> panel. There, you can see all job metadata: the location of the dataset, hyperparameters, and more.</li>
				<li>The training log is visible in the notebook, and it's also stored in Amazon CloudWatch Logs, under the <strong class="source-inline">/aws/sagemaker/TrainingJobs</strong> prefix. Here are the first few lines, showing the infrastructure being provisioned, as explained earlier, in the <em class="italic">Using fully managed infrastructure</em> section:<p class="source-code"><strong class="bold">Starting - Starting the training job...</strong></p><p class="source-code"><strong class="bold">Starting - Launching requested ML instances......</strong></p><p class="source-code"><strong class="bold">Starting - Preparing the instances for training...</strong></p><p class="source-code"><strong class="bold">Downloading - Downloading input data...</strong></p><p class="source-code"><strong class="bold">Training - Training image download completed.</strong></p></li>
				<li>At the end of the training log, we see information on the <strong class="bold">mean square error</strong> (<strong class="bold">MSE</strong>) and loss metrics:<p class="source-code"><strong class="bold">#quality_metric: host=algo-1, validation mse &lt;loss&gt;=13.7226685169</strong></p><p class="source-code"><strong class="bold">#quality_metric: host=algo-1, validation absolute_loss &lt;loss&gt;=2.86944983987</strong></p></li>
				<li>Once training is complete, the model is copied automatically to S3, and SageMaker tells us how long the job took:<p class="source-code"><strong class="bold">Uploading - Uploading generated training model</strong></p><p class="source-code"><strong class="bold">Completed - Training job completed</strong></p><p class="source-code"><strong class="bold">Training seconds: 49</strong></p><p class="source-code"><strong class="bold">Billable seconds: 49</strong></p><p>We mentioned earlier that the cost of an <strong class="source-inline">ml.m5.large</strong> instance is $0.128 per hour. As we trained for 49 seconds, this job cost us (49/3600)*0.128= $0.00174 – less than a fifth of a penny. Any time spent setting up infrastructure ourselves would have certainly cost more!</p></li>
				<li>Looking at the output <a id="_idIndexMarker321"/>location in our S3 bucket, we see the model artifact:<p class="source-code"><strong class="bold">%%bash -s "$ll_estimator.output_path"</strong></p><p class="source-code"><strong class="bold">aws s3 ls --recursive $1</strong></p><p>You should see the model artifact <strong class="source-inline">model.tar.gz</strong>.</p></li>
			</ol>
			<p>We'll see in <a href="B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237"><em class="italic">Chapter 11</em></a>, <em class="italic">Deploying Machine Learning Models</em>, what's inside that artifact, and how to deploy the model outside of SageMaker. For now, let's deploy it to a real-time endpoint.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor083"/>Deploying a model</h2>
			<p>This is my favorite <a id="_idIndexMarker322"/>part of SageMaker; we only <a id="_idIndexMarker323"/>need one line of code to deploy a model to an <strong class="bold">HTTPS endpoint</strong>:</p>
			<ol>
				<li value="1">It's good practice to create identifiable and unique endpoint names. We could also let SageMaker create one for us during deployment:<p class="source-code">from time import strftime, gmtime</p><p class="source-code">timestamp = strftime('%d-%H-%M-%S', gmtime())</p><p class="source-code">endpoint_name = 'linear-learner-demo-'+timestamp</p><p class="source-code">print(endpoint_name)</p><p>Here, the endpoint name is <strong class="source-inline">linear-learner-demo-29-08-37-25</strong>.</p></li>
				<li>We deploy the model using the <strong class="source-inline">deploy()</strong> API. As this is a test endpoint, we use the smallest endpoint instance available, <strong class="source-inline">ml.t2.medium</strong>. In the eu-west-1 region, this will only cost us $0.07 per hour:<p class="source-code">ll_predictor = ll_estimator.deploy(</p><p class="source-code">    endpoint_name=endpoint_name,</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.t2.medium')</p><p>When the endpoint is created, we can see it in the <strong class="bold">SageMaker components and registries</strong> | <strong class="bold">Endpoints</strong> panel in SageMaker Studio.</p></li>
				<li>A few minutes later, the endpoint is in service. We can use the <strong class="source-inline">predict()</strong> API to send it a CSV sample for prediction. We set serialization using built-in functions:<p class="source-code">ll_predictor.serializer =   </p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">ll_predictor.deserializer =</p><p class="source-code">    sagemaker.deserializers.CSVDeserializer()</p><p class="source-code">test_sample = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'</p><p class="source-code">response = ll_predictor.predict(test_sample)</p><p class="source-code">print(response)</p><p>The prediction output tells us that this house should cost $30,173:</p><p class="source-code"> <strong class="bold">[['30.17342185974121']]</strong></p><p>We can also predict multiple samples at a time:</p><p class="source-code">test_samples = [</p><p class="source-code">'0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98',</p><p class="source-code">'0.02731,0.00,7.070,0,0.4690,6.4210,78.90,4.9671,2,242.0,17.80,9.14']</p><p class="source-code">response = ll_predictor.predict(test_samples)</p><p class="source-code">print(response)</p><p>Now the<a id="_idIndexMarker324"/> prediction output is as follows:</p><p class="source-code"><strong class="bold"> [['30.413358688354492'],['24.884408950805664']]</strong></p></li>
			</ol>
			<p>When we're done working with the endpoint, <strong class="bold">we shouldn't forget to delete it to avoid unnecessary charges</strong>.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor084"/>Cleaning up</h2>
			<p>Deleting an <a id="_idIndexMarker325"/>endpoint is as simple as calling the <strong class="source-inline">delete_endpoint()</strong> API:</p>
			<p class="source-code">ll_predictor.delete_endpoint()</p>
			<p>At the risk of repeating myself, the topics covered in this section are extremely important, so please make sure you're completely familiar with them, as we'll constantly <a id="_idIndexMarker326"/>use them in the rest of the book. Please spend some time reading the service and SDK documentation as well:</p>
			<ul>
				<li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html">https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html</a></li>
				<li><a href="https://sagemaker.readthedocs.io">https://sagemaker.readthedocs.io</a> </li>
			</ul>
			<p>Now let's explore other built-in algorithms. You'll see that the workflow and the code are very similar!</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor085"/>Working with more built-in algorithms</h1>
			<p>In the rest of this chapter, we will run <a id="_idIndexMarker327"/>more examples with built-in algorithms, both in supervised and unsupervised mode. This will help you become very familiar with the SageMaker SDK and learn how to solve actual machine learning problems. The following list shows some of these algorithms:</p>
			<ul>
				<li>Classification with XGBoost</li>
				<li>Recommendation with Factorization Machines</li>
				<li>Dimensionality reduction with PCA</li>
				<li>Anomaly detection with Random Cut Forest</li>
			</ul>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor086"/>Regression with XGBoost</h2>
			<p>Let's train a model <a id="_idIndexMarker328"/>on the Boston Housing dataset with the <strong class="bold">XGBoost</strong> algorithm (<a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a>). As we will see in <a href="B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending Machine Learning Services Using Built-In Frameworks </em>, SageMaker <a id="_idIndexMarker329"/>also supports XGBoost scripts:</p>
			<ol>
				<li value="1">We reuse the dataset preparation steps from the previous examples.</li>
				<li>We find the name of the XGBoost container. As several versions are supported, we select the latest one (1.3.1 at the time of writing):<p class="source-code">from sagemaker import image_uris</p><p class="source-code">region = sess.boto_session.region_name     </p><p class="source-code">container = image_uris.retrieve('xgboost', region, </p><p class="source-code">                                version='latest')</p></li>
				<li>We configure the <strong class="source-inline">Estimator</strong> function. The code is strictly identical to the code used with <strong class="source-inline">LinearLearner</strong>:<p class="source-code">xgb_estimator = Estimator(</p><p class="source-code">   container,</p><p class="source-code">   role=sagemaker.get_execution_role(),</p><p class="source-code">   instance_count=1,</p><p class="source-code">   instance_type='ml.m5.large',</p><p class="source-code">   output_path='s3://{}/{}/output'.format(bucket, </p><p class="source-code">                                          prefix))</p></li>
				<li>Taking a look at the hyperparameters (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html">https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html</a>), we see that the only required one is <strong class="source-inline">num_round</strong>. As it's not obvious which <a id="_idIndexMarker330"/>value to set, we'll go for a large value, and we'll also define the <strong class="source-inline">early_stopping_rounds</strong> parameter in order to avoid overfitting. Of course, we need to set the objective for a regression problem:<p class="source-code">xgb_estimator.set_hyperparameters(</p><p class="source-code">    objective='reg:linear',</p><p class="source-code">    num_round=200,</p><p class="source-code">    early_stopping_rounds=10)</p></li>
				<li>We define the training input, just like in the previous example:<p class="source-code">from sagemaker import TrainingInput</p><p class="source-code">training_data_channel = TrainingInput(</p><p class="source-code">    s3_data=training_data_path, </p><p class="source-code">    content_type='text/csv')</p><p class="source-code">validation_data_channel = TrainingInput(</p><p class="source-code">    s3_data=validation_data_path,  </p><p class="source-code">    content_type='text/csv')</p></li>
				<li>We then launch the training job:<p class="source-code">xgb_estimator.fit(</p><p class="source-code">    {'train': training_data_channel, </p><p class="source-code">     'validation': validation_data_channel})</p></li>
				<li>The job only ran for 22 rounds, meaning that <strong class="bold">early stopping</strong> was triggered. Looking at the training log, we <a id="_idIndexMarker331"/>see that round #12 was actually <a id="_idIndexMarker332"/>the best one, with a <strong class="bold">root mean square error</strong> (<strong class="bold">RMSE</strong>) of 2.43126:<p class="source-code"><strong class="bold">[12]#011train-rmse:1.25702#011validation-rmse:2.43126</strong></p><p class="source-code"><strong class="bold">&lt;output removed&gt;</strong></p><p class="source-code"><strong class="bold">[22]#011train-rmse:0.722193#011validation-rmse:2.43355</strong></p></li>
				<li>Deploying still takes one line of code:<p class="source-code">from time import strftime, gmtime</p><p class="source-code">timestamp = strftime('%d-%H-%M-%S', gmtime())</p><p class="source-code">endpoint_name = 'xgb-demo'+'-'+timestamp</p><p class="source-code">xgb_predictor = xgb_estimator.deploy(</p><p class="source-code">    endpoint_name=endpoint_name,</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.t2.medium')</p></li>
				<li>Once the model is deployed, we use the <strong class="source-inline">predict()</strong> API again to send it a CSV sample:<p class="source-code">test_sample = '0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,4.98'</p><p class="source-code">xgb_predictor.serializer =</p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">xgb_predictor.deserializer =</p><p class="source-code">    sagemaker.deserializers.CSVDeserializer()</p><p class="source-code">response = xgb_predictor.predict(test_sample)</p><p class="source-code">print(response)</p><p>The result <a id="_idIndexMarker333"/>tells us that this house should cost $23,754:</p><p class="source-code"><strong class="bold">[['23.73023223876953']]</strong></p></li>
				<li>Finally, we delete the endpoint when we're done:<p class="source-code">xgb_predictor.delete_endpoint()</p></li>
			</ol>
			<p>As you can see, the SageMaker workflow is pretty simple and makes it easy to experiment quickly with different algorithms without having to rewrite all your code.</p>
			<p>Let's move on to the Factorization Machines algorithm. In the process, we will learn about the highly efficient RecordIO-wrapped protobuf format.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor087"/>Recommendation with Factorization Machines</h2>
			<p>Factorization Machines is a generalization <a id="_idIndexMarker334"/>of <a id="_idIndexMarker335"/>linear models (<a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf</a>). It's well-suited for high-dimension sparse datasets, such as user-item interaction matrices for recommendation. </p>
			<p>In this example, we're going <a id="_idIndexMarker336"/>to train a recommendation model based on the <strong class="bold">MovieLens</strong> dataset (<a href="https://grouplens.org/datasets/movielens/">https://grouplens.org/datasets/movielens/</a>).</p>
			<p>The dataset exists in several versions. To minimize training times, we'll use the 100k version. It contains 100,000 ratings (integer values from 1 to 5) assigned by 943 users to 1,682 movies. The dataset is already split for training and validation.</p>
			<p>As you know <a id="_idIndexMarker337"/>by now, training and deploying with SageMaker is very simple. Most of the code will be identical to the two previous examples, which is great! This lets us focus on understanding and preparing data.</p>
			<h3>Understanding sparse datasets</h3>
			<p>Imagine building a matrix to store this dataset. It would have 943 lines (one per user) and 1,682 <a id="_idIndexMarker338"/>columns (one per movie). Cells would store the ratings. The following diagram shows a basic example:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="Images/B17705_04_2.jpg" alt="Figure 4.2 – Sparse matrix&#13;&#10;" width="1134" height="388"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Sparse matrix</p>
			<p>Hence, the matrix would have 943*1,682=1,586,126 cells. However, as only 100,000 ratings are present, 93.69% of cells would be empty. Storing our dataset this way would be extremely inefficient. It would needlessly consume RAM, storage, and network bandwidth to store and transfer lots of zero values!</p>
			<p>In fact, things are much worse, as the algorithm expects the input dataset to look like in the following diagram:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="Images/B17705_04_3.jpg" alt="Figure 4.3 – Sparse matrix&#13;&#10;" width="1217" height="568"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Sparse matrix</p>
			<p>Why do we need to store data this way? The answer is simple: Factorization Machines is a <strong class="bold">supervised learning</strong> algorithm, so we need to train it on labeled <a id="_idIndexMarker339"/>samples. </p>
			<p>Looking at the preceding diagram, we see that each line represents a movie review. The matrix on the left <a id="_idIndexMarker340"/>stores its one-hot encoded features (users and movies), and the vector on the right stores its label. For instance, the last line tells us that user 4 has given movie 5 a "5" rating. </p>
			<p>The size of this matrix is 100,000 lines by 2,625 columns (943 movies plus 1,682 movies). The total number of cells is 262,500,000, which are only 0.076% full (200,000 / 262,500,000). If we used a 32-bit value for each cell, we would need almost a gigabyte of memory to store this matrix. This is horribly inefficient but still manageable.</p>
			<p>Just for fun, let's do the same exercise for the largest version of MovieLens, which has 25 million ratings, 62,000 movies, and 162,000 users. The matrix would have 25 million lines and 224,000 columns, for a total of 5,600,000,000,000 cells. Yes, that's 5.6 trillion cells, and although they would be 99.999% empty, we would still need over 20 terabytes of RAM to store them. Ouch. If that's not bad enough, consider recommendation models with millions of users and products: the numbers are mind-boggling!</p>
			<p>Instead of using a <a id="_idIndexMarker341"/>plain matrix, we'll use a <strong class="bold">sparse matrix</strong>, a data structure specifically designed and optimized for sparse datasets. <strong class="source-inline">SciPy</strong> has exactly the object we need, named <strong class="source-inline">lil_matrix</strong> (<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html</a>). This will help us to get rid of all these nasty zeros.</p>
			<h3>Understanding protobuf and RecordIO</h3>
			<p>So how will we pass this sparse matrix to the SageMaker algorithm? As you <a id="_idIndexMarker342"/>would expect, we're going to serialize the object and store it in S3. We're not going to use Python serialization, however. Instead, we're going <a id="_idIndexMarker343"/>to use <strong class="source-inline">protobuf</strong> (<a href="https://developers.google.com/protocol-buffers/">https://developers.google.com/protocol-buffers/</a>), a popular and efficient serialization mechanism. </p>
			<p>In addition, we're going to store the protobuf-encoded data in a <a id="_idIndexMarker344"/>record format called <strong class="bold">RecordIO</strong> (<a href="https://mxnet.apache.org/api/faq/recordio/">https://mxnet.apache.org/api/faq/recordio/</a>). Our dataset will be stored as a sequence <a id="_idIndexMarker345"/>of records in a single file. This has the following benefits:</p>
			<ul>
				<li>A single file is easier to move around: who wants to deal with thousands of individual files that can get lost or corrupted?</li>
				<li>A sequential file is faster to read, which makes the training process more efficient.</li>
				<li>A sequence of records is easy to split for distributed training.</li>
			</ul>
			<p>Don't worry if you're not familiar with protobuf and RecordIO. The SageMaker SDK includes utility functions that hide their complexity.</p>
			<h3>Building a Factorization Machines model on MovieLens</h3>
			<p>We will begin <a id="_idIndexMarker346"/>building the model using the following steps:</p>
			<ol>
				<li value="1">In a Jupyter <a id="_idIndexMarker347"/>notebook, we first download and extract the MovieLens dataset:<p class="source-code">%%sh</p><p class="source-code">wget <strong class="bold">http://files.grouplens.org/datasets/movielens/ml-100k.zip</strong></p><p class="source-code">unzip ml-100k.zip</p></li>
				<li>As the dataset is ordered by user ID, we shuffle it as a precaution. Then, we take a look at the first few lines:<p class="source-code">%cd ml-100k</p><p class="source-code">!shuf ua.base -o ua.base.shuffled</p><p class="source-code">!head -5 ua.base.shuffled</p><p>We see four columns: the user ID, the movie ID, the rating, and a timestamp (which we'll ignore in our model):</p><p class="source-code"><strong class="bold">378  43  3  880056609</strong></p><p class="source-code"><strong class="bold">919  558 5  875372988</strong></p><p class="source-code"><strong class="bold">90   285 5  891383687</strong></p><p class="source-code"><strong class="bold">249  245 2  879571999</strong></p><p class="source-code"><strong class="bold">416  64  5  893212929</strong></p></li>
				<li>We define sizing constants:<p class="source-code">num_users = 943</p><p class="source-code">num_movies = 1682</p><p class="source-code">num_features = num_users+num_movies</p><p class="source-code">num_ratings_train = 90570</p><p class="source-code">num_ratings_test = 9430</p></li>
				<li>Now, let's write a function <a id="_idIndexMarker348"/>to load a dataset <a id="_idIndexMarker349"/>into a sparse matrix. Based on the previous explanation, we go through the dataset line by line. In the X matrix, we set the appropriate user and movie columns to <strong class="source-inline">1</strong>. We also store the rating in the Y vector:<p class="source-code">import csv</p><p class="source-code">import numpy as np</p><p class="source-code">from scipy.sparse import lil_matrix</p><p class="source-code">def loadDataset(filename, lines, columns):</p><p class="source-code">    X = lil_matrix((lines, columns)).astype('float32')</p><p class="source-code">    Y = []</p><p class="source-code">    line=0</p><p class="source-code">    with open(filename,'r') as f:</p><p class="source-code">        samples=csv.reader(f,delimiter='\t')</p><p class="source-code">        for userId,movieId,rating,timestamp in samples:</p><p class="source-code">            X[line,int(userId)-1] = 1</p><p class="source-code">            X[line,int(num_users)+int(movieId)-1] = 1</p><p class="source-code">            Y.append(int(rating))</p><p class="source-code">            line=line+1       </p><p class="source-code">    Y=np.array(Y).astype('float32')</p><p class="source-code">    return X,Y</p></li>
				<li>We then process the <a id="_idIndexMarker350"/>training and<a id="_idIndexMarker351"/> test datasets:<p class="source-code">X_train, Y_train = loadDataset('ua.base.shuffled', </p><p class="source-code">                               num_ratings_train,  </p><p class="source-code">                               num_features)</p><p class="source-code">X_test, Y_test = loadDataset('ua.test',</p><p class="source-code">                             num_ratings_test, </p><p class="source-code">                             num_features)</p></li>
				<li>We check that the shapes are what we expect:<p class="source-code">print(X_train.shape)</p><p class="source-code">print(Y_train.shape)</p><p class="source-code">print(X_test.shape)</p><p class="source-code">print(Y_test.shape)</p><p>This displays the dataset shapes:</p><p class="source-code"><strong class="bold">(90570, 2625)</strong></p><p class="source-code"><strong class="bold">(90570,)</strong></p><p class="source-code"><strong class="bold">(9430, 2625)</strong></p><p class="source-code"><strong class="bold">(9430,)</strong></p></li>
				<li>Now, let's <a id="_idIndexMarker352"/>write a function that converts a dataset to RecordIO-wrapped <strong class="source-inline">protobuf</strong>, and uploads it to an S3 bucket. We first create an in-memory binary <a id="_idIndexMarker353"/>stream with <strong class="source-inline">io.BytesIO()</strong>. Then, we use the lifesaving <strong class="source-inline">write_spmatrix_to_sparse_tensor()</strong> function to write the sample matrix and the label vector to that buffer in <strong class="source-inline">protobuf</strong> format. Finally, we use <strong class="source-inline">boto3</strong> to upload the buffer to S3:<p class="source-code">import io, boto3</p><p class="source-code">import sagemaker.amazon.common as smac</p><p class="source-code">def writeDatasetToProtobuf(X, Y, bucket, prefix, key):</p><p class="source-code">    buf = io.BytesIO()</p><p class="source-code">    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)</p><p class="source-code">    buf.seek(0)</p><p class="source-code">    obj = '{}/{}'.format(prefix, key)  </p><p class="source-code">    </p><p class="source-code">    boto3.resource('s3').Bucket(bucket).Object(obj).</p><p class="source-code">    upload_fileobj(buf)</p><p class="source-code">    return 's3://{}/{}'.format(bucket,obj)</p><p>Had our <a id="_idIndexMarker354"/>data been stored <a id="_idIndexMarker355"/>in a <strong class="source-inline">numpy</strong> array instead of <strong class="source-inline">lilmatrix</strong>, we would have used the <strong class="source-inline">write_numpy_to_dense_tensor()</strong> function instead. It has the same effect.</p></li>
				<li>We apply this function to both datasets, and we store their S3 paths:<p class="source-code">import sagemaker</p><p class="source-code">sess   = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()</p><p class="source-code">prefix = 'fm-movielens'</p><p class="source-code">train_key      = 'train.protobuf'</p><p class="source-code">train_prefix   = '{}/{}'.format(prefix, 'train')</p><p class="source-code">test_key       = 'test.protobuf'</p><p class="source-code">test_prefix    = '{}/{}'.format(prefix, 'test')</p><p class="source-code">output_prefix  = 's3://{}/{}/output'.format(bucket, </p><p class="source-code">                                            prefix)</p><p class="source-code">train_data = writeDatasetToProtobuf(X_train, Y_train, </p><p class="source-code">             bucket, train_prefix, train_key)    </p><p class="source-code">test_data  = writeDatasetToProtobuf(X_test, Y_test, </p><p class="source-code">             bucket, test_prefix, test_key)    </p></li>
				<li>Taking a look at the S3 bucket in a terminal, we see that the training dataset only takes 5.5 MB. The combination of sparse matrix, protobuf, and RecordIO has paid off:<p class="source-code"><strong class="bold">$ aws s3 ls s3://sagemaker-eu-west-1-123456789012/fm-movielens/train/train.protobuf</strong></p><p class="source-code"><strong class="bold">5796480 train.protobuf</strong></p></li>
				<li>What <a id="_idIndexMarker356"/>comes next is SageMaker <a id="_idIndexMarker357"/>business as usual. We find the name of the Factorization Machines container, configure the <strong class="source-inline">Estimator</strong> function, and set the hyperparameters:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region = sess.boto_session.region_name    </p><p class="source-code">container=retrieve('factorization-machines', region)</p><p class="source-code">fm=sagemaker.estimator.Estimator(</p><p class="source-code">    container,</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.c5.xlarge',</p><p class="source-code">    output_path=output_prefix)</p><p class="source-code">fm.set_hyperparameters(</p><p class="source-code">    feature_dim=num_features,</p><p class="source-code">    predictor_type='regressor',</p><p class="source-code">    num_factors=64,</p><p class="source-code">    epochs=10)</p><p>Looking at the documentation (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html">https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html</a>), we see that the required hyperparameters are <strong class="source-inline">feature_dim</strong>, <strong class="source-inline">predictor_type</strong>, and <strong class="source-inline">num_factors</strong>. The default setting for <strong class="source-inline">epochs</strong> is <strong class="source-inline">1</strong>, which feels a little low, so we use <strong class="source-inline">10</strong> instead.</p></li>
				<li>We then launch the training <a id="_idIndexMarker358"/>job. Did you notice that we didn't <a id="_idIndexMarker359"/>configure training inputs? We're simply passing the location of the two <strong class="source-inline">protobuf</strong> files. As <strong class="source-inline">protobuf</strong> is the default format for Factorization Machines (as well as other built-in algorithms), we can save a step:<p class="source-code">fm.fit({'train': train_data, 'test': test_data})</p></li>
				<li>Once the job is over, we deploy the model to a real-time endpoint:<p class="source-code">endpoint_name = 'fm-movielens-100k'</p><p class="source-code">fm_predictor = fm.deploy(</p><p class="source-code">    endpoint_name=endpoint_name,</p><p class="source-code">    instance_type='ml.t2.medium', </p><p class="source-code">    initial_instance_count=1)</p></li>
				<li>We'll now send samples to the endpoint in JSON format (https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html#fm-inputoutput). For this purpose, we write a custom serializer to convert input data to JSON. The default JSON deserializer will be used automatically since we set the content type to <strong class="source-inline">'application/json'</strong>:<p class="source-code">import json</p><p class="source-code">from sagemaker.deserializers import JSONDeserializer</p><p class="source-code">from sagemaker.serializers import JSONSerializer</p><p class="source-code">class FMSerializer(JSONSerializer):</p><p class="source-code">    def serialize(self, data):</p><p class="source-code">        js = {'instances': []}</p><p class="source-code">        for row in data:</p><p class="source-code">            js['instances'].append({'features':   </p><p class="source-code">                            row.tolist()})</p><p class="source-code">        return json.dumps(js)</p><p class="source-code">fm_predictor.serializer = FMSerializer()</p><p class="source-code">fm_predictor.deserializer = JSONDeserializer()</p></li>
				<li>We send the first three samples of the test set for prediction:<p class="source-code">result = fm_predictor.predict(X_test[:3].toarray())</p><p class="source-code">print(result)</p><p>The prediction looks like this:</p><p class="source-code"><strong class="bold">{'predictions': [{'score': 3.3772034645080566}, {'score': 3.4299235343933105}, {'score': 3.6053106784820557}]}</strong></p></li>
				<li>Using this <a id="_idIndexMarker360"/>model, we could fill all the empty cells in the recommendation matrix. For each user, we would simply predict <a id="_idIndexMarker361"/>the score of all movies, and store, say, the top 50 movies. That information would be stored in a backend, and the corresponding metadata (title, genre, and so on) would be displayed to the user in a frontend application.</li>
				<li>Finally, we delete the endpoint:<p class="source-code">fm_predictor.delete_endpoint()</p></li>
			</ol>
			<p>So far, we've only used supervised learning algorithms. In the next section, we'll move on to unsupervised learning with Principal Component Analysis.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor088"/>Using Principal Component Analysis</h2>
			<p><strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) is a dimension reductionality algorithm. It's often applied <a id="_idIndexMarker362"/>as a preliminary step before regression <a id="_idIndexMarker363"/>or classification. Let's use it on the <strong class="source-inline">protobuf</strong> dataset built in the Factorization Machines example. Its 2,625 columns are a good candidate for dimensionality reduction! We will use PCA by taking the following steps:</p>
			<ol>
				<li value="1">Starting from the processed dataset, we configure <strong class="source-inline">Estimator</strong> for PCA. By now, you should (almost) be able to do this with your eyes closed:<p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region = sess.boto_session.region_name   </p><p class="source-code">container = retrieve('pca', region) </p><p class="source-code">pca = sagemaker.estimator.Estimator(</p><p class="source-code">    container=container,</p><p class="source-code">    role=sagemaker.get_execution_role(),</p><p class="source-code">    instance_count=1,                               </p><p class="source-code">    instance_type='ml.c5.xlarge',</p><p class="source-code">    output_path=output_prefix)</p></li>
				<li>We then set the hyperparameters. The required ones are the initial number of features, the number of principal components to compute, and the batch size:<p class="source-code">pca.set_hyperparameters(feature_dim=num_features,</p><p class="source-code">                        num_components=64,</p><p class="source-code">                        mini_batch_size=1024)</p></li>
				<li>We train and deploy the model:<p class="source-code">pca.fit({'train': train_data, 'test': test_data})</p><p class="source-code">pca_predictor = pca.deploy(</p><p class="source-code">    endpoint_name='pca-movielens-100k',</p><p class="source-code">    instance_type='ml.t2.medium',</p><p class="source-code">    initial_instance_count=1)</p></li>
				<li>Then, we predict the first test sample, using <a id="_idIndexMarker364"/>the same serialization code as in the previous example:<p class="source-code">import json</p><p class="source-code">from sagemaker.deserializers import JSONDeserializer</p><p class="source-code">from sagemaker.serializers import JSONSerializer</p><p class="source-code">class PCASerializer(JSONSerializer):</p><p class="source-code">    def serialize(self, data):</p><p class="source-code">        js = {'instances': []}</p><p class="source-code">        for row in data:</p><p class="source-code">            js['instances'].append({'features': </p><p class="source-code">                            row.tolist()})</p><p class="source-code">        return json.dumps(js)</p><p class="source-code">pca_predictor.serializer = PCASerializer()</p><p class="source-code">pca_predictor.deserializer = JSONDeserializer()</p><p class="source-code">result = pca_predictor.predict(X_test[0].toarray())</p><p class="source-code">print(result)</p><p>This prints out the 64 principal components of the test sample. In real life, we typically <a id="_idIndexMarker365"/>would process the dataset with this model, save the results, and use them to train a regression model:</p><p class="source-code"><strong class="bold">{'projections': [{'projection': [-0.008711372502148151, 0.0019895541481673717, 0.002355781616643071, 0.012406938709318638, -0.0069608548656105995, -0.009556426666676998, &lt;output removed&gt;]}]} </strong></p></li>
			</ol>
			<p>Don't forget to <a id="_idIndexMarker366"/>delete the endpoint when you're done. Then, let's run one more unsupervised learning example to conclude this chapter!</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor089"/>Detecting anomalies with Random Cut Forest</h2>
			<p><strong class="bold">Random Cut Forest</strong> (<strong class="bold">RCF</strong>) is an unsupervised <a id="_idIndexMarker367"/>learning algorithm for anomaly detection (<a href="https://proceedings.mlr.press/v48/guha16.pdf">https://proceedings.mlr.press/v48/guha16.pdf</a>). We're going to apply it to a subset of the <a id="_idIndexMarker368"/>household electric power consumption <a id="_idIndexMarker369"/>dataset (<a href="https://archive.ics.uci.edu/ml/">https://archive.ics.uci.edu/ml/</a>), available in the GitHub repository for this book. Data is aggregated <a id="_idIndexMarker370"/>hourly over a period of a little less than a year (just under 8,000 values):</p>
			<ol>
				<li value="1">In a Jupyter notebook, we load the dataset with <strong class="source-inline">pandas</strong>, and we display the first few lines:<p class="source-code">import pandas as pd</p><p class="source-code">df = pd.read_csv('item-demand-time.csv', dtype = object, names=['timestamp','value','client'])</p><p class="source-code">df.head(3)</p><p>As shown in the following screenshot, the dataset has three columns – an hourly timestamp, the power consumption value (in kilowatt-hours), and the client ID:</p><div id="_idContainer094" class="IMG---Figure"><img src="Images/B17705_04_4.jpg" alt="Figure 4.4 – Viewing the columns&#13;&#10;" width="658" height="216"/></div><p class="figure-caption">Figure 4.4 – Viewing the columns</p></li>
				<li>Using <strong class="source-inline">matplotlib</strong>, we plot the dataset to get a quick idea of what it looks like:<p class="source-code">import matplotlib</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df.value=pd.to_numeric(df.value)</p><p class="source-code">df_plot=df.pivot(index='timestamp',columns='item',</p><p class="source-code">                 values='value')</p><p class="source-code">df_plot.plot(figsize=(40,10))</p><p>The plot <a id="_idIndexMarker371"/>is shown in the following <a id="_idIndexMarker372"/>diagram. We see three time series corresponding to three different clients:</p><div id="_idContainer095" class="IMG---Figure"><img src="Images/B17705_04_5.jpg" alt="Figure 4.5 – Viewing the dataset&#13;&#10;" width="1272" height="327"/></div><p class="figure-caption">Figure 4.5 – Viewing the dataset</p></li>
				<li>There are two issues with this dataset. First, it contains several time series: RCF can only train a model on a single series. Second, RCF requires <strong class="bold">integer values</strong>. Let's solve both problems with <strong class="source-inline">pandas</strong> – we only keep the <strong class="source-inline">"client_12"</strong> time series, we multiply its values by 100, and cast them to the integer type:<p class="source-code">df = df[df['item']=='client_12']</p><p class="source-code">df = df.drop(['item', 'timestamp'], axis=1)</p><p class="source-code">df.value *= 100</p><p class="source-code">df.value = df.value.astype('int32')</p><p class="source-code">df.head()</p><p>The following diagram shows <a id="_idIndexMarker373"/>the first lines of the transformed dataset:</p><div id="_idContainer096" class="IMG---Figure"><img src="Images/B17705_04_6.jpg" alt="Figure 4.6 – The values of the first lines&#13;&#10;" width="143" height="223"/></div><p class="figure-caption">Figure 4.6 – The values of the first lines</p></li>
				<li>We plot it <a id="_idIndexMarker374"/>again to check that it looks as expected. Note the large drop right after step 2000, highlighted by a box in the following screenshot. This is clearly an anomaly, and hopefully, our model will catch it:<div id="_idContainer097" class="IMG---Figure"><img src="Images/B17705_04_7.jpg" alt="Figure 4.7 – Viewing a single time series&#13;&#10;" width="533" height="407"/></div><p class="figure-caption">Figure 4.7 – Viewing a single time series</p></li>
				<li>As in the previous examples, we save the dataset to a CSV file, which we upload to S3:<p class="source-code">import sagemaker</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">bucket = sess.default_bucket()</p><p class="source-code">prefix = 'electricity'</p><p class="source-code">df.to_csv('electricity.csv', index=False, </p><p class="source-code">          header=False)</p><p class="source-code">training_data_path = sess.upload_data(</p><p class="source-code">                       path='electricity.csv', </p><p class="source-code">                       key_prefix=prefix + </p><p class="source-code">                                  '/input/training')</p></li>
				<li>Then, we define the <strong class="bold">training channel</strong>. There are a couple of quirks that we haven't met <a id="_idIndexMarker375"/>before. SageMaker <a id="_idIndexMarker376"/>generally doesn't have many of these, and reading the documentation goes a long way in pinpointing them (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html">https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html</a>). <p>First, the <strong class="bold">content type</strong> must state that data is not labeled. The reason for this is that RCF can accept an optional test channel where anomalies are labeled (<strong class="source-inline">label_size=1</strong>). Even though the training channel never has labels, we still need to tell RCF.</p><p>Second, the only <strong class="bold">distribution policy</strong> supported in RCF is <strong class="source-inline">ShardedByS3Key</strong>. This policy splits the dataset across the different instances in the training cluster, instead of sending them a full copy. We won't run distributed training here, but we need to set that policy nonetheless:</p><p class="source-code">training_data_channel = </p><p class="source-code">    sagemaker.TrainingInput(</p><p class="source-code">        s3_data=training_data_path,                                                            </p><p class="source-code">        content_type='text/csv;label_size=0',                                         </p><p class="source-code">        distribution='ShardedByS3Key')</p><p class="source-code">rcf_data = {'train': training_data_channel}</p></li>
				<li>The rest is business as usual: train and deploy! Once <a id="_idIndexMarker377"/>again, we reuse the <a id="_idIndexMarker378"/>code for the previous examples, and it's almost unchanged:<p class="source-code">from sagemaker.estimator import Estimator</p><p class="source-code">from sagemaker.image_uris import retrieve</p><p class="source-code">region = sess.boto_session.region_name</p><p class="source-code">container = retrieve('randomcutforest', region)</p><p class="source-code">rcf_estimator = Estimator(container,</p><p class="source-code">    role= sagemaker.get_execution_role(),</p><p class="source-code">    instance_count=1,</p><p class="source-code">    instance_type='ml.m5.large',</p><p class="source-code">    output_path='s3://{}/{}/output'.format(bucket, </p><p class="source-code">                                           prefix))</p><p class="source-code">rcf_estimator.set_hyperparameters(feature_dim=1)</p><p class="source-code">rcf_estimator.fit(rcf_data)</p><p class="source-code">endpoint_name = 'rcf-demo'</p><p class="source-code">rcf_predictor = rcf_estimator.deploy(</p><p class="source-code">    endpoint_name=endpoint_name,</p><p class="source-code">    initial_instance_count=1,</p><p class="source-code">    instance_type='ml.t2.medium')</p></li>
				<li>After a few <a id="_idIndexMarker379"/>minutes, the model is deployed. We convert the input time series to a Python list, and we send it to <a id="_idIndexMarker380"/>the endpoint for prediction. We use CSV and JSON, respectively, for serialization and deserialization:<p class="source-code">rcf_predictor.serializer =</p><p class="source-code">    sagemaker.serializers.CSVSerializer()</p><p class="source-code">rcf_predictor.deserializer =</p><p class="source-code">    sagemaker.deserializers.JSONDeserializer()</p><p class="source-code">values = df['value'].astype('str').tolist()</p><p class="source-code">response = rcf_predictor.predict(values)</p><p class="source-code">print(response)</p><p>The response contains the anomaly score for each value in the time series. It looks like this:</p><p class="source-code"><strong class="bold">{'scores': [{'score': 1.0868037776}, {'score': 1.5307718138}, {'score': 1.4208102841} …</strong></p></li>
				<li>We then convert this response to a Python list, and we then compute its mean and its standard deviation:<p class="source-code">from statistics import mean,stdev</p><p class="source-code">scores = []</p><p class="source-code">for s in response['scores']:</p><p class="source-code">    scores.append(s['score'])</p><p class="source-code">score_mean = mean(scores)</p><p class="source-code">score_std = stdev(scores)</p></li>
				<li>We plot a subset of the time series and the corresponding scores. Let's focus on the "[2000:2500]" interval, as this is where we saw a large drop. We also plot a line <a id="_idIndexMarker381"/>representing the mean plus <a id="_idIndexMarker382"/>three standard deviations (99.7% of the score distribution) – any score largely exceeding the line is likely to be an anomaly:<p class="source-code">df[2000:2500].plot(figsize=(40,10))</p><p class="source-code">plt.figure(figsize=(40,10))</p><p class="source-code">plt.plot(scores[2000:2500])</p><p class="source-code">plt.autoscale(tight=True)</p><p class="source-code">plt.axhline(y=score_mean+3*score_std, color='red')</p><p class="source-code">plt.show()</p><p>The drop is clearly visible in the following diagram:</p><div id="_idContainer098" class="IMG---Figure"><img src="Images/B17705_04_8.jpg" alt="Figure 4.8 – Zooming in on an anomaly&#13;&#10;" width="1265" height="319"/></div><p class="figure-caption">Figure 4.8 – Zooming in on an anomaly</p><p>As you can see on the following score plot, its score is sky high! Beyond a doubt, this value is an anomaly:</p><div id="_idContainer099" class="IMG---Figure"><img src="Images/B17705_04_9.jpg" alt="Figure 4.9 – Viewing anomaly scores&#13;&#10;" width="1263" height="320"/></div><p class="figure-caption">Figure 4.9 – Viewing anomaly scores</p><p>Exploring <a id="_idIndexMarker383"/>other intervals of the <a id="_idIndexMarker384"/>time series, we could certainly find more. Who said machine learning wasn't fun?</p></li>
				<li>Finally, we delete the endpoint:<p class="source-code">rcf_predictor.delete_endpoint()</p></li>
			</ol>
			<p>Having gone through five complete examples, you should now be familiar with built-in algorithms, the SageMaker workflow, and the SDK. To fully master these topics, I would recommend experimenting with your datasets and running additional examples available at <a href="https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms">https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms</a>.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor090"/>Summary</h1>
			<p>As you can see, built-in algorithms are a great way to quickly train and deploy models without having to write any machine learning code.</p>
			<p>In this chapter, you learned about the SageMaker workflow, and how to implement it with a handful of APIs from the SageMaker SDK, without ever worrying about infrastructure. </p>
			<p>You learned how to work with data in CSV and RecordIO-wrapped protobuf format, the latter being the preferred format for large-scale training on bulky datasets. You also learned how to build models with important algorithms for supervised and unsupervised learning: Linear Learner, XGBoost, Factorization Machines, PCA, and Random Cut Forest. </p>
			<p>In the next chapter, you will learn how to use additional built-in algorithms to build computer vision models.</p>
		</div>
	</div></body></html>
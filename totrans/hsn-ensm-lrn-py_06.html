<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Stacking</h1>
                </header>
            
            <article>
                
<p><strong>Stacking</strong> is the second ensemble learning technique that we will study. Together with voting, it belongs to the non-generative methods class, as they both use individually trained classifiers as base learners.</p>
<p>In this chapter, we will present the main ideas behind stacking, its strengths and weaknesses, and how to select base learners. Furthermore, we will go through the processes of implementing stacking for both regression and classification problems with scikit-learn.</p>
<p>The main topics covered in this chapter are as follows:</p>
<ul>
<li>The methodology of stacking and using a meta-learner to combine predictions</li>
<li>The motivation behind using stacking</li>
<li>The strengths and weaknesses of stacking</li>
<li>Selecting base learners for an ensemble</li>
<li>Implementing stacking for regression and classification problems</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter04">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter04</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2XJgyD2">http://bit.ly/2XJgyD2</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Meta-learning</h1>
                </header>
            
            <article>
                
<p><strong>Meta-learning</strong> is a broad machine learning term. It has a number of meanings, but it generally entails utilizing metadata for a specific problem in order to solve it. Its applications range from solving a problem more efficiently, to designing entirely new learning algorithms. It is a growing research field that has recently yielded impressive results by designing novel deep learning architectures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking</h1>
                </header>
            
            <article>
                
<p>Stacking is a form of meta-learning. The main idea is that we use base learners in order to generate metadata for the problem's dataset and then utilize another learner called a meta-learner, in order to process the metadata. Base learners are considered to be level 0 learners, while the meta learner is considered a level 1 learner. In other words, the meta learner is stacked on top of the base learners, hence the name stacking.</p>
<p>A more intuitive way to describe the ensemble is to present an analogy with voting. In voting, we combined a number of base learners' predictions in order to increase their performance. In stacking, instead of explicitly defining the combination rule, we train a model that learns how to best combine the base learners' predictions. The meta-learner's input dataset consists of the base learners' predictions (metadata), as shown in figure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-593 image-border" src="assets/8becb6b8-de8a-4fb7-9718-921f8c998776.png" style="width:42.00em;height:36.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Stacking ensemble data flow, from original data to the base learners, creating metadata for the meta-learner</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating metadata</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, we need metadata in order to both train and operate our ensemble. During the operation phase, we simply pass the data from our base learners. On the other hand, the training phase is a little more complicated. We want our meta-learner to discover strengths and weaknesses between our base learners. Although some would argue that we could train the base learners on the train set, predict on it, and use the predictions in order to train our meta-learner, this would induce variance. Our meta-learner would discover the strengths and weaknesses of data that has already been seen (by the base learners). As we want to generate models with decent predictive (out-of-sample) performance, instead of descriptive (in-sample) capabilities, another approach must be utilized.</p>
<p>Another approach would be to split our training set into a base learner train set and a meta-learner train (validation) set. This way, we would still retain a true test set where we can measure the ensemble's performance. The drawback of this approach is that we must donate some of the instances to the validation set. Furthermore, both the validation set size and the train set size will be smaller than the original train set size. Thus, the preferred approach is to utilize <strong>K-fold cross validation</strong>. For each <em>K</em>, the base learners will be trained on the <em>K</em>-1 folds and predict on the <em>K</em>th fold, generating 100/<em>K</em> percent of the final training metadata. By repeating the process <em>K</em> times, one for each fold, we will have generated metadata for the whole training dataset. The process is depicted in the following diagram. The final result is a set of metadata for the whole dataset, where the metadata is generated on out-of-sample data (from the perspective of the base learners, for each fold):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-594 image-border" src="assets/63d4726b-b178-4827-aadd-36539dd7fe8d.png" style="width:56.33em;height:31.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Creating metadata with five-fold cross-validation</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deciding on an ensemble's composition</h1>
                </header>
            
            <article>
                
<p>We described stacking as an advanced form of voting. Similarly to voting (and most ensemble learning techniques for that matter), stacking is dependent on the diversity of its base learners. If the base learners exhibit the same characteristics and performance throughout the problem's domain, it will be difficult for the meta-learner to <span>dramatically </span>improve their collective performance. Furthermore, a complex meta-learner will be needed. If the base learners are diverse and exhibit different performance characteristics in different domains of the problem, even a simple meta-learner will be able to greatly improve their collective performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting base learners</h1>
                </header>
            
            <article>
                
<p>It is generally a good idea to mix different learning algorithms, in order to capture both linear and non-linear relationships between the features themselves, as well as the target variable. Take, for example, the following dataset, which exhibits both linear and non-linear relationships between the feature (<em>x</em>) and the target variable (<em>y</em>). It is evident that neither a single linear nor a single non-linear regression will be able to fully model the data. A stacking ensemble with a linear and non-linear regression will be able to greatly outperform either of the two models. Even without stacking, by hand-crafting a simple rule, (for example "use the linear model if x is in the spaces [0, 30] or [60, 100], else use the non-linear") we can greatly outperform the two models:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-595 image-border" src="assets/5e1ab97e-f28c-4e60-b92a-d9550621d653.png" style="width:29.42em;height:22.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Combination of x=5 and x-squared for the example dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selecting the meta-learner</h1>
                </header>
            
            <article>
                
<p>Generally, the meta-learner should be a relatively simple machine learning algorithm, in order to avoid overfitting. Furthermore, additional steps should be taken in order to regularize the meta-learner. For example, if a decision tree is used, then the tree's maximum depth should be limited. If a regression model is used, a regularized regression (such as elastic net or ridge regression) should be preferred. If there is a need for more complex models in order to increase the ensemble's predictive performance, a multi-level stack could be used, in which the number of models and each individual model's complexity reduces as the stack's level increases:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-596 image-border" src="assets/7206e732-7010-4481-a15a-9667fd20e787.png" style="width:32.50em;height:38.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Level stacking ensemble. Each level has simpler models than the previous level</div>
<div>
<p>Another really important characteristic of the meta-learner should be the ability to handle correlated inputs and especially to not make any assumptions about the independence of features from one another, as naive Bayes classifiers do. The inputs to the meta-learner (metadata) will be highly correlated. This happens because all base learners are trained to predict the same target. Thus, their predictions will come from an approximation of the same function. Although the predicted values will vary, they will be close to each other.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Python implementation</h1>
                </header>
            
            <article>
                
<p><span>Although scikit-learn does implement most ensemble methods that we cover in this book, stacking is not one of them. In this section, we will implement custom stacking solutions for both regression and classification problems.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking for regression</h1>
                </header>
            
            <article>
                
<div>
<p>Here, we will try to create a stacking ensemble for the diabetes regression dataset. The ensemble will consist of a 5-neighbor <strong>k-Nearest Neighbors</strong> (<strong>k-NN</strong>), a decision tree limited to a max depth of four, and a ridge regression (a regularized form of least squares regression). The meta-learner will be a simple <strong>Ordinary Least Squares</strong> (<strong>OLS</strong>) linear regression.</p>
<p>First, we have to import the required libraries and data. Scikit-learn provides a convenient method to split data into K-folds, with the <kbd>KFold</kbd> class from the <kbd>sklearn.model_selection</kbd> module. As in previous chapters, we use the first 400 instances for training and the remaining instances for testing:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.neighbors import KNeighborsRegressor<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.linear_model import LinearRegression, Ridge<br/>from sklearn.model_selection import KFold<br/>from sklearn import metrics<br/>import numpy as np<br/>diabetes = load_diabetes()<br/><br/>train_x, train_y = diabetes.data[:400], diabetes.target[:400]<br/>test_x, test_y = diabetes.data[400:], diabetes.target[400:]</pre>
<p>In the following code, we instantiate the base and meta-learners. In order to have ease of access to the individual base learners later on, we store each base learner in a list, called <kbd>base_learners</kbd>:</p>
<pre># --- SECTION 2 ---<br/># Create the ensemble's base learners and meta-learner<br/># Append base learners to a list for ease of access<br/>base_learners = []<br/>knn = KNeighborsRegressor(n_neighbors=5)<br/><br/>base_learners.append(knn)<br/>dtr = DecisionTreeRegressor(max_depth=4 , random_state=123456)<br/><br/>base_learners.append(dtr)<br/>ridge = Ridge()<br/><br/>base_learners.append(ridge)<br/>meta_learner = LinearRegression()</pre>
<p>After instantiating our learners, we need to create the metadata for the training set. We split the training set into five folds by first creating a <kbd>KFold</kbd> object, specifying the number of splits (K) with <kbd>KFold(n_splits=5)</kbd>, and then calling <kbd>KF.split(train_x)</kbd>. This, in turn, returns a generator for the train and test indices of the <span>five</span> splits. For each of these splits, we use the data indicated by <kbd>train_indices</kbd> (four folds) to train our base learners and create metadata on the data corresponding to <kbd>test_indices</kbd>. Furthermore, we store the metadata for each classifier in the <kbd>meta_data</kbd> array and the corresponding targets in the <kbd>meta_targets</kbd> array. Finally, we transpose <kbd>meta_data</kbd> in order to get a (instance, feature) shape:</p>
</div>
<pre># --- SECTION 3 ---<br/># Create the training metadata<br/><br/># Create variables to store metadata and their targets<br/>meta_data = np.zeros((len(base_learners), len(train_x)))<br/>meta_targets = np.zeros(len(train_x))<br/><br/># Create the cross-validation folds<br/>KF = KFold(n_splits=5)<br/>meta_index = 0<br/>for train_indices, test_indices in KF.split(train_x):<br/>  # Train each learner on the K-1 folds <br/>  # and create metadata for the Kth fold<br/>  for i in range(len(base_learners)):<br/>    learner = base_learners[i]<br/>    learner.fit(train_x[train_indices], train_y[train_indices])<br/>    predictions = learner.predict(train_x[test_indices])<br/>    meta_data[i][meta_index:meta_index+len(test_indices)] = \<br/>                              predictions<br/><br/>  meta_targets[meta_index:meta_index+len(test_indices)] = \<br/>                          train_y[test_indices]<br/>  meta_index += len(test_indices)<br/><br/># Transpose the metadata to be fed into the meta-learner<br/>meta_data = meta_data.transpose()</pre>
<p class="NormalPACKT">For the test set, we do not need to split it into folds. We simply train the base learners on the whole train set and predict on the test set. Furthermore, we evaluate each base learner and store the evaluation metrics, in order to compare them with the ensemble's performance. As this is a regression problem, we use R-squared and <strong>Mean Squared Error</strong> (<strong>MSE</strong>) as evaluation metrics:</p>
<pre># --- SECTION 4 ---<br/># Create the metadata for the test set and evaluate the base learners<br/>test_meta_data = np.zeros((len(base_learners), len(test_x)))<br/>base_errors = []<br/>base_r2 = []<br/>for i in range(len(base_learners)):<br/>  learner = base_learners[i]<br/>  learner.fit(train_x, train_y)<br/>  predictions = learner.predict(test_x)<br/>  test_meta_data[i] = predictions<br/><br/>  err = metrics.mean_squared_error(test_y, predictions)<br/>  r2 = metrics.r2_score(test_y, predictions)<br/><br/>  base_errors.append(err)<br/>  base_r2.append(r2)<br/><br/>test_meta_data = test_meta_data.transpose()</pre>
<p class="NormalPACKT">Now, that we have the metadata for both the train and test sets, we can train our meta-learner on the train set and evaluate on the test set:</p>
<pre># --- SECTION 5 ---<br/># Fit the meta-learner on the train set and evaluate it on the test set<br/>meta_learner.fit(meta_data, meta_targets)<br/>ensemble_predictions = meta_learner.predict(test_meta_data)<br/><br/>err = metrics.mean_squared_error(test_y, ensemble_predictions)<br/>r2 = metrics.r2_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 6 ---<br/># Print the results <br/>print('ERROR R2 Name')<br/>print('-'*20)<br/>for i in range(len(base_learners)):<br/>  learner = base_learners[i]<br/>  print(f'{base_errors[i]:.1f} {base_r2[i]:.2f} {learner.__class__.__name__}')<br/>print(f'{err:.1f} {r2:.2f} Ensemble')</pre>
<p class="NormalPACKT">We get the following output:</p>
<pre>ERROR R2 Name<br/>--------------------<br/>2697.8 0.51 KNeighborsRegressor<br/>3142.5 0.43 DecisionTreeRegressor<br/>2564.8 0.54 Ridge<br/>2066.6 0.63 Ensemble</pre>
<p class="mce-root">As is evident, r-squared has improved by over 16% from the best base learner (ridge regression), while MSE has improved by almost 20%. This is a considerable improvement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking for classification</h1>
                </header>
            
            <article>
                
<p>Stacking is a viable method for both regression and classification. In this section, we will use it to classify the breast cancer dataset. Again, we will use three base learners. A 5-neighbor k-NN, a decision tree limited to a max depth of 4, and a simple neural network with 1 hidden layer of 100 neurons. For the meta-learner, we utilize a simple logistic regression.</p>
<p>Again, we load the required libraries and split the data into a train and test set:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.neural_network import MLPClassifier<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import KFold<br/>from sklearn import metrics<br/>import numpy as np<br/>bc = load_breast_cancer()<br/><br/>train_x, train_y = bc.data[:400], bc.target[:400]<br/>test_x, test_y = bc.data[400:], bc.target[400:]</pre>
<p class="mce-root"/>
<p>We instantiate the base learners and the meta-learner. Note that <kbd>MLPClassifier</kbd> has a <kbd>hidden_layer_sizes =(100,)</kbd> parameter, which specifies the number of neurons for each hidden layer. Here, we have a single layer of 100 neurons:</p>
<pre># --- SECTION 2 ---<br/># Create the ensemble's base learners and meta-learner<br/># Append base learners to a list for ease of access<br/>base_learners = []<br/><br/>knn = KNeighborsClassifier(n_neighbors=2)<br/>base_learners.append(knn)<br/><br/>dtr = DecisionTreeClassifier(max_depth=4, random_state=123456)<br/>base_learners.append(dtr)<br/><br/>mlpc = MLPClassifier(hidden_layer_sizes =(100, ), <br/>           solver='lbfgs', random_state=123456)<br/>base_learners.append(mlpc)<br/><br/>meta_learner = LogisticRegression(<span>solver='lbfgs'</span>)</pre>
<p>Again, using <kbd>KFolds</kbd>, we split the train set into five folds in order to train on four folds and generate metadata for the remaining fold, repeated five times. Note that we use <kbd>learner.predict_proba(train_x[test_indices])[:,0]</kbd> in order to get the predicted probability that the instance belongs to in the first class. Given that we have only two classes, this is sufficient. For <em>N</em> classes, we would have to either save <em>N</em>-1 features or simply use <kbd>learner.predict</kbd>, in order to save the predicted class:</p>
<pre># --- SECTION 3 ---<br/># Create the training metadata<br/><br/># Create variables to store metadata and their targets<br/>meta_data = np.zeros((len(base_learners), len(train_x)))<br/>meta_targets = np.zeros(len(train_x))<br/><br/># Create the cross-validation folds<br/>KF = KFold(n_splits=5)<br/>meta_index = 0<br/>for train_indices, test_indices in KF.split(train_x):<br/>   # Train each learner on the K-1 folds and create <br/>   # metadata for the Kth fold<br/>   for i in range(len(base_learners)):<br/>   learner = base_learners[i]<br/><br/>   learner.fit(train_x[train_indices], train_y[train_indices])<br/><span>   predictions = learner.predict_proba(train_x[test_indices])[:,0]</span><br/><br/>   meta_data[i][meta_index:meta_index+len(test_indices)] = predictions<br/><br/>   meta_targets[meta_index:meta_index+len(test_indices)] = \<br/>                           train_y[test_indices]<br/>   meta_index += len(test_indices)<br/><br/># Transpose the metadata to be fed into the meta-learner<br/>meta_data = meta_data.transpose()</pre>
<p>Then, we train the base classifiers on the train set and create metadata for the test set, as well as evaluating their accuracy with <kbd>metrics.accuracy_score(test_y, learner.predict(test_x))</kbd>:</p>
<pre class="mce-root"># --- SECTION 4 ---<br/># Create the metadata for the test set and evaluate the base learners<br/>test_meta_data = np.zeros((len(base_learners), len(test_x)))<br/>base_acc = []<br/>for i in range(len(base_learners)):<br/>  learner = base_learners[i]<br/>  learner.fit(train_x, train_y)<br/>  predictions = learner.predict_proba(test_x)[:,0]<br/>  test_meta_data[i] = predictions<br/><br/>  acc = metrics.accuracy_score(test_y, learner.predict(test_x))<br/>  base_acc.append(acc)<br/>test_meta_data = test_meta_data.transpose()</pre>
<p>Finally, we fit the meta-learner on the train metadata, evaluate its performance on the test data, and print both the ensemble's and the individual learner's accuracy:</p>
<pre># --- SECTION 5 ---<br/># Fit the meta-learner on the train set and evaluate it on the test set<br/>meta_learner.fit(meta_data, meta_targets)<br/>ensemble_predictions = meta_learner.predict(test_meta_data)<br/><br/>acc = metrics.accuracy_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 6 ---<br/># Print the results<br/>print('Acc Name')<br/>print('-'*20)<br/>for i in range(len(base_learners)):<br/>  learner = base_learners[i]<br/>  print(f'{base_acc[i]:.2f} {learner.__class__.__name__}')<br/>print(f'{acc:.2f} Ensemble')</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">The final output is as follows:</p>
<pre class="mce-root">Acc Name<br/>--------------------<br/>0.86 KNeighborsClassifier<br/>0.88 DecisionTreeClassifier<br/>0.92 MLPClassifier  <br/>0.93 Ensemble</pre>
<p class="mce-root">Here, we can see that the meta-learner was only able to improve the ensemble's performance by 1%, compared to the best performing base learner. If we try to utilize the <kbd>learner.predict</kbd> method to generate our metadata, we see that the ensemble actually under performs, compared to the neural network:</p>
<pre>Acc Name<br/>--------------------<br/>0.86 KNeighborsClassifier<br/>0.88 DecisionTreeClassifier<br/>0.92 MLPClassifier<br/>0.91 Ensemble</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a stacking regressor class for scikit-learn</h1>
                </header>
            
            <article>
                
<p>We can utilize the preceding code in order to create a reusable class that orchestrates the ensemble's training and prediction. All scikit-learn classifiers use the standard <kbd>fit(x, y)</kbd> and <kbd>predict(x)</kbd> methods, in order to train and predict respectively. First, we import the required libraries and declare the class and its constructor. The constructor's argument is a list of lists of scikit-learn classifiers. Each sub-list contains the level's learners. Thus, it is easy to construct a multi-level stacking ensemble. For example, a three-level ensemble can be constructed with <kbd>StackingRegressor([ [l11, l12, l13],[ l21, l22], [l31] ])</kbd>. We create a list of each stacking level's size (the number of learners) and also create deep copies of the base learners. The classifier in the last list is considered to be the meta-learner:</p>
<div class="packt_infobox">All of the following code, up to (not including) Section 5 (comment labels), is part of the <kbd>StackingRegressor</kbd> class. It should be properly indented if it is copied to a Python editor.</div>
<pre># --- SECTION 1 ---<br/># Libraries<br/>import numpy as np<br/><br/>from sklearn.model_selection import KFold<br/>from copy import deepcopy<br/><br/>class StackingRegressor():<br/>  # --- SECTION 2 ---<br/>  # The constructor <br/>  def __init__(self, learners):<br/>    # Create a list of sizes for each stacking level<br/>    # And a list of deep copied learners <br/>    self.level_sizes = []<br/>    self.learners = []<br/>    for learning_level in learners:<br/>      self.level_sizes.append(len(learning_level))<br/>      level_learners = []<br/>      for learner in learning_level:<br/>        level_learners.append(deepcopy(learner))<br/>      self.learners.append(level_learners)</pre>
<p>In following the constructor definition, we define the <kbd>fit</kbd> function. The only difference from the simple stacking script we presented in the preceding section is that instead of creating metadata for the meta-learner, we create a list of metadata, one for each stacking level. We save the metadata and targets in the <kbd>meta_data, meta_targets</kbd> lists and use <kbd>data_z, target_z</kbd> as the corresponding variables for each level. Furthermore, we train the level's learners on the metadata of the previous level. We initialize the metadata lists with the original training set and targets:</p>
<pre>  # --- SECTION 3 ---<br/>  # The fit function. Creates training metadata for every level<br/>  # and trains each level on the previous level's metadata<br/>  def fit(self, x, y):<br/>    # Create a list of training metadata, one for each stacking level<br/>    # and another one for the targets. For the first level, <br/>    # the actual data is used.<br/>    meta_data = [x]<br/>    meta_targets = [y]<br/>    for i in range(len(self.learners)):<br/>      level_size = self.level_sizes[i]<br/><br/>      # Create the metadata and target variables for this level<br/>      data_z = np.zeros((level_size, len(x)))<br/>      target_z = np.zeros(len(x))<br/><br/>      train_x = meta_data[i]<br/>      train_y = meta_targets[i]<br/><br/>      # Create the cross-validation folds<br/>      KF = KFold(n_splits=5)<br/>      meta_index = 0<br/>      for train_indices, test_indices in KF.split(x):<br/>        # Train each learner on the K-1 folds and create<br/>        # metadata for the Kth fold<br/>        for j in range(len(self.learners[i])):<br/><br/>          learner = self.learners[i][j]<br/>          learner.fit(train_x[train_indices], <br/>                train_y[train_indices])<br/>          predictions = learner.predict(train_x[test_indices])<br/><br/>          data_z[j][meta_index:meta_index+len(test_indices)] = \<br/>                              predictions<br/><br/>        target_z[meta_index:meta_index+len(test_indices)] = \<br/>                          train_y[test_indices]<br/>        meta_index += len(test_indices)<br/><br/>      # Add the data and targets to the metadata lists<br/>      data_z = data_z.transpose()<br/>      meta_data.append(data_z)<br/>      meta_targets.append(target_z)<br/><br/><br/>      # Train the learner on the whole previous metadata<br/>      for learner in self.learners[i]:<br/>        learner.fit(train_x, train_y)</pre>
<p class="mce-root">Finally, we define the <kbd>predict</kbd> function, which creates metadata for each level for the provided test set, using the same logic as was used in <kbd>fit</kbd> (storing each level's metadata). The function returns the metadata for each level, as they are also the predictions of each level. The ensemble's output can be accessed with <kbd>meta_data[-1]</kbd>:</p>
<pre class="mce-root"><br/>  # --- SECTION 4 ---<br/>  # The predict function. Creates metadata for the test data and returns<br/>  # all of them. The actual predictions can be accessed with <br/>  # meta_data[-1]<br/>  def predict(self, x):<br/><br/>    # Create a list of training metadata, one for each stacking level<br/>    meta_data = [x]<br/>    for i in range(len(self.learners)):<br/>      level_size = self.level_sizes[i]<br/><br/>      data_z = np.zeros((level_size, len(x)))<br/><br/>      test_x = meta_data[i]<br/><br/>      # Create the cross-validation folds<br/>      KF = KFold(n_splits=5)<br/>      for train_indices, test_indices in KF.split(x):<br/>        # Train each learner on the K-1 folds and create<br/>        # metadata for the Kth fold<br/>        for j in range(len(self.learners[i])):<br/><br/>          learner = self.learners[i][j]<br/>          predictions = learner.predict(test_x)<br/>          data_z[j] = predictions<br/><br/>      # Add the data and targets to the metadata lists<br/>      data_z = data_z.transpose()<br/>      meta_data.append(data_z)<br/><br/>    # Return the meta_data the final layer's prediction can be accessed<br/>    # With meta_data[-1]<br/>    return meta_data</pre>
<p class="mce-root">If we instantiate <kbd>StackingRegressor</kbd> with the same meta-learner and base learners as our regression example, we can see that it performs exactly the same! In order to access intermediate predictions, we must access the level's index plus one, as the data in <kbd>meta_data[0]</kbd> is the original test data:</p>
<pre class="mce-root"># --- SECTION 5 ---<br/># Use the classifier<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.neighbors import KNeighborsRegressor<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.linear_model import LinearRegression, Ridge<br/>from sklearn import metrics<br/>diabetes = load_diabetes()<br/><br/>train_x, train_y = diabetes.data[:400], diabetes.target[:400]<br/>test_x, test_y = diabetes.data[400:], diabetes.target[400:]<br/><br/>base_learners = []<br/><br/>knn = KNeighborsRegressor(n_neighbors=5)<br/>base_learners.append(knn)<br/><br/>dtr = DecisionTreeRegressor(max_depth=4, random_state=123456)<br/>base_learners.append(dtr)<br/><br/>ridge = Ridge()<br/>base_learners.append(ridge)<br/><br/>meta_learner = LinearRegression()<br/><br/># Instantiate the stacking regressor<br/>sc = StackingRegressor([[knn,dtr,ridge],[meta_learner]])<br/><br/># Fit and predict<br/>sc.fit(train_x, train_y)<br/>meta_data = sc.predict(test_x)<br/><br/># Evaluate base learners and meta-learner<br/>base_errors = []<br/>base_r2 = []<br/>for i in range(len(base_learners)):<br/>  learner = base_learners[i]<br/>  predictions = meta_data[1][:,i]<br/>  err = metrics.mean_squared_error(test_y, predictions)<br/>  r2 = metrics.r2_score(test_y, predictions)<br/>  base_errors.append(err)<br/>  base_r2.append(r2)<br/><br/>err = metrics.mean_squared_error(test_y, meta_data[-1])<br/>r2 = metrics.r2_score(test_y, meta_data[-1])<br/><br/># Print the results<br/>print('ERROR R2 Name')<br/>print('-'*20)<br/>for i in range(len(base_learners)):<br/>  learner = base_learners[i]<br/>  print(f'{base_errors[i]:.1f} {base_r2[i]:.2f} <br/>      {learner.__class__.__name__}')<br/>print(f'{err:.1f} {r2:.2f} Ensemble')</pre>
<p class="mce-root">The results match with our previous example's result:</p>
<pre class="mce-root">ERROR R2 Name<br/>--------------------<br/>2697.8 0.51 KNeighborsRegressor<br/>3142.5 0.43 DecisionTreeRegressor<br/>2564.8 0.54 Ridge<br/>2066.6 0.63 Ensemble</pre>
<p class="mce-root">In order to further clarify the relationships between the <kbd>meta_data</kbd> and <kbd>self.learners</kbd> lists, we graphically depict their interactions as follows. We initialize <kbd>meta_data[0]</kbd> for the sake of code simplicity. While it can be misleading to store the actual input data in the <kbd>meta_data</kbd> list, it avoids the need to handle the first level of base learners in a different way than the rest:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-597 image-border" src="assets/90f001b3-9e06-4847-8dc0-7ed5beb2952f.png" style="width:25.33em;height:22.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The relationships between each level of meta_data and self.learners</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented an ensemble learning method called stacking (or stacked generalization). It can be seen as a more advanced method of voting. We first presented the basic concept of stacking, how to properly create the metadata, and how to decide on the ensemble's composition. We presented one regression and one classification implementation for stacking. Finally, we presented an implementation of an ensemble class  (implemented similarly to scikit-learn classes), which makes it easier to use multi-level stacking ensembles. <span>The following are some key points to remember from this chapter<span><span>.</span></span></span></p>
<p><strong>Stacking</strong> can consist of many <strong>levels</strong>. Each level generates <strong>metadata</strong> for the next.<span> </span><span>You should create each level's metadata by splitting the train set into <strong>K folds</strong> and iteratively <strong>train on K-1 folds</strong>, while creating <strong>metadata for the Kth fold</strong>. </span><span>After creating the metadata, you should train the current level on the whole train set. </span><span>Base learners must be diverse. The meta-learner should be a relatively simple algorithm that is resistant to overfitting. </span><span>If possible, try to induce regularization in the meta-learner. For example, limit the maximum depth if you use a decision tree or use a regularized regression. </span><span>The meta-learner should be able to handle correlated inputs relatively well. </span><span>You should not be afraid to <strong>add under-performing models</strong> to the ensemble, as long as they introduce new information to the metadata (that is, they handle the dataset differently from the other models). In the next chapter, we will introduce the first generative ensemble method, Bagging.</span></p>


            </article>

            
        </section>
    </body></html>
- en: 'Chapter 10: Advanced Training Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned when and how to scale training jobs using
    features such as **Pipe mode** and **distributed training**, as well as alternatives
    to **S3** for dataset storage.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll conclude our exploration of training techniques. In the
    first part of the chapter, you'll learn how to slash down your training costs
    with **managed spot training**, how to squeeze every drop of accuracy from your
    models with **automatic model tuning**, and how to crack models open with **SageMaker
    Debugger**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second part of the chapter, we''ll introduce two new SageMaker capabilities
    that help you build more efficient workflows and higher quality models: **SageMaker
    Feature Store** and **SageMaker Clarify**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing training costs with managed spot training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing hyperparameters with automatic model tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring models with SageMaker Debugger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing features and building datasets with SageMaker Feature Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting bias and explaining predictions with SageMaker Clarify
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create it. You should also familiarize yourself with the AWS free tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to install and configure the AWS **Command-Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: You will need a working `pandas`, `numpy`, and more).
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing training costs with managed spot training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we trained the **image classification** algorithm on
    the **ImageNet** dataset. The job ran for a little less than 4 hours. At about
    $290 per hour, this job cost us roughly $1,160\. That's a lot of money… but is
    it really?
  prefs: []
  type: TYPE_NORMAL
- en: Comparing costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you throw your arms up the air yelling "*What is he thinking?*", please
    consider how much it would cost your organization to own and run this training
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: A back-of-the-envelope calculation for capital expenditure (servers, storage,
    GPUs, 100 Gbit/s networking equipment) says at least $1.5M. As far as operational
    expenditure is concerned, hosting costs won't be cheap, as each equivalent server
    will require 4-5 kW of power. That's enough to fill one rack at your typical hosting
    company, so even if high-density racks are available, you'll need several. Add
    bandwidth, cross connects, and so on, and my gut feeling says it would cost about
    $15K per month (much more in certain parts of the world).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would need to add hardware support contracts (say, 10% per year, so $150K).
    Depreciating this cluster over 5 years, total monthly costs would be ($1.5M +
    60*$15K + 5*$150K)/60 = $52.5K. Let's round it to $55K to account for labor costs
    for server maintenance and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using conservative estimates, this spend is equivalent to 190 hours of training
    with the large $290-an-hour cluster we've used for our ImageNet example. As we
    will see later in this chapter, managed spot training routinely delivers savings
    of 70%. So, now the spend would be equivalent to about 633 hours of ImageNet training
    per month.
  prefs: []
  type: TYPE_NORMAL
- en: This amounts to 87% usage (633/720) month in, month out, and it's very unlikely
    you'd keep your training cluster that busy. Add downtime, accelerated depreciation
    caused by hardware innovation, hardware insurance costs, the opportunity cost
    of not investing $1.5M in other ventures, and so on, and the business case for
    physical infrastructure gets worse by the minute.
  prefs: []
  type: TYPE_NORMAL
- en: Financials matter, but the worst thing is that you'd only have one cluster.
    What if a potential business opportunity required another one? Would you spend
    another $1.5M? If not, would you have to time-share the existing cluster? Of course,
    only you could decide what's best for your organization. Just make sure that you
    look at the big picture.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how you can easily enjoy that 70% cost reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Amazon EC2 Spot Instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At any given time, **Amazon** **EC2** has more capacity than needed. This allows
    customers to add on-demand capacity to their platforms whenever they need to.
    On-demand instances may be created explicitly using an API call, or automatically
    if **Auto Scaling** is configured. Once a customer has acquired an on-demand instance,
    they will keep it until they decide to release it, either explicitly or automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '**Spot Instances** are a simple way to tap into this unused capacity and to
    enjoy very significant discounts (50-70% are typical). You can request them in
    the same way, and they behave the same too. The only difference is that should
    AWS need the capacity to build on-demand instances, your Spot Instance may be
    reclaimed. It will receive an interruption notification two minutes before being
    forcefully terminated.'
  prefs: []
  type: TYPE_NORMAL
- en: This isn't as bad as it sounds. Depending on regions and instance families,
    Spot Instances may not be reclaimed very often, and customers routinely keep them
    for days, if not more. In addition, you can architecture your application for
    this requirement, for example, by running stateless workloads on Spot Instances
    and relying on managed services for data storage. The cost benefit is too good
    to pass!
  prefs: []
  type: TYPE_NORMAL
- en: 'Going to the `p3dn.24xlarge` for the last three months, where the spot price
    has been 60-70% cheaper than the on-demand price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Viewing the spot price of p3dn.24xlarge'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Viewing the spot price of p3dn.24xlarge
  prefs: []
  type: TYPE_NORMAL
- en: 'These are EC2 prices, but the same discount rates apply to SageMaker prices.
    Discounts vary across instance types, regions, and even availability zones. You
    can use the `describe-spot-price-history` API to collect this information programmatically
    and use it in your workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-spot-price-history.html
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see what this means for SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding managed spot training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training with Spot Instances is available in all SageMaker configurations:
    single-instance training, distributed training, built-in algorithms, frameworks,
    and your own algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting a couple of estimator parameters is all it takes. You don't need to
    worry about handling notifications and interruptions. SageMaker automatically
    does it for you.
  prefs: []
  type: TYPE_NORMAL
- en: If a training job is interrupted, SageMaker regains adequate spot capacity and
    relaunches the training job. If the algorithm uses checkpointing, training resumes
    from the latest checkpoint. If not, the job restarts from the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'How much work is required to implement checkpointing depends on the algorithm
    you''re using:'
  prefs: []
  type: TYPE_NORMAL
- en: The three built-in algorithms for computer vision and **XGBoost** support checkpointing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All other built-in algorithms don't. You can still train them with Spot Instances.
    However, the maximum running time is limited to 60 minutes to minimize potential
    waste. If your training job takes longer than 60 minutes, you should try scaling
    it. If that's not enough, you'll have to use on-demand instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **deep learning containers** for **TensorFlow**, **PyTorch**, **Apache**
    **MXNet**, and **Hugging Face** come with built-in checkpointing, and you don't
    need to modify your training script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use other frameworks or your own custom code, you need to implement checkpointing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, checkpoints are saved inside the training container. The default
    path is `/opt/ml/checkpoints`, and you can customize it with an estimator parameter.
    SageMaker also automatically persists these checkpoints to a user-defined S3 path.
    If your training job is interrupted and relaunched, checkpoints are automatically
    copied inside the container. Your code can check for their presence and load the
    appropriate one to resume training.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that checkpointing is available even when you train with on-demand
    instances. This may come in handy if you'd like to store checkpoints in S3 for
    further inspection or for incremental training. The only restriction is that checkpointing
    is not available with **Local mode**.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, checkpointing does slow down jobs, especially for large
    models. However, this is a small price to pay to avoid restarting long-running
    jobs from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's add managed spot training to the **object detection** job we ran
    in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091), *Training Computer
    Vision Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Using managed spot training with object detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Switching from on-demand training to managed spot training is very simple. We
    just have to set the maximum duration of the training job, including any time
    spent waiting for Spot Instances to be available.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set a maximum running time of 2 hours, plus 8 hours for any spot delay.
    If either one of these bounds is exceeded, the job will be terminated automatically.
    This is helpful in killing runaway jobs that last much longer than expected or
    jobs that are stuck waiting for spot instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We train with the same configuration as before: Pipe mode and `dist_sync` mode.
    As the first epoch completes, the training log tells us that checkpointing is
    active. A new checkpoint is saved automatically each time the validation metric
    improves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training job is complete, the training log tells us how much we saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Not only is this job 70% cheaper than its on-demand counterpart, but it's also
    less than half the price of our original single-instance job. This means that
    we could use more instances and accelerate our training job for the same budget.
    Indeed, managed spot training lets you optimize the duration of a job and its
    cost. Instead of complex capacity planning, you can set a training budget that
    fits your business requirements, and then grab as much infrastructure as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try another example where we implement checkpointing in **Keras**.
  prefs: []
  type: TYPE_NORMAL
- en: Using managed spot training and checkpointing with Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we'll build a simple `Sequential` API in TensorFlow 2.1.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing with Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s first look at the Keras script itself. For the sake of brevity, only
    important steps are presented here. You can find the full code in the GitHub repository
    for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Script mode, we store dataset paths and hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we load the dataset and normalize pixel values to the [0,1] range. We
    also one-hot encode class labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We build a `Sequential` model: two convolution blocks (`Conv2D` / `BatchNormalization`
    / `ReLU` / `MaxPooling2D` / `Dropout`), then two fully connected blocks (`Dense`
    / `BatchNormalization` / `ReLU` / `Dropout`), and finally, a `softmax` output
    layer for the 10 classes in the dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We compile the model using the **categorical cross-entropy** loss function
    and the **Adam** optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a Keras callback to checkpoint the model each time validation accuracy
    improves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We train the model, adding the callback we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When training is complete, we save the model in the **TensorFlow Serving**
    format, which is required to deploy on SageMaker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's look at our training notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Training with managed spot training and checkpointing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the same workflow as before:'
  prefs: []
  type: TYPE_NORMAL
- en: We download the Fashion-MNIST dataset and save it to a local directory. We upload
    the dataset to S3, and we define the S3 location where SageMaker should copy the
    checkpoints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We configure a `TensorFlow` estimator, enabling managed spot training and passing
    the S3 output location for checkpoints. This time, we use an `ml.g4dn.xlarge`
    instance. This very cost-effective GPU instance ($0.822 in `eu-west-1`) is more
    than enough for a small model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We launch training as usual, and the job hits 93.11% accuracy. Training lasts
    289 seconds, and we're only billed for 87 seconds, thanks to a 69.9% discount.
    The total cost is 1.98 cents! Who said GPU training had to be costly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the training log, we see that a checkpoint is created every time validation
    accuracy improves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While the job is running, we also see that checkpoints are copied to S3:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If our spot job gets interrupted, SageMaker will copy checkpoints inside the
    container so that we can use them to resume training. This requires some logic
    in our Keras script to load the latest checkpoint. Let's see how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Resuming training from a checkpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is a pretty simple process—look for checkpoints, and resume training from
    the latest one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We list the checkpoint directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If checkpoints are present, we find the most recent and its epoch number. Then,
    we load the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If no checkpoint is present, we build the model as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We compile the model, and we launch training, passing the number of the last
    epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How can we test this? There is no way to intentionally cause a spot interruption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the trick: start a new training job with existing checkpoints in the
    `checkpoint_s3_uri` path, and increase the number of epochs. This will simulate
    resuming an interrupted job.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting the number of epochs to 25 and keeping the checkpoints in `s3://sagemaker-eu-west-1-123456789012/keras2`
  prefs: []
  type: TYPE_NORMAL
- en: '`fashion-mnist/checkpoints`, we launch the training job again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the training log, we see that the latest checkpoint is loaded and that training
    resumes at epoch 21:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We also see that new checkpoints are created as validation accuracy improves,
    and they''re copied to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it's not difficult to set up checkpointing in SageMaker, and
    you should be able to do the same for other frameworks. Thanks to this, you can
    enjoy the deep discount provided by managed spot training without the risk of
    losing any work if an interruption occurs. Of course, you can use checkpointing
    on its own to inspect intermediate training results, or for incremental training.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we''re going to introduce another important feature: automatic
    model tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing hyperparameters with automatic model tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyperparameters have a huge influence on the training outcome. Just like in
    **chaos theory**, tiny variations of a single hyperparameter can cause wild swings
    in accuracy. In most cases, the "why?" evades us, leaving us perplexed about what
    to try next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the years, several techniques have been devised to try to solve the problem
    of selecting optimal hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual search**: This means using our best judgment and experience to select
    the "best" hyperparameters. Let''s face it: this doesn''t really work, especially
    with deep learning and its horde of training and network architecture parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Grid search**: This entails systematically exploring the hyperparameter space,
    zooming in on hot spots, and repeating the process. This is much better than a
    manual search. However, this usually requires training hundreds of jobs. Even
    with scalable infrastructure, the time and dollar budgets can be significant.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Random search**: This refers to selecting hyperparameters at random. Unintuitive
    as it sounds, James Bergstra and Yoshua Bengio (of Turing Award fame) proved in
    2012 that this technique delivers better models than a grid search with the same
    compute budget'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.jmlr.org/papers/v13/bergstra12a.html](http://www.jmlr.org/papers/v13/bergstra12a.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Hyperparameter optimization** (HPO): This means using optimization techniques
    to select hyperparameters, such as **Bayesian optimization** and **Gaussian process
    regression**. With the same compute budget, HPO typically delivers results with
    10x fewer training epochs than other techniques.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding automatic model tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker includes an **automatic model tuning** capability that lets you easily
    explore hyperparameter ranges and quickly optimize any training metric with a
    limited number of jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model tuning supports both random search and HPO. The former is an interesting
    baseline that helps you to check whether the latter is indeed overperforming.
    You can find a very detailed comparison in this excellent blog post:'
  prefs: []
  type: TYPE_NORMAL
- en: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/
  prefs: []
  type: TYPE_NORMAL
- en: Model tuning is completely agnostic to the algorithm you're using. It works
    with built-in algorithms, and the documentation lists the hyperparameters that
    can be tuned. It also works with all frameworks and custom containers, and hyperparameters
    are passed in the same way.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each hyperparameter that we want to optimize, we have to define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A type (parameters can either be an integer, continuous, or categorical)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A range of values to explore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scaling type (linear, logarithmic, or reverse logarithmic, or auto)—this lets
    us control how a specific parameter range will be explored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also define the metric we want to optimize for. It can be any numerical value
    as long as it's visible in the training log and you can pass a regular expression
    to extract it.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we launch the tuning jobs, passing all of these parameters as well as
    the number of training jobs to run and their degree of parallelism. With Bayesian
    optimization, you'll get the best results with sequential jobs (no parallelism),
    as optimization can be applied after each job. Having said that, running a small
    number of jobs in parallel is acceptable. Random search has no restrictions on
    parallelism as jobs are completely unrelated.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the `deploy()` API on the tuner object deploys the best model. If tuning
    is is still in progress, it will deploy the best model so far, which can be useful
    for early testing.
  prefs: []
  type: TYPE_NORMAL
- en: Let's run the first example with a built-in algorithm and learn about the model
    tuning API.
  prefs: []
  type: TYPE_NORMAL
- en: Using automatic model tuning with object detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re going to optimize our object detection job. Looking at the documentation,
    we can see the list of tunable hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tuning.html
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to optimize the learning rate, momentum, and weight decay:'
  prefs: []
  type: TYPE_NORMAL
- en: We set up the input channels using Pipe mode. There's no change here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We also configure the estimator as usual, setting up managed spot training
    to minimize costs. We''ll train on a single instance for maximum accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the same hyperparameters as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the three extra hyperparameters we want to tune. We explicitly set
    logarithmic scaling for the learning rate, to make sure that different orders
    of magnitude are explored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set the metric to optimize for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We put everything together, using the `HyperparameterTuner` object. We decide
    to run 30 jobs, with two jobs in parallel. We also enable early stopping to weed
    out low performing jobs, saving us time and money:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We launch training on the tuner object (not on the estimator) without waiting
    for it to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the moment, **SageMaker Studio** doesn''t provide a convenient view of tuning
    jobs. Instead, we can track progress in the **Hyperparameter tuning jobs** section
    of the SageMaker console, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Viewing tuning jobs in the SageMaker console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Viewing tuning jobs in the SageMaker console
  prefs: []
  type: TYPE_NORMAL
- en: The job runs for 17 hours (wall time). 22 jobs completed and 8 stopped early.
    The total training time is 30 hours and 15 minutes. Applying the 70% spot discount,
    the total cost is 25.25 * $4.131 * 0.3 = $37.48.
  prefs: []
  type: TYPE_NORMAL
- en: 'How well did this tuning job do? With default hyperparameters, our standalone
    training job reached a `0.2453`. Our tuning job hits `0.6337`, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Tuning job results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Tuning job results
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph for validation mAP is shown in the next image. It tells me that we
    could probably train a little longer and get extra accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Viewing the mAP metric'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Viewing the mAP metric
  prefs: []
  type: TYPE_NORMAL
- en: One idea would be to launch a single training job with the best hyperparameters
    and let it run for more epochs. We could also resume the tuning job using `deploy()`
    on the tuner object and test our model just like any SageMaker model.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, automatic model tuning is extremely powerful. By running a small
    number of jobs, we improved our metric by 158%! The cost is negligible compared
    to the time you would spend experimenting with other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, running the same tuning job using the random strategy delivers a top
    accuracy of 0.52\. We would certainly need to run many more training jobs to even
    hope hitting 0.6315.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now try to optimize the Keras example we used earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using automatic model tuning with Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automatic model tuning can easily be used any algorithm on SageMaker, which
    of course includes all frameworks. Let's see how this works with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we trained our Keras CNN on the Fashion MNIST dataset
    for 20 epochs and reached a validation accuracy of 93.11%. Let's see if we can
    improve it with automatic model tuning. In the process, we'll also learn how to
    optimize for any metric present in the training log, not just metrics that are
    predefined in SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing on a custom metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Modifying our training script, we install the `keras-metrics` package ([https://github.com/netrack/keras-metrics](https://github.com/netrack/keras-metrics))
    and add the **precision**, **recall**, and **f1 score** metrics to the training
    log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After 20 epochs, the metrics now look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted to optimize on the f1 score, we would define the tuner metrics
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: That's all it takes. As long as a metric is printed in the training log, you
    can use it to tune models.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing our Keras model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s run our tuning job:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the metrics for `HyperparameterTuner` like so, optimizing for accuracy
    and also displaying the f1 score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the parameter ranges to explore:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We use the same estimator (20 epochs with spot instances) and we define the
    tuner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We launch the tuning job. While it''s running, we can use the **SageMaker SDK**
    to display the list of training jobs and their properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the table visible in the next screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Viewing information on a tuning job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Viewing information on a tuning job
  prefs: []
  type: TYPE_NORMAL
- en: The tuning job runs for 2 hours and 8 minutes (wall time). Top validation accuracy
    is 93.46% – a decent improvement over our baseline.
  prefs: []
  type: TYPE_NORMAL
- en: We could certainly do better by training longer. However, the longer we train
    for, the more overfitting becomes a concern. We can alleviate it with early stopping,
    which can be implemented with a Keras callback. However, we should make sure that
    the job reports the metric for the best epoch, not for the last epoch. How can
    we display this in the training log? With another callback!
  prefs: []
  type: TYPE_NORMAL
- en: Adding callbacks for early stopping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adding a Keras callback for early stopping is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We add a built-in callback for early stopping, based on validation accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We add a custom callback to store validation accuracy at the end of each epoch,
    and to display the best one at the end of training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We add these two callbacks to the training API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Testing with a few individual jobs, the last lines of the training log now
    look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the notebook, we update our metric definition in order to extract the best
    validation accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training for 60 epochs this time (about 3 hours wall time), top validation accuracy
    is now at 93.78%. It looks like this is as good as it gets by tweaking the learning
    rate and the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: Using automatic model tuning for architecture search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our neural network has plenty more hyperparameters: number of convolution filters,
    dropout, and so on. Let''s try to optimize these as well:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We modify our training script to add command-line parameters for the following
    network parameters, which are used by Keras layers in our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you certainly guessed, the parameters let us set values for the number of
    convolution filters in each layer, the dropout value for convolution layers, and
    the dropout value for fully connected layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Accordingly, in the notebook, we define these hyperparameters and their ranges.
    For the learning rate and the batch size, we use narrow ranges centered on the
    optimal values discovered by the previous tuning job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We launch the tuning job, running 50 jobs two at a time for 100 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The tuning job runs for about 12 hours, for a total cost of about $15\. Top
    validation accuracy hits 94.09%. Compared to our baseline, automatic model tuning
    has improved the accuracy of our model by almost 1 percentage point – a very significant
    gain. If this model is used to predict 1 million samples a day, this translates
    to over 10,000 additional accurate predictions!
  prefs: []
  type: TYPE_NORMAL
- en: In total, we've spent less about $50 on tuning our Keras model. Whatever business
    metric would be improved by the extra accuracy, it's fair to say that this spend
    would be recouped in no time. As many customers have told me, automatic model
    tuning pays for itself, and then some.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our exploration of automatic model tuning, one of my favorite
    features in SageMaker. You can find more examples at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's learn about SageMaker Debugger, and how it can help us to understand
    what's happening inside our models.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring models with SageMaker Debugger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker Debugger lets you configure *debugging rules* for your training job.
    These rules will inspect its internal state and check for specific unwanted conditions
    that could be developing during training. SageMaker Debugger includes a long list
    of built-in rules ([https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html)),
    and you can add your own written in Python.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you can save and inspect the model state (gradients, weights, and
    so on) as well as the training state (metrics, optimizer parameters, and so on).
    At each training step, the **tensors** storing these values may be saved in near-real-time
    in an S3 bucket, making it possible to visualize them while the model is training.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can select the tensor **collections** that you'd like to save,
    how often, and so on. Depending on the framework you use, different collections
    are available. You can find more information at [https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md).
    Last but not least, you can save either raw tensor data or tensor reductions to
    limit the amount of data involved. Reductions include min, max, median, and more.
  prefs: []
  type: TYPE_NORMAL
- en: If you are working with the built-in containers for supported versions of TensorFlow,
    PyTorch, Apache MXNet, or the built-in XGBoost algorithm, you can use SageMaker
    Debugger out of the box, without changing a line of code in your script. Yes,
    you read that right. All you have to do is add extra parameters to the estimator,
    as we will in the next examples.
  prefs: []
  type: TYPE_NORMAL
- en: With other versions, or with your own containers, minimal modifications are
    required. You can find the latest information and examples at [https://github.com/awslabs/sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger).
  prefs: []
  type: TYPE_NORMAL
- en: Debugging rules and saving tensors can be configured on the same training job.
    For clarity, we'll run two separate examples. First, let's use the XGBoost and
    Boston Housing example from [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging an XGBoost job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will configure several built-in rules, train our model, and check
    the status of all rules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a look at the list of built-in rules, we decide to use `overtraining`
    and `overfit`. Each rule has extra parameters that we could tweak. We stick to
    defaults, and we configure the `Estimator` accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We set hyperparameters and launch training without waiting for the training
    job to complete. The training log won''t be visible in the notebook, but it will
    still be available in **CloudWatch Logs**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition to the training job, one debugging job per rule is running under
    the hood, and we can check their statuses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This tells us that the debugger jobs are running:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the same cell once the training job is complete, we see that no rule
    was triggered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Had a rule been triggered, we would get an error message, and the training job
    would be stopped. Inspecting tensors stored in S3 would help us understand what
    went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting an XGBoost job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s configure a new training job that saves all tensor collections available
    for XGBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We configure the `Estimator`, passing a `DebuggerHookConfig` object. We save
    three tensor collections at each training step: metrics, feature importance, and
    average **SHAP** ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    values. These help us understand how each feature in a data sample contributes
    to increasing or decreasing the predicted value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For larger models and datasets, this could generate a lot of data, which would
    take a long time to load and analyze. We would either increase the save interval
    or save tensor reductions instead of full tensors:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the training job has started, we can create a trial and load data that
    has already been saved. As this job is very short, we see all data within a minute
    or so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can list the name of all tensors that were saved:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also list the name of all tensors in a given collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each tensor, we can access training steps and values. Let''s plot feature
    information from the `average_shap` and `feature_importance` collections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We build the `average_shap` plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see it in the following screenshot – **dis**, **crim**, and **nox**
    have the largest average values:![Figure 10.6 – Plotting average SHAP values over
    time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17705_10_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.6 – Plotting average SHAP values over time
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We build the `feature_importance/weight` plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see it in the following screenshot – **crim**, **age**, and **dis**
    have the largest weights:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Plotting feature weights over time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Plotting feature weights over time
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's use SageMaker Debugger on our Keras and Fashion-MNIST example.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging and inspecting a Keras job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can inspect and debug a Keras job using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The default behavior in TensorFlow 2.x is eager mode, where gradients are not
    available. Hence, we disable eager mode in our script, which is the only modification
    required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We start from the same estimator. The dataset has 70,000 samples (60,000 for
    training, plus 10,000 for validation). With 30 epochs and a batch size of 128,
    our training job will have about 16,400 steps (70,000 * 30 / 128). Saving tensors
    at each step feels like overkill. Let''s save them every 100 steps instead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the built-in rules available for TensorFlow, we decide to set up
    `poor_weight_initialization`, `dead_relu,` and `check_input_images`. We need to
    specify the index of channel information in the input tensor. It''s 4 for TensorFlow
    (batch size, height, width, and channels):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Looking at the collections available for TensorFlow, we decide to save metrics,
    losses, outputs, weights, and gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As training starts, we see the rules being launched in the training log:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When training is complete, we check the status of the debugging rules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a trial using the same tensors saved in S3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s inspect the filters in the first convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As defined in our training script, the first convolution layer has 64 filters.
    Each one is 3x3 pixels, with a single channel (2D). Accordingly, gradients have
    the same shape.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We write a function to plot filter weights and gradients over time, and we
    plot weights in the last filter of the first convolution layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see the graph in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Plotting the weights of a convolution filter over time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – Plotting the weights of a convolution filter over time
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SageMaker Debugger makes it really easy to inspect training
    jobs. If you work with the built-in containers that support it, you don't need
    to modify your code. All configuration takes place in the estimator.
  prefs: []
  type: TYPE_NORMAL
- en: You can find additional examples at [https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples),
    including some advanced use cases such as real-time visualization and model pruning.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the first part of the chapter, where we learned how to optimize
    the cost of training jobs with managed spot training, their accuracy with automatic
    model tuning, and how to inspect their internal state with SageMaker Debugger.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we're going to dive into two advanced capabilities that
    will help us build better training workflows – SageMaker Feature Store and SageMaker
    Clarify.
  prefs: []
  type: TYPE_NORMAL
- en: Managing features and building datasets with SageMaker Feature Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, we''ve engineered our training and validation features in a notebook
    or in a SageMaker Processing script, before storing them as S3 objects. Then,
    we used these objects as-is to train and evaluate models. This is a perfectly
    reasonable workflow. However, the following questions may arise as your machine
    learning workflows grow and mature:'
  prefs: []
  type: TYPE_NORMAL
- en: How can we apply a well-defined schema to our features?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we select a subset of our features to build different datasets?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we store and manage different feature versions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we discover and reuse feature engineering by other teams?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we access engineered features at prediction time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker Feature Store is designed to answer these questions. Let's add it
    to the classification training workflow we built with BlazingText and Amazon Reviews
    in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training Natural
    Language Processing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Engineering features with SageMaker Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can reuse our previous SageMaker Processing job almost as-is. The only difference
    is the output format of the engineered data. In the original job, we saved it
    as a plain text file according to the input format expected by BlazingText. This
    format is inconvenient for SageMaker Feature Store, as we need easy access to
    each column. CSV doesn''t work either as reviews contain commas, so we decide
    to use TSV instead:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, we add a few lines to our processing script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running our SageMaker Processing job as before, we now see two outputs: a plain
    text output for BlazingText (in case we wanted to train directly on the full dataset)
    and a TSV output that we''ll ingest in SageMaker Feature Store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s load the TSV file in a `pandas` dataframe and display the first few
    rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the table visible in the next image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Viewing the first rows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Viewing the first rows
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's create a feature group where we'll ingest this data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a feature group
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **feature group** is a resource that stores a collection of related features.
    Feature groups are organized in rows, which have a unique identifier and a timestamp.
    Each row contains key-value pairs, where each pair represents a feature name and
    a feature value.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define the name of our feature group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we set the name of the feature that contains a unique identifier – `review_id`
    works perfectly here, and you could use any unique value present in your data
    source, such as a primary key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we add a timestamp column to all rows in our `pandas` dataframe. If your
    data source already contains a timestamp, you can reuse that value, either in
    the **float64** format or in the **UNIX** date/time format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our dataframe now looks like the following picture:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Viewing timestamps'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17705_10_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.10 – Viewing timestamps
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to define a schema for the feature group. We can either provide
    it explicitly in a JSON document or let SageMaker pick it up from the pandas dataframe.
    We use the second option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then load feature definitions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we create the feature group, passing the S3 location where features
    will be stored. This is where we''ll query them to build datasets. We enable the
    online store, which will give us low-latency access to features at prediction
    time. We also add a description and tags which make it easier to discover the
    feature group:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a few seconds, the feature group is ready and visible in SageMaker Studio,
    under **Components and registries** / **Feature Store**, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Viewing a feature group'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.11 – Viewing a feature group
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's ingest data.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SageMaker Feature Store lets us ingest data in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Call the `PutRecord()` API to ingest a single record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the `ingest()` API to upload the contents of a `pandas` dataframe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we used **SageMaker Data Wrangler** for feature engineering, use an auto-generated
    notebook to create a feature group and ingest data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use the second option here, which is as simple as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Once ingestion is complete, features are stored at the S3 location we specified,
    as well as in a dedicated low-latency backend. Let's use the former to build a
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Querying features to build a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we create the feature group, SageMaker automatically adds a new table for
    it in the **AWS Glue Data Catalog**. This makes it easy to use **Amazon Athena**
    to query data and build datasets on demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we''d like to build a dataset that contains best-selling cameras
    with at least 1,000 reviews:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we write an SQL query that computes the average rating for each camera,
    counts how many reviews each camera received, only keeps cameras with at least
    1,000 reviews, and orders cameras by descending average rating:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we use Athena to query our feature group, store selected rows in a `pandas`
    dataframe, and display the first few rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the table visible in the next image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Viewing query results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.12 – Viewing query results
  prefs: []
  type: TYPE_NORMAL
- en: From then on, it's business as usual. We can save this dataframe to a CSV file
    and use it to train models. You'll find an end-to-end example in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring other capabilities of SageMaker Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over time, we could store different versions of the same feature – that is,
    several records with the same identifier but with different timestamps. This would
    allow us to retrieve earlier versions of a dataset – "time traveling" in our data
    with a simple SQL query.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, features are also available in the online store. We can
    retrieve individual records with the `GetRecord()` API and use features at prediction
    time whenever needed.
  prefs: []
  type: TYPE_NORMAL
- en: Again, you'll find code samples for both capabilities in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: To close this chapter, let's look at Amazon SageMaker Clarify, a capability
    that helps us build higher quality models by detecting potential bias present
    in datasets and models.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting bias in datasets and explaining predictions with SageMaker Clarify
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **machine learning** (**ML**) model is only as good as the dataset it was
    built from. If a dataset is inaccurate or unfair in representing the reality it's
    supposed to capture, a corresponding model is very likely to learn this biased
    representation and perpetuate it in its predictions. As ML practitioners, we need
    to be aware of these problems, understand how they impact predictions, and limit
    that impact whenever possible.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we'll work with the **Adult Data Set**, available at the **UCI
    Machine Learning Repository** ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml),
    Dua, D. and Graff, C., 2019). This dataset describes a binary classification task,
    where we try to predict if an individual earns less or more than $50,000 per year.
    Here, we'd like to check whether this dataset introduces gender bias or not. In
    other words, does it help us build models that predict equally well for men and
    women?
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The dataset you'll find in the GitHub repository has been slightly processed.
    The label column has been moved to the front as per XGBoost requirements. Categorical
    variables have been one-hot encoded.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a bias analysis with SageMaker Clarify
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker Clarify computes pre-training and post-training metrics that help
    us understand how a model predicts.
  prefs: []
  type: TYPE_NORMAL
- en: Post-training metrics obviously require a trained model, so we first train a
    binary classification model with XGBoost. It's nothing we haven't seen many times
    already, and you'll find the code in the GitHub repository. This model hits a
    validation AuC of 92.75%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once training is complete, we can proceed with the bias analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias analyses run as SageMaker Processing jobs. Accordingly, we create a `SageMakerClarifyProcessor`
    object with our infrastructure requirements. As the job is small-scale, we use
    a single instance. For larger jobs, we could use an increased instance count,
    and the analysis would automatically run on **Spark**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a `DataConfig` object describing the dataset to analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Likewise, we create a `ModelConfig` object describing the model to analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, we create a `BiasConfig` object describing the metrics to compute.
    The `label_values_or_threshold` defines the label value for the positive outcome
    (1, indicating a revenue higher than $50K). The `facet_name` defines the feature
    on which we'd like to run the analysis (`Sex_`), and `facet_values_or_threshold`
    defines the feature value for the potentially disadvantaged group (1, indicating
    women).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We're now ready to run the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Running a bias analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Putting everything together, we launch the analysis with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Once the analysis is complete, the results are visible in SageMaker Studio.
    A report is also generated and stored in S3 in HTML, PDF, and notebook format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In **Experiments and trials**, we locate our SageMaker Clarify job, and we
    right-click on **Open trial details**. Selecting **Bias report**, we see bias
    metrics, as shown in the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Viewing bias metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.13 – Viewing bias metrics
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing bias metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you''d like to learn more about bias metrics, what they mean, and how they''re
    computed, I highly recommend these resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/aws/amazon-sagemaker-clarify](https://github.com/aws/amazon-sagemaker-clarify)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at two pre-training metrics, **Class Imbalance** (**CI**) and **Difference
    in Positive Proportions in Labels** (**DPL**), and one post-training metric, **Difference
    in Positive Proportions in Predicted Labels** (**DPPL**).
  prefs: []
  type: TYPE_NORMAL
- en: A non-zero value of CI indicates that the dataset is imbalanced. Here, the difference
    between the men fraction and the women fraction is 0.35\. Indeed, the men group
    is about two-thirds of the dataset, the women group is about one-third. This isn't
    a very severe imbalance, but we should also look at the proportion of positive
    labels for each class.
  prefs: []
  type: TYPE_NORMAL
- en: The DPL measures if each class has the same proportion of positive labels. In
    other words, does the dataset contain the same ratio of men and women earning
    $50K? The DPL is non-zero (0.20), which tells us that men have a higher ratio
    of $50K earners.
  prefs: []
  type: TYPE_NORMAL
- en: The DPPL is a post-training metric similar to the DPL. Its value (0.18) shows
    that the model unfortunately picked up the bias present in the dataset, only lightly
    reducing it. Indeed, the model predicts a more favorable outcome for men (over-predicting
    $50K earners) and a less favorable outcome for women (under-predicting 50K earners).
  prefs: []
  type: TYPE_NORMAL
- en: That's clearly a problem. Although the model has a rather nice validation AuC
    (92.75%), it doesn't predict both classes equally well.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the data and try to mitigate this issue, let's run an explainability
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Running an explainability analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker Clarify can compute local and global SHAP ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    values. They help us understand feature importance, and how individual feature
    values contribute to a positive or negative outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias analyses run as SageMaker Processing jobs, and the process is similar:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a `DataConfig` object describing the dataset to analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a `SHAPConfig` object describing how we''d like to compute SHAP values
    – that is, which baseline to use (I use the test set where I removed labels),
    how many samples to use (twice the number of features plus 2048, a common default),
    and how to aggregate values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we run the analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Results in available in SageMaker Studio, under `Sex` feature is by far the
    most important, which confirms the bias analysis. Ethical considerations aside,
    this doesn't seem to make a lot of sense from a business perspective. Features
    such as education or capital gain should be more important.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Viewing feature importance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17705_10_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.14 – Viewing feature importance
  prefs: []
  type: TYPE_NORMAL
- en: Local SHAP values have also been computed and stored in S3\. We could use them
    to understand how feature values impact the prediction of each individual sample.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we can try to mitigate the bias we detected in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This dataset combines two problems. First, it contains more men than women.
    Second, the men group has a higher proportion of positive outcomes. The combination
    of these two problems leads to a situation where the dataset contains a disproportionately
    low number of women who earn more than $50K. This makes it harder for the model
    to learn in a fair way, and it tends to favor the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias mitigation techniques include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling the majority class by removing majority samples to rebalance the
    dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling the minority class by adding more samples through duplication of
    existing ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding synthetic samples to the minority class by generating new samples that
    have statistical properties similar to existing samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Altering data shouldn't be done lightly, especially in organizations operating
    in regulated industries. This can have serious business, compliance, and legal
    consequences. Please make sure to get approval before doing this in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try a combined approach based on the **imbalanced-learn** open source
    library (https://imbalanced-learn.org). First, we''ll add synthetic samples to
    the minority class with the **Synthetic Minority Oversampling Technique** (**SMOTE**)
    algorithm, in order to match the ratio of $50K earners present in the majority
    samples. Then, we''ll undersample the majority class to match the number of samples
    of the minority class. The result will be a perfectly balanced dataset, where
    both classes have the same size and the same ratio of $50K earners. Let''s get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to compute the ratios for both classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following result, showing that the majority class (class
    0) has a much larger ratio of $50k earners:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we generate synthetic minority samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we rebuild the dataset with the original majority class and the rebalanced
    minority class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we undersample the original majority class to rebalance ratios:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We count both classes and compute their ratios again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This displays the following results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training with this rebalanced dataset, and using the same test set, we get a
    validation AuC of 92.95%, versus 92.75% for the original model. Running a new
    bias analysis, CI is zero, and the DPL and DPPL are close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Not only have we built a model that predicts more fairly, but it's also a little
    bit more accurate. For once, it looks like we got the best of both worlds!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter concludes our exploration of training techniques. You learned about
    managed spot training, a simple way to slash training costs by 70% or more. You
    also saw how checkpointing helps to resume jobs that have been interrupted. Then,
    you learned about automatic model tuning, a great way to extract more accuracy
    from your models by exploring hyperparameter ranges. You learned about SageMaker
    Debugger, an advanced capability that automatically inspects training jobs for
    unwanted conditions and saves tensor collections to S3 for inspection and visualization.
    Finally, we discovered two capabilities that help you build higher quality workflows
    and models, SageMaker Feature Store and SageMaker Clarify.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll study model deployment in detail.
  prefs: []
  type: TYPE_NORMAL

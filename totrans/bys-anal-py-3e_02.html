<html><head></head><body>
<section id="chapter-3-hierarchical-models" class="level2 chapterHead" data-number="1.7">&#13;
<h1 class="chapterHead" data-number="1.7">Chapter 3<br/>&#13;
<span id="x1-670003"/>Hierarchical Models</h1>&#13;
<blockquote>&#13;
<p>Hierarchical models are one honking great idea – let’s do more of those! - The zen of Bayesian modeling</p>&#13;
</blockquote>&#13;
<p>In <em>Chapter <a href="CH02.xhtml#x1-440002">2</a></em>, we saw a tips example where we had multiple groups in our data, one for each of Thursday, Friday, Saturday, and Sunday. We decided to model each group separately. That’s sometimes fine, but we should be aware of our assumptions. By modeling each group independently, we are assuming the groups are unrelated. In other words, we are assuming that knowing the tip for one day does not give us any information about the tip for another day. That could be too strong an assumption. Would it be possible to build a model that allows us to share information between groups? That’s not only possible, but is also the main topic of this chapter. Lucky you!</p>&#13;
<p>In this chapter, we will cover the following topics:</p>&#13;
<ul>&#13;
<li><p>Hierarchical models</p></li>&#13;
<li><p>Partial pooling</p></li>&#13;
<li><p>Shrinkage</p></li>&#13;
</ul>&#13;
<p><span id="x1-67001r145"/></p>&#13;
<section id="sharing-information-sharing-priors" class="level3 sectionHead" data-number="1.7.1">&#13;
<h2 class="sectionHead" data-number="1.7.1">3.1 <span id="x1-680001"/>Sharing information, sharing priors</h2>&#13;
<p>Hierarchical models are also <span id="dx1-68001"/>known as multilevel models, mixed-effects models, random-effects models, or <span id="dx1-68002"/>nested models. They are particularly useful when dealing with data that can be described as grouped or having different levels, such as data nested within geographic regions (for example, cities belonging to a province and provinces belonging to a country), or with a hierarchical structure (such as students nested within schools, or patients nested within hospitals) or repeated measurements on the same individuals.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file87.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-68003r1"/><strong>Figure 3.1</strong>: The differences between a pooled model, an unpooled model, and a hierarchical model</p>&#13;
<p>Hierarchical models are a natural way to <span id="dx1-68004"/>share information between groups. In a hierarchical model, the parameters of the prior distributions are themselves given a prior distribution. These higher-level priors are <span id="dx1-68005"/>often called hyperpriors; ”hyper” means ”over” in Greek. Having hyperpriors allows the model to share <span id="dx1-68006"/>information between groups, while still allowing differences between groups. In other words, we can think of the parameters of the prior distributions as belonging to a common population of parameters. <em>Figure <a href="#x1-68003r1">3.1</a></em> shows a diagram with the high-level differences between a pooled model (a single group), an unpooled model (all separated groups), and a hierarchical model, also known as a <span id="dx1-68007"/>partially pooled model.</p>&#13;
<p>The concept of a <span id="dx1-68008"/>hierarchical model may seem confusingly simple, almost trivial, but it has subtle implications. That is why in the rest of this chapter we will use different examples to understand their implications. I am sure that these examples will not only help you better understand this concept but will also convince you that it is a very useful tool to apply to your own problems. <span id="x1-68009r148"/></p>&#13;
</section>&#13;
<section id="hierarchical-shifts" class="level3 sectionHead" data-number="1.7.2">&#13;
<h2 class="sectionHead" data-number="1.7.2">3.2 <span id="x1-690002"/>Hierarchical shifts</h2>&#13;
<p><span id="dx1-69001"/> <span id="dx1-69002"/></p>&#13;
<p>Proteins are molecules formed by 20 units called amino acids. Each amino acid can appear in a protein 0 or more times. Just as a melody is defined by a sequence of musical notes, a protein is defined by a sequence of amino acids. Some musical note variations can result in small variations of the melody and other variations in completely different melodies. Something similar happens with proteins. One way to study proteins is by using nuclear magnetic resonance (the same technique used for medical imaging). This technique allows us to measure various quantities, one of which is called a chemical shift. You may remember that we saw an example using chemical shifts in <em>Chapter <a href="CH02.xhtml#x1-440002">2</a></em>.</p>&#13;
<p>Suppose we want to compare a theoretical method of computing chemical shift against the experimental observations to evaluate the ability of the theoretical method to reproduce the experimental values. Luckily for us, someone has already run the experiments and carried out the theoretical calculations, and we just need to compare them. The following dataset contains chemical shift values for a set of proteins. If you inspect the <code>cs_data </code>DataFrame, you will see that it has four columns:</p>&#13;
<ol>&#13;
<li><div id="x1-69004x1">&#13;
<p>The first is a code that identifies the protein (you can get a lot of information about that protein by entering that code at <a href="https://www.rcsb.org/" class="url">https://www.rcsb.org/</a>)</p>&#13;
</div></li>&#13;
<li><div id="x1-69006x2">&#13;
<p>The second column has the name of the amino acid (you might notice that there are only 19 unique names; one of the amino acids is missing from this dataset)</p>&#13;
</div></li>&#13;
<li><div id="x1-69008x3">&#13;
<p>The third contains theoretical values of chemical shift (calculated using quantum methods)</p>&#13;
</div></li>&#13;
<li><div id="x1-69010x4">&#13;
<p>The fourth has experimental values</p>&#13;
</div></li>&#13;
</ol>&#13;
<p>Now that we have the data, how should we proceed? One option is to take the empirical differences and fit a Gaussian or maybe Student’s t model. Because amino acids are a family of chemical compounds, it would make sense to assume they are all the same and estimate a single Gaussian for all the differences. But you may argue that there are 20 different kinds of amino acids, each one with different chemical properties, and hence a better choice is to fit 20 separated Gaussians. What should we do?</p>&#13;
<p>Let’s take a moment to think about which option is the best. If we combine all the data, our estimates are going to be more accurate, but we will not be able to get information from individual groups (amino acids). On the contrary, if we treat them as separate groups, we will get a much more detailed analysis but with less accuracy. What should we do?</p>&#13;
<p>When in doubt, everything! (Not sure this is good general advice for your life, but I like the song <a href="https://www.youtube.com/watch?v=1di09XZUlIw" class="url">https://www.youtube.com/watch?v=1di09XZUlIw</a>). We can build a hierarchical model; that way, we allow estimates at the group level but with the restriction that they all belong to a larger group or population. To better understand this, let’s build a hierarchical model for the chemical shift data.</p>&#13;
<p>To see the difference between a non-hierarchical (unpooled) model and a hierarchical one, we are going to build two models. The first one is essentially the same as the <code>comparing_groups </code>model from <em>Chapter <a href="CH02.xhtml#x1-440002">2</a></em>:</p>&#13;
<p><span id="x1-69011r1"/> <span id="x1-69012"/><strong>Code 3.1</strong></p>&#13;
<pre id="listing-38" class="source-code"><code>with pm.Model(coords=coords) as cs_nh: </code>&#13;
<code>    μ = pm.Normal('μ', mu=0, sigma=10, dims="aa") </code>&#13;
<code>    σ = pm.HalfNormal('σ', sigma=10, dims="aa") </code>&#13;
<code>    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) </code>&#13;
<code>    idata_cs_nh = pm.sample()</code></pre>&#13;
<p>Now, we will build the hierarchical version of the model. We are adding two hyperpriors, one for the mean of <em>μ</em> and one for the standard deviation of <em>μ</em>. We are leaving <em>σ</em> without hyperpriors; in other words, we are assuming that the variance between observed and theoretical values should be the same for all groups. This is a modeling choice, and you may face a problem where this seems unacceptable and consider it necessary to add a hyperprior for <em>σ</em>; feel free to do that:</p>&#13;
<p><span id="x1-69018r2"/> <span id="x1-69019"/><strong>Code 3.2</strong></p>&#13;
<pre id="listing-39" class="source-code"><code>with pm.Model(coords=coords) as cs_h: </code>&#13;
<code>    # hyper_priors </code>&#13;
<code>    μ_mu = pm.Normal('μ_mu', mu=0, sigma=10) </code>&#13;
<code>    μ_sd = pm.HalfNormal('μ_sd', 10) </code>&#13;
<code>    # priors </code>&#13;
<code>    μ = pm.Normal('μ', mu=μ_mu, sigma=μ_sd, dims="aa") </code>&#13;
<code>    σ = pm.HalfNormal('σ', sigma=10, dims="aa") </code>&#13;
<code>    # likelihood </code>&#13;
<code>    y = pm.Normal('y', mu=μ[idx], sigma=σ[idx], observed=diff) </code>&#13;
<code>    idata_cs_h = pm.sample()</code></pre>&#13;
<p><em>Figure <a href="#x1-69030r2">3.2</a></em> shows the graphical representation of the <code>cs_h </code>and <code>cs_nh </code>models. We can see that we have one more level for <code>cs_h </code>representing the hyperpriors for <em>μ</em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file88.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-69030r2"/><strong>Figure 3.2</strong>: Graph representation of the non-hierarchical (left) and hierarchical (right) models for the chemical shift data. Each subfigure was generated with the <code>pm.model_to_graphviz(.) </code>function</p>&#13;
<p>We are going to compare the results using ArviZ’s <code>plot_forest </code>function. We can pass more than one model to this function. This is useful when we want to compare the values of parameters from different models such as with the present example. In <em>Figure <a href="#x1-69031r3">3.3</a></em>, we have a plot for the 40 estimated means, one per amino acid (20) for each of the two models. We also have their 94% HDI and the inter-quantile range (the central 50% of the distribution). The vertical dashed line is the global mean according to the hierarchical model. This value is close to zero, as expected for theoretical values faithfully reproducing experimental ones.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file89.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-69031r3"/><strong>Figure 3.3</strong>: Chemical shift differences for the hierarchical and non-hierarchical models</p>&#13;
<p>The most relevant part of this plot is that the estimates from the hierarchical model are pulled toward the partially pooled mean, or equivalently they are shrunken in comparison to the unpooled estimates. You will also notice that the effect is more notorious for those groups farther away from the mean (such as <code>PRO</code>) and that the uncertainty is on par with or smaller than that from the non-hierarchical model. The estimates are partially pooled because we have one estimate for each group, but estimates for individual groups restrict each other through the hyperprior. Therefore, we get an intermediate situation between having a single group with all chemical shifts together and having 20 separate groups, one per amino acid. And that, ladies, gentlemen, and non-binary-gender-fluid people, is the beauty of hierarchical models. <span id="x1-69032r149"/></p>&#13;
</section>&#13;
<section id="water-quality" class="level3 sectionHead" data-number="1.7.3">&#13;
<h2 class="sectionHead" data-number="1.7.3">3.3 <span id="x1-700003"/>Water quality</h2>&#13;
<p><span id="dx1-70001"/></p>&#13;
<p>Suppose we want to analyze the quality of water in a city, so we take samples by dividing the city into neighborhoods. We may think we have two options for analyzing this data:</p>&#13;
<ul>&#13;
<li><p>Study each neighborhood as a separate entity</p></li>&#13;
<li><p>Pool all the data together and estimate the water quality of the city as a single big group</p></li>&#13;
</ul>&#13;
<p>You have probably already noticed the pattern here. We can justify the first option by saying we obtain a more detailed view of the problem, which otherwise could become invisible or less evident if we average the data. The second option can be justified by saying that if we pool the data, we obtain a bigger sample size and hence a more accurate estimation. But we already know we have a third option: we can do a hierarchical model!</p>&#13;
<p>For this example, we are going to use synthetic data. I love using synthetic data; it is a great way to understand things. If you don’t understand something, simulate it! There are many uses for synthetic data. Here, we are going to imagine we have collected water samples from three different regions of the same city and measured the lead content of water; samples with concentrations of lead above recommendations from the World Health Organization are marked with zero and samples with values below the recommendations are marked with one. This is a very simple scenario. In a more realistic example, we would have a continuous measurement of lead concentration and probably many more groups. Nevertheless, for our current purposes, this example is good enough to uncover the details of hierarchical models. We can generate the synthetic data with the following code:</p>&#13;
<p><span id="x1-70002r3"/> <span id="x1-70003"/><strong>Code 3.3</strong></p>&#13;
<pre id="listing-40" class="source-code"><code>N_samples = [30, 30, 30] </code>&#13;
<code>G_samples = [18, 18, 18] </code>&#13;
<code>group_idx = np.repeat(np.arange(len(N_samples)), N_samples) </code>&#13;
<code>data = [] </code>&#13;
<code>for i in range(0, len(N_samples)): </code>&#13;
<code>    data.extend(np.repeat([1, 0], [G_samples[i], N_samples[i]-G_samples[i]]))</code></pre>&#13;
<p>We are simulating an experiment where we have measured three groups, each one consisting of a certain number of samples; we store the total number of samples per group in the <code>N_samples </code>list. Using the <code>G_samples </code>list, we keep a record of the number of good-quality samples per group. The rest of the code is there just to generate a list of the data, filled with zeros and ones.</p>&#13;
<p>The model for this problem is similar to the one we used for the coin problem, except for two important features:</p>&#13;
<ul>&#13;
<li><p>We have defined two hyperpriors that will influence the Beta prior.</p></li>&#13;
<li><p>Instead of setting hyperpriors on the parameters <em>α</em> and <em>β</em>, we are defining the Beta distribution in terms of <em>μ</em>, the mean, and <em>ν</em>, the concentration (or precision) of the Beta distribution. The precision is analog to the inverse of the standard deviation; the larger the value of <em>ν</em>, the more concentrated the Beta distribution will be. In statistical notation, our model is as follows:</p></li>&#13;
</ul>&#13;
<div class="math-display">&#13;
<img src="../media/file90.jpg" class="math-display" alt=" μ ∼ Beta(𝛼μ,𝛽μ) ν ∼ ℋ𝒩 (σν) θi ∼ Beta(μ,ν) y ∼ Bernoulli(θ) i i "/>&#13;
</div>&#13;
<p>Notice that we are using the subscript <em>i</em> to indicate that the model has groups with different values for some of the parameters. Using Kruschke diagrams (see <em>Figure <a href="#x1-70010r4">3.4</a></em>), we can recognize that the new model has one additional level compared to the one from <em>Figure <a href="CH01.xhtml#x1-39001r14">1.14</a></em>. Notice also that for this model, we are parametrizing the Beta prior distribution in terms of <em>μ</em> and <em>ν</em> instead of <em>α</em> and <em>β</em>. This is a common practice in Bayesian statistics, and it is done because <em>μ</em> and <em>ν</em> are more intuitive parameters than <em>α</em> and <em>β</em>.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file91.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-70010r4"/><strong>Figure 3.4</strong>: Hierarchical model</p>&#13;
<p>Let’s write the model in PyMC:</p>&#13;
<p><span id="x1-70011r4"/> <span id="x1-70012"/><strong>Code 3.4</strong></p>&#13;
<pre id="listing-41" class="source-code"><code>with pm.Model() as model_h: </code>&#13;
<code>    # hypyerpriors </code>&#13;
<code>    μ = pm.Beta('μ', 1, 1) </code>&#13;
<code>    ν = pm.HalfNormal('ν', 10) </code>&#13;
<code>    # prior </code>&#13;
<code>    <em>θ</em> = pm.Beta('<em>θ</em>', mu=μ, nu=ν, shape=len(N_samples)) </code>&#13;
<code>    # likelihood </code>&#13;
<code>    y = pm.Bernoulli('y', p=<em>θ</em>[group_idx], observed=data) </code>&#13;
<code> </code>&#13;
<code>    idata_h = pm.sample()</code></pre>&#13;
<p><span id="x1-70023r154"/></p>&#13;
</section>&#13;
<section id="shrinkage" class="level3 sectionHead" data-number="1.7.4">&#13;
<h2 class="sectionHead" data-number="1.7.4">3.4 <span id="x1-710004"/>Shrinkage</h2>&#13;
<p><span id="dx1-71001"/></p>&#13;
<p>To show you one of the main consequences of hierarchical models, I will require your assistance, so please join me in a brief experiment. I will need you to print and save the summary computed with <code>az.summary(idata_h)</code>. Then, I want you to rerun the model two more times after making small changes to the synthetic data. Remember to save the summary after each run. In total, we will have three runs:</p>&#13;
<ul>&#13;
<li><p>One run setting all the elements of <code>G_samples </code>to 18</p></li>&#13;
<li><p>One run setting all the elements of <code>G_samples </code>to 3</p></li>&#13;
<li><p>One last run setting one element to 18 and the other two to 3</p></li>&#13;
</ul>&#13;
<p>Before continuing, please take a moment to think about the outcome of this experiment. Focus on the estimated mean value of <em>θ</em> in each experiment. Based on the first two runs of the model, could you predict the outcome for the third case?</p>&#13;
<p>If we put the result in a table, we get something more or less like this; remember that small variations could occur due to the stochastic nature of the sampling process:</p>&#13;
<table id="TBL-7" class="tabular">&#13;
<tbody>&#13;
&#13;
<tr id="TBL-7-1-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-7-1-1" class="td11" style="text-align: center; white-space: nowrap;">G_samples</td>&#13;
<td id="TBL-7-1-2" class="td11" style="text-align: center; white-space: nowrap;">Mean</td>&#13;
</tr>&#13;
&#13;
<tr id="TBL-7-2-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-7-2-1" class="td11" style="text-align: center; white-space: nowrap;">18, 18, 18</td>&#13;
<td id="TBL-7-2-2" class="td11" style="text-align: center; white-space: nowrap;">0.6, 0.6, 0.6</td>&#13;
</tr>&#13;
<tr id="TBL-7-3-" class="odd" style="vertical-align:baseline;">&#13;
<td id="TBL-7-3-1" class="td11" style="text-align: center; white-space: nowrap;">3, 3, 3</td>&#13;
<td id="TBL-7-3-2" class="td11" style="text-align: center; white-space: nowrap;">0.11, 0.11, 0.11</td>&#13;
</tr>&#13;
<tr id="TBL-7-4-" class="even" style="vertical-align:baseline;">&#13;
<td id="TBL-7-4-1" class="td11" style="text-align: center; white-space: nowrap;">18, 3, 3</td>&#13;
<td id="TBL-7-4-2" class="td11" style="text-align: center; white-space: nowrap;">0.55, 0.13, 0.13</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p class="IMG---Caption"><span id="x1-71002r1"/> <span id="x1-71003"/><strong>Table 3.1</strong>: Sample data and corresponding means</p>&#13;
<p>In the first row, we can see that for a dataset of 18 good samples out of 30, we get a mean value for <em>θ</em> of 0.6; remember that now the mean of <em>θ</em> is a vector of 3 elements, 1 per group. Then, on the second row, we have only 3 good samples out of 30 and the mean of <em>θ</em> is 0.11. These results should not be surprising; our estimates are practically the same as the empirical means. The interesting part comes in the third row. Instead of getting a mix of the mean estimates of <em>θ</em> from the other two rows, such as 0.6, 0.11, and 0.11, we get different values, namely 0.55, 0.13, and 0.13.</p>&#13;
<p>What on Earth happened? Did we make a mistake somewhere? Nothing like that. What we are seeing is that the estimates have shrunk toward the common mean. This is totally OK; indeed this is just a consequence of our model. By using hyperpriors, we are estimating the parameters of the Beta prior distribution from the data. Each group is informing the rest, and each group is informed by the estimation of the others.</p>&#13;
<p><em>Figure <a href="#x1-71004r5">3.5</a></em> shows the posterior estimates of <em>μ</em> and <em>ν</em> plugged into a Beta distribution. In other words, this is the posterior distribution of the inferred Beta prior distribution.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file92.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-71004r5"/><strong>Figure 3.5</strong>: Posterior distribution of the inferred Beta prior distribution</p>&#13;
<p>Why is shrinkage desirable? Because it contributes to more stable inferences. This is, in many ways, similar to what we saw with the Student’s t-distribution and the outliers; using a heavy-tailed distribution results in a more robust model to data points away from the mean. Introducing hyperpriors results in a more conservative model, one that is less responsive to extreme values in individual groups. Imagine that the sample sizes are different from each neighborhood, some small, some large; the smaller the sample size, the easier it is to get bogus results. At an extreme, if you take only one sample in a given neighborhood, you may just hit the only really old lead pipe in the whole neighborhood or, on the contrary, the only one made out of PVC. In one case, you will overestimate the bad quality and in the other underestimate it. Under a hierarchical model, the misestimation of one group will be ameliorated by the information provided by the other groups. A larger sample size will also do the trick but, more often than not, that is not an option.</p>&#13;
<p>The amount of shrinkage depends on the data; a group with more data will pull the estimate of the other groups harder than a group with fewer data points. If several groups are similar and one group is different, the similar groups are going to inform the others of their similarity and reinforce a common estimation, while pulling the estimation for the less similar group toward them; this is exactly what we saw in the previous example. The hyperpriors also have a role in modulating the amount of shrinkage. We can effectively use an informative prior distribution to shrink our estimate to some reasonable value if we have trustworthy information about the group-level distribution.</p>&#13;
<div id="tcolobox-7" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>Shrinkage</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>In a hierarchical model, groups sharing a common hyperprior are effectively sharing information through the hyperprior. This results in shrinkage, that is, individual estimates are shrunk toward the common mean. By partially pooling the data, we are modeling the groups as some middle ground between the groups being independent of each other and being a single big group.</p>&#13;
</div>&#13;
</div>&#13;
<p>Nothing prevents us from building a hierarchical model with just two groups, but we would prefer to have several groups. Intuitively, the reason is that getting shrinkage is like assuming each group is a data point, and we are estimating the standard deviation at the group level. Generally, we do not trust an estimation with too few data points unless we have a strong prior informing our estimation. Something similar is true for a hierarchical model. <span id="x1-71005r159"/></p>&#13;
</section>&#13;
<section id="hierarchies-all-the-way-up" class="level3 sectionHead" data-number="1.7.5">&#13;
<h2 class="sectionHead" data-number="1.7.5">3.5 <span id="x1-720005"/>Hierarchies all the way up</h2>&#13;
<p><span id="dx1-72001"/></p>&#13;
<p>Various data structures lend themselves to hierarchical descriptions that can encompass multiple levels. For example, consider professional football (soccer) players. As in many other sports, players have different positions. We may be interested in estimating some skill metrics for each player, for the positions, and for the overall group of professional football players. This kind of hierarchical structure can be found in many other domains as well:</p>&#13;
<ul>&#13;
<li><p>Medical research: Suppose we are interested in estimating the effectiveness of different drugs for treating a particular disease. We can categorize patients based on their demographic information, disease severity, and other relevant factors and build a hierarchical model to estimate the probability of cure or treatment success for each subgroup. We can then use the parameters of the subgroup distribution to estimate the overall probability of cure or treatment success for the entire patient population.</p></li>&#13;
<li><p>Environmental science: Suppose we are interested in estimating the impact of a certain pollutant on a particular ecosystem. We can categorize different habitats within the ecosystem (e.g., rivers, lakes, forests, wetlands) and build a hierarchical model to estimate the distribution of pollutant levels within each habitat. We can then use the parameters of the habitat distribution to estimate the overall distribution of pollutant levels within the ecosystem.</p></li>&#13;
<li><p>Market research: Suppose we are interested in understanding the purchasing behavior of consumers for a particular product across different regions. We can categorize consumers based on their demographic information (e.g., age, gender, income, education) and build a hierarchical model to estimate the distribution of purchasing behavior for each subgroup. We can then use the parameters of the subgroup distribution to estimate the distribution of purchasing behavior for the overall group of consumers.</p></li>&#13;
</ul>&#13;
<p>Going back to our football players, we have collected data from the <em>Premier League</em>, <em>Ligue 1</em>, <em>Bundesliga</em>, <em>Serie A</em>, and <em>La Liga</em> over the course of four years (2017 to 2020). Let’s suppose we are interested in the goals-per-shot metric. This is what statisticians usually call a <em>success rate</em>, and we can estimate it with a Binomial model where the parameter <em>n</em> is the number of shots and the observations <em>y</em> is the number of goals. This leaves us with an unknown value for <em>p</em>. In previous examples, we have been calling this parameter <em>θ</em> and we have used a Beta distribution to model it. We will do the same now, but hierarchically. See <em>Figure <a href="#x1-72002r6">3.6</a></em> for a graphical representation of the entire model.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file93.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-72002r6"/><strong>Figure 3.6</strong>: Hierarchical model for the football players example. Notice that we have one more level than in previous hierarchical models</p>&#13;
<p>In our model, <em>θ</em> represents the success rate for each player, and thus it is a vector of size <code>n_players</code>. We use a Beta distribution to model <em>θ</em>. The hyperparameters of the Beta distribution will be the vectors <em>μ</em><sub><em>p</em></sub> and <em>ν</em><sub><em>p</em></sub>, which are vectors of size 4, representing the four positions in our dataset (defender <code>DF</code>, midfielder <code>MF</code>, forward <code>FW</code>, and goalkeeper <code>GK</code>). We will need to properly index the vectors <em>μ</em><sub><em>p</em></sub> and <em>ν</em><sub><em>p</em></sub> to match the total number of players. Finally, we will have two global parameters, <em>μ</em> and <em>ν</em>, representing the professional football players.</p>&#13;
<p>The PyMC model is defined in the following block of code. <code>pm.Beta(’</code><em>μ</em><code>’, 1.7, 5.8) </code>was chosen with the help of PreliZ as a prior with 95% of the mass between 0 and 0.5. This is an example of a weakly informative prior, as there is little doubt that a success rate of 0.5 is a high value. Sports statistics are well-studied, and there is a lot of prior information that could be used to define stronger priors. For this example, we will settle on this prior. A similar justification can be done for the prior <code>pm.Gamma(’<img src="../media/nu.PNG" style="width:0.75em; vertical-align: -0.10em;"/>’, mu=125, sigma=50)</code>, which we define as the maximum entropy Gamma prior with 90% of the mass between 50 and 200:</p>&#13;
<p><span id="x1-72003r5"/> <span id="x1-72004"/><strong>Code 3.5</strong></p>&#13;
<pre id="listing-42" class="source-code"><code>coords = {"pos": pos_codes} </code>&#13;
<code>with pm.Model(coords=coords) as model_football: </code>&#13;
<code>    # Hyper parameters </code>&#13;
<code>    μ = pm.Beta('μ', 1.7, 5.8) </code>&#13;
<code>    ν = pm.Gamma('ν', mu=125, sigma=50) </code>&#13;
<code>    # Parameters for positions </code>&#13;
<code>    μ_p = pm.Beta('μ_p', </code>&#13;
<code>                       mu=μ, </code>&#13;
<code>                       nu=ν, </code>&#13;
<code>                       dims = "pos") </code>&#13;
<code>    ν_p = pm.Gamma('ν_p', mu=125, sigma=50, dims="pos") </code>&#13;
<code>    # Parameter for players </code>&#13;
<code>    <em>θ</em> = pm.Beta('<em>θ</em>', </code>&#13;
<code>                    mu=μ_p[pos_idx], </code>&#13;
<code>                    nu=ν_p[pos_idx]) </code>&#13;
<code>    _ = pm.Binomial('gs', n=football.shots.values, p=<em>θ</em>, </code>&#13;
<code>                    observed=football.goals.values) </code>&#13;
<code> </code>&#13;
<code>    idata_football = pm.sample()</code></pre>&#13;
<p>In the top panel of <em>Figure <a href="#x1-72024r7">3.7</a></em>, we have the posterior distribution for the global parameter <em>μ</em>. The posterior distribution is close to 0.1. This means that overall for a professional football player (from a top league), the probability of scoring a goal is on average 10%. This is a reasonable value, as scoring goals is not an easy task and we are not discriminating positions, i.e, we are considering players whose main role is not scoring goals. In the middle panel, we have the estimated <em>μ</em><sub><em>p</em></sub> value for the forward position; as expected, it is higher than the global parameter <em>μ</em>. In the bottom panel, we have the estimated <em>θ</em> value for Lionel Messi, with a value of 0.17, which is higher than the global parameter <em>μ</em> and the forward position <em>μ</em><sub><em>p</em></sub> value. This is also expected, as Lionel Messi is the best football player in the world, and his main role is scoring goals.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file94.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-72024r7"/><strong>Figure 3.7</strong>: Posterior distribution for the mean global parameter (top), mean forward position (middle), and the <em>θ</em> parameter for Messi (bottom)</p>&#13;
<p><em>Figure <a href="#x1-72025r8">3.8</a></em> shows a forest plot for the posterior distribution for the parameter <em>μ</em><sub><em>p</em></sub>. The posterior distribution for the forward position is centered around 0.13, as we have already seen, and is the highest of the four. This makes sense as the role of the players at a forward position is scoring goals as well as assisting them. The lowest value of <em>μ</em><sub><em>p</em></sub> is for the goalkeeper position. This is expected, as the main role is to stop the opposing team from scoring, and not to score goals. The interesting aspect is that the uncertainty is very high; this is because we have very few goalkeepers scoring goals in our dataset, three to be precise. The posterior distributions for the defender and midfielder positions are somewhat in the middle, being slightly higher for the midfielder. We can explain this as the main role of a midfielder is to defend and attack, and thus the probability of scoring a goal is higher than a defender but lower than a forward.</p>&#13;
<div class="IMG---Figure">&#13;
<img src="../media/file95.png" alt="PIC"/>&#13;
</div>&#13;
<p class="IMG---Caption"><span id="x1-72025r8"/><strong>Figure 3.8</strong>: Posterior distribution for the parameter <em>μ</em></p>&#13;
<p><em>p, the mean position</em></p>&#13;
<div id="tcolobox-8" class="tcolorbox coolbox">&#13;
<div class="tcolorbox-title">&#13;
<p>You Need to Know When to Stop</p>&#13;
</div>&#13;
<div class="tcolorbox-content">&#13;
<p>It is possible to create hierarchical models with as many levels as we want. But unless the problem necessitates additional structure, adding more levels than required does not enhance the quality of our model or inferences. Instead, we will get entangled in a web of hyperpriors and hyperparameters without the ability to assign any meaningful interpretation to them. The goal of building models is to make sense of data, and thus useful models are usually those that reflect and take advantage of the structure of the data.</p>&#13;
</div>&#13;
</div>&#13;
<p><span id="x1-72026r161"/></p>&#13;
</section>&#13;
<section id="summary-2" class="level3 sectionHead" data-number="1.7.6">&#13;
<h2 class="sectionHead" data-number="1.7.6">3.6 <span id="x1-730006"/>Summary</h2>&#13;
<p>In this chapter, we have presented one of the most important concepts to learn from this book: hierarchical models. We can build hierarchical models every time we can identify subgroups in our data. In such cases, instead of treating the subgroups as separate entities or ignoring the subgroups and treating them as a single group, we can build a model to partially pool information among groups. The main effect of this partial pooling is that the estimates of each subgroup will be biased by the estimates of the rest of the subgroups. This effect is known as shrinkage and, in general, is a very useful trick that helps to improve inferences by making them more conservative (as each subgroup informs the others by pulling estimates toward it) and more informative. We get estimates at the subgroup level and the group level.</p>&#13;
<p>Paraphrasing the Zen of Python, we can certainly say <em>hierarchical models are one</em> <em>honking great idea, let’s do more of those!</em> In the following chapters, we will keep building hierarchical models and learn how to use them to build better models. We will also discuss how hierarchical models are related to the pervasive overfitting/underfitting issue in statistics and machine learning in <em>Chapter <a href="CH05.xhtml#x1-950005">5</a></em>. In <em>Chapter <a href="CH10.xhtml#x1-18900010">10</a></em>, we will discuss some technical problems that we may find when sampling from hierarchical models and how to diagnose and fix those problems. <span id="x1-73001r164"/></p>&#13;
</section>&#13;
<section id="exercises-2" class="level3 sectionHead" data-number="1.7.7">&#13;
<h2 class="sectionHead" data-number="1.7.7">3.7 <span id="x1-740007"/>Exercises</h2>&#13;
<ol>&#13;
<li><div id="x1-74002x1">&#13;
<p>Using your own words explain the following concepts in two or three sentences:</p>&#13;
<ul>&#13;
<li><p>Complete pooling</p></li>&#13;
<li><p>No pooling</p></li>&#13;
<li><p>Partial pooling</p></li>&#13;
</ul>&#13;
</div></li>&#13;
<li><div id="x1-74004x2">&#13;
<p>Repeat the exercise we did with <code>model_h</code>. This time, without a hierarchical structure, use a flat prior such as Beta(<em>α</em> = 1<em>,<em>β</em></em> = 1). Compare the results of both models.</p>&#13;
</div></li>&#13;
<li><div id="x1-74006x3">&#13;
<p>Create a hierarchical version of the tips example from <em>Chapter <a href="CH02.xhtml#x1-440002">2</a></em>, by partially pooling across the days of the week. Compare the results to those obtained without the hierarchical structure.</p>&#13;
</div></li>&#13;
<li><div id="x1-74008x4">&#13;
<p>For each subpanel in <em>Figure <a href="#x1-72024r7">3.7</a></em>, add a reference line representing the empirical mean value at each level, that is, the global mean, the forward mean, and Messi’s mean. Compare the empirical values to the posterior mean values. What do you observe?</p>&#13;
</div></li>&#13;
<li><div id="x1-74010x5">&#13;
<p>Amino acids are usually grouped into categories such as <code>polar</code>, <code>non-polar</code>, <code>charged</code>, and <code>special</code>. Build a hierarchical model similar to <code>cs_h </code>but including a group effect for the amino acid category. Compare the results to those obtained in this chapter.</p>&#13;
</div></li>&#13;
</ol>&#13;
</section>&#13;
<section id="join-our-community-discord-space-3" class="level3 likesectionHead" data-number="1.7.8">&#13;
<h2 class="likesectionHead" data-number="1.7.8"><span id="x1-750007"/>Join our community Discord space</h2>&#13;
<p>Join our Discord community to meet like-minded people and learn alongside more than 5000 members at: <a href="https://packt.link/bayesian">https://packt.link/bayesian</a></p>&#13;
<p><img src="../media/file1.png" alt="PIC"/></p>&#13;
<p><span id="x1-75001r147"/></p>&#13;
</section>&#13;
</section>&#13;
</body></html>
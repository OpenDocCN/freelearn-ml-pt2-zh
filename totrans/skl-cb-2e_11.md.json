["```py\nimport numpy as np\nimport pandas as pd\n\ndata_web_address = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n\ncolumn_names = ['pregnancy_x',\n'plasma_con',\n'blood_pressure',\n'skin_mm',\n'insulin',\n'bmi',\n'pedigree_func',\n'age',\n'target']\n\nfeature_names = column_names[:-1]\n\nall_data = pd.read_csv(data_web_address , names=column_names) \n\nX = all_data[feature_names]\ny = all_data['target']\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler \n\nscaler = StandardScaler()\nscaler.fit(X_train) \nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```", "```py\nfrom sklearn.linear_model import Perceptron\n\npr = Perceptron()\npr.fit(X_train_scaled, y_train) \nPerceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n verbose=0, warm_start=False)\n```", "```py\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n\nskf = StratifiedKFold(n_splits=3)\ncross_val_score(pr, X_train_scaled, y_train, cv=skf,scoring='roc_auc').mean()\n\n0.76832628835771022\n```", "```py\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nprint \"Classification accuracy : \", accuracy_score(y_test, pr.predict(X_test_scaled))\nprint \"ROC-AUC Score : \",roc_auc_score(y_test, pr.predict(X_test_scaled))\n\nClassification accuracy : 0.681818181818\nROC-AUC Score : 0.682592592593\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\n\nparam_dist = {'alpha': [0.1,0.01,0.001,0.0001], \n 'penalty': [None, 'l2','l1','elasticnet'],\n 'random_state': [7],\n 'class_weight':['balanced',None],'eta0': [0.25,0.5,0.75,1.0], \n 'warm_start':[True,False], 'max_iter':[50], 'tol':[1e-3]}\n\ngs_perceptron = GridSearchCV(pr, param_dist, scoring='roc_auc',cv=skf).fit(X_train_scaled, y_train)\n```", "```py\ngs_perceptron.best_params_\n\n{'alpha': 0.001,\n 'class_weight': None,\n 'eta0': 0.5,\n 'max_iter': 50,\n 'penalty': 'l2',\n 'random_state': 7,\n 'tol': 0.001,\n 'warm_start': True}\n\ngs_perceptron.best_score_\n\n0.79221656570311072\n```", "```py\nbest_perceptron = gs_perceptron.best_estimator_\n```", "```py\nfrom sklearn.ensemble import BaggingClassifier \n\nfrom sklearn.ensemble import BaggingClassifier \nparam_dist = {\n 'max_samples': [0.5,1.0],\n 'max_features' : [0.5,1.0],\n 'oob_score' : [True, False],\n 'n_estimators': [100],\n 'n_jobs':[-1],\n 'base_estimator__alpha': [0.001,0.002],\n 'base_estimator__penalty': [None, 'l2','l1','elasticnet'], }\n\nensemble_estimator = BaggingClassifier(base_estimator = best_perceptron)\nbag_perceptrons = GridSearchCV(ensemble_estimator, param_dist,scoring='roc_auc',cv=skf,n_jobs=-1).fit(X_train_scaled, y_train)\n```", "```py\nbag_perceptrons.best_score_\n\n0.83299842529587864\n\nbag_perceptrons.best_params_\n\n{'base_estimator__alpha': 0.001,\n 'base_estimator__penalty': 'l1',\n 'max_features': 1.0,\n 'max_samples': 1.0,\n 'n_estimators': 100,\n 'n_jobs': -1,\n 'oob_score': True}\n```", "```py\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\n\ncali_housing = fetch_california_housing()\n\nX = cali_housing.data\ny = cali_housing.target\n```", "```py\nbins = np.arange(6)\nbinned_y = np.digitize(y, bins)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=binned_y)\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_scaled = scaler.transform(X_train)\n```", "```py\nX_test_scaled = scaler.transform(X_test)\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neural_network import MLPRegressor\n\nparam_grid = {'alpha': [10,1,0.1,0.01],\n 'hidden_layer_sizes' : [(50,50,50),(50,50,50,50,50)],\n 'activation': ['relu','logistic'],\n 'solver' : ['adam']\n }\n\npre_gs_inst = RandomizedSearchCV(MLPRegressor(random_state=7),\n param_distributions = param_grid,\n cv=3,\n n_iter=15,\n random_state=7)\npre_gs_inst.fit(X_train_scaled, y_train)\n\npre_gs_inst.best_score_\n\n0.7729679848718175\n\npre_gs_inst.best_params_\n\n{'activation': 'relu',\n 'alpha': 0.01,\n 'hidden_layer_sizes': (50, 50, 50),\n 'solver': 'adam'}\n```", "```py\nfrom __future__ import division\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\n\n#From within an ipython notebook\n%matplotlib inline\n\ncali_housing = fetch_california_housing()\n\nX = cali_housing.data\ny = cali_housing.target\n```", "```py\nbins = np.arange(6)\nbinned_y = np.digitize(y, bins)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train_prin, X_test_prin, y_train_prin, y_test_prin = train_test_split(X, y,test_size=0.2,stratify=binned_y,random_state=7)\n\nbinned_y_train_prin = np.digitize(y_train_prin, bins)\n\nX_1, X_stack, y_1, y_stack = train_test_split(X_train_prin,y_train_prin,test_size=0.33,stratify=binned_y_train_prin,random_state=7 )\n```", "```py\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPRegressor\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nmlp_pipe = Pipeline(steps=[('scale', StandardScaler()), ('neural_net', MLPRegressor())])\n\nparam_grid = {'neural_net__alpha': [0.02,0.01,0.005],\n 'neural_net__hidden_layer_sizes' : [(50,50,50)],\n 'neural_net__activation': ['relu'],\n 'neural_net__solver' : ['adam']\n }\n\n neural_net_gs = GridSearchCV(mlp_pipe, param_grid = param_grid,cv=3, n_jobs=-1)\n neural_net_gs.fit(X_1, y_1)\n```", "```py\nneural_net_gs.best_params_\n\n {'neural_net__activation': 'relu',\n 'neural_net__alpha': 0.01,\n 'neural_net__hidden_layer_sizes': (50, 50, 50),\n 'neural_net__solver': 'adam'}\n\nneural_net_gs.best_score_\n\n 0.77763106799320014\n```", "```py\nnn_best = neural_net_gs.best_estimator_\n\nimport pickle\n\nf = open('nn_best.save', 'wb')\npickle.dump(nn_best, f, protocol = pickle.HIGHEST_PROTOCOL)\nf.close()\n```", "```py\nfrom sklearn.model_selection import RandomizedSearchCV\n from sklearn.ensemble import GradientBoostingRegressor\n\n param_grid = {'learning_rate': [0.1,0.05,0.03,0.01],\n 'loss': ['huber'],\n 'max_depth': [5,7,10],\n 'max_features': [0.4,0.6,0.8,1.0],\n 'min_samples_leaf': [2,3,5],\n 'n_estimators': [100],\n 'warm_start': [True], 'random_state':[7]\n }\n\n boost_gs = RandomizedSearchCV(GradientBoostingRegressor(), param_distributions = param_grid,cv=3, n_jobs=-1,n_iter=25)\n boost_gs.fit(X_1, y_1)\n```", "```py\nboost_gs.best_score_\n\n0.82767651150013244\n\nboost_gs.best_params_\n\n{'learning_rate': 0.1, 'loss': 'huber', 'max_depth': 10, 'max_features': 0.4, 'min_samples_leaf': 5, 'n_estimators': 100, 'random_state': 7, 'warm_start': True}\n```", "```py\ngbt_inst = GradientBoostingRegressor(**{'learning_rate': 0.1,\n 'loss': 'huber',\n 'max_depth': 10,\n 'max_features': 0.4,\n 'min_samples_leaf': 5,\n 'n_estimators': 4000,\n 'warm_start': True, 'random_state':7}).fit(X_1, y_1)\n```", "```py\ndef pickle_func(filename, saved_object):\n import pickle\n\n f = open(filename, 'wb')\n pickle.dump(saved_object, f, protocol = pickle.HIGHEST_PROTOCOL)\n f.close()\n\n return None\n\npickle_func('grad_boost.save', gbt_inst)\n```", "```py\nfrom sklearn.ensemble import BaggingRegressor,GradientBoostingRegressor\n from sklearn.model_selection import RandomizedSearchCV\n\n param_dist = {\n 'max_samples': [0.5,1.0],\n 'max_features' : [0.5,1.0],\n 'oob_score' : [True, False],\n 'base_estimator__min_samples_leaf': [4,5],\n 'n_estimators': [20]\n }\n\n single_estimator = GradientBoostingRegressor(**{'learning_rate': 0.1,\n 'loss': 'huber',\n 'max_depth': 10,\n 'max_features': 0.4,\n 'n_estimators': 20,\n 'warm_start': True, 'random_state':7})\n\n ensemble_estimator = BaggingRegressor(base_estimator = single_estimator)\n\n pre_gs_inst_bag = RandomizedSearchCV(ensemble_estimator,\n param_distributions = param_dist,\n cv=3,\n n_iter = 5,\n n_jobs=-1)\n\n pre_gs_inst_bag.fit(X_1, y_1)\n```", "```py\npre_gs_inst_bag.best_score_\n\n0.78087218305611195\n\npre_gs_inst_bag.best_params_\n\n {'base_estimator__min_samples_leaf': 5,\n 'max_features': 1.0,\n 'max_samples': 1.0,\n 'n_estimators': 20,\n 'oob_score': True}\n```", "```py\npickle_func('bag_gbm.save', pre_gs_inst_bag.best_estimator_)\n```", "```py\ndef handle_X_set(X_train_set_in):\n X_train_set = X_train_set_in.copy()\n\n y_pred_nn = neural_net.predict(X_train_set)\n y_pred_gbt = gbt.predict(X_train_set)\n y_pred_bag = bag_gbm.predict(X_train_set)\n\n preds_df = pd.DataFrame(columns = ['nn', 'gbt','bag'])\n\n preds_df['nn'] = y_pred_nn\n preds_df['gbt'] = y_pred_gbt\n preds_df['bag'] = y_pred_bag\n\n return preds_df\n\ndef predict_from_X_set(X_train_set_in):\n X_train_set = X_train_set_in.copy() \n return final_etr.predict(handle_X_set(X_train_set)) \n```", "```py\ndef pickle_load_func(filename):\n f = open(filename, 'rb')\n to_return = pickle.load(f)\n f.close()\n\n return to_return\n\nneural_net = pickle_load_func('nn_best.save')\ngbt = pickle_load_func('grad_boost.save')\nbag_gbm = pickle_load_func('bag_gbm.save')\n```", "```py\npreds_df = handle_X_set(X_stack)\nprint (preds_df.corr())\n\n nn       gbt       bag\nnn   1.000000  0.867669  0.888655\ngbt  0.867669  1.000000  0.981368\nbag  0.888655  0.981368  1.000000\n```", "```py\nfrom sklearn.ensemble import ExtraTreesRegressor\n from sklearn.model_selection import RandomizedSearchCV\n\n param_dist = {'max_features' : ['sqrt','log2',1.0],\n 'min_samples_leaf' : [1, 2, 3, 7, 11],\n 'n_estimators': [50, 100],\n 'oob_score': [True, False]}\n\n pre_gs_inst = RandomizedSearchCV(ExtraTreesRegressor(warm_start=True,bootstrap=True,random_state=7),\n param_distributions = param_dist,\n cv=3,\n n_iter = 15,random_state=7)\n\n pre_gs_inst.fit(preds_df.values, y_stack)\n```", "```py\npre_gs_inst.best_params_\n\n{'max_features': 1.0,\n 'min_samples_leaf': 11,\n 'n_estimators': 100,\n 'oob_score': False}\n```", "```py\nfinal_etr = ExtraTreesRegressor(**{'max_features': 1.0,\n 'min_samples_leaf': 11,\n 'n_estimators': 3000,\n 'oob_score': False, 'random_state':7}).fit(preds_df.values, y_stack)\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(final_etr, preds_df.values, y_stack, cv=3).mean()\n\n0.82212054913537747\n```", "```py\ny_pred = predict_from_X_set(X_test_prin)\n\n from sklearn.metrics import r2_score, mean_absolute_error\n\n print \"R-squared\",r2_score(y_test_prin, y_pred)\n print \"MAE : \",mean_absolute_error(y_test_prin, y_pred)\n print \"MAPE : \",(np.abs(y_test_prin- y_pred)/y_test_prin).mean()\n\nR-squared 0.839538887753\nMAE :  0.303109168851\nMAPE :  0.168643891048\n```", "```py\ndef handle_X_set_sp(X_train_set_in):\n X_train_set = X_train_set_in.copy()\n\n y_pred_nn = neural_net.predict(X_train_set)\n y_pred_gbt = gbt.predict(X_train_set)\n y_pred_bag = bag_gbm.predict(X_train_set)\n\n #only change in function: include input vectors in training dataframe\n preds_df = pd.DataFrame(X_train_set, columns = cali_housing.feature_names)\n\n preds_df['nn'] = y_pred_nn\n preds_df['gbt'] = y_pred_gbt\n preds_df['bag'] = y_pred_bag\n\n return preds_df\n\ndef predict_from_X_set_sp(X_train_set_in):\n X_train_set = X_train_set_in.copy()\n\n #change final estimator's name to final_etr_sp and use handle_X_set_sp within this function\n return final_etr_sp.predict(handle_X_set_sp(X_train_set))\n```", "```py\npreds_df_sp = handle_X_set_sp(X_stack)\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {'max_features' : ['sqrt','log2',1.0],\n 'min_samples_leaf' : [1, 2, 3, 7, 11],\n 'n_estimators': [50, 100],\n 'oob_score': [True, False]}\n\npre_gs_inst_2 = RandomizedSearchCV(ExtraTreesRegressor(warm_start=True,bootstrap=True,random_state=7),\n param_distributions = param_dist,\n cv=3,\n n_iter = 15,random_state=7)\n\npre_gs_inst_2.fit(preds_df_sp.values, y_stack)\n```", "```py\n{'max_features': 'log2',\n 'min_samples_leaf': 2,\n 'n_estimators': 100,\n 'oob_score': False}\n\nfinal_etr_sp = ExtraTreesRegressor(**{'max_features': 'log2',\n 'min_samples_leaf': 2,\n 'n_estimators': 3000,\n 'oob_score': False,'random_state':7}).fit(preds_df_sp.values, y_stack)\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(final_etr_sp, preds_df_sp.values, y_stack, cv=3).mean()\n\n0.82978653642597144\n```", "```py\ny_pred = predict_from_X_set_sp(X_test_prin)\n\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint \"R-squared\",r2_score(y_test_prin, y_pred)\nprint \"MAE : \",mean_absolute_error(y_test_prin, y_pred)\nprint \"MAPE : \",(np.abs(y_test_prin- y_pred)/y_test_prin).mean()\n\nR-squared 0.846455829258\nMAE :  0.295381654368\nMAPE :  0.163374936923\n```"]
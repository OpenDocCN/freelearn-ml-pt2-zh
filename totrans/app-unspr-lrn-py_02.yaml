- en: '*Chapter 2*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hierarchical Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the hierarchical clustering algorithm from scratch by using packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform agglomerative clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare k-means with hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will use hierarchical clustering to build stronger groupings
    which make more logical sense.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will expand on the basic ideas that we built in *Chapter
    1*, *Introduction to Clustering*, by surrounding clustering with the concept of
    similarity. Once again, we will be implementing forms of the Euclidean distance
    to capture the notion of similarity. It is important to bear in mind that the
    Euclidean distance just happens to be one of the most popular distance metrics
    and not the only one! Through these distance metrics, we will expand on the simple
    neighbor calculations that we explored in the previous chapter by introducing
    the concept of hierarchy. By using hierarchy to convey clustering information,
    we can build stronger groupings that make more logical sense.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Refresher
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Chapter 1*, *Introduction to Clustering*, covered both the high-level intuition
    and in-depth details of one of the most basic clustering algorithms: k-means.
    While it is indeed a simple approach, do not discredit it; it will be a valuable
    addition to your toolkit as you continue your exploration of the unsupervised
    learning world. In many real-world use cases, companies experience groundbreaking
    discoveries through the simplest methods, such as k-means or linear regression
    (for supervised learning). As a refresher, let''s quickly walk through what clusters
    are and how k-means works to find them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: The attributes that separate supervised and unsupervised problems'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: The attributes that separate supervised and unsupervised problems'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you were given a random collection of data without any guidance, you would
    likely start your exploration using basic statistics â€“ for example, what the mean,
    median, and mode values are of each of the features. Remember that, from a high-level
    data model that simply exists, knowing whether it is supervised or unsupervised
    learning is ascribed by the data goals that you have set for yourself or that
    were set by your manager. If you were to determine that one of the features was
    actually a label and you wanted to see how the remaining features in the dataset
    influence it, this would become a supervised learning problem. However, if after
    initial exploration you realize that the data you have is simply a collection
    of features without a target in mind (such as a collection of health metrics,
    purchase invoices from a web store, and so on), then you could analyze it through
    unsupervised methods.
  prefs: []
  type: TYPE_NORMAL
- en: A classic example of unsupervised learning is finding clusters of similar customers
    in a collection of invoices from a web store. Your hypothesis is that by understanding
    which people are most similar, you can create more granular marketing campaigns
    that appeal to each cluster's interests. One way to achieve these clusters of
    similar users is through k-means.
  prefs: []
  type: TYPE_NORMAL
- en: k-means Refresher
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: k-means clustering works by finding "k" number clusters in your data through
    pairwise Euclidean distance calculations. "K" points (also called centroids) are
    randomly initialized in your data and the distance is calculated from each data
    point to each of the centroids. The minimum of these distances designates which
    cluster a data point belongs to. Once every point has been assigned to a cluster,
    the mean intra-cluster data point is calculated as the new centroid. This process
    is repeated until the newly-calculated cluster centroid no longer changes position.
  prefs: []
  type: TYPE_NORMAL
- en: The Organization of Hierarchy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both the natural and human-made world contain many examples of organizing systems
    into hierarchies and why, for the most part, it makes a lot of sense. A common
    representation that is developed from these hierarchies can be seen in tree-based
    data structures. Imagine that you had a parent node with any number of child nodes
    that could subsequently be parent nodes themselves. By organizing concepts into
    a tree structure, you can build an information-dense diagram that clearly shows
    how things are related to their peers and their larger abstract concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example from the natural world to help illustrate this concept can be seen
    in how we view the hierarchy of animals, which goes from parent classes to individual
    species:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Navigating the relationships of animal species in a hierarchical
    tree structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Navigating the relationships of animal species in a hierarchical
    tree structure'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Figure 2.2, you can see an example of how relational information between
    varieties of animals can be easily mapped out in a way that both saves space and
    still transmits a large amount of information. This example can be seen as both
    a tree of its own (showing how cats and dogs are different but both domesticated
    animals), and as a potential piece of a larger tree that shows a breakdown of
    domesticated versus non-domesticated animals.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the event that most of you are not biologists, let''s move back toward the
    concept of a web store selling products. If you sold a large variety of products,
    then you would likely want to create a hierarchical system of navigation for your
    customers. By withholding all of the information in your product catalog, customers
    will only be exposed to the path down the tree that matches their interests. An
    example of the hierarchical benefits of navigation can be seen in Figure 2.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Navigating product categories in a hierarchical tree structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.3: Navigating product categories in a hierarchical tree structure'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, the benefits of a hierarchical system of navigation cannot be overstated
    in terms of improving your customer experience. By organizing information into
    a hierarchical structure, you can build an intuitive structure out of your data
    that demonstrates explicit nested relationships. If this sounds like another approach
    to finding clusters in your data, then you're definitely on the right track! Through
    the use of similar distance metrics such as the Euclidean distance from k-means,
    we can develop a tree that shows the many cuts of data that allow a user to subjectively
    create clusters at their discretion.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Until this point, we have shown that hierarchies can be excellent structures
    in which to organize information that clearly show nested relationships among
    data points. While this is helpful in gaining an understanding of the parent/child
    relationships between items, it can also be very handy when forming clusters.
    Expanding on the animal example of the prior section, imagine that you were simply
    presented with two features of animals: their height (measured from the tip of
    the nose to the end of the tail) and their weight. Using this information, you
    then have to recreate the same structure in order to identify which records in
    your dataset correspond to dogs or cats, as well as their relative subspecies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you are only given animal heights and weights, you won''t be able to
    extrapolate the specific names of each species. However, by analyzing the features
    that you have been provided, you can develop a structure within the data that
    serves as an approximation of what animal species exist in your data. This perfectly
    sets the stage for an unsupervised learning problem that is well solved with hierarchical
    clustering. In the following plot, you will see the two features that we created
    on the left: with animal height in the left-hand column and animal weight in the
    right-hand column. This is then charted on a two-axis plot with the height on
    the x axis and the weight on the y axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: An example of a two-feature dataset comprising animal height
    and animal weight'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.4: An example of a two-feature dataset comprising animal height and
    animal weight'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One way to approach hierarchical clustering is by starting with each data point
    serving as its own cluster and recursively joining the similar points together
    to form clusters â€“ this is known as **agglomerative** hierarchical clustering.
    We will go into more detail about the different ways of approaching hierarchical
    clustering in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: In the agglomerative hierarchical clustering approach, the concept of data point
    similarity can be thought of in the paradigm that we saw during k-means. In k-means,
    we used the Euclidean distance to calculate the distance from the individual points
    to the centroids of the expected "k" clusters. For this approach to hierarchical
    clustering, we will reuse the same distance metric to determine the similarity
    between the records in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, by grouping individual records from the data with their most similar
    records recursively, you end up building a hierarchy from the bottom up. The individual
    single-member clusters join together into one single cluster at the top of our
    hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to Perform Hierarchical Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand how agglomerative hierarchical clustering works, we can trace
    the path of a simple toy program as it merges together to form a hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: Given n sample data points, view each point as an individual "cluster" with
    just that one point as a member.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the pairwise Euclidean distance between the centroids of all the clusters
    in your data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group the closest point pairs together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Step 2* and *Step 3* until you reach a single cluster containing all
    the data in your set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot a dendrogram to show how your data has come together in a hierarchical
    structure. A dendrogram is simply a diagram that is used to represent a tree structure,
    showing an arrangement of clusters from top to bottom.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide what level you want to create the clusters at.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An Example Walk-Through of Hierarchical Clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While slightly more complex than k-means, hierarchical clustering does not
    change too much from a logistical perspective. Here is a simple example that walks
    through the preceding steps in slightly more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a list of four sample data points, view each point as a centroid that
    is also its own cluster with the point indices from 0 to 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clusters (4): [ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Centroids (4): [ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the pairwise Euclidean distance between the centroids of all the
    clusters. In the matrix displayed in the following diagram, the point indices
    are between 0 and 3 both horizontally and vertically, showing the distance between
    the respective points. Along the diagonal are extremely high values to ensure
    that we do not select a point as its own neighbor (since it technically is the
    "closest" point). Notice that the values are mirrored across the diagonal:![Figure
    2.5: An array of distances'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C12626_02_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.5: An array of distances'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Group the closest point pairs together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this case, points [1,7] and [-5,9] join into a cluster since they are closest,
    with the remaining two points left as single-member clusters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.6: An array of distances'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_02_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.6: An array of distances'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here are the resulting three clusters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the centroid of the two-member cluster, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Add the centroid to the two single-member centroids and recalculate the distances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clusters (3):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Centroids (3):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following diagram, with the shortest distance
    called using a red arrow:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.7: An array of distances'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_02_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.7: An array of distances'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since it has the shortest distance, point [-9,4] is added to cluster 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clusters (2):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With only point (4,-2) left as the furthest distance away from its neighbors,
    you can just add it to cluster 1 to unify all the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clusters (1):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot a dendrogram to show the relationship between the points and the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.8: A dendrogram showing the relationship between the points and
    the clusters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: A dendrogram showing the relationship between the points and the
    clusters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the end of this process you can visualize the hierarchical structure that
    you created through a dendrogram. This plot shows how data points are similar
    and will look familiar to the hierarchical tree structures that we discussed earlier.
    Once you have this dendrogram structure, you can interpret how the data points
    relate to each other and subjectively decide at which "level" the clusters should
    exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Revisiting the previous animal taxonomy example from that involved dog and
    cat species, imagine that you were presented with the following dendrogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: An animal taxonomy dendrogram'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: An animal taxonomy dendrogram'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The great thing about hierarchical clustering and dendrograms is that you can
    see the entire breakdown of potential clusters to choose from. If you were just
    interested in grouping your species dataset into dogs and cats, you could stop
    clustering at the first level of the grouping. However, if you wanted to group
    all species into domesticated or non-domesticated animals, you could stop clustering
    at level two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7: Building a Hierarchy'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s try implementing the preceding hierarchical clustering approach in Python.
    With the framework for the intuition laid out, we can now explore the process
    of building a hierarchical cluster with some helper functions provided in `SciPy`.
    This exercise uses `SciPy`, an open source library that packages functions that
    are helpful in scientific and technical computing; examples of this include easy
    implementations of linear algebra and calculus-related methods. In addition to
    `SciPy`, we will be using Matplotlib to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate some dummy data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.10: A plot of the dummy data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_02_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.10: A plot of the dummy data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: After plotting this simple toy example, it should be pretty clear that our dummy
    data is comprised of eight clusters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can easily generate the distance matrix using the built-in `SciPy` package,
    ''`linkage`'':'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.11: A matrix of the distances'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_02_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.11: A matrix of the distances'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In the first situation, you can see that customizing the hyperparameters really
    drives the performance when finding the ideal linkage matrix. If you recall our
    previous steps, linkage works by simply calculating the distances between each
    of the data points. In the `linkage` function, we have the option to select both
    the metric and the method (we will cover more on this later).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After we determine the linkage matrix, we can easily pass it through the dendrogram
    function provided by `SciPy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.12: A dendrogram of the distances'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_02_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.12: A dendrogram of the distances'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This plot will give us some perspective on the potential breakouts of our data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using this information, we can wrap up our exercise on hierarchical clustering
    by using the `fcluster` function from `SciPy`. The number `3` in the
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'following example represents the maximum inter-cluster distance threshold hyperparameter
    that you will set. This hyperparameter is tunable based on the dataset that you
    are looking at; however, it is supplied for you as `3` for this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.13: A scatter plot of the distances'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.13: A scatter plot of the distances'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By simply calling a few helper functions provided by SciPy, you can easily implement
    agglomerative clustering in just a few lines of code. While SciPy does help with
    many of the intermediate steps, this is still an example that is a bit more verbose
    than what you will probably see in your regular work. We will cover more streamlined
    implementations later.
  prefs: []
  type: TYPE_NORMAL
- en: Linkage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Exercise 7*, *Building a Hierarchy*, you implemented hierarchical clustering
    using what is known as **Centroid Linkage**. Linkage is the concept of determining
    how you can calculate the distances between clusters and is dependent on the type
    of problem you are facing. Centroid linkage was chosen for the first activity
    as it essentially mirrors the new centroid search that we used in k-means. However,
    this is not the only option when it comes to clustering data points together.
    Two other popular choices for determining distances between clusters are single
    linkage and complete linkage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Single Linkage** works by finding the minimum distance between a pair of
    points between two clusters as its criteria for linkage. Put simply, it essentially
    works by combining clusters based on the closest points between the two clusters.
    This is expressed mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: dist(a,b) = min( dist( a[i]), b[j] ) )
  prefs: []
  type: TYPE_NORMAL
- en: '**Complete Linkage** is the opposite of single linkage and it works by finding
    the maximum distance between a pair of points between two clusters as its criteria
    for linkage. Put simply, it works by combining clusters based on the furthest
    points between the two clusters. This is mathematically expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: dist(a,b) = max( dist( a[i]), b[j] ) )
  prefs: []
  type: TYPE_NORMAL
- en: Determining what linkage criteria is best for your problem is as much about
    art as it is about science and it is heavily dependent on your particular dataset.
    One reason to choose single linkage is that your data is similar in a nearest-neighbor
    sense, therefore, when there are differences, then the data is extremely dissimilar.
    Since single linkage works by finding the closest points, it will not be affected
    by these distant outliers. Conversely, complete linkage may be a better option
    if your data is distant in terms of inter-cluster, however, it is quite dense
    intra-cluster. Centroid linkage has similar benefits but falls apart if the data
    is very noisy and there are less clearly defined "centers" of clusters. Typically,
    the best approach is to try a few different linkage criteria options and to see
    which fits your data in a way that's most relevant to your goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2: Applying Linkage Criteria'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recall the dummy data of the eight clusters that we generated in the previous
    exercise. In the real world, you may be given real data that resembles discrete
    Gaussian blobs in the same way. Imagine that the dummy data represents different
    groups of shoppers in a particular store. The store manager has asked you to analyze
    the shopper data in order to classify the customers into different groups, so
    that they can tailor marketing materials to each group.
  prefs: []
  type: TYPE_NORMAL
- en: Using the data already generated in the previous exercise, or by generating
    new data, you are going to analyze which linkage types do the best job of grouping
    the customers into distinct clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have generated the data, view the documents supplied using SciPy to
    understand what linkage types are available in the `linkage` function. Then, evaluate
    the linkage types by applying them to your data. The linkage types you should
    test are shown in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: By completing this activity, you will gain an understanding of the linkage criteria
    â€“ which is important to understand how effective your hierarchical clustering
    is. The aim is to gain an understanding of how linkage criteria play a role in
    different datasets and how it can make a useless clustering into a valid one.
  prefs: []
  type: TYPE_NORMAL
- en: You may realize that we have not covered all of the previously mentioned linkage
    types â€“ a key part of this activity is to learn how to parse the docstrings provided
    using packages to explore all of their capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps required to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the dataset that we created in *Exercise 7*, *Building a Hierarchy*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a list with all the possible linkage method hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loop through each of the methods in the list that you just created and display
    the effect they have on the same dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should generate a plot for each linkage type and use the plots to comment
    on which linkage types are most suitable for this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plots that you will generate should look similar to the ones in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14: The expected scatter plots for all methods'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.14: The expected scatter plots for all methods'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity is on page 310.
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative versus Divisive Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our instances of hierarchical clustering so far have all been agglomerative
    â€“ that is, they have been built from the bottom up. While this is typically the
    most common approach for this type of clustering, it is important to know that
    it is not the only way a hierarchy can be created. The opposite hierarchical approach,
    that is, built from the top up, can also be used to create your taxonomy. This
    approach is called **Divisive** Hierarchical Clustering and works by having all
    the data points in your dataset in one massive cluster. Many of the internal mechanics
    of the divisive approach will prove to be quite similar to the agglomerative approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15: Agglomerative versus divisive hierarchical clustering'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.15: Agglomerative versus divisive hierarchical clustering'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with most problems in unsupervised learning, deciding the best approach is
    often highly dependent on the problem you are faced with solving.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you are an entrepreneur who has just bought a new grocery store
    and needs to stock it with goods. You receive a large shipment of food and drink
    in a container, but you've lost track of all the shipment information! In order
    to most effectively sell your products, you must group similar products together
    (your store will be a huge mess if you just put everything on the shelves in a
    random order). Setting out on this organizational goal, you can take either a
    bottom-up or top-down approach. On the bottom-up side, you will go through the
    shipping container and think of everything as disorganized â€“ you will then pick
    up a random object and find its most similar product. For example, you may pick
    up apple juice and realize that it makes sense to group it together with orange
    juice. With the top-down approach, you will view everything as organized in one
    large group. Then, you will move through your inventory and split the groups based
    on the largest differences in similarity. For example, you may originally think
    that apple juice and tofu go together, but on second thoughts, they are really
    different. Therefore, you will break them into smaller, dissimilar groups.
  prefs: []
  type: TYPE_NORMAL
- en: In general, it helps to think of agglomerative as the bottom-up approach and
    divisive as the top-down approach â€“ but how do they trade off in performance?
    Due to the greedy nature of Agglomerative, it has the potential to be fooled by
    local neighbors and not see the larger implications of clusters it forms at any
    given time. On the flip side, the divisive approach has the benefit of seeing
    the entire data distribution as one from the beginning and choosing the best way
    to break down clusters. This insight into what the entire dataset looks like is
    helpful for potentially creating more accurate clusters and should not be overlooked.
    Unfortunately, a top-down approach, typically, trades off greater accuracy with
    deeper complexity. In practice, an agglomerative approach works most of the time
    and should be the preferred starting point when it comes to hierarchical clustering.
    If, after reviewing the hierarchies, you are unhappy with the results, it may
    help to take a divisive approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 8: Implementing Agglomerative Clustering with scikit-learn'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In most real-world use cases, you will likely find yourself implementing hierarchical
    clustering with a package that abstracts everything away, such as scikit-learn.
    Scikit-learn is a free package that is indispensable when it comes to machine
    learning in Python. It conveniently provides highly optimized forms of the most
    popular algorithms, such as regression, classification, and, of book, clustering.
    By using an optimized package such as scikit-learn, your work becomes much easier.
    However, you should only use it when you fully understand how hierarchical clustering
    works from the prior sections. The following exercise will compare two potential
    routes that you can take when forming clusters â€“ using SciPy and scikit-learn.
    By completing the exercise, you will learn what the pros and cons are of each,
    and which suits you best from a user perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn makes implementation as easy as just a few lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, we assign the model to the `ac` variable, by passing in parameters that
    we are familiar with, such as `affinity` (the distance function) and `linkage`
    (explore your options as we did in *Activity 2*, *Implementing Linkage Criteria*).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After instantiating our model into a variable, we can simply pass through the
    dataset we are interested in in order to determine where the cluster memberships
    lie using `.fit_predict()` and assigning it to an additional variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can then compare how each of the approaches work by comparing the final
    cluster results through plotting. Let''s take a look at the clusters from the
    scikit-learn approach:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output for the clusters from the scikit-learn approach:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.16: A plot of the Scikit-Learn approach'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12626_02_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.16: A plot of the Scikit-Learn approach'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Take a look at the clusters from the SciPy Learn approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17: A plot of the SciPy approach'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.17: A plot of the SciPy approach'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in our example problem, the two converge to basically the same
    clusters. While this is great from a toy-problem perspective, you will soon learn,
    in the next activity, that small changes to the input parameters can lead to wildly
    different results!
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3: Comparing k-means with Hierarchical Clustering'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are managing a store's inventory and receive a large shipment of wine, but
    the brand labels have fallen off the bottles during transit. Fortunately, your
    supplier has provided you with the chemical readings for each bottle, along with
    their respective serial numbers. Unfortunately, you aren't able to open each bottle
    of wine and taste test the difference â€“ you must find a way to group the unlabeled
    bottles back together according to their chemical readings! You know from the
    order list that you ordered three different types of wine and are given only two
    wine attributes to group the wine types back together. In this activity, we will
    be using the wine dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The wine dataset can be downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/).
    It can be accessed at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activ](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activity03)ity03.
  prefs: []
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science'
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this activity is to implement k-means and hierarchical clustering
    on the wine dataset and to explore which approach ends up being more accurate
    or easier for you to use. You can try different combinations of scikit-learn implementations
    and use helper functions in SciPy and NumPy. You can use the silhouette score
    to compare the different clustering methods and visualize the clusters on a graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected Outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: After completing this activity, you will have gained an understanding of how
    k-means and hierarchical clustering work on similar datasets. You will likely
    notice that one method performs better than the other depending on how the data
    is shaped. Another key outcome from this activity is gaining an understanding
    of how important hyperparameters are in any given use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary packages from scikit-learn (`KMeans`, `AgglomerativeClustering`,
    and `silhouette_score`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the wine dataset into the pandas DataFrame and print a small sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the wine dataset to understand its data structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the sklearn implementation of k-means on the wine dataset, knowing that
    there are three wine types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the sklearn implementation of hierarchical clustering on the wine dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the predicted clusters from k-means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the predicted clusters from hierarchical clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the silhouette score of each clustering method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Plot the predicted clusters from the k-means clustering method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.18: The expected clusters from the k-means method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.18: The expected clusters from the k-means method'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the predicted clusters from the agglomerative clustering method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19: The expected clusters from the agglomerative method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12626_02_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.19: The expected clusters from the agglomerative method'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity is on page 312.
  prefs: []
  type: TYPE_NORMAL
- en: k-means versus Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have expanded our understanding of how k-means clustering works,
    it is important to explore where hierarchical clustering fits into the picture.
    As mentioned in the linkage criteria section, there is some potential direct overlap
    when it comes to grouping data points together using centroids. Universal to all
    of the approaches mentioned so far, is also the use of a distance function to
    determine similarity. Due to our in-depth exploration in the previous chapter,
    we have kept using the Euclidean distance, but we understand that any distance
    function can be used to determine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, here are some quick highlights for choosing one clustering method
    over another:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering benefits from not needing to pass in an explicit "k"
    number of clusters apriori. This means that you can find all the potential clusters
    and decide which clusters make the most sense after the algorithm has completed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering benefits from a simplicity perspective â€“ oftentimes, in business
    use cases, there is a challenge to find methods that can be explained to non-technical
    audiences but still be accurate enough to generate quality results. k-means can
    easily fill this niche.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering has more parameters to tweak than k-means clustering
    when it comes to dealing with abnormally shaped data. While k-means is great at
    finding discrete clusters, it can falter when it comes to mixed clusters. By tweaking
    the parameters in hierarchical clustering, you may find better results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanilla k-means clustering works by instantiating random centroids and finding
    the closest points to those centroids. If they are randomly instantiated in areas
    of the feature space that are far away from your data, then it can end up taking
    quite some time to converge, or it may never even get to that point. Hierarchical
    clustering is less prone to falling prey to this weakness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed how hierarchical clustering works and where it
    may be best employed. In particular, we discussed various aspects of how clusters
    can be subjectively chosen through the evaluation of a dendrogram plot. This is
    a huge advantage compared to k-means clustering if you have absolutely no idea
    of what you''re looking for in the data. Two key parameters that drive the success
    of hierarchical clustering were also discussed: the agglomerative versus divisive
    approach and linkage criteria. Agglomerative clustering takes a bottom-up approach
    by recursively grouping nearby data together until it results in one large cluster.
    Divisive clustering takes a top-down approach by starting with the one large cluster
    and recursively breaking it down until each data point falls into its own cluster.
    Divisive clustering has the potential to be more accurate since it has a complete
    view of the data from the start; however, it adds a layer of complexity that can
    decrease the stability and increase the runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linkage criteria grapples with the concept of how distance is calculated between
    candidate clusters. We have explored how centroids can make an appearance again
    beyond k-means clustering, as well as single and complete linkage criteria. Single
    linkage finds cluster distances by comparing the closest points in each cluster,
    while complete linkage finds cluster distances by comparing more distant points
    in each cluster. From the understanding that you have gained in this chapter,
    you are now able to evaluate how both k-means and hierarchical clustering can
    best fit the challenge that you are working on. In the next chapter, we will cover
    a clustering approach that will serve us best in the highly complex data: **DBSCAN**
    (Density-Based Spatial Clustering of Applications with Noise).'
  prefs: []
  type: TYPE_NORMAL

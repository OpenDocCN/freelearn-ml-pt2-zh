<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 12. Bigger Data"><div class="titlepage"><div><div><h1 class="title"><a id="ch12"/>Chapter 12. Bigger Data</h1></div></div></div><p>It's not easy to say what big data is. We will adopt an operational definition: when data is so large that it becomes cumbersome to work with, we will talk about <span class="strong"><strong>big data</strong></span>. In some areas, this might mean petabytes of data or trillions of transactions: data which will not fit into a single hard drive. In other cases, it may be one hundred times smaller, but still difficult to work with.</p><p>Why has data itself become an issue? While computers keep getting faster and have more memory, the size of the data has grown as well. In fact, data has grown faster than computational speed and few algorithms scale linearly with the size of the input data—taken together, this means that data has grown faster than our ability to process it.</p><p>We will first build on some of the experience of the previous chapters and work with what we can call medium data setting (not quite big data, but not small either). For this, we will use a package called <a id="id601" class="indexterm"/>
<span class="strong"><strong>jug</strong></span>, which allows us to perform the following tasks:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Break up your pipeline into tasks</li><li class="listitem" style="list-style-type: disc">Cache (memoize) intermediate results</li><li class="listitem" style="list-style-type: disc">Make use of multiple cores, including multiple computers on a grid</li></ul></div><p>The next step is to move to true <span class="emphasis"><em>big data</em></span> and we will see how to use the cloud for computation purpose. In particular, you will learn about the Amazon Web Services infrastructure. In this section, we introduce another Python package called StarCluster to manage clusters.</p><div class="section" title="Learning about big data"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec67"/>Learning about big data</h1></div></div></div><p>The expression "big data" does <a id="id602" class="indexterm"/>not mean a specific amount of data, neither in the number of examples nor in the number of gigabytes, terabytes, or petabytes occupied by the data. It means that data has been growing faster than processing power. This implies the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Some of the methods and techniques that worked well in the past now need to be redone or replaced as they do not scale well to the new size of the input data</li><li class="listitem" style="list-style-type: disc">Algorithms cannot assume that all the input data can fit in RAM</li><li class="listitem" style="list-style-type: disc">Managing data becomes a major task in itself</li><li class="listitem" style="list-style-type: disc">Using computer clusters or multicore machines becomes a necessity and not a luxury</li></ul></div><p>This chapter will <a id="id603" class="indexterm"/>focus on this last piece of the puzzle: how to use multiple cores (either on the same machine or on separate machines) to speed up and organize your computations. This will also be useful in other medium-sized data tasks.</p><div class="section" title="Using jug to break up your pipeline into tasks"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec96"/>Using jug to break up your pipeline into tasks</h2></div></div></div><p>Often, we<a id="id604" class="indexterm"/> have a simple pipeline: we preprocess the initial data, compute features, and then call a machine learning algorithm with the resulting features.</p><p>Jug is a package developed by Luis Pedro Coelho, one of the authors of this book. It's open source (using the liberal MIT License) and can be useful in many areas, but was designed specifically around data analysis problems. It simultaneously solves several problems, for example:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It can <span class="emphasis"><em>memoize</em></span> results to disk (or a database), which means that if you ask it to compute something you have already computed before, the result is instead read from disk.</li><li class="listitem" style="list-style-type: disc">It can use multiple cores or even multiple computers on a cluster. Jug was also designed to work very well in batch computing environments, which use queuing systems such as <a id="id605" class="indexterm"/><span class="strong"><strong>PBS</strong></span> (<span class="strong"><strong>Portable Batch System</strong></span>), <span class="strong"><strong>LSF</strong></span> (<span class="strong"><strong>Load Sharing Facility</strong></span>), or <span class="strong"><strong>Grid Engine</strong></span>. This will be used in<a id="id606" class="indexterm"/> the second half of the <a id="id607" class="indexterm"/>chapter as we build online clusters and dispatch jobs to them.</li></ul></div></div><div class="section" title="An introduction to tasks in jug"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec97"/>An introduction to tasks in jug</h2></div></div></div><p>Tasks<a id="id608" class="indexterm"/> are the basic building block of jug. A task<a id="id609" class="indexterm"/> is composed of a function and values for its arguments. Consider this simple example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def double(x):</strong></span>
<span class="strong"><strong>    return 2*x</strong></span>
</pre></div><p>In this chapter, the code examples will generally have to be typed in script files. Thus, they will not be shown with the <code class="literal">&gt;&gt;&gt;</code> marker. Commands that should be typed at the shell will be indicated by preceding them with <code class="literal">$</code>.</p><p>A task could be "call double with argument 3". Another task would be "call double with argument 642.34". Using jug, we can build these tasks as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>from jug import Task</strong></span>
<span class="strong"><strong>t1 = Task(double, 3)</strong></span>
<span class="strong"><strong>t2 = Task(double, 642.34)</strong></span>
</pre></div><p>Save this to a file<a id="id610" class="indexterm"/> called <code class="literal">jugfile.py</code> (which is just a regular Python file). Now, we can run <code class="literal">jug execute</code> to run the tasks. This is something you type on the command line, not at the Python prompt, so we show it marked with a dollar sign (<code class="literal">$</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ jug execute</strong></span>
</pre></div><p>You will also get some feedback on the tasks (jug will say that two tasks named <code class="literal">double</code> were run). Run <code class="literal">jug execute</code> again and it will tell you that it did nothing! It does not need to. In this case, we gained little, but if the tasks took a long time to compute, it would have been very useful.</p><p>You may notice that a new directory also appeared on your hard drive named <code class="literal">jugfile.jugdata</code> with a few weirdly named files. This is the memoization cache. If you remove it, <code class="literal">jug execute</code> will run all your tasks again.</p><p>Often, it's good to distinguish between pure functions, which simply take their inputs and return a result, from more general functions that can perform actions (such as reading from files, writing to files, accessing global variables, modify their arguments, or anything that the language allows). Some programming languages, such as Haskell, even have syntactic ways to distinguish pure from impure functions.</p><p>With jug, your tasks do not need to be perfectly pure. It's even recommended that you use tasks to read in your data or write out your results. However, accessing and modifying global variables will not work well: the tasks may be run in any order in different processors. The exceptions are global constants, but even this may confuse the memoization system (if the value is changed between runs). Similarly, you should not modify the input values. Jug has a debug mode (use <code class="literal">jug execute --debug</code>), which slows down your computation, but will give you useful error messages if you make this sort of mistake.</p><p>The preceding code works, but is a bit cumbersome. You are always repeating the <code class="literal">Task(function, argument)</code> construct. Using a bit of Python magic, we can make the code even more natural as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>from jug import TaskGenerator</strong></span>
<span class="strong"><strong>from time import sleep</strong></span>

<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def double(x):</strong></span>
<span class="strong"><strong>    sleep(4)</strong></span>
<span class="strong"><strong>    return 2*x</strong></span>

<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def add(a, b):</strong></span>
<span class="strong"><strong>    return a + b</strong></span>

<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def print_final_result(oname, value):</strong></span>
<span class="strong"><strong>    with open(oname, 'w') as output:</strong></span>
<span class="strong"><strong>        output.write('Final result: {}\n'.format(value))</strong></span>


<span class="strong"><strong>y = double(2)</strong></span>
<span class="strong"><strong>z = double(y)</strong></span>

<span class="strong"><strong>y2 = double(7)</strong></span>
<span class="strong"><strong>z2 = double(y2)</strong></span>
<span class="strong"><strong>print_final_result('output.txt', add(z,z2))</strong></span>
</pre></div><p>Except for<a id="id611" class="indexterm"/> the use of <code class="literal">TaskGenerator</code>, the preceding code could be a standard Python file! However, using <code class="literal">TaskGenerator</code>, it actually creates a series of tasks and it is now possible to run it in a way that takes advantage of multiple processors. Behind the scenes, the decorator transforms your functions so that they do not actually execute when called, but create a <code class="literal">Task</code> object. We also take advantage of the fact that we can pass tasks to other tasks and this results in a dependency being generated.</p><p>You may have noticed that we added a few <code class="literal">sleep(4)</code> calls in the preceding code. This simulates running a long computation. Otherwise, this example is so fast that there is no point in using multiple processors.</p><p>We start by running <code class="literal">jug status</code>, which results in the output shown in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_12_10.jpg" alt="An introduction to tasks in jug"/></div><p>Now, we start two processes simultaneously (using the <code class="literal">&amp;</code> operator in the background):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ jug execute &amp;</strong></span>
<span class="strong"><strong>$ jug execute &amp;</strong></span>
</pre></div><p>Now, we run <code class="literal">jug status</code> again:</p><div class="mediaobject"><img src="images/2772OS_12_11.jpg" alt="An introduction to tasks in jug"/></div><p>We can see that <a id="id612" class="indexterm"/>the two initial double operators are running at the same time. After about 8 seconds, the whole process will finish and the <code class="literal">output.txt</code> file will be written.</p><p>By the way, if your file was called anything other than <code class="literal">jugfile.py</code>, you would then have to specify it explicitly on the command line. For example, if your file was called <code class="literal">analysis.py</code>, you would run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ jug execute analysis.py</strong></span>
</pre></div><p>This is the only disadvantage of not using the name <code class="literal">jugfile.py</code>. So, feel free to use more meaningful names.</p></div><div class="section" title="Looking under the hood"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec98"/>Looking under the hood</h2></div></div></div><p>How does<a id="id613" class="indexterm"/> jug <a id="id614" class="indexterm"/>work? At the basic level, it's very simple. A <code class="literal">Task</code> is a function plus its argument. Its arguments may be either values or other tasks. If a task takes other tasks, there is a dependency between the two tasks (and the second one cannot be run until the results of the first task are available).</p><p>Based on this, jug recursively computes a hash for each task. This hash value encodes the whole computation to get the result. When you run <code class="literal">jug execute</code>, for each task, there is a little loop that runs the logic depicted in the following flowchart:</p><div class="mediaobject"><img src="images/2772OS_12_12.jpg" alt="Looking under the hood"/></div><p>The default <a id="id615" class="indexterm"/>backend writes the file to disk (in this funny directory named <code class="literal">jugfile.jugdata/</code>). Another backend is available, which uses a Redis database. With <a id="id616" class="indexterm"/>proper locking, which jug takes care of, this also allows for many processors to execute tasks; each process will independently look at all the tasks and run the ones that have not run yet and then write them back to the shared backend. This works on either the same machine (using multicore processors) or in multiple machines as long as they all have access to the same backend (for example, using a network disk or the Redis databases). In the second half of this chapter, we will discuss computer clusters, but for now let's focus on multiple cores.</p><p>You can also understand why it's able to memoize intermediate results. If the backend already has the result of a task, it's not run again. On the other hand, if you change the task, even in minute ways (by altering one of the parameters), its hash will change. Therefore, it will be rerun. Furthermore, all tasks that depend on it will also have their hashes changed and they will be rerun as well.</p></div><div class="section" title="Using jug for data analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec99"/>Using jug for data analysis</h2></div></div></div><p>Jug is <a id="id617" class="indexterm"/>a generic framework, but it's ideally suited for medium-scale data analysis. As you develop your analysis pipeline, it's good to have <a id="id618" class="indexterm"/>intermediate results automatically saved. If you have already computed the preprocessing step before and are only changing the features you compute, you do not want to recompute the preprocessing step. If you have already computed the features, but want to try combining a few new ones into the mix, you also do not want to recompute all your other features.</p><p>Jug is also specifically optimized to work with NumPy arrays. Whenever your tasks return or receive NumPy arrays, you are taking advantage of this optimization. Jug is another piece of this ecosystem where everything works together.</p><p>We will now look back at <a class="link" href="ch10.html" title="Chapter 10. Computer Vision">Chapter 10</a>, <span class="emphasis"><em>Computer Vision</em></span>. In that chapter, we learned how to compute features on images. Remember that the basic pipeline consisted of the following features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Loading image files</li><li class="listitem" style="list-style-type: disc">Computing features</li><li class="listitem" style="list-style-type: disc">Combining these features</li><li class="listitem" style="list-style-type: disc">Normalizing the features</li><li class="listitem" style="list-style-type: disc">Creating a classifier</li></ul></div><p>We are going to redo this exercise, but this time with the use of jug. The advantage of this version is that it's now possible to add a new feature or classifier without having to recompute all of the pipeline.</p><p>We start with a few imports as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>from jug import TaskGenerator</strong></span>
<span class="strong"><strong>import mahotas as mh</strong></span>
<span class="strong"><strong>from glob import glob</strong></span>
</pre></div><p>Now, we define the first task generators and feature computation functions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def compute_texture(im):</strong></span>
<span class="strong"><strong>    from features import texture</strong></span>
<span class="strong"><strong>    imc = mh.imread(im)</strong></span>
<span class="strong"><strong>    return texture(mh.colors.rgb2gray(imc))</strong></span>

<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def chist_file(fname):</strong></span>
<span class="strong"><strong>    from features import chist</strong></span>
<span class="strong"><strong>    im = mh.imread(fname)</strong></span>
<span class="strong"><strong>    return chist(im)</strong></span>
</pre></div><p>The <code class="literal">features</code> module we import is the one from <a class="link" href="ch10.html" title="Chapter 10. Computer Vision">Chapter 10</a>, <span class="emphasis"><em>Computer Vision</em></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>We write functions that take the filename as input instead of the image array. Using the full images would also work, of course, but this is a small optimization. A filename is a string, which is small if it gets written to the backend. It's also very fast to compute a hash if needed. It also ensures that the images are only loaded by the processes that need them.</p></div></div><p>We can <a id="id619" class="indexterm"/>use <code class="literal">TaskGenerator</code> on any function. This<a id="id620" class="indexterm"/> is true even for functions, which we did not write, such as <code class="literal">np.array</code>, <code class="literal">np.hstack</code>, or the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import numpy as np</strong></span>
<span class="strong"><strong>to_array = TaskGenerator(np.array)</strong></span>
<span class="strong"><strong>hstack = TaskGenerator(np.hstack)</strong></span>

<span class="strong"><strong>haralicks = []</strong></span>
<span class="strong"><strong>chists = []</strong></span>
<span class="strong"><strong>labels = []</strong></span>

<span class="strong"><strong># Change this variable to point to</strong></span>
<span class="strong"><strong># the location of the dataset on disk</strong></span>
<span class="strong"><strong>basedir = '../SimpleImageDataset/'</strong></span>
<span class="strong"><strong># Use glob to get all the images</strong></span>
<span class="strong"><strong>images = glob('{}/*.jpg'.format(basedir))</strong></span>

<span class="strong"><strong>for fname in sorted(images):</strong></span>
<span class="strong"><strong>    haralicks.append(compute_texture(fname))</strong></span>
<span class="strong"><strong>    chists.append(chist_file(fname))</strong></span>
<span class="strong"><strong>    # The class is encoded in the filename as xxxx00.jpg</strong></span>
<span class="strong"><strong>    labels.append(fname[:-len('00.jpg')])</strong></span>

<span class="strong"><strong>haralicks = to_array(haralicks)</strong></span>
<span class="strong"><strong>chists = to_array(chists)</strong></span>
<span class="strong"><strong>labels = to_array(labels)</strong></span>
</pre></div><p>One small inconvenience of using jug is that we must always write functions to output the results to files, as shown in the preceding examples. This is a small price to pay for the extra convenience of using jug.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def accuracy(features, labels):</strong></span>
<span class="strong"><strong>    from sklearn.linear_model import LogisticRegression</strong></span>
<span class="strong"><strong>    from sklearn.pipeline import Pipeline</strong></span>
<span class="strong"><strong>    from sklearn.preprocessing import StandardScaler</strong></span>
<span class="strong"><strong>    from sklearn import cross_validation</strong></span>

<span class="strong"><strong>    clf = Pipeline([('preproc', StandardScaler()),</strong></span>
<span class="strong"><strong>                ('classifier', LogisticRegression())])</strong></span>
<span class="strong"><strong>    cv = cross_validation.LeaveOneOut(len(features))</strong></span>
<span class="strong"><strong>    scores = cross_validation.cross_val_score(</strong></span>
<span class="strong"><strong>        clf, features, labels, cv=cv)</strong></span>
<span class="strong"><strong>    return scores.mean()</strong></span>
</pre></div><p>Note that <a id="id621" class="indexterm"/>we are only importing <code class="literal">sklearn</code> inside this<a id="id622" class="indexterm"/> function. This is a small optimization. This way, <code class="literal">sklearn</code> is only imported when it's really needed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scores_base = accuracy(haralicks, labels)</strong></span>
<span class="strong"><strong>scores_chist = accuracy(chists, labels)</strong></span>

<span class="strong"><strong>combined = hstack([chists, haralicks])</strong></span>
<span class="strong"><strong>scores_combined  = accuracy(combined, labels)</strong></span>
</pre></div><p>Finally, we write and call a function to print out all results. It expects its argument to be a list of pairs with the name of the algorithm and the results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def print_results(scores):</strong></span>
<span class="strong"><strong>    with open('results.image.txt', 'w') as output:</strong></span>
<span class="strong"><strong>        for k,v in scores:</strong></span>
<span class="strong"><strong>            output.write('Accuracy [{}]: {:.1%}\n'.format(</strong></span>
<span class="strong"><strong>                k, v.mean()))</strong></span>

<span class="strong"><strong>print_results([</strong></span>
<span class="strong"><strong>        ('base', scores_base),</strong></span>
<span class="strong"><strong>        ('chists', scores_chist),</strong></span>
<span class="strong"><strong>        ('combined' , scores_combined),</strong></span>
<span class="strong"><strong>        ])</strong></span>
</pre></div><p>That's it. Now, on the shell, run the following command to run this pipeline using jug:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ jug execute image-classification.py</strong></span>
</pre></div></div><div class="section" title="Reusing partial results"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec100"/>Reusing partial results</h2></div></div></div><p>For example, let's say <a id="id623" class="indexterm"/>you want to add a new feature (or even a set of features). As we saw in <a class="link" href="ch10.html" title="Chapter 10. Computer Vision">Chapter 10</a>, <span class="emphasis"><em>Computer Vision</em></span>, this is easy to do by changing the feature computation code. However, this would imply recomputing all the features again, which is wasteful, particularly, if you want to test new features and techniques quickly.</p><p>We now add a set of features, that is, another type of texture feature called linear binary patterns. This is implemented in mahotas; we just need to call a function, but we wrap it in <code class="literal">TaskGenerator</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>@TaskGenerator</strong></span>
<span class="strong"><strong>def compute_lbp(fname):</strong></span>
<span class="strong"><strong>    from mahotas.features import lbp</strong></span>
<span class="strong"><strong>    imc = mh.imread(fname)</strong></span>
<span class="strong"><strong>    im = mh.colors.rgb2grey(imc)</strong></span>
<span class="strong"><strong>    # The parameters 'radius' and 'points' are set to typical values</strong></span>
<span class="strong"><strong>    # check the documentation for their exact meaning</strong></span>
<span class="strong"><strong>    return lbp(im, radius=8, points=6)</strong></span>
</pre></div><p>We replace the previous loop to have an extra function call:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>lbps = []</strong></span>
<span class="strong"><strong>for fname in sorted(images):</strong></span>
<span class="strong"><strong>    # the rest of the loop as before</strong></span>
<span class="strong"><strong>    lbps.append(compute_lbp(fname))</strong></span>
<span class="strong"><strong>lbps = to_array(lbps)</strong></span>
</pre></div><p>We call accuracy with these newer features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scores_lbps = accuracy(lbps, labels)</strong></span>
<span class="strong"><strong>combined_all = hstack([chists, haralicks, lbps])</strong></span>
<span class="strong"><strong>scores_combined_all = accuracy(combined_all, labels)</strong></span>

<span class="strong"><strong>print_results([</strong></span>
<span class="strong"><strong>        ('base', scores_base),</strong></span>
<span class="strong"><strong>        ('chists', scores_chist),</strong></span>
<span class="strong"><strong>        ('lbps', scores_lbps),</strong></span>
<span class="strong"><strong>        ('combined' , scores_combined),</strong></span>
<span class="strong"><strong>        ('combined_all' , scores_combined_all),</strong></span>
<span class="strong"><strong>        ])</strong></span>
</pre></div><p>Now, when you run <code class="literal">jug execute</code> again, the new features will be computed, but the old features will be loaded from the cache. This is when jug can be very powerful. It ensures that you always get the results you want while saving you from unnecessarily recomputing cached results. You will also see that adding this feature set improves on the previous methods.</p><p>Not all features <a id="id624" class="indexterm"/>of jug could be mentioned in this chapter, but here is a summary of the most potentially interesting ones we didn't cover in the main text:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">jug invalidate</code>: This <a id="id625" class="indexterm"/>declares that all results from a given function should be considered invalid and in need of recomputation. This will also recompute any downstream computation, which depended (even indirectly) on the invalidated results.</li><li class="listitem" style="list-style-type: disc"><code class="literal">jug status --cache</code>: If <code class="literal">jug status</code> takes<a id="id626" class="indexterm"/> too long, you can use the <code class="literal">--cache</code> flag to cache the status and speed it up. Note that this will not detect any changes to the jugfile, but you can always use <code class="literal">--cache --clear</code> to remove the cache and start again.</li><li class="listitem" style="list-style-type: disc"><code class="literal">jug cleanup</code>: This <a id="id627" class="indexterm"/>removes any extra files in the memoization cache. This is a garbage collection operation.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note16"/>Note</h3><p>There are other, more advanced features, which allow you to look at values that have been computed inside the jugfile. Read up on features such as barriers in the jug <a id="id628" class="indexterm"/>documentation online at <a class="ulink" href="http://jug.rtfd.org">http://jug.rtfd.org</a>.</p></div></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Using Amazon Web Services"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec68"/>Using Amazon Web Services</h1></div></div></div><p>When you have a lot of data and a lot of computation to be performed, you might start to crave more computing power. Amazon (<a class="ulink" href="http://aws.amazon.com">http://aws.amazon.com</a>) allows <a id="id629" class="indexterm"/>you to rent computing power by the hour. Thus, you can access a large amount of computing power without having to precommit by purchasing a large number of machines (including the costs of managing the infrastructure). There are other competitors in this market, but Amazon is the largest player, so we briefly cover it here.</p><p>
<span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) is <a id="id630" class="indexterm"/>a large set of services. We will focus only on the<a id="id631" class="indexterm"/> <span class="strong"><strong>Elastic Compute Cloud</strong></span> (<span class="strong"><strong>EC2</strong></span>) service. This service offers you virtual machines and disk space, which can be allocated and deallocated quickly.</p><p>There are three <a id="id632" class="indexterm"/>modes of use. First is a reserved mode, whereby you prepay to have cheaper per-hour access, a fixed per-hour rate, and a variable rate, which depends on the overall compute market (when there is less demand, the costs are lower; when there is more demand, the prices go up).</p><p>On top of this general system, there are several types of machines available with varying costs, from a single core to a multicore system with a lot of RAM or even graphical processing units (GPUs). We will later see that you can also get several of the cheaper machines and build yourself a virtual cluster. You can also choose to get a Linux or Windows server (with Linux being slightly cheaper). In this chapter, we will work on our examples on Linux, but most of this information will be valid for Windows machines as well.</p><p>For testing, you<a id="id633" class="indexterm"/> can use a single machine in the <span class="strong"><strong>free tier</strong></span>. This<a id="id634" class="indexterm"/> allows you to play around with the system, get used to the interface, and so on. Note, though, that this machine contains a slow CPU.</p><p>The resources can be managed through a web interface. However, it's also possible to do so programmatically and to write scripts that allocate virtual machines, format hard disks, and perform all operations that are possible through the web interface. In fact, while the web interface changes very frequently (and some of the screenshots we show in the book may be out of date by the time it goes to press), the programmatic interface is more stable and the general architecture has remained stable since the service was introduced.</p><p>Access <a id="id635" class="indexterm"/>to AWS services is performed through a traditional username/password system, although Amazon calls the username an <span class="emphasis"><em>access key</em></span><a id="id636" class="indexterm"/> and the password a<a id="id637" class="indexterm"/> <span class="emphasis"><em>secret key</em></span>. They probably do so to keep it separate from the username/password you use to access the web interface. In fact, you can create as many access/secret key pairs as you wish and give them different permissions. This is helpful for a larger team where a senior user with access to the full web panel can create other keys for developers with fewer privileges.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>Amazon.com has several regions. These correspond to physical regions of the world: West coast US, East coast US, several Asian locations, a South American one, and two European ones. If you will be transferring data, it's best to keep it close to where you will be transferring to and from. Additionally, keep in mind that if you are handling user information, there may be regulatory issues regulating their transfer to another jurisdiction. In this case, do check with an informed counsel on the implications of transferring data about European customers to the US or any other similar transfer.</p></div></div><p>Amazon Web Services is a very large topic and there are various books exclusively available to cover AWS. The purpose of this chapter is to give you an overall impression of what is available and what is possible with AWS. In the practical spirit of this book, we do this by working through examples, but we will not exhaust all possibilities.</p><div class="section" title="Creating your first virtual machines"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec101"/>Creating your first virtual machines</h2></div></div></div><p>The first step<a id="id638" class="indexterm"/> is to go to <a class="ulink" href="http://aws.amazon.com/">http://aws.amazon.com/</a> and create an account. These steps are similar to any other online service. A single machine is free, but to get more, you will need a credit card. In<a id="id639" class="indexterm"/> this example, we will use a few machines, so it may cost you a few dollars if you want to run through it. If you are not ready to take out a credit card just yet, you can certainly read the chapter to learn what AWS provides without going through the examples. Then you can make a more informed decision on whether to sign up.</p><p>Once you sign up for AWS and log in, you will be taken to the console. Here, you will see the many services that AWS provides, as depicted in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_12_01.jpg" alt="Creating your first virtual machines"/></div><p>We pick and click on <span class="strong"><strong>EC2</strong></span> (the top element on the leftmost column—this is the panel shown as it was when this book was written. Amazon regularly makes minor changes, so you may see something slightly different from what we present in the book). We now see the EC2 management console, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_12_02.jpg" alt="Creating your first virtual machines"/></div><p>In the top-right corner, you <a id="id640" class="indexterm"/>can pick your region (see the Amazon regions information box). Note that <span class="emphasis"><em>you will only see information about the region that you have selected at the moment</em></span>. Thus, if you mistakenly select the <a id="id641" class="indexterm"/>wrong region (or have machines running in multiple regions), your machines may not appear (this seems to be a common pitfall of using the EC2 web management console).</p><p>In EC2 parlance, a running server is called an <a id="id642" class="indexterm"/>
<span class="strong"><strong>instance</strong></span>. We select <span class="strong"><strong>Launch Instance</strong></span>, which leads to the following screen asking us to select the operating system to use:</p><div class="mediaobject"><img src="images/2772OS_12_03.jpg" alt="Creating your first virtual machines"/></div><p>Select the <span class="strong"><strong>Amazon Linux</strong></span> option (if you are familiar with one of the other offered Linux distributions, such <a id="id643" class="indexterm"/>as Red Hat, SUSE, or Ubuntu, you can also select one of these, but the configurations will be slightly different). Now<a id="id644" class="indexterm"/> that you have selected the software, you will need to select the hardware. In the next screen, you will be asked to select which type of machine to use:</p><div class="mediaobject"><img src="images/2772OS_12_04.jpg" alt="Creating your first virtual machines"/></div><p>We will start with one instance of the <span class="strong"><strong>t2.micro</strong></span> type (the <span class="strong"><strong>t1.micro</strong></span> type was an older, even less powerful machine). This<a id="id645" class="indexterm"/> is the <a id="id646" class="indexterm"/>smallest possible machine and it's free. Keep clicking on <span class="strong"><strong>Next</strong></span> and accept all of the defaults until you come to the screen mentioning a key pair:</p><div class="mediaobject"><img src="images/2772OS_12_06.jpg" alt="Creating your first virtual machines"/></div><p>We will pick the <a id="id647" class="indexterm"/>name <code class="literal">awskeys</code> for the key pair. Then check <span class="strong"><strong>Create a new key pair</strong></span>. Name the key pair file <code class="literal">awskeys.pem</code>. Download<a id="id648" class="indexterm"/> and save this file somewhere safe! This is the SSH (Secure Shell) key that will enable you to log in to your cloud machine. Accept the remaining defaults and your instance will launch.</p><p>You will now need to wait a few minutes for your instance to come up. Eventually, the instance will be shown in green with the <a id="id649" class="indexterm"/>status as <span class="strong"><strong>running</strong></span>:</p><div class="mediaobject"><img src="images/2772OS_12_08.jpg" alt="Creating your first virtual machines"/></div><p>In the preceding screenshot, you should see the Public IP which can be used to log in to the instance as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ssh -i awskeys.pem ec2-user@54.93.165.5</strong></span>
</pre></div><p>Therefore, we<a id="id650" class="indexterm"/> will be calling the <code class="literal">ssh</code> command<a id="id651" class="indexterm"/> and passing it the key files that we downloaded earlier as the identity (using the <code class="literal">-i</code> option). We are logging in as user <code class="literal">ec2-user</code> at the machine with the IP address as 54.93.165.5. This address will, of course, be different in your case. If you choose another distribution for your instance, the username may also change. In this case, try logging in as <code class="literal">root</code>, <code class="literal">ubuntu</code> (for Ubuntu distribution), or <code class="literal">fedora</code> (for fedora distribution).</p><p>Finally, if you are running a Unix-style operating system (including Mac OS), you may have to tweak its permissions by calling the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ chmod 600 awskeys.pem</strong></span>
</pre></div><p>This sets the read/write permission for the current user only. SSH will otherwise give you an ugly warning.</p><p>Now you should be able to log in to your machine. If everything is okay, you should see the banner, as shown in the following screenshot:</p><div class="mediaobject"><img src="images/2772OS_12_09.jpg" alt="Creating your first virtual machines"/></div><p>This is a<a id="id652" class="indexterm"/> regular <a id="id653" class="indexterm"/>Linux box where you have <code class="literal">sudo</code> permission: you can run any command as the superuser by prefixing it with <code class="literal">sudo</code>. You can run the <code class="literal">update</code> command it recommends to get your machine up to speed.</p><div class="section" title="Installing Python packages on Amazon Linux"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec20"/>Installing Python packages on Amazon Linux</h3></div></div></div><p>If you prefer <a id="id654" class="indexterm"/>another distribution, you can use your knowledge of that distribution to install Python, NumPy, and others. Here, we will do it on the standard Amazon distribution. We start <a id="id655" class="indexterm"/>by installing several basic Python packages as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo yum -y install python-devel \</strong></span>
<span class="strong"><strong>    python-pip numpy scipy python-matplotlib</strong></span>
</pre></div><p>To compile mahotas, we will also need a C++ compiler:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo yum -y install gcc-c++</strong></span>
</pre></div><p>Finally, we install <code class="literal">git</code> to make sure that we can get the latest version of the code for the book:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo yum -y install git</strong></span>
</pre></div><p>In this system, pip is installed as <code class="literal">python-pip</code>. For convenience, we will use pip to upgrade itself. We will then use pip to install the necessary packages as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo pip-python install -U pip</strong></span>
<span class="strong"><strong>$ sudo pip install scikit-learn jug mahotas</strong></span>
</pre></div><p>At this point, you can install any other package you wish using pip.</p></div><div class="section" title="Running jug on our cloud machine"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec21"/>Running jug on our cloud machine</h3></div></div></div><p>We can<a id="id656" class="indexterm"/> now download the data and code for the book using this sequence of commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone \
https://github.com/luispedro/BuildingMachineLearningSystemsWithPython</strong></span>
<span class="strong"><strong>$ cd BuildingMachineLearningSystemsWithPython</strong></span>
<span class="strong"><strong>$ cd ch12</strong></span>
</pre></div><p>Finally, we run<a id="id657" class="indexterm"/> this following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ jug execute</strong></span>
</pre></div><p>This would work just fine, but we would have to wait a long time for the results. Our free tier machine (of type t2.micro) is not very fast and only has a single processor. So, we will <span class="emphasis"><em>upgrade our machine</em></span>!</p><p>We go back to the EC2 console, and right-click on the running instance to get the pop-up menu. We need to first stop the instance. This is the virtual machine equivalent to powering off. You can stop your machines at any time. At this point, you stop paying for them. Note that you are still using disk space, which also has a cost, billed separately. You can terminate the instance, which will also destroy the disk. This loses any information saved on the machine.</p><p>Once the machine is stopped, the <span class="strong"><strong>Change instance type</strong></span> option becomes available. Now, we can select a more powerful instance, for example, a <span class="strong"><strong>c1.xlarge</strong></span> instance with eight cores. The machine is still off, so you need to start it again (the virtual equivalent to booting up).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip23"/>Tip</h3><p>AWS offers several instance types at different price points. As this information is constantly being revised as more powerful options are introduced and prices change (generally, getting cheaper), we cannot give you many details in the book, but you can find the most up-to-date information on Amazon's website.</p></div></div><p>We need to wait for the instance to come back up. Once it has, look up its IP address in the same fashion as we did before. When you change instance types, your instance will get a new address assigned to it.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip24"/>Tip</h3><p>You can assign a fixed IP to an instance using Amazon.com's Elastic IPs functionality, which you will find on the left-hand side of the EC2 console. This is useful if you find yourself creating and modifying instances very often. There is a small cost associated with this feature.</p></div></div><p>With eight cores, you can run eight jug processes simultaneously, as illustrated in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ # the loop below runs 8 times</strong></span>
<span class="strong"><strong>$ for counter in $(seq 8); do</strong></span>
<span class="strong"><strong>&gt;     jug execute &amp;</strong></span>
<span class="strong"><strong>&gt; done</strong></span>
</pre></div><p>Use <code class="literal">jug status</code> to <a id="id658" class="indexterm"/>check <a id="id659" class="indexterm"/>whether these eight jobs are, in fact, running. After your jobs are finished (which should now happen pretty fast), you can stop the machine and downgrade it again to a <span class="strong"><strong>t2.micro</strong></span> instance to save money. The micro instance can be used for free (within certain limits), while the <span class="strong"><strong>c1.xlarge</strong></span> one we used costs 0.064 US dollars per hour (as of February 2015—check the AWS website for up-to-date information).</p></div></div><div class="section" title="Automating the generation of clusters with StarCluster"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec102"/>Automating the generation of clusters with StarCluster</h2></div></div></div><p>As we just learned, we<a id="id660" class="indexterm"/> can spawn machines using the web interface, but it quickly becomes tedious and error prone. Fortunately, Amazon has an API. This means that we can write scripts, which <a id="id661" class="indexterm"/>perform all the operations we discussed earlier, automatically. Even better, others have already developed tools that can be used to mechanize and automate many of the processes you want to perform with AWS.</p><p>A group at MIT developed exactly such a tool called StarCluster. It happens to be a Python package, so you can install it with Python tools as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo pip install starcluster</strong></span>
</pre></div><p>You can run this from an Amazon machine or from your local machine. Either option will work.</p><p>We will need to specify what our cluster will look like. We do so by editing a configuration file. We generate a template configuration file by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ starcluster help</strong></span>
</pre></div><p>Then pick the option of generating the configuration file in <code class="literal">~/.starcluster/config</code>. Once this is done, we will manually edit it.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip25"/>Tip</h3><p>
<span class="strong"><strong>Keys, keys, and more keys</strong></span>
</p><p>There are three completely different types of keys that are important when dealing with AWS. First, there is a standard username/password combination, which you use to log in to the website. Second, there is the SSH key system, which is a public/private key system implemented with files; with your public key file, you can log in to remote machines. Third, there is the AWS access key/secret key system, which is just a form of username/password that allows you to have multiple users on the same account (including adding different permissions to each one, but we will not cover these advanced features in this book).</p><p>To look up our access/secret keys, we go back to the AWS Console, click on our name on the top-right, and select <span class="strong"><strong>Security Credentials</strong></span>. Now at the bottom of the screen, there should be our access key, which may look something like <span class="strong"><strong>AAKIIT7HHF6IUSN3OCAA</strong></span>, which we will use as an example in this chapter.</p></div></div><p>Now, edit the configuration file. This is a standard <code class="literal">.ini</code> file: a text file where sections start by having<a id="id662" class="indexterm"/> their names <a id="id663" class="indexterm"/>in brackets and options are specified in the <code class="literal">name=value</code> format. The first section is the <code class="literal">aws info</code> section and you should copy and paste your keys here:</p><div class="informalexample"><pre class="programlisting">[aws info]
AWS_ACCESS_KEY_ID =  AAKIIT7HHF6IUSN3OCAA
AWS_SECRET_ACCESS_KEY = &lt;your secret key&gt;</pre></div><p>Now we come to the fun part, that is, defining a cluster. StarCluster allows you to define as many different clusters as you wish. The starting file has one called smallcluster. It's defined in the <code class="literal">cluster smallcluster</code> section. We will edit it to read as follows:</p><div class="informalexample"><pre class="programlisting">[cluster smallcluster]
KEYNAME = mykey
CLUSTER_SIZE = 16</pre></div><p>This changes the number of nodes to 16 instead of the default of two. We can additionally specify which type of instance each node will be and what the initial image is (remember, an image is used to initialized the virtual hard disk, which defines what operating system you will be running and what software is installed). StarCluster has a few predefined images, but you can also build your own.</p><p>We need to create a new SSH key with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ starcluster createkey mykey -o ~/.ssh/mykey.rsa</strong></span>
</pre></div><p>Now that we have configured a sixteen node cluster and set up the keys, let's try it out:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ starcluster start smallcluster</strong></span>
</pre></div><p>This may take a few minutes as it allocates seventeen new machines. Why seventeen when our cluster is only sixteen nodes? StarCluster always creates a master node. All of these nodes have the same filesystem, so anything we create on the master node will also be seen by the worker nodes. This also means that we can use jug on these clusters.</p><p>These clusters can be<a id="id664" class="indexterm"/> used as you wish, but they come pre-equipped with a job queue engine, which makes it ideal for batch processing. The process of using them is simple:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">You log in to the master node.</li><li class="listitem">You prepare your scripts on the master (or better yet, have them prepared before hand).</li><li class="listitem">You submit jobs to the queue. A job can be any Unix command. The scheduler will find free nodes and run your job.</li><li class="listitem">You wait for the jobs to finish.</li><li class="listitem">You read the results on the master node. You can also now kill all the slave nodes to save money. In any case, do not leave your system running when you do not need it anymore! Otherwise, this will cost you (in dollars-and-cents).</li></ol></div><p>Before logging<a id="id665" class="indexterm"/> in to the cluster, we will copy our data to it (remember we had earlier cloned the repository onto <code class="literal">BuildingMachineLearningSystemsWithPython</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ dir=BuildingMachineLearningSystemsWithPython</strong></span>
<span class="strong"><strong>$ starcluster put smallcluster $dir $dir</strong></span>
</pre></div><p>We used the <code class="literal">$dir</code> variable to make the command line fit in a single line. We can log in to the master node with a single command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ starcluster sshmaster smallcluster</strong></span>
</pre></div><p>We could also have looked up the address of the machine that was generated and used an <code class="literal">ssh</code> command as we did earlier, but using the preceding command, it does not matter what the address was, as StarCluster takes care of it behind the scenes for us.</p><p>As we said earlier, StarCluster provides a batch queuing system for its clusters; you write a script to perform your actions, put it on the queue, and it will run in any available node.</p><p>At this point, we need to install some packages again. Fortunately, StarCluster has already done half the work. If this was a real project, we would set up a script to perform all the initialization for us. StarCluster can do this. As this is a tutorial, we just run the installation step again:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install jug mahotas scikit-learn</strong></span>
</pre></div><p>We can use the same jugfile system as before, except that now, instead of running it directly on the master, we schedule it on the cluster.</p><p>First, write a very simple wrapper script as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/usr/bin/env bash</strong></span>
<span class="strong"><strong>jug execute jugfile.py</strong></span>
</pre></div><p>Call it <code class="literal">run-jugfile.sh</code> and use <code class="literal">chmod +x run-jugfile.sh</code> to give it executable permission. Now, we can schedule sixteen jobs on the cluster by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ for c in $(seq 16); do</strong></span>
<span class="strong"><strong>&gt;    qsub -cwd run-jugfile.sh</strong></span>
<span class="strong"><strong>&gt; done</strong></span>
</pre></div><p>This will create <a id="id666" class="indexterm"/>16 jobs, each of which will run the <code class="literal">run-jugfile.sh</code> script, which we will simply call jug. You can still use the master as you wish. In particular, you can, at any moment, run <code class="literal">jug status</code> and see the status of the computation. In fact, jug was developed in exactly such an environment, so it works very well in it.</p><p>Eventually, the computation will finish. At this point, we need to first save our results. Then, we can kill off all the nodes. We create a directory <code class="literal">~/results</code> and copy our results here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># mkdir ~/results</strong></span>
<span class="strong"><strong># cp results.image.txt ~/results</strong></span>
</pre></div><p>Now, log off the cluster back to our worker machine:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># exit</strong></span>
</pre></div><p>Now, we are <a id="id667" class="indexterm"/>back at our AWS machine (notice the <code class="literal">$</code> sign in the next code examples). First, we copy the results back to this computer using the <code class="literal">starcluster get</code> command (which is the mirror image of <code class="literal">put</code> we used before):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ starcluster get smallcluster results results</strong></span>
</pre></div><p>Finally, we should kill all the nodes to save money as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ starcluster stop smallcluster</strong></span>
<span class="strong"><strong>$ starcluster terminate smallcluster</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>Note that terminating will really destroy the filesystem and all your results. In our case, we have copied the final results to safety manually. Another possibility is to have the cluster write to a filesystem, which is not allocated and destroyed by StarCluster, but is available to you on a regular instance; in fact, the flexibility of these tools is immense. However, these advanced manipulations could not all fit in this chapter.</p><p>StarCluster has <a id="id668" class="indexterm"/>excellent documentation online at <a class="ulink" href="http://star.mit.edu/cluster/">http://star.mit.edu/cluster/</a>, which you<a id="id669" class="indexterm"/> should read for more information about all the possibilities of this tool. We have seen only a small fraction of the functionality and used only the default settings here.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec69"/>Summary</h1></div></div></div><p>We saw how to use jug, a little Python framework to manage computations in a way that takes advantage of multiple cores or multiple machines. Although this framework is generic, it was built specifically to address the data analysis needs of its author (who is also an author of this book). Therefore, it has several aspects that make it fit in with the rest of the Python machine learning environment.</p><p>You also learned about AWS, the Amazon Cloud. Using cloud computing is often a more effective use of resources than building in-house computing capacity. This is particularly true if your needs are not constant and are changing. StarCluster even allows for clusters that automatically grow as you launch more jobs and shrink as they terminate.</p><p>This is the end of the book. We have come a long way. You learned how to perform classification when we labeled data and clustering when we do not. You learned about dimensionality reduction and topic modeling to make sense of large datasets. Towards the end, we looked at some specific applications (such as music genre classification and computer vision). For implementations, we relied on Python. This language has an increasingly expanding ecosystem of numeric computing packages built on top of NumPy. Whenever possible, we relied on scikit-learn, but used other packages when necessary. Due to the fact that they all use the same basic data structure (the NumPy multidimensional array), it's possible to mix functionality from different packages seamlessly. All of the packages used in this book are open source and available for use in any project.</p><p>Naturally, we did not cover every machine learning topic. In the Appendix, we provide pointers to a selection of other resources that will help interested readers learn more about machine learning.</p></div></div>


  <div id="sbo-rt-content"><div class="appendix" title="Appendix A. Where to Learn More Machine Learning"><div class="titlepage"><div><div><h1 class="title"><a id="appA"/>Appendix A. Where to Learn More Machine Learning</h1></div></div></div><p>We are at the end of our book and now take a moment to look at what else is out there that could be useful for our readers.</p><p>There are many wonderful resources<a id="id670" class="indexterm"/> out there to learn more about machine learning—way too much to cover them all here. The following list can therefore represent only a small, and very biased, sampling of the resources the authors think are best at the time of writing.</p><div class="section" title="Online courses"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec70"/>Online courses</h1></div></div></div><p>Andrew Ng is a professor at Stanford who runs an online course in machine learning as a massive open <a id="id671" class="indexterm"/>online <a id="id672" class="indexterm"/>course at Coursera (<a class="ulink" href="http://www.coursera.org">http://www.coursera.org</a>). It is <a id="id673" class="indexterm"/>free of charge, but may represent a significant time investment.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Books"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec71"/>Books</h1></div></div></div><p>This book is <a id="id674" class="indexterm"/>focused on the practical side of machine learning. We did not present the thinking behind the algorithms or the theory that justifies them. If you are interested in that aspect of machine learning, then we recommend <span class="emphasis"><em>Pattern Recognition and Machine Learning</em></span> by Christopher Bishop. This is a classical introductory text in the field. It will teach you the nitty-gritty of most of the algorithms we used in this book.</p><p>If you want to move beyond the introduction and learn all the gory mathematical details, then <span class="emphasis"><em>Machine Learning: A Probabilistic Perspective</em></span> by Kevin P. Murphy is an excellent option (<a class="ulink" href="http://www.cs.ubc.ca/~murphyk/MLbook">www.cs.ubc.ca/~murphyk/MLbook</a>). It's very recent (published in 2012) and contains the cutting edge <a id="id675" class="indexterm"/>of ML research. This 1100 page book can also serve as a reference as very little of machine learning has been left out.</p><div class="section" title="Question and answer sites"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec103"/>Question and answer sites</h2></div></div></div><p>MetaOptimize (<a class="ulink" href="http://metaoptimize.com/qa">http://metaoptimize.com/qa</a>) is a <a id="id676" class="indexterm"/>machine <a id="id677" class="indexterm"/>learning question and answer website where many very knowledgeable researchers and practitioners interact.</p><p>Cross Validated (<a class="ulink" href="http://stats.stackexchange.com">http://stats.stackexchange.com</a>) is a general statistics question and <a id="id678" class="indexterm"/>answer site, which<a id="id679" class="indexterm"/> often features machine learning questions as well.</p><p>As<a id="id680" class="indexterm"/> mentioned in the beginning of the book, if you have questions specific to particular parts of the book, feel free to ask them at <a id="id681" class="indexterm"/>TwoToReal (<a class="ulink" href="http://www.twotoreal.com">http://www.twotoreal.com</a>). We try to be as quick as possible to jump in and help as best as we can.</p></div><div class="section" title="Blogs"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec104"/>Blogs</h2></div></div></div><p>Here is an<a id="id682" class="indexterm"/> obviously non-exhaustive list of blogs, which are<a id="id683" class="indexterm"/> interesting to someone working in machine learning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Machine Learning Theory: <a class="ulink" href="http://hunch.net">http://hunch.net</a><p>The average pace is approximately one post per month. Posts are more theoretical. They offer additional value in brain teasers.</p></li><li class="listitem" style="list-style-type: disc">Text &amp; Data Mining by practical means: <a class="ulink" href="http://textanddatamining.blogspot.de">http://textanddatamining.blogspot.de</a><p>Average pace is one post per month, very practical, always surprising approaches.</p></li><li class="listitem" style="list-style-type: disc">Edwin Chen's Blog: <a class="ulink" href="http://blog.echen.me">http://blog.echen.me</a><p>The average pace is one post per month, providing more applied topics.</p></li><li class="listitem" style="list-style-type: disc">Machined Learnings: <a class="ulink" href="http://www.machinedlearnings.com">http://www.machinedlearnings.com</a><p>The average pace is one post per month, providing more applied topics.</p></li><li class="listitem" style="list-style-type: disc">FlowingData: <a class="ulink" href="http://flowingdata.com">http://flowingdata.com</a><p>The average pace is one post per day, with the posts revolving more around statistics.</p></li><li class="listitem" style="list-style-type: disc">Simply Statistics: <a class="ulink" href="http://simplystatistics.org">http://simplystatistics.org</a><p>Several posts per month, focusing on statistics and big data.</p></li><li class="listitem" style="list-style-type: disc">Statistical Modeling, Causal Inference, and Social Science: <a class="ulink" href="http://andrewgelman.com">http://andrewgelman.com</a><p>One post per <a id="id684" class="indexterm"/>day with often funny reads when the author points out flaws in popular media, using statistics.</p></li></ul></div></div><div class="section" title="Data sources"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec105"/>Data sources</h2></div></div></div><p>If you want<a id="id685" class="indexterm"/> to play around with algorithms, you<a id="id686" class="indexterm"/> can obtain many datasets from the Machine Learning Repository at the University of California at Irvine (UCI). You can find it at <a class="ulink" href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>.</p></div><div class="section" title="Getting competitive"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec106"/>Getting competitive</h2></div></div></div><p>An excellent <a id="id687" class="indexterm"/>way to learn more about machine learning is by trying out a competition! Kaggle (<a class="ulink" href="http://www.kaggle.com">http://www.kaggle.com</a>) is a marketplace <a id="id688" class="indexterm"/>of ML competitions and was already mentioned in the introduction. On the website, you will find several different competitions with different structures and often cash prizes.</p><p>The supervised learning competitions almost always follow the following format: you (and every other competitor) are given access to labeled training data and testing data (without labels). Your task is to submit predictions for testing data. When the competition closes, whoever has the best accuracy wins. The prizes range from glory to cash.</p><p>Of course, winning something is nice, but you can gain a lot of useful experience just by participating. So, you have to stay tuned after the competition is over as participants start sharing their approaches in the forum. Most of the time, winning is not about developing a new algorithm, but cleverly preprocessing, normalizing, and combining existing methods.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="All that was left out"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec72"/>All that was left out</h1></div></div></div><p>We did not cover every machine learning package available for Python. Given the limited space, we chose to focus on scikit-learn. However, there are other options and we list a few of them here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">MDP<a id="id689" class="indexterm"/> toolkit (<a class="ulink" href="http://mdp-toolkit.sourceforge.net">http://mdp-toolkit.sourceforge.net</a>): Modular toolkit for data processing</li><li class="listitem" style="list-style-type: disc">PyBrain (<a class="ulink" href="http://pybrain.org">http://pybrain.org</a>): Python-based Reinforcement Learning, Artificial <a id="id690" class="indexterm"/>Intelligence, and Neural Network Library</li><li class="listitem" style="list-style-type: disc">Machine Learning Toolkit (Milk) (<a class="ulink" href="http://luispedro.org/software/milk">http://luispedro.org/software/milk</a>): This <a id="id691" class="indexterm"/>package was developed by one of the authors of this book and covers some algorithms and techniques that are not included in scikit-learn</li><li class="listitem" style="list-style-type: disc">Pattern (<a class="ulink" href="http://www.clips.ua.ac.be/pattern">http://www.clips.ua.ac.be/pattern</a>): A package that combines web<a id="id692" class="indexterm"/> mining, natural language processing, and machine learning, having wrapper APIs for Google, Twitter, and Wikipedia.</li></ul></div><p>A more general resource is <a class="ulink" href="http://mloss.org">http://mloss.org</a>, which is a repository of open source machine learning software. As is usually the case with repositories such as this one, the quality varies between excellent well maintained software and projects that were one-offs and then abandoned. It may be worth checking out whether your problem is very specific and none of the more general packages address it.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec73"/>Summary</h1></div></div></div><p>We are now truly at the end. We hope you enjoyed the book and feel well equipped to start your own machine learning adventure.</p><p>We also hope you learned the importance of carefully testing your methods. In particular, the importance of using correct cross-validation method and not report training test results, which are an over-inflated estimate of how good your method really is.</p></div></div>
</body></html>
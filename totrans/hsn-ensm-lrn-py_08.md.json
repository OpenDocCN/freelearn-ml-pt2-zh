["```py\n# --- SECTION 1 ---\n# Libraries and data loading\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\n\n# --- SECTION 2 ---\n# Print the original sample's statistics\ntarget = diabetes.target\n\nprint(np.mean(target))\nprint(np.std(target))\n```", "```py\n# --- SECTION 3 ---\n# Create the bootstrap samples and statistics\nbootstrap_stats = []\nfor _ in range(10000):\n    bootstrap_sample = np.random.choice(target, size=len(target))\n    mean = np.mean(bootstrap_sample)\n    std = np.std(bootstrap_sample)\n    bootstrap_stats.append((mean, std))\n\nbootstrap_stats = np.array(bootstrap_stats)\n```", "```py\n# --- SECTION 4 ---\n# plot the distributions\nplt.figure()\nplt.subplot(2,1,1)\nstd_err = np.std(bootstrap_stats[:,0])\nplt.title('Mean, Std. Error: %.2f'%std_err)\nplt.hist(bootstrap_stats[:,0], bins=20)\n\nplt.subplot(2,1,2)\nstd_err = np.std(bootstrap_stats[:,1])\nplt.title('Std. Dev, Std. Error: %.2f'%std_err)\nplt.hist(bootstrap_stats[:,1], bins=20)\nplt.show()\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom sklearn.datasets import load_digits\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport numpy as np\ndigits = load_digits()\n\ntrain_size = 1500\ntrain_x, train_y = digits.data[:train_size], digits.target[:train_size]\ntest_x, test_y = digits.data[train_size:], digits.target[train_size:]\n```", "```py\n# --- SECTION 2 ---\n# Create our bootstrap samples and train the classifiers\n\nensemble_size = 10\nbase_learners = []\n\nfor _ in range(ensemble_size):\n # We sample indices in order to access features and targets\n bootstrap_sample_indices = np.random.randint(0, train_size, size=train_size)\n bootstrap_x = train_x[bootstrap_sample_indices]\n bootstrap_y = train_y[bootstrap_sample_indices]\n dtree = DecisionTreeClassifier()\n dtree.fit(bootstrap_x, bootstrap_y)\n base_learners.append(dtree)\n```", "```py\n# --- SECTION 3 ---\n# Predict with the base learners and evaluate them\n\nbase_predictions = []\nbase_accuracy = []\nfor learner in base_learners:\n predictions = learner.predict(test_x)\n base_predictions.append(predictions)\n acc = metrics.accuracy_score(test_y, predictions)\n base_accuracy.append(acc)\n```", "```py\n# Combine the base learners' predictions \n\nensemble_predictions = []\n# Find the most voted class for each test instance\nfor i in range(len(test_y)):\n    counts = [0 for _ in range(10)]\n    for learner_predictions in base_predictions:\n        counts[learner_predictions[i]] = counts[learner_predictions[i]]+1\n    # Find the class with most votes \n    final = np.argmax(counts)\n    # Add the class to the final predictions \n    ensemble_predictions.append(final)\n\nensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)\n```", "```py\n# --- SECTION 5 ---\n# Print the accuracies\nprint('Base Learners:')\nprint('-'*30)\nfor index, acc in enumerate(sorted(base_accuracy)):\n print(f'Learner {index+1}: %.2f' % acc)\nprint('-'*30)\nprint('Bagging: %.2f' % ensemble_acc)\n```", "```py\n\nBase Learners:\n------------------------------\nLearner 1: 0.72\nLearner 2: 0.72\nLearner 3: 0.73\nLearner 4: 0.73\nLearner 5: 0.76\nLearner 6: 0.76\nLearner 7: 0.77\nLearner 8: 0.77\nLearner 9: 0.79\nLearner 10: 0.79\n------------------------------\nBagging: 0.88\n```", "```py\ndef create_learner(train_x, train_y):\n # We sample indices in order to access features and targets\n bootstrap_sample_indices = np.random.randint(0, train_size, size=train_size)\n bootstrap_x = train_x[bootstrap_sample_indices]\n bootstrap_y = train_y[bootstrap_sample_indices]\n dtree = DecisionTreeClassifier()\n dtree.fit(bootstrap_x, bootstrap_y)\n return dtree\n\ndef predict(learner, test_x):\n return learner.predict(test_x)\n```", "```py\n# Original Section 2\nwith ProcessPoolExecutor() as executor:\n futures = []\n for _ in range(ensemble_size):\n future = executor.submit(create_learner, train_x, train_y)\n futures.append(future)\n\nfor future in futures:\n base_learners.append(future.result())\n\n# Original Section 3\nbase_predictions = []\n base_accuracy = []\n with ProcessPoolExecutor() as executor:\n futures = []\n for learner in base_learners:\n future = executor.submit(predict, learner, test_x)\n futures.append(future)\n\nfor future in futures:\n predictions = future.result()\n base_predictions.append(predictions)\n acc = metrics.accuracy_score(test_y, predictions)\n base_accuracy.append(acc)\n```", "```py\n# --- SECTION 1 ---\n# Libraries and data loading\nfrom sklearn.datasets import load_digits\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn import metrics\n\ndigits = load_digits()\n\ntrain_size = 1500\ntrain_x, train_y = digits.data[:train_size], digits.target[:train_size]\ntest_x, test_y = digits.data[train_size:], digits.target[train_size:]\n```", "```py\n# --- SECTION 2 ---\n# Create the ensemble\nensemble_size = 10\nensemble = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n n_estimators=ensemble_size,\n oob_score=True)\n\n# --- SECTION 3 ---\n# Train the ensemble\nensemble.fit(train_x, train_y)\n\n# --- SECTION 4 ---\n# Evaluate the ensemble\nensemble_predictions = ensemble.predict(test_x)\n\nensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# --- SECTION 5 ---\n# Print the accuracy\nprint('Bagging: %.2f' % ensemble_acc)\n```", "```py\n# --- SECTION 1 ---\n # Libraries and data loading\n from sklearn.datasets import load_diabetes\n from sklearn.tree import DecisionTreeRegressor\n from sklearn.ensemble import BaggingRegressor\n from sklearn import metrics\n import numpy as np\n diabetes = load_diabetes()\n\nnp.random.seed(1234)\n\ntrain_x, train_y = diabetes.data[:400], diabetes.target[:400]\ntest_x, test_y = diabetes.data[400:], diabetes.target[400:]\n```", "```py\n# --- SECTION 2 ---\n# Create the ensemble and a single base learner for comparison\nestimator = DecisionTreeRegressor(max_depth=6)\nensemble = BaggingRegressor(base_estimator=estimator,\nn_estimators=10)\n\n# --- SECTION 3 ---\n# Train and evaluate both the ensemble and the base learner\nensemble.fit(train_x, train_y)\nensemble_predictions = ensemble.predict(test_x)\n\nestimator.fit(train_x, train_y)\nsingle_predictions = estimator.predict(test_x)\n\nensemble_r2 = metrics.r2_score(test_y, ensemble_predictions)\nensemble_mse = metrics.mean_squared_error(test_y, ensemble_predictions)\n\nsingle_r2 = metrics.r2_score(test_y, single_predictions)\nsingle_mse = metrics.mean_squared_error(test_y, single_predictions)\n\n# --- SECTION 4 ---\n# Print the metrics\nprint('Bagging r-squared: %.2f' % ensemble_r2)\nprint('Bagging MSE: %.2f' % ensemble_mse)\nprint('-'*30)\nprint('Decision Tree r-squared: %.2f' % single_r2)\nprint('Decision Tree MSE: %.2f' % single_mse)\n```", "```py\n Bagging r-squared: 0.52\n Bagging MSE: 2679.12\n ------------------------------\n Decision Tree r-squared: 0.15\n Decision Tree MSE: 4733.35\n```"]
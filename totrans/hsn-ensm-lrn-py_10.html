<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Random Forests</h1>
                </header>
            
            <article>
                
<p>Bagging is generally used to reduce variance of a model. It achieves it by creating an ensemble of base learners, each one trained on a unique bootstrap sample of the original train set. This forces diversity between the base learners. Random Forests expand on bagging by inducing randomness not only on each base learner's train samples, but in the features as well. Furthermore, their performance is similar to boosting techniques, although they do not require as much fine-tuning as boosting methods.</p>
<p>In this chapter, we will provide the basic background of random forests, as well as discuss the strengths and weaknesses of the method. Finally, we will present usage examples, using the scikit-learn implementation. The main topics covered in this chapter are as follows:</p>
<ul>
<li>How Random Forests build their base learners</li>
<li>How randomness can be utilized in order to build better random forest ensembles</li>
<li>The strengths and weaknesses of Random Forests</li>
<li>Utilizing scikit-learn's implementation for regression and classification</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You will require basic knowledge of machine learning techniques and algorithms. Furthermore, a knowledge of python conventions and syntax is required. Finally, familiarity with the NumPy library will greatly help the reader to understand some custom algorithm implementations.</p>
<p>The code files of this chapter can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07</a></p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/2LY5OJR">http://bit.ly/2LY5OJR</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding random forest trees</h1>
                </header>
            
            <article>
                
<p>In this section, we will go over the methodology of building a basic random forest tree. There are other methods that can be employed, but they all strive to achieve the same goal: diverse trees that serve as the ensemble's base learners.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building trees</h1>
                </header>
            
            <article>
                
<p>As mentioned in <span class="cdp-organizer-chapter-number"><a href="57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml">Chapter 1</a>,</span><span> </span><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>A Machine Learning Refresher</em>,</span></span> create a tree by selecting at each node a single feature and split point, such that the train set is best split. When an ensemble is created, we wish the base learners to be as uncorrelated (diverse) as possible.</p>
<p>Bagging is able to produce reasonably uncorrelated trees by diversifying each tree's train set through bootstrapping. But bagging only diversifies the trees by acting on one axis: each set's instances. There is still a second axis on which we can introduce diversity, the features. By selecting a subset of the available features during training, the generated base learners can be even more diverse. In random forests, for each tree and at each node, only a subset of the available features is considered when choosing the best feature/split point combination. The number of features that will be selected can be optimized by hand, but one-third of all features for regression problems and the square root of all features are considered to be a good starting point.</p>
<p>The algorithm's steps are as follows:</p>
<ol>
<li>Select the number of features <em>m</em> that will be considered at each node</li>
<li>For each base learner, do the following:
<ol>
<li>Create a bootstrap train sample</li>
<li>Select the node to split</li>
<li>Select <em>m</em> features randomly</li>
<li>Pick the best feature and split point from <em>m</em></li>
<li>Split the node into two nodes</li>
<li>Repeat from step 2-2 until a stopping criterion is met, such as maximum tree depth</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Illustrative example</h1>
                </header>
            
            <article>
                
<div>
<p>In order to better illustrate the process, let's consider the following dataset, indicating whether a second shoulder dislocation has occurred after the first (recurrence):</p>
</div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Age</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Operated</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sex</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Recurrence</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>15</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>m</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>45</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>f</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>30</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>m</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>18</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>m</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>52</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>f</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Shoulder dislocation recurrence dataset</span></span></div>
<p>In order to build a Random Forest tree, we must first decide the number of features that will be considered in each split. As we have three features, we will use the square root of 3, which is approximately 1.7. Usually, we use the floor of this number (we round it down to the closest integer), but as we want to illustrate the process, we will use two features in order to better demonstrate it. For the first tree, we generate a bootstrap sample. The second row is an instance that was chosen twice from the original dataset:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Age</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Operated</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Sex</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Recurrence</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>15</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>m</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>15</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>m</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>30</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>m</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>18</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>m</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>52</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>n</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>f</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>y</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>The bootstrap sample</span></div>
<p>Next, we create the root node. First, we randomly select two features to consider. We choose <strong>operated</strong> and <strong>sex</strong>. The best split is given for <strong>operated</strong>, as we get a leaf with 100% accuracy and one node with 50% accuracy. The resulting tree is depicted as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-614 image-border" src="assets/2585e8c5-1dc8-4d1d-842b-0ca00748fc81.png" style="width:21.58em;height:16.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">The tree after the first split</div>
<p>Next, we again select two features at random and the one that offers the best split. We now choose <strong>operated</strong> and <strong>age</strong>. As both misclassified instances were not operated, the best split is offered through the age feature.</p>
<p>Thus, the final tree is a tree with three leaves, where if someone is operated they have a recurrence, while if they are not operated and are over the age of 18 they do not:</p>
<div class="packt_infobox">
<p>Note that medical research indicates that young males have the highest chance for shoulder dislocation recurrence. The dataset here is a toy example that does not reflect real<span>ity.<br/></span></p>
</div>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-615 image-border" src="assets/dcf0023c-0cb0-499e-9972-8eb6714ef467.png" style="width:22.83em;height:21.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">The final decision tree</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extra trees</h1>
                </header>
            
            <article>
                
<p>Another method to create trees in a Random Forest ensemble is Extra Trees (extremely<span> </span>randomized trees). The main difference with the previous method is that the feature and split point combination does not have to be the optimal. Instead, a number of split points are randomly generated, one for each available feature. The best split point of those generated is selected. The algorithm constructs a tree as follows:</p>
<ol>
<li>Select the number of features<span> </span><em>m</em><span> </span>that will be considered at each node and the minimum number of samples<span> </span><em>n</em><span> </span>in order to split a node</li>
<li>For each base learner, do the following:
<ol>
<li>Create a bootstrap train sample</li>
<li>Select the node to split (the node must have at least<span> </span><em>n</em><span> </span>samples)</li>
<li>Select<span> </span><em>m</em><span> </span>features randomly</li>
<li>Randomly generate<span> </span><em>m</em><span> </span>split points, with values between the minimum and maximum value of each feature</li>
<li>Select the best of these split points</li>
<li>Split the node into two nodes and repeat from step 2-2 until there are no available nodes</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating forests</h1>
                </header>
            
            <article>
                
<p>By creating a number of trees using any valid randomization method, we have essentially created a forest, hence the algorithm's name. After generating the ensemble's trees, their predictions must be combined in order to have a functional ensemble. This is usually achieved through majority voting for classification problems and through averaging for regression problems. There are a number of hyperparameters associated with Random Forests, such as the number of features to consider at each node split, the number of trees in the forest, and the individual tree's size. As mentioned earlier, a good starting point for the number of features to consider is as follows:</p>
<ul>
<li>The square root of the number of total features for classification problems</li>
<li>One-third of the number of total features for regression problems</li>
</ul>
<p>The total number of trees can be fine-tuned by hand, as the ensemble's error converges to a limit when this number increases. Out-of-bag errors can be utilized to find an optimal value. Finally, the size of each tree can be a deciding factor in overfitting. Thus, if overfitting is observed, the tree size should be reduced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing forests</h1>
                </header>
            
            <article>
                
<p>Random Forests provide information about the underlying dataset that most of other methods cannot easily provide. A prominent example is the importance of each individual feature in the dataset. One method to estimate feature importance is to use the Gini index for each node of each tree and compare each feature's cumulative value. Another method uses the out-of-bag samples. First, the out-of-bag accuracy is recorded for all base learners. Then, a single feature is chosen and its values are shuffled in the out-of-bag samples. This results in out-of-bag sample sets with the same statistical properties as the original sets, but any predictive power that the chosen feature might have is removed (as there is now zero correlation between the selected feature's values and the target). The difference in accuracy between the original and the partially random dataset is used as measure for the selected feature's importance.</p>
<p>Concerning bias and variance, although random forests seem to cope well with both, they are certainly not immune. Bias can appear when the available features are great in number, but only few are correlated to the target. When using the recommended number of features to consider at each split (for example, the square root of the number of total features), the probability that a relevant feature will be selected can be small. The following graph shows the probability that at least one relevant feature will be selected, as a function of relevant and irrelevant features (when the square root of the number of total features is considered at each split):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-616 image-border" src="assets/89e05ebd-3f52-4666-9694-b76675e7d70b.png" style="width:32.58em;height:28.00em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Probability to select at least one relevant feature as a function of the number of relevant and irrelevant features</div>
<div><br/>
<div class="packt_infobox">
<p>The Gini index measures the frequency of incorrect classifications, assuming that a randomly sampled instance would be classified according to the label distribution dictated by a specific node.</p>
</div>
<p>Variance can also appear in Random Forests, although the method is sufficiently resistant to it. Variance usually appears when the individual trees are allowed to grow fully. We have previously mentioned that as the number of trees increases, the error approximates a certain limit. Although this claim still holds true, it is possible that the limit itself overfits the data. Restricting the tree size (by increasing the minimum number of samples per leaf or reducing the maximum depth) can potentially help in such circumstances.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strengths and weaknesses</h1>
                </header>
            
            <article>
                
<p>Random Forests are a very robust ensemble learning method, able to reduce both bias and variance, similar to boosting. Furthermore, the algorithm's nature allows it to be fully parallelized, both during training, as well as during prediction. This is a considerable advantage over boosting methods, especially when large datasets are concerned. Furthermore, they require less hyperparameter fine-tuning, compared to boosting techniques, especially XGBoost.</p>
<p>The main weaknesses of random forests are their sensitivity to class imbalances, as well as the problem we mentioned earlier, which involves a low ratio of relevant to irrelevant features in the train set. Furthermore, when the data contains low-level non-linear patterns (such as in raw, high-resolution image recognition), Random Forests usually are outperformed by deep neural networks. Finally, Random Forests can be computationally expensive when very large datasets are used combined with unrestricted tree depth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using scikit-learn</h1>
                </header>
            
            <article>
                
<p>scikit-learn implements both conventional Random Forest trees, as well as Extra Trees. In this section, we will provide basic regression and classification examples with both algorithms, using the scikit-learn implementations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forests for classification</h1>
                </header>
            
            <article>
                
<p>The Random Forests classification class is implemented in<span> </span><kbd>RandomForestClassifier</kbd>, under the<span> </span><kbd>sklearn.ensemble</kbd><span> </span>package. It has a number of parameters, such as the ensemble's size, the maximum tree depth, the number of samples required to make or split a node, and many more.</p>
<p>In this example, we will try to classify the hand-written digits dataset, using the Random Forest classification ensemble. As usual, we load the required classes and data and set the seed for our random number generator:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_digits<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn import metrics<br/>import numpy as np<br/><br/>digits = load_digits()<br/><br/>train_size = 1500<br/>train_x, train_y = digits.data[:train_size], digits.target[:train_size]<br/>test_x, test_y = digits.data[train_size:], digits.target[train_size:]<br/><br/>np.random.seed(123456)</pre>
<p class="mce-root">Following this, we create the ensemble, by setting the<span> </span><kbd>n_estimators</kbd> and <kbd>n_jobs</kbd><span> </span>parameters. These parameters dictate the number of trees that will be generated and the number of parallel jobs that will be run. We train the ensemble using the<span> </span><kbd>fit</kbd><span> </span>function and evaluate it on the test set by measuring its achieved accuracy:</p>
<pre class="mce-root"># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 500<br/>ensemble = RandomForestClassifier(n_estimators=ensemble_size, n_jobs=4)<br/><br/># --- SECTION 3 ---<br/># Train the ensemble<br/>ensemble.fit(train_x, train_y)<br/><br/># --- SECTION 4 ---<br/># Evaluate the ensemble<br/>ensemble_predictions = ensemble.predict(test_x)<br/><br/>ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 5 ---<br/># Print the accuracy<br/>print('Random Forest: %.2f' % ensemble_acc)</pre>
<p class="mce-root">The classifier is able to achieve an accuracy of 93%, which is even higher than the previously best-performing method, XGBoost (<span class="cdp-organizer-chapter-number"><a href="a1a92022-31ce-4c9b-9712-6b8282fac1af.xhtml">Chapter 6</a>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Boosting</span></span></em>). We can visualize the approximation of the error limit we mentioned earlier, by plotting validation curves (from <span class="cdp-organizer-chapter-number"><a href="d7921006-351e-4c21-ab54-f1dc834557dc.xhtml">Chapter 2</a>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Getting Started with Ensemble Learning</span></span></em>) for a number of ensemble sizes. We test for sizes of 10, 50, 100, 150, 200, 250, 300, 350, and 400 trees. The curves are depicted in the following graph. We can see that the ensemble approaches a 10-fold cross-validation error of 96%:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-617 image-border" src="assets/fcce0cc8-1a54-41c7-8ebc-200301ca4071.png" style="width:34.67em;height:25.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Validation curves for a number of ensemble sizes</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forests for regression</h1>
                </header>
            
            <article>
                
<p>Scikit-learn also implements random forests for regression purposes in the <kbd>RandomForestRegressor</kbd> class. It is also highly parameterizable, with hyper-parameters concerning both the ensemble as a whole, as well as the individual trees. Here, we will generate an ensemble in order to model the diabetes regression dataset. The code follows the standard procedure of loading libraries and data, creating the ensemble and calling the <kbd>fit</kbd> and predict methods, along with calculating the MSE and R-squared values:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from copy import deepcopy<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.ensemble import RandomForestRegressor<br/>from sklearn import metrics<br/><br/>import numpy as np<br/><br/>diabetes = load_diabetes()<br/><br/>train_size = 400<br/>train_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]<br/>test_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]<br/><br/>np.random.seed(123456)<br/><br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 100<br/>ensemble = RandomForestRegressor(n_estimators=ensemble_size, n_jobs=4)<br/><br/># --- SECTION 3 ---<br/># Evaluate the ensemble<br/>ensemble.fit(train_x, train_y)<br/>predictions = ensemble.predict(test_x)<br/><br/># --- SECTION 4 ---<br/># Print the metrics<br/>r2 = metrics.r2_score(test_y, predictions)<br/>mse = metrics.mean_squared_error(test_y, predictions)<br/><br/>print('Random Forest:')<br/>print('R-squared: %.2f' % r2)<br/>print('MSE: %.2f' % mse)</pre>
<p>The ensemble is able to produce an R-squared of 0.51 and an MSE of 2722.67 on the test set. As the R-squared and MSE on the train set are 0.92 and 468.13 respectively, it is safe to assume that the ensemble overfits. This is a case where the error limit overfits, and thus we need to regulate the individual trees in order to achieve better results. By reducing the minimum number of samples required to be at each leaf node (increased to 20, from the default value of 2) through <kbd>min_samples_leaf=20</kbd>, we are able to increase R-squared to 0.6 and reduce MSE to 2206.6. Furthermore, by increasing the ensemble size to 1000, R-squared is further increased to 0.61 and MSE is further decreased to 2158.73.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extra trees for classification</h1>
                </header>
            
            <article>
                
<p>Apart from conventional Random Forests, scikit-learn also implements Extra Trees. The classification implementation lies in the<span> </span><kbd>ExtraTreesClassifier</kbd>, in the<span> </span><kbd>sklearn.ensemble</kbd><span> </span>package. Here, we repeat the hand-written digit recognition example, using the Extra Trees classifier:</p>
<pre><br/># --- SECTION 1 ---<br/># Libraries and data loading<br/>from sklearn.datasets import load_digits<br/>from sklearn.ensemble import ExtraTreesClassifier<br/>from sklearn import metrics<br/>import numpy as np<br/><br/>digits = load_digits()<br/><br/>train_size = 1500<br/>train_x, train_y = digits.data[:train_size], digits.target[:train_size]<br/>test_x, test_y = digits.data[train_size:], digits.target[train_size:]<br/><br/>np.random.seed(123456)<br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 500<br/>ensemble = ExtraTreesClassifier(n_estimators=ensemble_size, n_jobs=4)<br/><br/># --- SECTION 3 ---<br/># Train the ensemble<br/>ensemble.fit(train_x, train_y)<br/><br/># --- SECTION 4 ---<br/># Evaluate the ensemble<br/>ensemble_predictions = ensemble.predict(test_x)<br/><br/>ensemble_acc = metrics.accuracy_score(test_y, ensemble_predictions)<br/><br/># --- SECTION 5 ---<br/># Print the accuracy<br/>print('Extra Tree Forest: %.2f' % ensemble_acc)</pre>
<p class="mce-root"/>
<p class="mce-root">As you may notice, the only difference with the previous example is the switch from<span> </span><kbd>RandomForestClassifier</kbd><span> </span>to<span> </span><kbd>ExtraTreesClassifier</kbd>. Nonetheless, the ensemble achieves an even higher test accuracy score of 94%. Once again, we create validation curves for a number of ensemble sizes, depicted as follows. The 10-fold cross validation error limit for this ensemble is approximately at 97%, which further confirms that it outperforms the conventional Random Forest approach:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-618 image-border" src="assets/fbe8dc8b-dd5a-49f5-b170-43944c0d5d51.png" style="width:37.92em;height:28.33em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">Extra Trees validation curves for a number of ensemble sizes</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extra trees regression</h1>
                </header>
            
            <article>
                
<p>Finally, we present the regression implementation of Extra Trees, implemented in<span> </span><kbd>ExtraTreesRegressor</kbd>. In the following code, we repeat the previously presented example of modeling the diabetes dataset, using the regression version of Extra Trees:</p>
<pre># --- SECTION 1 ---<br/># Libraries and data loading<br/>from copy import deepcopy<br/>from sklearn.datasets import load_diabetes<br/>from sklearn.ensemble import ExtraTreesRegressor<br/>from sklearn import metrics<br/><br/>import numpy as np<br/><br/>diabetes = load_diabetes()<br/><br/>train_size = 400<br/>train_x, train_y = diabetes.data[:train_size], diabetes.target[:train_size]<br/>test_x, test_y = diabetes.data[train_size:], diabetes.target[train_size:]<br/><br/>np.random.seed(123456)<br/><br/># --- SECTION 2 ---<br/># Create the ensemble<br/>ensemble_size = 100<br/>ensemble = ExtraTreesRegressor(n_estimators=ensemble_size, n_jobs=4)<br/><br/># --- SECTION 3 ---<br/># Evaluate the ensemble<br/>ensemble.fit(train_x, train_y)<br/>predictions = ensemble.predict(test_x)<br/><br/># --- SECTION 4 ---<br/># Print the metrics<br/>r2 = metrics.r2_score(test_y, predictions)<br/>mse = metrics.mean_squared_error(test_y, predictions)<br/><br/>print('Extra Trees:')<br/>print('R-squared: %.2f' % r2)<br/>print('MSE: %.2f' % mse)</pre>
<p class="mce-root">Similar to the classification examples, Extra Trees outperform conventional random forests by achieving a test R-squared of 0.55 (0.04 better than Random Forests) and an MSE of 2479.18 (a difference of 243.49). Still, the ensemble seems to overfit, as it perfectly predicts in-sample data. By setting<span> </span><kbd>min_samples_leaf=10</kbd><span> </span>and the ensemble size to 1000, we are able to produce an R-squared of 0.62 and an MSE of 2114.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed Random Forests, an ensemble method utilizing decision trees as its base learners. We presented two basic methods of constructing the trees: the conventional Random Forests approach, where a subset of features is considered at each split, as well as Extra Trees, where the split points are chosen almost randomly. We discussed the basic characteristics of the ensemble method. Furthermore, we presented regression and classification examples using the scikit-learn implementations of Random Forests and Extra Trees. The key points of this chapter that summarize its contents are provided below.</p>
<p><strong><span>Random Forests</span></strong> use bagging in order to create train sets for their base learners. <span>At each node, each tree considers only a subset of the available features and computes the optimal feature/split point combination. </span><span>The number of features to consider at each point is a hyper-parameter that must be tuned. Good starting points are as follows:</span></p>
<ul>
<li>The square root of the total number of parameters for classification problems</li>
<li>One-third of the total number of parameters for regression problems</li>
</ul>
<p><strong>Extra trees</strong> and random forests use the <strong>whole dataset</strong> for each base learner. <span>In</span> <span>extra trees</span> <span>and random forests, instead of calculating the optimal feature/split-point combination of the feature subset at each node, a random split point is generated for each feature in the subset and the best is selected. </span><span>Random forests can give information regarding the importance of each feature. </span><span>Although relatively resistant to overfitting, random forests are not immune to it. </span><span>Random forests can exhibit high bias when the ratio of relevant to irrelevant features is low. </span><span>Random forests can exhibit high variance, although the ensemble size does not contribute to the</span> problem<span>. In the next chapter, we will present ensemble learning techniques that can be applied to unsupervised learning methods (clustering).</span></p>


            </article>

            
        </section>
    </body></html>
["```py\ngzip *ubyte.gz -d \n```", "```py\nimport os\nimport struct\nimport numpy as np\ndef load_mnist(path, kind='train'):\n    \"\"\"Load MNIST data from 'path'\"\"\"\n    labels_path = os.path.join(path,\n                               '%s-labels-idx1-ubyte' % kind)\n    images_path = os.path.join(path,\n                               '%s-images-idx3-ubyte' % kind)\n\n    with open(labels_path, 'rb') as lbpath:\n        magic, n = struct.unpack('>II',\n                                 lbpath.read(8))\n        labels = np.fromfile(lbpath,\n                             dtype=np.uint8)\n\n    with open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack(\">IIII\",\n                                               imgpath.read(16))\n        images = np.fromfile(imgpath,\n                             dtype=np.uint8).reshape(\n                             len(labels), 784)\n        images = ((images / 255.) - .5) * 2\n\n    return images, labels \n```", "```py\nmagic, n = struct.unpack('>II', lbpath.read(8))\nlabels = np.fromfile(lbpath, dtype=np.uint8) \n```", "```py\n[offset] [type]           [value]           [description]\n0000      32 bit integer  0x00000801(2049)  magic number (MSB first)\n0004      32 bit integer  60000             number of items\n0008      unsigned byte   ??                label\n0009      unsigned byte   ??                label\n........\nxxxx      unsigned byte   ??                label \n```", "```py\nimages = ((images / 255.) - .5) * 2 \n```", "```py\nto the same directory in which this code was executed.)\n```", "```py\n>>> X_train, y_train = load_mnist('', kind='train')\n>>> print('Rows: %d, columns: %d'\n...       % (X_train.shape[0], X_train.shape[1]))\nRows: 60000, columns: 784\n>>> X_test, y_test = load_mnist('', kind='t10k')\n>>> print('Rows: %d, columns: %d'\n...       % (X_test.shape[0], X_test.shape[1]))\nRows: 10000, columns: 784 \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> fig, ax = plt.subplots(nrows=2, ncols=5,\n...                        sharex=True, sharey=True)\n>>> ax = ax.flatten()\n>>> for i in range(10):\n...     img = X_train[y_train == i][0].reshape(28, 28)\n...     ax[i].imshow(img, cmap='Greys')\n>>> ax[0].set_xticks([])\n>>> ax[0].set_yticks([])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> fig, ax = plt.subplots(nrows=5,\n...                        ncols=5,\n...                        sharex=True,\n...                        sharey=True)\n>>> ax = ax.flatten()\n>>> for i in range(25):\n...     img = X_train[y_train == 7][i].reshape(28, 28)\n...     ax[i].imshow(img, cmap='Greys')\n>>> ax[0].set_xticks([])\n>>> ax[0].set_yticks([])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nmnist_scaled.npz:\n```", "```py\n>>> import numpy as np\n>>> np.savez_compressed('mnist_scaled.npz',\n...                     X_train=X_train,\n...                     y_train=y_train,\n...                     X_test=X_test,\n...                     y_test=y_test) \n```", "```py\n>>> mnist = np.load('mnist_scaled.npz') \n```", "```py\n>>> mnist.files\n['X_train', 'y_train', 'X_test', 'y_test'] \n```", "```py\n>>> X_train = mnist['X_train'] \n```", "```py\n>>> X_train, y_train, X_test, y_test = [mnist[f] for\n...                                     f in mnist.files] \n```", "```py\n>>> from sklearn.datasets import fetch_openml\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = fetch_openml('mnist_784', version=1,\n...                     return_X_y=True)\n>>> y = y.astype(int)\n>>> X = ((X / 255.) - .5) * 2\n>>> X_train, X_test, y_train, y_test =\\\n...  train_test_split(\n...     X, y, test_size=10000,\n...     random_state=123, stratify=y) \n```", "```py\nfrom neuralnet import NeuralNetMLP \n```", "```py\nimport numpy as np\nimport sys\nclass NeuralNetMLP(object):\n    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n\n    Parameters\n    ------------\n    n_hidden : int (default: 30)\n        Number of hidden units.\n    l2 : float (default: 0.)\n        Lambda value for L2-regularization.\n        No regularization if l2=0\\. (default)\n    epochs : int (default: 100)\n        Number of passes over the training set.\n    eta : float (default: 0.001)\n        Learning rate.\n    shuffle : bool (default: True)\n        Shuffles training data every epoch\n        if True to prevent circles.\n    minibatch_size : int (default: 1)\n        Number of training examples per minibatch.\n    seed : int (default: None)\n        Random seed for initializing weights and shuffling.\n\n    Attributes\n    -----------\n    eval_ : dict\n        Dictionary collecting the cost, training accuracy,\n        and validation accuracy for each epoch during training.\n\n    \"\"\"\n    def __init__(self, n_hidden=30,\n                 l2=0., epochs=100, eta=0.001,\n                 shuffle=True, minibatch_size=1, seed=None):\n\n        self.random = np.random.RandomState(seed)\n        self.n_hidden = n_hidden\n        self.l2 = l2\n        self.epochs = epochs\n        self.eta = eta\n        self.shuffle = shuffle\n        self.minibatch_size = minibatch_size\n\n    def _onehot(self, y, n_classes):\n        \"\"\"Encode labels into one-hot representation\n\n        Parameters\n        ------------\n        y : array, shape = [n_examples]\n            Target values.\n\n        Returns\n        -----------\n        onehot : array, shape = (n_examples, n_labels)\n\n        \"\"\"\n        onehot = np.zeros((n_classes, y.shape[0]))\n        for idx, val in enumerate(y.astype(int)):\n            onehot[val, idx] = 1.\n        return onehot.T\n\n    def _sigmoid(self, z):\n        \"\"\"Compute logistic function (sigmoid)\"\"\"\n        return 1\\. / (1\\. + np.exp(-np.clip(z, -250, 250)))\n\n    def _forward(self, X):\n        \"\"\"Compute forward propagation step\"\"\"\n\n        # step 1: net input of hidden layer\n        # [n_examples, n_features] dot [n_features, n_hidden]\n        # -> [n_examples, n_hidden]\n        z_h = np.dot(X, self.w_h) + self.b_h\n\n        # step 2: activation of hidden layer\n        a_h = self._sigmoid(z_h)\n\n        # step 3: net input of output layer\n        # [n_examples, n_hidden] dot [n_hidden, n_classlabels]\n        # -> [n_examples, n_classlabels]\n\n        z_out = np.dot(a_h, self.w_out) + self.b_out\n        # step 4: activation output layer\n        a_out = self._sigmoid(z_out)\n\n        return z_h, a_h, z_out, a_out\n\n    def _compute_cost(self, y_enc, output):\n        \"\"\"Compute cost function.\n\n        Parameters\n        ----------\n        y_enc : array, shape = (n_examples, n_labels)\n            one-hot encoded class labels.\n        output : array, shape = [n_examples, n_output_units]\n            Activation of the output layer (forward propagation)\n\n        Returns\n        ---------\n        cost : float\n            Regularized cost\n\n        \"\"\"\n        L2_term = (self.l2 *\n                   (np.sum(self.w_h ** 2.) +\n                    np.sum(self.w_out ** 2.)))\n\n        term1 = -y_enc * (np.log(output))\n        term2 = (1\\. - y_enc) * np.log(1\\. - output)\n        cost = np.sum(term1 - term2) + L2_term\n        return cost\n\n    def predict(self, X):\n        \"\"\"Predict class labels\n\n        Parameters\n        -----------\n        X : array, shape = [n_examples, n_features]\n            Input layer with original features.\n\n        Returns:\n        ----------\n        y_pred : array, shape = [n_examples]\n            Predicted class labels.\n\n        \"\"\"\n        z_h, a_h, z_out, a_out = self._forward(X)\n        y_pred = np.argmax(z_out, axis=1)\n        return y_pred\n\n    def fit(self, X_train, y_train, X_valid, y_valid):\n        \"\"\" Learn weights from training data.\n\n        Parameters\n        -----------\n        X_train : array, shape = [n_examples, n_features]\n            Input layer with original features.\n        y_train : array, shape = [n_examples]\n            Target class labels.\n        X_valid : array, shape = [n_examples, n_features]\n            Sample features for validation during training\n        y_valid : array, shape = [n_examples]\n            Sample labels for validation during training\n\n        Returns:\n        ----------\n        self\n\n        \"\"\"\n        n_output = np.unique(y_train).shape[0] # no. of class\n                                               #labels\n        n_features = X_train.shape[1]\n\n        ########################\n        # Weight initialization\n        ########################\n\n        # weights for input -> hidden\n        self.b_h = np.zeros(self.n_hidden)\n        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n                                      size=(n_features,\n                                            self.n_hidden))\n\n        # weights for hidden -> output\n        self.b_out = np.zeros(n_output)\n        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n                                        size=(self.n_hidden,\n                                              n_output))\n\n        epoch_strlen = len(str(self.epochs)) # for progr. format.\n        self.eval_ = {'cost': [], 'train_acc': [], 'valid_acc': \\\n                      []}\n\n        y_train_enc = self._onehot(y_train, n_output)\n\n        # iterate over training epochs\n        for i in range(self.epochs):\n\n            # iterate over minibatches\n            indices = np.arange(X_train.shape[0])\n\n            if self.shuffle:\n                self.random.shuffle(indices)\n\n            for start_idx in range(0, indices.shape[0] -\\\n                                   self.minibatch_size +\\\n                                   1, self.minibatch_size):\n                batch_idx = indices[start_idx:start_idx +\\\n                                    self.minibatch_size]\n\n                # forward propagation\n                z_h, a_h, z_out, a_out = \\\n                    self._forward(X_train[batch_idx])\n\n                ##################\n                # Backpropagation\n                ##################\n\n                # [n_examples, n_classlabels]\n                delta_out = a_out - y_train_enc[batch_idx]\n\n                # [n_examples, n_hidden]\n                sigmoid_derivative_h = a_h * (1\\. - a_h)\n\n                # [n_examples, n_classlabels] dot [n_classlabels,\n                #                                 n_hidden]\n                # -> [n_examples, n_hidden]\n                delta_h = (np.dot(delta_out, self.w_out.T) *\n                           sigmoid_derivative_h)\n\n                # [n_features, n_examples] dot [n_examples,\n                #                               n_hidden]\n                # -> [n_features, n_hidden]\n                grad_w_h = np.dot(X_train[batch_idx].T, delta_h)\n                grad_b_h = np.sum(delta_h, axis=0)\n\n                # [n_hidden, n_examples] dot [n_examples,\n                #                            n_classlabels]\n                # -> [n_hidden, n_classlabels]\n                grad_w_out = np.dot(a_h.T, delta_out)\n                grad_b_out = np.sum(delta_out, axis=0)\n\n                # Regularization and weight updates\n                delta_w_h = (grad_w_h + self.l2*self.w_h)\n                delta_b_h = grad_b_h # bias is not regularized\n                self.w_h -= self.eta * delta_w_h\n                self.b_h -= self.eta * delta_b_h\n\n                delta_w_out = (grad_w_out + self.l2*self.w_out)\n                delta_b_out = grad_b_out # bias is not regularized\n                self.w_out -= self.eta * delta_w_out\n                self.b_out -= self.eta * delta_b_out\n\n            #############\n            # Evaluation\n            #############\n\n            # Evaluation after each epoch during training\n            z_h, a_h, z_out, a_out = self._forward(X_train)\n\n            cost = self._compute_cost(y_enc=y_train_enc,\n                                      output=a_out)\n\n            y_train_pred = self.predict(X_train)\n            y_valid_pred = self.predict(X_valid)\n\n            train_acc = ((np.sum(y_train ==\n                          y_train_pred)).astype(np.float) /\n                         X_train.shape[0])\n            valid_acc = ((np.sum(y_valid ==\n                          y_valid_pred)).astype(np.float) /\n                         X_valid.shape[0])\n\n            sys.stderr.write('\\r%0*d/%d | Cost: %.2f '\n                             '| Train/Valid Acc.: %.2f%%/%.2f%% '\n                              %\n                             (epoch_strlen, i+1, self.epochs,\n                              cost,\n                              train_acc*100, valid_acc*100))\n            sys.stderr.flush()\n\n            self.eval_['cost'].append(cost)\n            self.eval_['train_acc'].append(train_acc)\n            self.eval_['valid_acc'].append(valid_acc)\n\n        return self \n```", "```py\n>>> nn = NeuralNetMLP(n_hidden=100,\n...                   l2=0.01,\n...                   epochs=200,\n...                   eta=0.0005,\n...                   minibatch_size=100,\n...                   shuffle=True,\n...                   seed=1) \n```", "```py\n>>> nn.fit(X_train=X_train[:55000],\n...        y_train=y_train[:55000],\n...        X_valid=X_train[55000:],\n...        y_valid=y_train[55000:])\n200/200 | Cost: 5065.78 | Train/Valid Acc.: 99.28%/97.98% \n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(range(nn.epochs), nn.eval_['cost'])\n>>> plt.ylabel('Cost')\n>>> plt.xlabel('Epochs')\n>>> plt.show() \n```", "```py\n>>> plt.plot(range(nn.epochs), nn.eval_['train_acc'],\n...          label='training')\n>>> plt.plot(range(nn.epochs), nn.eval_['valid_acc'],\n...          label='validation', linestyle='--')\n>>> plt.ylabel('Accuracy')\n>>> plt.xlabel('Epochs')\n>>> plt.legend(loc='lower right')\n>>> plt.show() \n```", "```py\n>>> y_test_pred = nn.predict(X_test)\n>>> acc = (np.sum(y_test == y_test_pred)\n...        .astype(np.float) / X_test.shape[0])\n>>> print('Test accuracy: %.2f%%' % (acc * 100))\nTest accuracy: 97.54% \n```", "```py\n>>> miscl_img = X_test[y_test != y_test_pred][:25]\n>>> correct_lab = y_test[y_test != y_test_pred][:25]\n>>> miscl_lab = y_test_pred[y_test != y_test_pred][:25]\n>>> fig, ax = plt.subplots(nrows=5,\n...                        ncols=5,\n...                        sharex=True,\n...                        sharey=True,)\n>>> ax = ax.flatten()\n>>> for i in range(25):\n...     img = miscl_img[i].reshape(28, 28)\n...     ax[i].imshow(img,\n...                  cmap='Greys',\n...                  interpolation='nearest')\n...     ax[i].set_title('%d) t: %d p: %d'\n...     % (i+1, correct_lab[i], miscl_lab[i]))\n>>> ax[0].set_xticks([])\n>>> ax[0].set_yticks([])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nself.w_h -= self.eta * delta_w_h\nself.b_h -= self.eta * delta_b_h\nself.w_out -= self.eta * delta_w_out\nself.b_out -= self.eta * delta_b_out \n```"]
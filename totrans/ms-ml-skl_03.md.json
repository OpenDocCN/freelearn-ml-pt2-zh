["```py\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> onehot_encoder = DictVectorizer()\n>>> instances = [\n>>>     {'city': 'New York'},\n>>>     {'city': 'San Francisco'},\n>>>     {'city': 'Chapel Hill'}>>> ]\n>>> print onehot_encoder.fit_transform(instances).toarray()\n[[ 0\\.  1\\.  0.] [ 0\\.  0\\.  1.][ 1\\.  0\\.  0.]]\n```", "```py\ncorpus = [\n    'UNC played Duke in basketball',\n    'Duke lost the basketball game'\n]\n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n>>>     'UNC played Duke in basketball',\n>>>     'Duke lost the basketball game'\n>>> ]\n>>> vectorizer = CountVectorizer()\n>>> print vectorizer.fit_transform(corpus).todense()\n>>> print vectorizer.vocabulary_\n[[1 1 0 1 0 1 0 1]\n [1 1 1 0 1 0 1 0]]\n{u'duke': 1, u'basketball': 0, u'lost': 4, u'played': 5, u'game': 2, u'unc': 7, u'in': 3, u'the': 6}\n```", "```py\ncorpus = [\n    'UNC played Duke in basketball',\n    'Duke lost the basketball game',\n    'I ate a sandwich'\n]\n```", "```py\n{u'duke': 2, u'basketball': 1, u'lost': 5, u'played': 6, u'in': 4, u'game': 3, u'sandwich': 7, u'unc': 9, u'ate': 0, u'the': 8} \n```", "```py\nUNC played Duke in basketball = [[0 1 1 0 1 0 1 0 0 1]]\nDuke lost the basketball game = [[0 1 1 1 0 1 0 0 1 0]]\nI ate a sandwich = [[1 0 0 0 0 0 0 1 0 0]]\n```", "```py\n>>> from sklearn.metrics.pairwise import euclidean_distances\n>>> counts = [\n>>>     [0, 1, 1, 0, 0, 1, 0, 1],\n>>>     [0, 1, 1, 1, 1, 0, 0, 0],\n>>>     [1, 0, 0, 0, 0, 0, 1, 0]\n>>> ]\n>>> print 'Distance between 1st and 2nd documents:', euclidean_distances(counts[0], counts[1])\n>>> print 'Distance between 1st and 3rd documents:', euclidean_distances(counts[0], counts[2])\n>>> print 'Distance between 2nd and 3rd documents:', euclidean_distances(counts[1], counts[2])\nDistance between 1st and 2nd documents: [[ 2.]]\nDistance between 1st and 3rd documents: [[ 2.44948974]]\nDistance between 2nd and 3rd documents: [[ 2.44948974]]\n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n>>>     'UNC played Duke in basketball',\n>>>     'Duke lost the basketball game',\n>>>     'I ate a sandwich'\n>>> ]\n>>> vectorizer = CountVectorizer(stop_words='english')\n>>> print vectorizer.fit_transform(corpus).todense()\n>>> print vectorizer.vocabulary_\n[[0 1 1 0 0 1 0 1]\n [0 1 1 1 1 0 0 0]\n [1 0 0 0 0 0 1 0]]\n{u'duke': 2, u'basketball': 1, u'lost': 4, u'played': 5, u'game': 3, u'sandwich': 6, u'unc': 7, u'ate': 0}\n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n>>>     'He ate the sandwiches',\n>>>     'Every sandwich was eaten by him'\n>>> ]\n>>> vectorizer = CountVectorizer(binary=True, stop_words='english')\n>>> print vectorizer.fit_transform(corpus).todense()\n>>> print vectorizer.vocabulary_\n[[1 0 0 1]\n [0 1 1 0]]\n{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}\n```", "```py\ncorpus = [\n    'I am gathering ingredients for the sandwich.',\n    'There were many wizards at the gathering.'\n]\n```", "```py\n>>> import nltk\n>>> nltk.download()\n```", "```py\n>>> from nltk.stem.wordnet import WordNetLemmatizer\n>>> lemmatizer = WordNetLemmatizer()\n>>> print lemmatizer.lemmatize('gathering', 'v')\n>>> print lemmatizer.lemmatize('gathering', 'n')\ngather\ngathering\n```", "```py\n>>> from nltk.stem import PorterStemmer\n>>> stemmer = PorterStemmer()\n>>> print stemmer.stem('gathering')\ngather\n```", "```py\n>>> from nltk import word_tokenize\n>>> from nltk.stem import PorterStemmer\n>>> from nltk.stem.wordnet import WordNetLemmatizer\n>>> from nltk import pos_tag\n>>> wordnet_tags = ['n', 'v']\n>>> corpus = [\n>>>     'He ate the sandwiches',\n>>>     'Every sandwich was eaten by him'\n>>> ]\n>>> stemmer = PorterStemmer()\n>>> print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus]\n>>> def lemmatize(token, tag):\n>>>     if tag[0].lower() in ['n', 'v']:\n>>>         return lemmatizer.lemmatize(token, tag[0].lower())\n>>>     return token\n>>> lemmatizer = WordNetLemmatizer()\n>>> tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n>>> print 'Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus]\nStemmed: [['He', 'ate', 'the', 'sandwich'], ['Everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\nLemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n```", "```py\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich']\n>>> vectorizer = CountVectorizer(stop_words='english')\n>>> print vectorizer.fit_transform(corpus).todense()\n[[2 1 3 1 1]]\n{u'sandwich': 2, u'wizard': 4, u'dog': 1, u'transfigured': 3, u'ate': 0}\n```", "```py\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> corpus = [\n>>>     'The dog ate a sandwich and I ate a sandwich',\n>>>     'The wizard transfigured a sandwich'\n>>> ]\n>>> vectorizer = TfidfVectorizer(stop_words='english')\n>>> print vectorizer.fit_transform(corpus).todense()\n[[ 0.75458397  0.37729199  0.53689271  0\\.          0\\.        ]\n [ 0\\.          0\\.          0.44943642  0.6316672   0.6316672 ]]\n```", "```py\n>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> corpus = ['the', 'ate', 'bacon', 'cat']\n>>> vectorizer = HashingVectorizer(n_features=6)\n>>> print vectorizer.transform(corpus).todense()\n[[-1\\.  0\\.  0\\.  0\\.  0\\.  0.]\n [ 0\\.  0\\.  0\\.  1\\.  0\\.  0.]\n [ 0\\.  0\\.  0\\.  0\\. -1\\.  0.]\n [ 0\\.  1\\.  0\\.  0\\.  0\\.  0.]]\n```", "```py\n>>> from sklearn import datasets\n>>> digits = datasets.load_digits()\n>>> print 'Digit:', digits.target[0]\n>>> print digits.images[0]\n>>> print 'Feature vector:\\n', digits.images[0].reshape(-1, 64)\nDigit: 0\n[[  0\\.   0\\.   5\\.  13\\.   9\\.   1\\.   0\\.   0.]\n [  0\\.   0\\.  13\\.  15\\.  10\\.  15\\.   5\\.   0.]\n [  0\\.   3\\.  15\\.   2\\.   0\\.  11\\.   8\\.   0.]\n [  0\\.   4\\.  12\\.   0\\.   0\\.   8\\.   8\\.   0.]\n [  0\\.   5\\.   8\\.   0\\.   0\\.   9\\.   8\\.   0.]\n [  0\\.   4\\.  11\\.   0\\.   1\\.  12\\.   7\\.   0.]\n [  0\\.   2\\.  14\\.   5\\.  10\\.  12\\.   0\\.   0.]\n [  0\\.   0\\.   6\\.  13\\.  10\\.   0\\.   0\\.   0.]]\nFeature vector:\n[[  0\\.   0\\.   5\\.  13\\.   9\\.   1\\.   0\\.   0\\.   0\\.   0\\.  13\\.  15\\.  10\\.  15.\n    5\\.   0\\.   0\\.   3\\.  15\\.   2\\.   0\\.  11\\.   8\\.   0\\.   0\\.   4\\.  12\\.   0.\n    0\\.   8\\.   8\\.   0\\.   0\\.   5\\.   8\\.   0\\.   0\\.   9\\.   8\\.   0\\.   0\\.   4.\n   11\\.   0\\.   1\\.  12\\.   7\\.   0\\.   0\\.   2\\.  14\\.   5\\.  10\\.  12\\.   0\\.   0.\n    0\\.   0\\.   6\\.  13\\.  10\\.   0\\.   0\\.   0.]]\n```", "```py\n>>> import numpy as nps\n>>> from skimage.feature import corner_harris, corner_peaks\n>>> from skimage.color import rgb2gray\n>>> import matplotlib.pyplot as plt\n>>> import skimage.io as io\n>>> from skimage.exposure import equalize_hist\n\n>>> def show_corners(corners, image):\n>>>     fig = plt.figure()\n>>>     plt.gray()\n>>>     plt.imshow(image)\n>>>     y_corner, x_corner = zip(*corners)\n>>>     plt.plot(x_corner, y_corner, 'or')\n>>>     plt.xlim(0, image.shape[1])\n>>>     plt.ylim(image.shape[0], 0)\n>>>     fig.set_size_inches(np.array(fig.get_size_inches()) * 1.5)\n>>>     plt.show()\n\n>>> mandrill = io.imread('/home/gavin/PycharmProjects/mastering-machine-learning/ch4/img/mandrill.png')\n>>> mandrill = equalize_hist(rgb2gray(mandrill))\n>>> corners = corner_peaks(corner_harris(mandrill), min_distance=2)\n>>> show_corners(corners, mandrill)\n```", "```py\n>>> import mahotas as mh\n>>> from mahotas.features import surf\n\n>>> image = mh.imread('zipper.jpg', as_grey=True)\n>>> print 'The first SURF descriptor:\\n', surf.surf(image)[0]\n>>> print 'Extracted %s SURF descriptors' % len(surf.surf(image))\nThe first SURF descriptor:\n[  6.73839947e+02   2.24033945e+03   3.18074483e+00   2.76324459e+03\n  -1.00000000e+00   1.61191475e+00   4.44035121e-05   3.28041690e-04\n   2.44845817e-04   3.86297608e-04  -1.16723672e-03  -8.81290243e-04\n   1.65414959e-03   1.28393061e-03  -7.45077384e-04   7.77655540e-04\n   1.16078772e-03   1.81434398e-03   1.81736394e-04  -3.13096961e-04\n    3.06559785e-04   3.43443699e-04   2.66200498e-04  -5.79522387e-04\n   1.17893036e-03   1.99547411e-03  -2.25938217e-01  -1.85563853e-01\n   2.27973631e-01   1.91510135e-01  -2.49315698e-01   1.95451021e-01\n   2.59719480e-01   1.98613061e-01  -7.82458546e-04   1.40287015e-03\n   2.86712113e-03   3.15971628e-03   4.98444730e-04  -6.93986983e-04\n   1.87531652e-03   2.19041521e-03   1.80681053e-01  -2.70528820e-01\n   2.32414943e-01   2.72932870e-01   2.65725332e-01   3.28050743e-01\n   2.98609869e-01   3.41623138e-01   1.58078002e-03  -4.67968721e-04\n   2.35704122e-03   2.26279888e-03   6.43115065e-06   1.22501486e-04\n   1.20064616e-04   1.76564805e-04   2.14148537e-03   8.36243899e-05\n   2.93382280e-03   3.10877776e-03   4.53469215e-03  -3.15254535e-04\n   6.92437341e-03   3.56880279e-03  -1.95228401e-04   3.73674995e-05\n   7.02700555e-04   5.45156362e-04]\nExtracted 994 SURF descriptors\n```", "```py\n>>> from sklearn import preprocessing\n>>> import numpy as np\n>>> X = np.array([\n>>>     [0., 0., 5., 13., 9., 1.],\n>>>     [0., 0., 13., 15., 10., 15.],\n>>>     [0., 3., 15., 2., 0., 11.]\n>>> ])\n>>> print preprocessing.scale(X)\n[[ 0\\.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n [ 0\\.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n [ 0\\.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n```"]
<html><head></head><body><div class="chapter" title="Chapter&#xA0;12.&#xA0;Specialized Machine Learning Topics"><div class="titlepage"><div><div><h1 class="title"><a id="ch12"/>Chapter 12. Specialized Machine Learning Topics</h1></div></div></div><p>Congratulations on reaching this point in your machine learning journey! If you have not already started work on your own projects, you will do so soon. And in doing so, you may find that the task of turning data into action is more difficult than it first appeared.</p><p>As you gathered data, you may have realized that the information was trapped in a proprietary format or spread across pages on the Web. Making matters worse, after spending hours reformatting the data, maybe your computer slowed to a crawl after running out of memory. Perhaps R even crashed or froze your machine. Hopefully, you were undeterred, as these issues can be remedied with a bit more effort.</p><p>This chapter covers techniques that may not apply to every project, but will prove useful for working around such specialized issues. You might find the information particularly useful if you tend to work with data that is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Stored in unstructured or proprietary formats such as web pages, web APIs, or spreadsheets</li><li class="listitem" style="list-style-type: disc">From a specialized domain such as bioinformatics or social network analysis</li><li class="listitem" style="list-style-type: disc">Too large to fit in memory or analyses take a very long time to complete</li></ul></div><p>You're not alone if you suffer from any of these problems. Although there is no panacea—these issues are the bane of the data scientist as well as the reason data skills are in high demand—through the dedicated efforts of the R community, a number of R packages provide a head start toward solving the problem.</p><p>This chapter provides a cookbook of such solutions. Even if you are an experienced R veteran, you may discover a package that simplifies your workflow. Or, perhaps one day, you will author a package that makes work easier for everybody else!</p><div class="section" title="Working with proprietary files and databases"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec50"/>Working with proprietary files and databases</h1></div></div></div><p>Unlike the <a id="id967" class="indexterm"/>examples in this book, real-world data is rarely packaged in a <a id="id968" class="indexterm"/>simple CSV form that can be downloaded from a website. Instead, significant effort is needed to prepare data for analysis. Data must be collected, merged, sorted, filtered, or reformatted to meet the requirements of the learning algorithm. This <a id="id969" class="indexterm"/>process is informally known as <span class="strong"><strong>data munging</strong></span> or <span class="strong"><strong>data wrangling</strong></span>.</p><p>Data preparation <a id="id970" class="indexterm"/>has become even more important, as the size of typical datasets has grown from megabytes to gigabytes, and data is gathered from unrelated and messy sources, many of which are stored in massive databases. Several packages and resources for retrieving and working with proprietary data formats and databases are listed in the following sections.</p><div class="section" title="Reading from and writing to Microsoft Excel, SAS, SPSS, and Stata files"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec119"/>Reading from and writing to Microsoft Excel, SAS, SPSS, and Stata files</h2></div></div></div><p>A frustrating<a id="id971" class="indexterm"/> aspect of data analysis is <a id="id972" class="indexterm"/>the large amount of work<a id="id973" class="indexterm"/> required to pull and combine data from various <a id="id974" class="indexterm"/>proprietary formats. Vast troves of data exist<a id="id975" class="indexterm"/> in files and databases that simply need to be <a id="id976" class="indexterm"/>unlocked for use in R. Thankfully, packages <a id="id977" class="indexterm"/>exist for exactly this purpose.</p><p>What used to<a id="id978" class="indexterm"/> be a tedious and time-consuming process, requiring knowledge of specific tricks and tools across multiple R packages, has been made trivial by a relatively new R package called <code class="literal">rio</code> (an acronym for R input and output). This package, by Chung-hong Chan, Geoffrey CH Chan, Thomas J. Leeper, and Christopher Gandrud, is described as a "Swiss-army knife for data". It is capable of importing and exporting a large variety of file formats, including but not limited to: tab-separated (<code class="literal">.tsv</code>), comma-separated (<code class="literal">.csv</code>), JSON (<code class="literal">.json</code>), Stata (<code class="literal">.dta</code>), SPSS (<code class="literal">.sav</code> and <code class="literal">.por</code>), Microsoft Excel (<code class="literal">.xls</code> and <code class="literal">.xlsx</code>), Weka (<code class="literal">.arff</code>), and SAS (<code class="literal">.sas7bdat</code> and <code class="literal">.xpt</code>).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note43"/>Note</h3><p>For the <a id="id979" class="indexterm"/>complete list of file types <code class="literal">rio</code> can import and export, as well as more detailed usage examples, see <a class="ulink" href="http://cran.r-project.org/web/packages/rio/vignettes/rio.html">http://cran.r-project.org/web/packages/rio/vignettes/rio.html</a>.</p></div></div><p>The <code class="literal">rio</code> package<a id="id980" class="indexterm"/> consists of three functions for working with proprietary data formats: <code class="literal">import()</code>, <code class="literal">export()</code>, and <code class="literal">convert()</code>. Each does exactly what you'd expect, given their name. Consistent with the package's philosophy of keeping things simple, each function uses the filename extension to guess the type of file to import, export, or convert.</p><p>For<a id="id981" class="indexterm"/> example, to import the credit <a id="id982" class="indexterm"/>data from previous chapters, which <a id="id983" class="indexterm"/>is stored in CSV format, simply<a id="id984" class="indexterm"/> type:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(rio)</strong></span>
<span class="strong"><strong>&gt; credit &lt;- import("credit.csv")</strong></span>
</pre></div><p>This <a id="id985" class="indexterm"/>creates the <code class="literal">credit</code> data frame as expected; as<a id="id986" class="indexterm"/> a bonus, not only did we not have to <a id="id987" class="indexterm"/>specify the CSV file type, <code class="literal">rio</code> automatically<a id="id988" class="indexterm"/> set <code class="literal">stringsAsFactors = FALSE</code> as well as other reasonable defaults.</p><p>To export the <code class="literal">credit</code> data frame to Microsoft Excel (<code class="literal">.xlsx</code>) format, use the <code class="literal">export()</code> function while specifying the desired filename, as follows. For other formats, simply change the file extension to the desired output type:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; export(credit, "credit.xlsx")</strong></span>
</pre></div><p>It is also possible to convert the CSV file to another format directly, without an import step, using the <code class="literal">convert()</code> function. For example, this converts the <code class="literal">credit.csv</code> file to Stata (<code class="literal">.dta</code>) format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; convert("credit.csv", "credit.dta")</strong></span>
</pre></div><p>Though the <code class="literal">rio</code> package covers many common proprietary data formats, it does not do everything. The next section covers other ways to get data into R via database queries.</p></div><div class="section" title="Querying data in SQL databases"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec120"/>Querying data in SQL databases</h2></div></div></div><p>Large<a id="id989" class="indexterm"/> datasets <a id="id990" class="indexterm"/>are often stored in <span class="strong"><strong>Database Management Systems</strong></span> (<span class="strong"><strong>DBMSs</strong></span>) such<a id="id991" class="indexterm"/> as Oracle, MySQL, PostgreSQL, Microsoft SQL, or SQLite. These systems allow the datasets to be accessed<a id="id992" class="indexterm"/> using a <span class="strong"><strong>Structured Query Language</strong></span> (<span class="strong"><strong>SQL</strong></span>), a programming language designed to pull data from databases. If your DBMS is <a id="id993" class="indexterm"/>configured to allow <span class="strong"><strong>Open Database Connectivity</strong></span> (<span class="strong"><strong>ODBC</strong></span>), the <code class="literal">RODBC</code> package by Brian Ripley can be used to import this data directly into an R data frame.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip139"/>Tip</h3><p>If you have trouble using ODBC to connect to your database, you may try one of the DMBS-specific R packages. These include <code class="literal">ROracle</code>, <code class="literal">RMySQL</code>, <code class="literal">RPostgresSQL</code>, and <code class="literal">RSQLite</code>. Though they will function largely similar to the instructions here, refer to the package documentation on CRAN for instructions specific to each package.</p></div></div><p>ODBC is a <a id="id994" class="indexterm"/>standard protocol for connecting to <a id="id995" class="indexterm"/>databases regardless of operating system or DBMS. If you were previously connected to an ODBC database, you most likely would have referred to it via its <span class="strong"><strong>Data Source Name</strong></span> (<span class="strong"><strong>DSN</strong></span>). You will need the DSN, plus a <a id="id996" class="indexterm"/>username and password (if your database requires it) to use <code class="literal">RODBC</code>.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip140"/>Tip</h3><p>The instructions to configure an ODBC connection are highly specific to the combination of the OS and DBMS. If you are having trouble setting up an ODBC connection, check with your database administrator. Another way to obtain help is via the <code class="literal">RODBC</code> package <code class="literal">vignette</code>, which can be accessed in R with the <code class="literal">vignette("RODBC")</code> command after the <code class="literal">RODBC</code> package has been installed.</p></div></div><p>To open a connection called <code class="literal">my_db</code> for the database with the <code class="literal">my_dsn</code> DSN, use the <code class="literal">odbcConnect()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(RODBC)</strong></span>
<span class="strong"><strong>&gt; my_db &lt;- odbcConnect("my_dsn")</strong></span>
</pre></div><p>Alternatively, if your ODBC connection requires a username and password, they should be specified while calling the <code class="literal">odbcConnect()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; my_db &lt;- odbcConnect("my_dsn",</strong></span>
<span class="strong"><strong>    uid = "my_username",</strong></span>
<span class="strong"><strong>    pwd = "my_password")</strong></span>
</pre></div><p>With an open database connection, we can use the <code class="literal">sqlQuery()</code> function to create an R data frame from the database rows pulled by an SQL query. This function, like the many functions that create data frames, allows us to specify <code class="literal">stringsAsFactors = FALSE</code> to prevent R from automatically converting character data into factors.</p><p>The <code class="literal">sqlQuery()</code> function uses typical SQL queries, as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; my_query &lt;- "select * from my_table where my_value = 1"</strong></span>
<span class="strong"><strong>&gt; results_df &lt;- sqlQuery(channel = my_db, query = sql_query,</strong></span>
<span class="strong"><strong>    stringsAsFactors = FALSE)</strong></span>
</pre></div><p>The<a id="id997" class="indexterm"/> resulting <code class="literal">results_df</code> object is a data frame <a id="id998" class="indexterm"/>containing all of the rows selected using the SQL query stored in <code class="literal">sql_query</code>.</p><p>Once you are done using the database, the connection can be closed using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; odbcClose(my_db)</strong></span>
</pre></div><p>Although R will automatically close ODBC connections at the end of an R session, it is a better practice to do so explicitly.</p></div></div></div>
<div class="section" title="Working with online data and services"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec51"/>Working with online data and services</h1></div></div></div><p>With the growing <a id="id999" class="indexterm"/>amount of data available from web-based sources, it is <a id="id1000" class="indexterm"/>increasingly important for machine learning projects to be able to access and interact with online services. R is able to read data from online sources natively, with some caveats. Firstly, by default, R cannot access secure websites (those using the <code class="literal">https://</code> rather than the <code class="literal">http://</code> protocol). Secondly, it is important to note that most web pages do not provide data in a form that R can understand. The<a id="id1001" class="indexterm"/> data would need to be <span class="strong"><strong>parsed</strong></span>, or broken apart and rebuilt into a structured form, before it can be useful. We'll discuss the workarounds shortly.</p><p>However, if neither of these caveats applies (that is, if data are already online on a nonsecure website and in a tabular form, like CSV, that R can understand natively), then R's <code class="literal">read.csv()</code> and <code class="literal">read.table()</code> functions will be able to access data from the Web just as if it were on your local machine. Simply supply the full URL for the dataset as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mydata &lt;- read.csv("http://www.mysite.com/mydata.csv")</strong></span>
</pre></div><p>R also provides functionality to download other files from the Web, even if R cannot use them directly. For a text file, try the <code class="literal">readLines()</code> function as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mytext &lt;- readLines("http://www.mysite.com/myfile.txt")</strong></span>
</pre></div><p>For other types <a id="id1002" class="indexterm"/>of files, the <code class="literal">download.file()</code> function can be<a id="id1003" class="indexterm"/> used. To download a file to R's current working directory, simply supply the URL and destination filename as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; download.file("http://www.mysite.com/myfile.zip", "myfile.zip")</strong></span>
</pre></div><p>Beyond this base functionality, there are numerous packages that extend R's capabilities to work with online data. The most basic of them will be covered in the sections that follow. As the Web is massive and ever-changing, these sections are far from a comprehensive set of all the ways R can connect to online data. There are literally hundreds of packages for everything from niche projects to massive ones.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note44"/>Note</h3><p>For the most complete and up-to-date list of packages, refer to the regularly updated <a id="id1004" class="indexterm"/>CRAN Web Technologies and Services task view at <a class="ulink" href="http://cran.r-project.org/web/views/WebTechnologies.html">http://cran.r-project.org/web/views/WebTechnologies.html</a>.</p></div></div><div class="section" title="Downloading the complete text of web pages"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec121"/>Downloading the complete text of web pages</h2></div></div></div><p>The <code class="literal">RCurl</code> package by Duncan Temple Lang supplies a more robust way<a id="id1005" class="indexterm"/> of accessing web pages <a id="id1006" class="indexterm"/>by providing an R interface to the <span class="strong"><strong>curl</strong></span> (client for URLs) utility, a command-line tool to transfer data over networks. The curl program <a id="id1007" class="indexterm"/>acts much like a programmable web browser; given a set of commands, it can access and download the content of nearly anything available on the Web. Unlike R, it can access secure websites as well as post data to online forms. It is an incredibly powerful utility.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note45"/>Note</h3><p>Precisely because it is so powerful, a complete curl tutorial is outside the scope of this <a id="id1008" class="indexterm"/>chapter. Instead, refer to the online <code class="literal">RCurl</code> documentation at <a class="ulink" href="http://www.omegahat.org/RCurl/">http://www.omegahat.org/RCurl/</a>.</p></div></div><p>After installing the <code class="literal">RCurl</code> package, downloading a page is as simple as typing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(RCurl) </strong></span>
<span class="strong"><strong>&gt; packt_page &lt;- ("https://www.packtpub.com/")</strong></span>
</pre></div><p>This will save the full text of the Packt Publishing homepage (including all the web markup) into the R character object named <code class="literal">packt_page</code>. As shown in the following lines, this is not very useful:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(packt_page, nchar.max=200)</strong></span>
<span class="strong"><strong> chr "&lt;!DOCTYPE html&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;\n\t&lt;head&gt;\n\t\t&lt;title&gt;Packt Publishing | Technology Books, eBooks &amp; Videos&lt;/title&gt;\n\t\t&lt;script&gt;\n\t\t\tdata"| __truncated__</strong></span>
</pre></div><p>The reason<a id="id1009" class="indexterm"/> that the first 200 characters <a id="id1010" class="indexterm"/>of the page look like nonsense is because the websites are written using <span class="strong"><strong>Hypertext Markup Language</strong></span> (<span class="strong"><strong>HTML</strong></span>), which <a id="id1011" class="indexterm"/>combines the page text with special tags that tell web browsers how to display the text. The <code class="literal">&lt;title&gt;</code> and <code class="literal">&lt;/title&gt;</code> tags here surround the page title, telling the browser that this is the Packt Publishing homepage. Similar tags are used to denote other portions of the page.</p><p>Though curl is the cross-platform standard to access online content, if you work with web data frequently in R, the <code class="literal">httr</code> package by Hadley Wickham builds upon the foundation of <code class="literal">RCurl</code> to make it more convenient and R-like. We can see some of the differences immediately by attempting to download the Packt Publishing homepage using the <code class="literal">httr</code> package's <code class="literal">GET()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(httr)</strong></span>
<span class="strong"><strong>&gt; packt_page &lt;- GET("https://www.packtpub.com")</strong></span>
<span class="strong"><strong>&gt; str(packt_page, max.level = 1)</strong></span>
<span class="strong"><strong>List of 9</strong></span>
<span class="strong"><strong> $ url        : chr "https://www.packtpub.com/"</strong></span>
<span class="strong"><strong> $ status_code: int 200</strong></span>
<span class="strong"><strong> $ headers    : List of 11</strong></span>
<span class="strong"><strong> $ all_headers: List of 1</strong></span>
<span class="strong"><strong> $ cookies    : list()</strong></span>
<span class="strong"><strong> $ content    : raw [1:58560] 3c 21 44 4f ...</strong></span>
<span class="strong"><strong> $ date       : POSIXct[1:1], format: "2015-05-24 20:46:40"</strong></span>
<span class="strong"><strong> $ times      : Named num [1:6] 0 0.000071 0.000079 ...</strong></span>
<span class="strong"><strong> $ request    : List of 5</strong></span>
</pre></div><p>Where the <code class="literal">getURL()</code> function in <code class="literal">RCurl</code> downloaded only the HTML, the <code class="literal">GET()</code> function returns a list with site properties in addition to the HTML. To access the page content itself, we need to use the <code class="literal">content()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(content(packt_page, type="text"), nchar.max=200)</strong></span>
<span class="strong"><strong> chr "&lt;!DOCTYPE html&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;\n\t&lt;head&gt;\n\t\t&lt;title&gt;Packt Publishing | Technology Books, eBooks &amp; Videos&lt;/title&gt;\n\t\t&lt;script&gt;\n\t\t\tdata"| __truncated__</strong></span>
</pre></div><p>In order to<a id="id1012" class="indexterm"/> use this data in an R program, it is <a id="id1013" class="indexterm"/>necessary to process the page to transform it into a structured format like a list or data frame. Functions to do so are discussed in the sections that follow.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note46"/>Note</h3><p>For detailed <code class="literal">httr</code> documentation <a id="id1014" class="indexterm"/>and tutorials, visit the project GitHub page at <a class="ulink" href="https://github.com/hadley/httr">https://github.com/hadley/httr</a>. The quickstart guide is particularly helpful to learn the base functionality.</p></div></div></div><div class="section" title="Scraping data from web pages"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec122"/>Scraping data from web pages</h2></div></div></div><p>Because there is<a id="id1015" class="indexterm"/> a consistent structure of the HTML tags<a id="id1016" class="indexterm"/> of many web pages, it is possible to write programs that look for desired sections of the page and extract them in order to compile them into a dataset. This process practice of harvesting data from websites and transforming it into a structured form is known<a id="id1017" class="indexterm"/> as <span class="strong"><strong>web scraping</strong></span>.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip141"/>Tip</h3><p>Though it is frequently used, scraping should be considered a last resort to get data from the Web. This is because any changes to the underlying HTML structure may break your code, requiring efforts to be fixed. Even worse, it may introduce unnoticed errors into your data. Additionally, many websites' terms of use agreements explicitly forbid automated data extraction, not to mention the fact that your program's traffic may overload their servers. Always check the site's terms before you begin your project; you may even find that the site offers its data freely via a developer agreement.</p></div></div><p>The <code class="literal">rvest</code> package (a pun on the term "harvest") by Hadley Wickham makes web scraping <a id="id1018" class="indexterm"/>a largely effortless process, assuming the data you want can be found in a consistent place within HTML.</p><p>Let's start with a simple example using the Packt Publishing homepage. We begin by downloading the page as before, using the <code class="literal">html()</code> function in the <code class="literal">rvest</code> package. Note that this function, when supplied with a URL, simply calls the <code class="literal">GET()</code> function in Hadley Wickham's <code class="literal">httr</code> package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(rvest)</strong></span>
<span class="strong"><strong>&gt; packt_page &lt;- html("https://www.packtpub.com")</strong></span>
</pre></div><p>Suppose we'd like to scrape the page title. Looking at the previous HTML code, we know that there is only one title per page wrapped within <code class="literal">&lt;title&gt;</code> and <code class="literal">&lt;/title&gt;</code> tags. To pull the title, we<a id="id1019" class="indexterm"/> supply the tag name to the <code class="literal">html_node()</code> function, as<a id="id1020" class="indexterm"/> follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; html_node(packt_page, "title")</strong></span>
<span class="strong"><strong>&lt;title&gt;Packt Publishing | Technology Books, eBooks &amp;amp; Videos&lt;/title&gt;</strong></span>
</pre></div><p>This keeps the HTML formatting in place, including the <code class="literal">&lt;title&gt;</code> tags and the <code class="literal">&amp;amp;</code> code, which is the HTML designation for the ampersand symbol. To translate this into plain text, we simply run it through the <code class="literal">html_text()</code> function, as shown:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; html_node(packt_page, "title") %&gt;% html_text()</strong></span>
<span class="strong"><strong>[1] "Packt Publishing | Technology Books, eBooks &amp; Videos"</strong></span>
</pre></div><p>Notice the use of the <code class="literal">%&gt;%</code> operator. This is known as a pipe, because it essentially "pipes" data from one function to another. The use of pipes allows the creation of powerful chains of functions to process HTML data.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note47"/>Note</h3><p>The pipe operator is a part of the <code class="literal">magrittr</code> package by Stefan Milton Bache and Hadley Wickham, installed <a id="id1021" class="indexterm"/>by default with the <code class="literal">rvest</code> package. The name is a play on René Magritte's famous painting of a pipe (you may recall seeing it in <a class="link" href="ch01.html" title="Chapter 1. Introducing Machine Learning">Chapter 1</a>, <span class="emphasis"><em>Introducing Machine Learning</em></span>). For more<a id="id1022" class="indexterm"/> information on the project, visit its GitHub page at <a class="ulink" href="https://github.com/smbache/magrittr">https://github.com/smbache/magrittr</a>.</p></div></div><p>Let's try a slightly more interesting example. Suppose we'd like to scrape a list of all the packages on the CRAN machine learning task view. We begin as in the same way we did it earlier, by downloading the HTML page using the <code class="literal">html()</code> function. Since we don't know how the page is structured, we'll also peek into HTML by typing <code class="literal">cran_ml</code>, the name of the R object we created:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(rvest)</strong></span>
<span class="strong"><strong>&gt; cran_ml &lt;- html("http://cran.r-project.org/web/views/MachineLearning.html")</strong></span>
<span class="strong"><strong>&gt; cran_ml</strong></span>
</pre></div><p>Looking over the output, we find that one section appears to have the data we're interested in. Note that only a subset of the output is shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  &lt;h3&gt;CRAN packages:&lt;/h3&gt;</strong></span>
<span class="strong"><strong>  &lt;ul&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/ahaz/index.html"&gt;ahaz&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/arules/index.html"&gt;arules&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/bigrf/index.html"&gt;bigrf&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/bigRR/index.html"&gt;bigRR&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/bmrm/index.html"&gt;bmrm&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/Boruta/index.html"&gt;Boruta&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/bst/index.html"&gt;bst&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/C50/index.html"&gt;C50&lt;/a&gt;&lt;/li&gt;</strong></span>
<span class="strong"><strong>    &lt;li&gt;&lt;a href="../packages/caret/index.html"&gt;caret&lt;/a&gt;&lt;/li&gt;</strong></span>
</pre></div><p>The <code class="literal">&lt;h3&gt;</code> tags <a id="id1023" class="indexterm"/>imply a header of size 3, while the <code class="literal">&lt;ul&gt;</code> and <code class="literal">&lt;li&gt;</code> tags refer to the creation of an unordered list and list items, respectively. The data elements we want are surrounded by <code class="literal">&lt;a&gt;</code> tags, which are hyperlink anchor tags that link to the CRAN page for each package.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip142"/>Tip</h3><p>Because the CRAN page is actively maintained and may be changed at any time, do not be surprised if your results differ from those shown here.</p></div></div><p>With this knowledge in hand, we can scrape the links much like we did previously. The one exception is that, because we expect to find more than one result, we need to use the <code class="literal">html_nodes()</code> function to return a vector of results rather than <code class="literal">html_node()</code>, which returns only a single item:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ml_packages &lt;- html_nodes(cran_ml, "a")</strong></span>
</pre></div><p>Let's peek at the result using the <code class="literal">head()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head(ml_packages, n = 7)</strong></span>
<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>&lt;a href="../packages/nnet/index.html"&gt;nnet&lt;/a&gt; </strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>&lt;a href="../packages/RSNNS/index.html"&gt;RSNNS&lt;/a&gt; </strong></span>

<span class="strong"><strong>[[3]]</strong></span>
<span class="strong"><strong>&lt;a href="../packages/rpart/index.html"&gt;rpart&lt;/a&gt; </strong></span>

<span class="strong"><strong>[[4]]</strong></span>
<span class="strong"><strong>&lt;a href="../packages/tree/index.html"&gt;tree&lt;/a&gt; </strong></span>

<span class="strong"><strong>[[5]]</strong></span>
<span class="strong"><strong>&lt;a href="../packages/rpart/index.html"&gt;rpart&lt;/a&gt; </strong></span>

<span class="strong"><strong>[[6]]</strong></span>
<span class="strong"><strong>&lt;a href="http://www.cs.waikato.ac.nz/~ml/weka/"&gt;Weka&lt;/a&gt; </strong></span>

<span class="strong"><strong>[[7]]</strong></span>
<span class="strong"><strong>&lt;a href="../packages/RWeka/index.html"&gt;RWeka&lt;/a&gt;</strong></span>
</pre></div><p>As we can <a id="id1024" class="indexterm"/>see on line 6, it looks like the links to some <a id="id1025" class="indexterm"/>other projects slipped in. This is because some packages are hyperlinked to additional websites; in this case, the <code class="literal">RWeka</code> package is linked to both CRAN and its homepage. To exclude these results, you might chain this output to another function that could look for the <code class="literal">/packages</code> string in the hyperlink.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip143"/>Tip</h3><p>In general, web scraping is always a process of iterate-and-refine as you identify more specific criteria to exclude or include specific cases. The most difficult cases may even require a human eye to achieve 100 percent accuracy.</p></div></div><p>These are simple examples that merely scratch the surface of what is possible with the <code class="literal">rvest</code> package. Using the pipe functionality, it is possible to look for tags nested within tags or specific classes of HTML tags. For these types of complex examples, refer to the package documentation.</p><div class="section" title="Parsing XML documents"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec67"/>Parsing XML documents</h3></div></div></div><p>XML is a<a id="id1026" class="indexterm"/> plaintext, human-readable, structured <a id="id1027" class="indexterm"/>markup language upon which many document formats have been based. It employs a tagging structure in some ways similar to HTML, but is far stricter about formatting. For this reason, it is a popular online format to store structured datasets.</p><p>The <code class="literal">XML</code> package <a id="id1028" class="indexterm"/>by Duncan Temple Lang provides a suite of R functionality based on the popular C-based <code class="literal">libxml2</code> parser to read and write XML documents. It is the grandfather of XML parsing packages in R and is still widely used.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note48"/>Note</h3><p>Information on the <code class="literal">XML</code> package, including simple examples to get you started quickly, can be <a id="id1029" class="indexterm"/>found on the project's website at <a class="ulink" href="http://www.omegahat.org/RSXML/">http://www.omegahat.org/RSXML/</a>.</p></div></div><p>Recently, the <code class="literal">xml2</code> package by Hadley Wickham has surfaced as an easier and more R-like interface to the <code class="literal">libxml2</code> library. The <code class="literal">rvest</code> package, which was covered earlier in this chapter, utilizes <code class="literal">xml2</code> behind the scenes to parse HTML. Moreover, <code class="literal">rvest</code> can be used to parse XML as well.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note49"/>Note</h3><p>The <code class="literal">xml2</code> GitHub<a id="id1030" class="indexterm"/> page is found at <a class="ulink" href="https://github.com/hadley/xml2">https://github.com/hadley/xml2</a>.</p></div></div><p>Because <a id="id1031" class="indexterm"/>parsing XML is so closely related to parsing<a id="id1032" class="indexterm"/> HTML, the exact syntax is not covered here. Please refer to these packages' documentation for examples.</p></div><div class="section" title="Parsing JSON from web APIs"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec68"/>Parsing JSON from web APIs</h3></div></div></div><p>Online <a id="id1033" class="indexterm"/>applications communicate with one another <a id="id1034" class="indexterm"/>using web-accessible functions known as <span class="strong"><strong>Application Programming Interfaces</strong></span> (<span class="strong"><strong>APIs</strong></span>). These interfaces act much like a typical <a id="id1035" class="indexterm"/>website; they receive a request from a client via a particular URL and return a response. The difference is that a normal website returns HTML meant for display in a web browser, while an API typically returns data in a structured form meant for processing by a machine.</p><p>Though it is not <a id="id1036" class="indexterm"/>uncommon to find XML-based APIs, perhaps the most common API data structure today is <span class="strong"><strong>JavaScript Object Notation</strong></span> (<span class="strong"><strong>JSON</strong></span>). Like XML, it is a standard, plaintext format, most often used for data structures and objects on the Web. The format has become popular recently due to its roots in browser-based JavaScript applications, but despite the pedigree, its utility is not limited to the Web. The ease in which JSON data structures can be understood by humans and parsed by machines makes it an appealing data structure for many types of projects.</p><p>JSON is based on a simple <code class="literal">{key: value}</code> format. The <code class="literal">{ }</code> brackets denote a JSON object, and the <code class="literal">key</code> and <code class="literal">value</code> parameters denote a property of the object and the status of the property. An object can have any number of properties and the properties themselves may be objects. For example, a JSON object for this book might look something like this:</p><div class="informalexample"><pre class="programlisting">{
  "title": "Machine Learning with R",
  "author": "Brett Lantz",
  "publisher": {
     "name": "Packt Publishing",
     "url": "https://www.packtpub.com"
  },
  "topics": ["R", "machine learning", "data mining"],
  "MSRP": 54.99
}</pre></div><p>This<a id="id1037" class="indexterm"/> example illustrates the data types available to<a id="id1038" class="indexterm"/> JSON: numeric, character, array (surrounded by <code class="literal">[</code> and <code class="literal">]</code> characters), and object. Not shown are the <code class="literal">null</code> and Boolean (<code class="literal">true</code> or <code class="literal">false</code>) values. The transmission of these types of objects from application to application and application to web browser, is what powers many of the most popular websites.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note50"/>Note</h3><p>For details<a id="id1039" class="indexterm"/> on the JSON format, go to <a class="ulink" href="http://www.json.org/">http://www.json.org/</a>.</p></div></div><p>Public-facing APIs allow programs like R to systematically query websites to retrieve results in the JSON format, using packages like <code class="literal">RCurl</code> and <code class="literal">httr</code>. Though a full tutorial on using web APIs is worthy of a separate book, the basic process relies on only a couple of steps—it's the details that are tricky.</p><p>Suppose we wanted to query the Google Maps API to locate the latitude and longitude of the Eiffel Tower in France. We first need to review the Google Maps API documentation to determine the URL and parameters needed to make this query. We then supply this information to the <code class="literal">httr</code> package's <code class="literal">GET()</code> function, adding a list of query parameters in order to apply the search address:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(httr)</strong></span>
<span class="strong"><strong>&gt; map_search &lt;-</strong></span>
<span class="strong"><strong>    GET("https://maps.googleapis.com/maps/api/geocode/json",</strong></span>
<span class="strong"><strong>          query = list(address = "Eiffel Tower"))</strong></span>
</pre></div><p>By typing the name of the resulting object, we can see some details about the request:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; map_search</strong></span>
<span class="strong"><strong>Response [https://maps.googleapis.com/maps/api/geocode/json?address=Eiffel%20T ower]</strong></span>
<span class="strong"><strong>  Status: 200</strong></span>
<span class="strong"><strong>  Content-Type: application/json; charset=UTF-8</strong></span>
<span class="strong"><strong>  Size: 2.34 kB</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>   "results" : [</strong></span>
<span class="strong"><strong>      {</strong></span>
<span class="strong"><strong>         "address_components" : [</strong></span>
<span class="strong"><strong>            {</strong></span>
<span class="strong"><strong>               "long_name" : "Eiffel Tower",</strong></span>
<span class="strong"><strong>               "short_name" : "Eiffel Tower",</strong></span>
<span class="strong"><strong>               "types" : [ "point_of_interest", "establishment" ]</strong></span>
<span class="strong"><strong>            },</strong></span>
<span class="strong"><strong>            {</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>To<a id="id1040" class="indexterm"/> access the resulting JSON, which the <code class="literal">httr</code> package <a id="id1041" class="indexterm"/>parsed automatically, we use the <code class="literal">content()</code> function. For brevity, only a handful of lines are shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; content(map_search)</strong></span>
<span class="strong"><strong>$results[[1]]$formatted_address</strong></span>
<span class="strong"><strong>[1] "Eiffel Tower, Champ de Mars, 5 Avenue Anatole France, 75007 Paris, France"</strong></span>

<span class="strong"><strong>$results[[1]]$geometry</strong></span>
<span class="strong"><strong>$results[[1]]$geometry$location</strong></span>
<span class="strong"><strong>$results[[1]]$geometry$location$lat</strong></span>
<span class="strong"><strong>[1] 48.85837</strong></span>

<span class="strong"><strong>$results[[1]]$geometry$location$lng</strong></span>
<span class="strong"><strong>[1] 2.294481</strong></span>
</pre></div><p>To access these contents individually, simply refer to them using list syntax. The names are based on the JSON objects returned by the Google API. For instance, the entire set of results is in an object appropriately named <code class="literal">results</code> and each result is numbered. In this case, we will access the formatted address property of the first result, as well as the latitude and longitude:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; content(map_search)$results[[1]]$formatted_address</strong></span>
<span class="strong"><strong>[1] "Eiffel Tower, Champ de Mars, 5 Avenue Anatole France, 75007 Paris, France"</strong></span>

<span class="strong"><strong>&gt; content(map_search)$results[[1]]$geometry$location$lat</strong></span>
<span class="strong"><strong>[1] 48.85837</strong></span>

<span class="strong"><strong>&gt; content(map_search)$results[[1]]$geometry$location$lng</strong></span>
<span class="strong"><strong>[1] 2.294481</strong></span>
</pre></div><p>These data elements could then be used in an R program as desired.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip144"/>Tip</h3><p>Because the Google Maps API may be updated in the future, if you find that your results differ from those shown here, please check the Packt Publishing support page for updated code.</p></div></div><p>On the <a id="id1042" class="indexterm"/>other hand, if you would like to do a <a id="id1043" class="indexterm"/>conversion to and from the JSON format outside the <code class="literal">httr</code> package, there are a number of packages that add this functionality. </p><p>The <code class="literal">rjson</code> package by Alex Couture-Beil was one of the earliest packages to allow R data structures to be converted back and forth from the JSON format. The syntax is simple. After installing the <code class="literal">rjson</code> package, to convert from an R object to a JSON string, we use the <code class="literal">toJSON()</code> function. Notice that the quote characters have escaped using the <code class="literal">\"</code> notation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(rjson)</strong></span>
<span class="strong"><strong>&gt; ml_book &lt;- list(book_title = "Machine Learning with R",</strong></span>
<span class="strong"><strong>                  author = "Brett Lantz")</strong></span>
<span class="strong"><strong>&gt; toJSON(ml_book)</strong></span>
<span class="strong"><strong>[1] "{\"book_title\":\"Machine Learning with R\",</strong></span>
<span class="strong"><strong>\"author\":\"Brett Lantz\"}"</strong></span>
</pre></div><p>To convert a JSON string into an R object, use the <code class="literal">fromJSON()</code> function. Quotation marks in the string need to be escaped, as shown:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ml_book_json &lt;- "{</strong></span>
<span class="strong"><strong>  \"title\": \"Machine Learning with R\",</strong></span>
<span class="strong"><strong>  \"author\": \"Brett Lantz\",</strong></span>
<span class="strong"><strong>  \"publisher\": {</strong></span>
<span class="strong"><strong>    \"name\": \"Packt Publishing\",</strong></span>
<span class="strong"><strong>    \"url\": \"https://www.packtpub.com\"</strong></span>
<span class="strong"><strong>  },</strong></span>
<span class="strong"><strong>  \"topics\": [\"R\", \"machine learning\", \"data mining\"],</strong></span>
<span class="strong"><strong>  \"MSRP\": 54.99</strong></span>
<span class="strong"><strong>}"</strong></span>

<span class="strong"><strong>&gt; ml_book_r &lt;- fromJSON(ml_book_json)</strong></span>
</pre></div><p>This results in a list structure in a form much like the original JSON:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; str(ml_book_r)</strong></span>
<span class="strong"><strong>List of 5</strong></span>
<span class="strong"><strong> $ title    : chr "Machine Learning with R"</strong></span>
<span class="strong"><strong> $ author   : chr "Brett Lantz"</strong></span>
<span class="strong"><strong> $ publisher:List of 2</strong></span>
<span class="strong"><strong>  ..$ name: chr "Packt Publishing"</strong></span>
<span class="strong"><strong>  ..$ url : chr "https://www.packtpub.com"</strong></span>
<span class="strong"><strong> $ topics   : chr [1:3] "R" "machine learning" "data mining"</strong></span>
<span class="strong"><strong> $ MSRP     : num 55</strong></span>
</pre></div><p>Recently, two <a id="id1044" class="indexterm"/>new JSON packages have arrived <a id="id1045" class="indexterm"/>on the scene. The first, <code class="literal">RJSONIO</code>, by Duncan Temple Lang was intended to be a faster and more extensible version of the <code class="literal">rjson</code> package, though they are now virtually identical. A second package, <code class="literal">jsonlite</code>, by Jeroen Ooms has quickly gained prominence as it creates data structures that are more consistent and R-like, especially while using data from web APIs. Which of these packages you use is a matter of preference; all three are virtually identical in practice as they each implement a <code class="literal">fromJSON()</code> and <code class="literal">toJSON()</code> function.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note51"/>Note</h3><a id="id1046" class="indexterm"/><p>For more information on the potential benefits of the <code class="literal">jsonlite</code> package, see: Ooms J. The jsonlite package: a practical and consistent mapping between JSON data and R objects. 2014. Available at: <a class="ulink" href="http://arxiv.org/abs/1403.2805">http://arxiv.org/abs/1403.2805</a>
</p></div></div></div></div></div>
<div class="section" title="Working with domain-specific data"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec52"/>Working with domain-specific data</h1></div></div></div><p>Machine learning has undoubtedly been applied to problems across every discipline. Although the <a id="id1047" class="indexterm"/>basic techniques are similar across all domains, some are so specialized that communities are formed to develop solutions to the challenges unique to the field. This leads to the discovery of new techniques and new terminology that is relevant only to domain specific problems.</p><p>This section covers a pair of domains that use machine learning techniques extensively, but require specialized knowledge to unlock their full potential. Since entire books have been written <a id="id1048" class="indexterm"/>on these topics, it will serve only as the briefest of introductions. For more details, seek out the help provided by the resources cited in each section.</p><div class="section" title="Analyzing bioinformatics data"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec123"/>Analyzing bioinformatics data</h2></div></div></div><p>The field of <span class="strong"><strong>bioinformatics</strong></span> is concerned with the application of computers and data analysis to<a id="id1049" class="indexterm"/> the biological domain, particularly<a id="id1050" class="indexterm"/> with regard to better understanding the <a id="id1051" class="indexterm"/>genome. As genetic data is unique compared to many other types, data analysis in the field of bioinformatics offers a number of unique challenges. For example, because living creatures have a tremendous number of genes and genetic sequencing is still relatively expensive, typical datasets are much wider than they are long; that is, they have more features (genes) than examples (creatures that have been sequenced). This creates problems while attempting to apply conventional visualizations, statistical tests, and machine learning methods to such data. Additionally, the increasing use <a id="id1052" class="indexterm"/>of proprietary <span class="strong"><strong>microarray</strong></span> "lab-on-a-chip" techniques requires highly specialized knowledge simply to load the genetic data.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note52"/>Note</h3><p>A CRAN task view, which lists some of R's specialized packages for statistical genetics<a id="id1053" class="indexterm"/> and bioinformatics, is available at <a class="ulink" href="http://cran.r-project.org/web/views/Genetics.html">http://cran.r-project.org/web/views/Genetics.html</a>.</p></div></div><p>The <span class="strong"><strong>Bioconductor</strong></span><a id="id1054" class="indexterm"/> project of the Fred Hutchinson Cancer Research Center in Seattle, Washington, aims to solve some of these problems by providing a standardized set of methods for analyzing genomic data. Using R as its foundation, Bioconductor adds bioinformatics-specific packages and documentation on top of the base R software.</p><p>Bioconductor <a id="id1055" class="indexterm"/>provides workflows to<a id="id1056" class="indexterm"/> analyze DNA and protein microarray data from common microarray platforms such as Affymetrix, Illumina, Nimblegen, and Agilent. Additional functionality includes sequence annotation, multiple testing procedures, specialized visualizations, tutorials, documentation, and much more.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note53"/>Note</h3><p>For more<a id="id1057" class="indexterm"/> information on the Bioconductor project, visit the project website at <a class="ulink" href="http://www.bioconductor.org">http://www.bioconductor.org</a>.</p></div></div></div><div class="section" title="Analyzing and visualizing network data"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec124"/>Analyzing and visualizing network data</h2></div></div></div><p>Social network<a id="id1058" class="indexterm"/> data and graph datasets <a id="id1059" class="indexterm"/>consist of structures that describe <a id="id1060" class="indexterm"/>connections, or <span class="strong"><strong>links</strong></span> (sometimes also called <span class="strong"><strong>edges</strong></span>), between<a id="id1061" class="indexterm"/> people or objects known as <span class="strong"><strong>nodes</strong></span>. With <span class="emphasis"><em>N</em></span> nodes, a <span class="emphasis"><em>N</em></span> x <span class="emphasis"><em>N = N<sub>2</sub></em></span> matrix of potential links can be created. This creates tremendous computational complexity as the number of nodes grows.</p><p>The field of <span class="strong"><strong>network analysis</strong></span><a id="id1062" class="indexterm"/> is concerned with statistical measures and visualizations that identify meaningful patterns of connections. For example, the following figure shows three clusters of circular nodes, all connected via a square node at the center. A network analysis may reveal the importance of the square node, among other key metrics.</p><div class="mediaobject"><img src="graphics/B03905_12_01.jpg" alt="Analyzing and visualizing network data"/></div><p>The <code class="literal">network</code> package by Carter T. Butts, David Hunter, and Mark S. Handcock offers a specialized data structure to work with networks. This data structure is necessary due to the fact that the matrix needed to store <span class="emphasis"><em>N<sub>2</sub></em></span> potential links will quickly exceed available memory; the <code class="literal">network</code> data structure uses a sparse representation to store only existent links, saving a great deal of memory if most relationships are nonexistent. A closely related package, <code class="literal">sna</code> (social network analysis), allows the analysis and visualization of the <code class="literal">network</code> objects.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note54"/>Note</h3><p>For more information on <code class="literal">network</code> and <code class="literal">sna</code>, including very detailed tutorials and documentation, refer to the project website hosted by the University of Washington at <a class="ulink" href="http://www.statnet.org/">http://www.statnet.org/</a>.</p></div></div><p>The <code class="literal">igraph</code> package<a id="id1063" class="indexterm"/> by Gábor Csárdi provides another set of tools to visualize and analyze network data. It is capable of calculating metrics for very large networks. An additional benefit of <code class="literal">igraph</code> is the fact that it has analogous packages for the Python and C programming languages, allowing it to be used to perform analyses virtually anywhere. As we will demonstrate shortly, it is very easy to use.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note55"/>Note</h3><p>For more<a id="id1064" class="indexterm"/> information on the <code class="literal">igraph</code> package, including demos and tutorials, visit the homepage at <a class="ulink" href="http://igraph.org/r/">http://igraph.org/r/</a>.</p></div></div><p>Using network data in R requires the use of specialized formats, as network data are not typically stored in typical tabular data structures like CSV files and data frames. As mentioned previously, because there are <span class="emphasis"><em>N<sub>2</sub></em></span> potential connections between <span class="emphasis"><em>N</em></span> network nodes, a tabular structure would quickly grow to be unwieldy for all but the smallest <span class="emphasis"><em>N</em></span> values. Instead, graph data are stored in a form that lists only the connections that are truly present; absent connections are inferred from the absence of data.</p><p>Perhaps<a id="id1065" class="indexterm"/> the simplest of such formats<a id="id1066" class="indexterm"/> is <span class="strong"><strong>edgelist</strong></span>, which is a text file with <a id="id1067" class="indexterm"/>one line per network <a id="id1068" class="indexterm"/>connection. Each node must be assigned <a id="id1069" class="indexterm"/>a unique identifier and the links between the nodes are defined by placing the connected nodes' identifiers together on a single line separated by a space. For instance, the following edgelist defines three connections between node 0 and nodes 1, 2, and 3:</p><div class="informalexample"><pre class="programlisting">0 1
0 2
0 3</pre></div><p>To load network data into R, the <code class="literal">igraph</code> package provides a <code class="literal">read.graph()</code> function that can read edgelist files as well as other more sophisticated formats like <span class="strong"><strong>Graph Modeling Language</strong></span> (<span class="strong"><strong>GML</strong></span>). To illustrate this functionality, we'll use a dataset describing<a id="id1070" class="indexterm"/> friendship among the members of a small karate club. To follow along, download the <code class="literal">karate.txt</code> file from the Packt Publishing website and save it in your R working directory. After you've installed the <code class="literal">igraph</code> package, the karate network can be read into R as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(igraph)</strong></span>
<span class="strong"><strong>&gt; karate &lt;- read.graph("karate.txt", "edgelist", directed = FALSE)</strong></span>
</pre></div><p>This will<a id="id1071" class="indexterm"/> create a sparse matrix object that <a id="id1072" class="indexterm"/>can be used for graphing <a id="id1073" class="indexterm"/>and network analysis. Note that the <code class="literal">directed = FALSE</code> parameter <a id="id1074" class="indexterm"/>forces the network to use undirected or bidirectional links between the nodes. Since the karate dataset describes friendship, it means that if person 1 is friends with person 2, then person 2 must be friends with person 1. On the other hand, if the dataset described fight outcomes, the fact that person 1 defeated person 2 would certainly not imply that person 2 defeated person 1. In this case, the <code class="literal">directed = TRUE</code> parameter should be set.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note56"/>Note</h3><p>The karate network dataset used here was compiled by <span class="emphasis"><em>M.E.J. Newman</em></span> of the University of Michigan. It was first presented in Zachary WW. <span class="emphasis"><em>An information flow model for conflict and fission in small groups</em></span>. Journal of Anthropological Research. 1977; 33:452-473.</p></div></div><p>To examine the graph, use the <code class="literal">plot()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; plot(karate)</strong></span>
</pre></div><p>This produces the following figure:</p><div class="mediaobject"><img src="graphics/B03905_12_02.jpg" alt="Analyzing and visualizing network data"/></div><p>Examining<a id="id1075" class="indexterm"/> the network visualization, it is <a id="id1076" class="indexterm"/>apparent that there are a <a id="id1077" class="indexterm"/>few highly connected members of the karate club. Nodes 1, 33, and 34 seem <a id="id1078" class="indexterm"/>to be more central than the others, which remain at the club periphery.</p><p>Using <code class="literal">igraph</code> to calculate graph metrics, it is possible to demonstrate our hunch analytically. The <span class="strong"><strong>degree</strong></span> of a node measures how many nodes it is linked to. The <code class="literal">degree()</code> function confirms our hunch that nodes 1, 33, and 34 are more connected than the others with <code class="literal">16</code>, <code class="literal">12</code>, and <code class="literal">17</code> connections, respectively:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; degree(karate)</strong></span>
<span class="strong"><strong> [1] 16  9 10  6  3  4  4  4  5  2  3  1  2  5  2  2  2  2</strong></span>
<span class="strong"><strong>[19]  2  3  2  2  2  5  3  3  2  4  3  4  4  6 12 17</strong></span>
</pre></div><p>Because some connections are more important than others, a variety of network measures have been developed to measure node connectivity with this consideration. A network metric <a id="id1079" class="indexterm"/>called <span class="strong"><strong>betweenness centrality</strong></span> is intended to capture the number of shortest paths between nodes that pass through each node. Nodes that are truly more central to the entire graph will have a higher betweenness centrality value, because they act as a bridge between the other nodes. We obtain a vector of the centrality measures using the <code class="literal">betweenness()</code> function, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; betweenness(karate)</strong></span>
<span class="strong"><strong> [1] 231.0714286  28.4785714  75.8507937   6.2880952</strong></span>
<span class="strong"><strong> [5]   0.3333333  15.8333333  15.8333333   0.0000000</strong></span>
<span class="strong"><strong> [9]  29.5293651   0.4476190   0.3333333   0.0000000</strong></span>
<span class="strong"><strong>[13]   0.0000000  24.2158730   0.0000000   0.0000000</strong></span>
<span class="strong"><strong>[17]   0.0000000   0.0000000   0.0000000  17.1468254</strong></span>
<span class="strong"><strong>[21]   0.0000000   0.0000000   0.0000000   9.3000000</strong></span>
<span class="strong"><strong>[25]   1.1666667   2.0277778   0.0000000  11.7920635</strong></span>
<span class="strong"><strong>[29]   0.9476190   1.5428571   7.6095238  73.0095238</strong></span>
<span class="strong"><strong>[33]  76.6904762 160.5515873</strong></span>
</pre></div><p>As <a id="id1080" class="indexterm"/>nodes 1 and 34 have much greater <a id="id1081" class="indexterm"/>betweenness values than the others, they<a id="id1082" class="indexterm"/> are more central to the karate club's friendship <a id="id1083" class="indexterm"/>network. These two individuals, with extensive personal friendship networks, may be the "glue" that holds the network together.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip145"/>Tip</h3><p>Betweenness centrality is only one of many metrics intended to capture a node's importance, and it isn't even the only measure of centrality. Refer to the <code class="literal">igraph</code> documentation for definitions of other network properties.</p></div></div><p>The <code class="literal">sna</code> and <code class="literal">igraph</code> packages are capable of computing many such graph metrics, which may then be used as inputs to machine learning functions. For example, suppose we were attempting to build a model predicting who would win an election for the club's president. The fact that nodes 1 and 34 are well-connected suggests that they may have the social capital needed for such a leadership role. These might be the highly valuable predictors of the election's results.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip146"/>Tip</h3><p>By combining network analysis with machine learning, services like Facebook, Twitter, and LinkedIn provide vast stores of network data to make predictions about the users' future behavior. A high-profile example is the 2012 U.S. Presidential campaign in which chief data scientist Rayid Ghani utilized Facebook data to identify people who might be persuaded to vote for Barack Obama.</p></div></div></div></div>
<div class="section" title="Improving the performance of R"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec53"/>Improving the performance of R</h1></div></div></div><p>R has a reputation for being slow and memory-inefficient, a reputation that is at least somewhat earned. These faults are largely unnoticed on a modern PC for datasets of many thousands of records, but datasets with a million records or more can exceed the limits of what is <a id="id1084" class="indexterm"/>currently possible with consumer-grade hardware. The problem worsens if the dataset contains many features or if complex learning algorithms are being used.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note57"/>Note</h3><p>CRAN has<a id="id1085" class="indexterm"/> a high-performance computing task view that lists <a id="id1086" class="indexterm"/>packages pushing the boundaries of what is possible in R. It can be viewed at <a class="ulink" href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">http://cran.r-project.org/web/views/HighPerformanceComputing.html</a>.</p></div></div><p>Packages that extend R past the capabilities of the base software are being developed rapidly. This work comes primarily on two fronts: some packages add the capability to manage extremely large datasets by making data operations faster or allowing the size of the data to exceed the amount of available system memory; others allow R to work faster, perhaps by spreading the work over additional computers or processors, utilizing specialized computer hardware, or providing machine learning algorithms optimized for big data problems.</p><div class="section" title="Managing very large datasets"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec125"/>Managing very large datasets</h2></div></div></div><p>Extremely<a id="id1087" class="indexterm"/> large datasets can cause R to<a id="id1088" class="indexterm"/> grind to a halt when the system runs out of memory to store data. Even if the entire dataset can fit into the available memory, additional memory overhead will be needed for data processing. Furthermore, very large datasets can take a long amount of time to analyze for no reason other than the sheer volume of records; even a quick operation can cause delays when performed many millions of times.</p><p>Years ago, many would perform data preparation outside R in another programming language, or use R but perform analyses on a smaller subset of data. However, this is no longer necessary, as several packages have been contributed to R to address these problems.</p><div class="section" title="Generalizing tabular data structures with dplyr"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec69"/>Generalizing tabular data structures with dplyr</h3></div></div></div><p>The <code class="literal">dplyr</code> package introduced in 2014 by Hadley Wickham and Romain Francois is perhaps <a id="id1089" class="indexterm"/>the most straightforward <a id="id1090" class="indexterm"/>way to begin working <a id="id1091" class="indexterm"/>with large datasets in R. Though other packages may exceed its capabilities in terms of raw speed or the raw size of the data, <code class="literal">dplyr</code> is still quite capable. More importantly, it is virtually transparent after the initial learning curve has passed.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note58"/>Note</h3><p>For more information on <code class="literal">dplyr</code>, including some very helpful tutorials, refer to the project's <a id="id1092" class="indexterm"/>GitHub page at <a class="ulink" href="https://github.com/hadley/dplyr">https://github.com/hadley/dplyr</a>.</p></div></div><p>Put simply, the package provides an object called <code class="literal">tbl</code>, which is an abstraction of tabular data. It acts much like a data frame, with several important exceptions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Key functionality has been written in C++, which according to the authors results in a 20x to 1000x performance increase for many operations.</li><li class="listitem" style="list-style-type: disc">R data frames are limited by available memory. The <code class="literal">dplyr</code> version of a data frame can be linked transparently to disk-based databases that can exceed what can be stored in memory.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">dplyr</code> package makes reasonable assumptions about data frames that optimize your effort as well as memory use. It doesn't automatically change data types. And, if possible, it avoids making copies of data by pointing to the original value instead.</li><li class="listitem" style="list-style-type: disc">New operators are introduced that allow common data transformations to be performed with much less code while remaining highly readable.</li></ul></div><p>Making the transition from data frames to <code class="literal">dplyr</code> is easy. To convert an existing data frame into a <code class="literal">tbl</code> object, use the <code class="literal">as.tbl()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(dplyr)</strong></span>
<span class="strong"><strong>&gt; credit &lt;- read.csv("credit.csv")</strong></span>
<span class="strong"><strong>&gt; credit_tbl &lt;- as.tbl(credit)</strong></span>
</pre></div><p>Typing the name of the table provides information about the object. Even here, we see a distinction between <code class="literal">dplyr</code> and typical R behavior; where as a traditional data frame would have displayed many rows of data, <code class="literal">dplyr</code> objects are more considerate of real-world needs. For example, <a id="id1093" class="indexterm"/>typing the name <a id="id1094" class="indexterm"/>of the<a id="id1095" class="indexterm"/> object provides output summarized in a form that fits a single screen:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_tbl</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03905_12_03.jpg" alt="Generalizing tabular data structures with dplyr"/></div><p>Connecting <code class="literal">dplyr</code> to an external database is straightforward as well. The <code class="literal">dplyr</code> package provides functions to connect to MySQL, PostgreSQL, and SQLite databases. These create a connection object that allows <code class="literal">tbl</code> objects to be pulled from the database.</p><p>Let's use the <code class="literal">src_sqlite()</code> function to create a SQLite database to store credit data. SQLite is a simple database that doesn't require a server. It simply connects to a database file, which we'll call <code class="literal">credit.sqlite3</code>. Since the file doesn't exist yet, we need to set the <code class="literal">create = TRUE</code> parameter to create the file. Note that for this step to work, you may require to install the <code class="literal">RSQLite</code> package if you have not already done so:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_db_conn &lt;- src_sqlite("credit.sqlite3", create = TRUE)</strong></span>
</pre></div><p>After creating the connection, we need to load the data into the database using the <code class="literal">copy_to()</code> function. This uses the <code class="literal">credit_tbl</code> object to create a database table within the database specified by <code class="literal">credit_db_conn</code>. The <code class="literal">temporary = FALSE</code> parameter forces the table to be created immediately. Since <code class="literal">dplyr</code> tries to avoid copying data unless it must, it will only create the table if it is explicitly asked to:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; copy_to(credit_db_conn, credit_tbl, temporary = FALSE)</strong></span>
</pre></div><p>Executing<a id="id1096" class="indexterm"/> the <code class="literal">copy_to()</code> function will store the data in the <code class="literal">credit.sqlite3</code> file, which can be transported to <a id="id1097" class="indexterm"/>other systems <a id="id1098" class="indexterm"/>as needed. To access this file later, simply reopen the database connection and create a <code class="literal">tbl</code> object, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit_db_conn &lt;- src_sqlite("credit.sqlite3")</strong></span>
<span class="strong"><strong>&gt; credit_tbl &lt;- tbl(credit_db_conn, "credit_tbl")</strong></span>
</pre></div><p>In spite of the fact that <code class="literal">dplyr</code> is routed through a database, the <code class="literal">credit_tbl</code> object here will act exactly like any other <code class="literal">tbl</code> object and will gain all the other benefits of the <code class="literal">dplyr</code> package.</p></div><div class="section" title="Making data frames faster with data.table"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec70"/>Making data frames faster with data.table</h3></div></div></div><p>The <code class="literal">data.table</code> package by Matt Dowle, Tom Short, Steve Lianoglou, and Arun Srinivasan provides<a id="id1099" class="indexterm"/> an enhanced version of a data frame <a id="id1100" class="indexterm"/>called a <span class="strong"><strong>data table</strong></span>. The <code class="literal">data.table</code> objects are typically much <a id="id1101" class="indexterm"/>faster than data frames for subsetting, joining, and grouping operations. For the largest datasets—those with many millions of rows—these objects may be substantially faster than even <code class="literal">dplyr</code> objects. Yet, because it is essentially an improved data frame, the resulting objects can still be used by any R function that accepts a data frame.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note59"/>Note</h3><p>The <code class="literal">data.table</code> project<a id="id1102" class="indexterm"/> can be found on GitHub at <a class="ulink" href="https://github.com/Rdatatable/data.table/wiki">https://github.com/Rdatatable/data.table/wiki</a>.</p></div></div><p>After installing the <code class="literal">data.table</code> package, the <code class="literal">fread()</code> function will read tabular files like CSVs into data table objects. For instance, to load the credit data used previously, type:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(data.table)</strong></span>
<span class="strong"><strong>&gt; credit &lt;- fread("credit.csv")</strong></span>
</pre></div><p>The credit data table can then be queried using syntax similar to R's <code class="literal">[row, col]</code> form, but optimized for speed and some additional useful conveniences. In particular, the data table structure allows the <code class="literal">row</code> portion to select rows using an abbreviated subsetting command, and the <code class="literal">col</code> portion to use a function that does something with the selected rows. For example, the following command computes the mean requested loan amount for people with a good credit history:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; credit[credit_history == "good", mean(amount)]</strong></span>
<span class="strong"><strong>[1] 3040.958</strong></span>
</pre></div><p>By building larger queries with this simple syntax, very complex operations can be performed on data tables. Since the data structure is optimized for speed, it can be used with large<a id="id1103" class="indexterm"/> datasets.</p><p>One limitation<a id="id1104" class="indexterm"/> of the <code class="literal">data.table</code> structures is that like data frames they are limited by the available system memory. The next two sections discuss packages that overcome this shortcoming at the expense of breaking compatibility with many R functions.</p><div class="tip" title="Tip" style=""><div class="inner"><h3 class="title"><a id="tip147"/>Tip</h3><p>The <code class="literal">dplyr</code> and <code class="literal">data.table</code> packages each have unique strengths. For an in-depth comparison, check out the following Stack Overflow discussion at <a class="ulink" href="http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly">http://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly</a>. It is also possible to have the best of both worlds, as <code class="literal">data.table</code> structures can be loaded into <code class="literal">dplyr</code> using the <code class="literal">tbl_dt()</code> function.</p></div></div></div><div class="section" title="Creating disk-based data frames with ff"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec71"/>Creating disk-based data frames with ff</h3></div></div></div><p>The <code class="literal">ff</code> package by Daniel Adler, Christian Gläser, Oleg Nenadic, Jens Oehlschlägel, and Walter Zucchini provides an alternative to a data frame (<code class="literal">ffdf</code>) that allows datasets of over two<a id="id1105" class="indexterm"/> billion rows to be <a id="id1106" class="indexterm"/>created, even if this far exceeds<a id="id1107" class="indexterm"/> the available system memory.</p><p>The <code class="literal">ffdf</code> structure has a physical component that stores the data on a disk in a highly efficient form, and a virtual component that acts like a typical R data frame, but transparently points to the data stored in the physical component. You can imagine the <code class="literal">ffdf</code> object as a map that points to a location of the data on a disk.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note60"/>Note</h3><p>The <code class="literal">ff</code> project is <a id="id1108" class="indexterm"/>on the Web at <a class="ulink" href="http://ff.r-forge.r-project.org/">http://ff.r-forge.r-project.org/</a>.</p></div></div><p>A downside of <code class="literal">ffdf</code> data structures is that they cannot be used natively by most R functions. Instead, the data must be processed in small chunks, and the results must be combined later on. The upside of chunking the data is that the task can be divided across several processors simultaneously using the parallel computing methods presented later in this chapter.</p><p>After installing the <code class="literal">ff</code> package, to read in a large CSV file, use the <code class="literal">read.csv.ffdf()</code> function, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(ff)</strong></span>
<span class="strong"><strong>&gt; credit &lt;- read.csv.ffdf(file = "credit.csv", header = TRUE)</strong></span>
</pre></div><p>Unfortunately, we cannot work directly with the <code class="literal">ffdf</code> object, as attempting to treat it like a traditional <a id="id1109" class="indexterm"/>data frame<a id="id1110" class="indexterm"/> results in an error message:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mean(credit$amount)</strong></span>
<span class="strong"><strong>[1] NA</strong></span>
<span class="strong"><strong>Warning message:</strong></span>
<span class="strong"><strong>In mean.default(credit$amount) :</strong></span>
<span class="strong"><strong>  argument is not numeric or logical: returning NA</strong></span>
</pre></div><p>The <code class="literal">ffbase</code> package<a id="id1111" class="indexterm"/> by Edwin de Jonge, Jan Wijffels, and Jan van der Laan addresses this issue somewhat by adding capabilities for basic analyses using <code class="literal">ff</code> objects. This makes it possible to use <code class="literal">ff</code> objects directly for data exploration. For instance, after installing the <code class="literal">ffbase</code> package, the mean function works as expected:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(ffbase)</strong></span>
<span class="strong"><strong>&gt; mean(credit$amount)</strong></span>
<span class="strong"><strong>[1] 3271.258</strong></span>
</pre></div><p>The package also provides other basic functionality such as mathematical operators, query functions, summary statistics, and wrappers to work with optimized machine learning algorithms like <code class="literal">biglm</code> (described later in this chapter). Though these do not completely eliminate the challenges of working with extremely large datasets, they make the process a bit more seamless.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note61"/>Note</h3><p>For more<a id="id1112" class="indexterm"/> information on advanced functionality, visit the <code class="literal">ffbase</code> project site at <a class="ulink" href="http://github.com/edwindj/ffbase">http://github.com/edwindj/ffbase</a>.</p></div></div></div><div class="section" title="Using massive matrices with bigmemory"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec72"/>Using massive matrices with bigmemory</h3></div></div></div><p>The <code class="literal">bigmemory</code> package by Michael J. Kane, John W. Emerson, and Peter Haverty allows <a id="id1113" class="indexterm"/>the use of<a id="id1114" class="indexterm"/> extremely large matrices that exceed the amount of available system memory. The matrices can be stored on a disk or in shared memory, allowing them to be used by other processes on the same computer or across a network. This facilitates parallel computing methods, such as the ones covered later in this chapter.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note62"/>Note</h3><p>Additional<a id="id1115" class="indexterm"/> documentation on the <code class="literal">bigmemory</code> package can be found at <a class="ulink" href="http://www.bigmemory.org/">http://www.bigmemory.org/</a>.</p></div></div><p>Because <code class="literal">bigmemory</code> matrices are intentionally unlike data frames, they cannot be used directly with most of the machine learning methods covered in this book. They also can only be used with numeric data. That said, since they are similar to a typical R matrix, it is easy to create smaller samples or chunks that can be converted into standard R data structures.</p><p>The authors also provide the <code class="literal">bigalgebra</code>, <code class="literal">biganalytics</code>, and <code class="literal">bigtabulate</code> packages, which allow simple analyses to be performed on the matrices. Of particular note is the <code class="literal">bigkmeans()</code> function in the <code class="literal">biganalytics</code> package, which performs k-means clustering as described in <a class="link" href="ch09.html" title="Chapter 9. Finding Groups of Data – Clustering with k-means">Chapter 9</a>, <span class="emphasis"><em>Finding Groups of Data – Clustering with k-means</em></span>. Due to the highly specialized nature of these packages, use cases are outside the scope of this chapter.</p></div></div><div class="section" title="Learning faster with parallel computing"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec126"/>Learning faster with parallel computing</h2></div></div></div><p>In the early days of computing, processors executed instructions in <span class="strong"><strong>serial</strong></span> which meant that<a id="id1116" class="indexterm"/> they were limited to performing a <a id="id1117" class="indexterm"/>single task at a time. The next instruction could not be started until the previous instruction was complete. Although it was widely known that many tasks could be completed more efficiently by completing the steps simultaneously, the technology simply did not exist yet.</p><div class="mediaobject"><img src="graphics/B03905_12_04.jpg" alt="Learning faster with parallel computing"/></div><p>This was addressed by the development of <span class="strong"><strong>parallel computing</strong></span> methods, which use a set of two or more <a id="id1118" class="indexterm"/>processors or computers to solve a larger problem. Many modern computers are designed for parallel computing. Even in the cases in which they have a single processor, they often have two or more <span class="strong"><strong>cores</strong></span> that are capable of working in parallel. This allows tasks to be accomplished independently of one another.</p><div class="mediaobject"><img src="graphics/B03905_12_05.jpg" alt="Learning faster with parallel computing"/></div><p>Networks of multiple computers called <span class="strong"><strong>clusters</strong></span> can also be used for parallel computing. A large cluster may include a variety of hardware and be separated over large distances. In this case, the cluster is<a id="id1119" class="indexterm"/> known as a <span class="strong"><strong>grid</strong></span>. Taken to an extreme, a cluster or grid of hundreds or thousands of computers running commodity hardware could be a very powerful system.</p><p>The catch, however, is that not every problem can be parallelized. Certain problems are more <a id="id1120" class="indexterm"/>conducive to parallel execution than others. One might expect that adding 100 processors would result in accomplishing 100 times the work in the same amount of time (that is, the overall execution time would be 1/100), but this is typically not the case. The reason is that it takes effort to manage the workers. Work must be divided into equal, nonoverlapping tasks, and each of the workers' results must be combined into one final answer.</p><p>So-called <span class="strong"><strong>embarrassingly parallel</strong></span> problems are ideal. It is easy to reduce these tasks into nonoverlapping blocks of work and recombine the results. An example of an embarrassingly<a id="id1121" class="indexterm"/> parallel machine learning task would be 10-fold <a id="id1122" class="indexterm"/>cross-validation; once the 10 samples are divided, each of the 10 blocks of work is independent, meaning that they do not affect the others. As you will soon see, this task can be sped up quite dramatically using parallel computing.</p><div class="section" title="Measuring execution time"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec73"/>Measuring execution time</h3></div></div></div><p>Efforts to <a id="id1123" class="indexterm"/>speed up R will be wasted if it is not possible to systematically measure how much time is saved. Although a stopwatch is one option, an easier solution would be to wrap the code in a <code class="literal">system.time()</code> function.</p><p>For example, on my laptop, the <code class="literal">system.time()</code> function notes that it takes about <code class="literal">0.093</code> seconds to generate a million random numbers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; system.time(rnorm(1000000))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.092   0.000   0.093</strong></span>
</pre></div><p>The same function can be used to evaluate the improvement in performance obtained by using the methods that were just described or any R function.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note63"/>Note</h3><p>For what it's worth, when the first edition was published, generating a million random numbers took 0.13 seconds. Although I'm now using a slightly more powerful computer, this reduction of about 30 percent of the processing time just two years later illustrates how quickly computer hardware and software are improving.</p></div></div></div><div class="section" title="Working in parallel with multicore and snow"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec74"/>Working in parallel with multicore and snow</h3></div></div></div><p>The <code class="literal">parallel</code> package, now included with R version 2.14.0 and higher, has lowered the entry barrier to<a id="id1124" class="indexterm"/> deploy parallel algorithms by <a id="id1125" class="indexterm"/>providing a standard framework to<a id="id1126" class="indexterm"/> set up worker processes that can complete tasks<a id="id1127" class="indexterm"/> simultaneously. It does this by including components of the <code class="literal">multicore</code> and <code class="literal">snow</code> packages, each taking a different approach towards multitasking.</p><p>If your computer is reasonably recent, you are likely to be able to use parallel processing. To determine the number of cores your machine has, use the <code class="literal">detectCores()</code> function as follows. Note that your output will differ depending on your hardware specifications:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(parallel)</strong></span>
<span class="strong"><strong>&gt; detectCores()</strong></span>
<span class="strong"><strong>[1] 8</strong></span>
</pre></div><p>The <code class="literal">multicore</code> package was developed by Simon Urbanek and allows parallel processing<a id="id1128" class="indexterm"/> on a single machine that has multiple <a id="id1129" class="indexterm"/>processors or processor cores. It <a id="id1130" class="indexterm"/>utilizes the multitasking capabilities of a computer's <a id="id1131" class="indexterm"/>operating system to <span class="strong"><strong>fork</strong></span> additional R sessions that share the same memory. It is perhaps the simplest way to get started with parallel processing in R. Unfortunately, because Windows does not support forking, this solution will not work everywhere.</p><p>An easy way to get started with the <code class="literal">multicore</code> functionality is to use the <code class="literal">mclapply()</code> function, which is a parallel version of <code class="literal">lapply()</code>. For instance, the following blocks of code illustrate how the task of generating a million random numbers can be divided across 1, 2, 4, and 8 cores. The <code class="literal">unlist()</code> function is used to combine the parallel results (a list) into a single vector after each core has completed its chunk of work:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; system.time(l1 &lt;- rnorm(1000000))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.094   0.003   0.097</strong></span>
 
<span class="strong"><strong>&gt; system.time(l2 &lt;- unlist(mclapply(1:2, function(x) {</strong></span>
<span class="strong"><strong>  rnorm(500000)}, mc.cores = 2)))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.106   0.045   0.076</strong></span>
 
<span class="strong"><strong>&gt; system.time(l4 &lt;- unlist(mclapply(1:4, function(x) {</strong></span>
<span class="strong"><strong>  rnorm(250000) }, mc.cores = 4)))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.135   0.055   0.063</strong></span>
 
<span class="strong"><strong>&gt; system.time(l8 &lt;- unlist(mclapply(1:8, function(x) {</strong></span>
<span class="strong"><strong>  rnorm(125000) }, mc.cores = 8)))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.123   0.058   0.055 </strong></span>
</pre></div><p>Notice how as the number of cores increases, the elapsed time decreases, and the benefit tapers off. Though this is a simple example, it can be adapted easily to many other tasks.</p><p>The <code class="literal">snow</code> package (simple networking of workstations) by Luke Tierney, A. J. Rossini, Na Li, and H. Sevcikova allows parallel computing on multicore or multiprocessor machines as<a id="id1132" class="indexterm"/> well as on a network of multiple<a id="id1133" class="indexterm"/> machines. It is slightly more difficult to <a id="id1134" class="indexterm"/>use, but offers much more power and flexibility. After<a id="id1135" class="indexterm"/> installing <code class="literal">snow</code>, to set up a cluster on a single machine, use the <code class="literal">makeCluster()</code> function with the number of cores to be used:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(snow)</strong></span>
<span class="strong"><strong>&gt; cl1 &lt;- makeCluster(4)</strong></span>
</pre></div><p>Because <code class="literal">snow</code> communicates via network traffic, depending on your operating system, you may receive a message to approve access through your firewall.</p><p>To confirm whether the cluster is operational, we can ask each node to report back its hostname. The <code class="literal">clusterCall()</code> function executes a function on each machine in the cluster. In this case, we'll define a function that simply calls the <code class="literal">Sys.info()</code> function and returns the <code class="literal">nodename</code> parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; clusterCall(cl1, function() { Sys.info()["nodename"] } )</strong></span>
<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>                  nodename </strong></span>
<span class="strong"><strong>"Bretts-Macbook-Pro.local" </strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>                  nodename </strong></span>
<span class="strong"><strong>"Bretts-Macbook-Pro.local" </strong></span>

<span class="strong"><strong>[[3]]</strong></span>
<span class="strong"><strong>                  nodename </strong></span>
<span class="strong"><strong>"Bretts-Macbook-Pro.local" </strong></span>

<span class="strong"><strong>[[4]]</strong></span>
<span class="strong"><strong>                  nodename </strong></span>
<span class="strong"><strong>"Bretts-Macbook-Pro.local"</strong></span>
</pre></div><p>Unsurprisingly, since all four nodes are running on a single machine, they report back the same hostname. To have the four nodes run a different command, supply them with a unique parameter via the <code class="literal">clusterApply()</code> function. Here, we'll supply each node with a different letter. Each node will then perform a simple function on its letter in parallel:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; clusterApply(cl1, c('A', 'B', 'C', 'D'),</strong></span>
<span class="strong"><strong>               function(x) { paste("Cluster", x, "ready!") })</strong></span>
<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>[1] "Cluster A ready!"</strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>[1] "Cluster B ready!"</strong></span>

<span class="strong"><strong>[[3]]</strong></span>
<span class="strong"><strong>[1] "Cluster C ready!"</strong></span>

<span class="strong"><strong>[[4]]</strong></span>
<span class="strong"><strong>[1] "Cluster D ready!"</strong></span>
</pre></div><p>Once<a id="id1136" class="indexterm"/> we're done with the cluster, it's important<a id="id1137" class="indexterm"/> to terminate the processes it spawned. This <a id="id1138" class="indexterm"/>will <a id="id1139" class="indexterm"/>free up the resources each node is using:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; stopCluster(cl1)</strong></span>
</pre></div><p>Using these simple commands, it is possible to speed up many machine learning tasks. For larger big data problems, much more complex <code class="literal">snow</code> configurations are possible. For instance, you may attempt to <a id="id1140" class="indexterm"/>configure a <span class="strong"><strong>Beowulf cluster</strong></span>—a network of many consumer-grade machines. In academic and industry research settings with dedicated computing clusters, <code class="literal">snow</code> can use the <code class="literal">Rmpi</code> package to access these high-performance <span class="strong"><strong>message-passing interface</strong></span> (<span class="strong"><strong>MPI</strong></span>) servers. Working with such clusters <a id="id1141" class="indexterm"/>requires the knowledge of network configurations and computing hardware, which is outside the scope of this book.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note64"/>Note</h3><p>For a much more<a id="id1142" class="indexterm"/> detailed introduction to <code class="literal">snow</code>, including some information on how to configure parallel computing on several computers over a network, see <a class="ulink" href="http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf">http://homepage.stat.uiowa.edu/~luke/classes/295-hpc/notes/snow.pdf</a>.</p></div></div></div><div class="section" title="Taking advantage of parallel with foreach and doParallel"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec75"/>Taking advantage of parallel with foreach and doParallel</h3></div></div></div><p>The <code class="literal">foreach</code> package by Steve Weston of Revolution Analytics provides perhaps the easiest way to<a id="id1143" class="indexterm"/> get started with parallel computing, particularly<a id="id1144" class="indexterm"/> if you are running R on <a id="id1145" class="indexterm"/>Windows, as some of the other packages are platform-specific.</p><p>The core of<a id="id1146" class="indexterm"/> the package is a new <code class="literal">foreach</code> looping construct. If you have worked with other programming languages, you may be familiar with it. Essentially, it allows looping over a number of items in a set without explicitly counting the number of items; in other words, <span class="emphasis"><em>for each</em></span> item in the set, <span class="emphasis"><em>do</em></span> something.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note65"/>Note</h3><p>In addition to the <code class="literal">foreach</code> package, Revolution Analytics (recently acquired by Microsoft) has developed high-performance, enterprise-ready R builds. Free versions are<a id="id1147" class="indexterm"/> available for trial and academic use. For more information, see their website at <a class="ulink" href="http://www.revolutionanalytics.com/">http://www.revolutionanalytics.com/</a>.</p></div></div><p>If you're thinking that R already provides a set of apply functions to loop over the sets of items (for example, <code class="literal">apply()</code>, <code class="literal">lapply()</code>, <code class="literal">sapply()</code>, and so on), you are correct. However, the <code class="literal">foreach</code> loop has an additional benefit: iterations of the loop can be completed in parallel using a very simple syntax. Let's see how this works.</p><p>Recall the command we've been using to generate a million random numbers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; system.time(l1 &lt;- rnorm(1000000))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.096   0.000   0.096 </strong></span>
</pre></div><p>After the <code class="literal">foreach</code> package has been installed, it can be expressed by a loop that generates four<a id="id1148" class="indexterm"/> sets of 250,000 random numbers in parallel. The <code class="literal">.combine</code> parameter <a id="id1149" class="indexterm"/>is an optional setting that tells <code class="literal">foreach</code> which function it should use to combine the final set of results from each loop iteration. In this case, since each iteration generates a set of random numbers, we simply use the <code class="literal">c()</code> concatenate function to create a single, combined vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(foreach)</strong></span>
<span class="strong"><strong>&gt; system.time(l4 &lt;- foreach(i = 1:4, .combine = 'c')</strong></span>
<span class="strong"><strong>                %do% rnorm(250000))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.106   0.003   0.109 </strong></span>
</pre></div><p>If you noticed that this function didn't result in a speed improvement, good catch! The reason is that by default, the <code class="literal">foreach</code> package runs each loop iteration in serial. The <code class="literal">doParallel</code> sister package provides a parallel backend for <code class="literal">foreach</code> that utilizes the <code class="literal">parallel</code> package included with R, which was described earlier in this chapter. After installing<a id="id1150" class="indexterm"/> the <code class="literal">doParallel</code> package, simply register <a id="id1151" class="indexterm"/>the number of cores and swap the <code class="literal">%do%</code> command with <code class="literal">%dopar%</code>, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(doParallel)</strong></span>
<span class="strong"><strong>&gt; registerDoParallel(cores = 4)</strong></span>
<span class="strong"><strong>&gt; system.time(l4p &lt;- foreach(i = 1:4, .combine = 'c')</strong></span>
<span class="strong"><strong>                       %dopar% rnorm(250000))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>  0.062   0.030   0.054</strong></span>
</pre></div><p>As shown in<a id="id1152" class="indexterm"/> the output, this code results in the expected performance<a id="id1153" class="indexterm"/> improvement, nearly cutting the execution time in half.</p><p>To close<a id="id1154" class="indexterm"/> the <code class="literal">doParallel</code> cluster, simply type:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; stopImplicitCluster()</strong></span>
</pre></div><p>Though the <a id="id1155" class="indexterm"/>cluster will be closed automatically at the conclusion of the R session, it is better form to do so explicitly.</p></div><div class="section" title="Parallel cloud computing with MapReduce and Hadoop"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec76"/>Parallel cloud computing with MapReduce and Hadoop</h3></div></div></div><p>The <span class="strong"><strong>MapReduce</strong></span> programming<a id="id1156" class="indexterm"/> model was developed at Google as a way to process their data on a large cluster of networked computers. MapReduce <a id="id1157" class="indexterm"/>defined parallel programming <a id="id1158" class="indexterm"/>as a two-step process:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>map</strong></span> step in which a<a id="id1159" class="indexterm"/> problem is divided into smaller tasks<a id="id1160" class="indexterm"/> that are distributed across the computers in the cluster</li><li class="listitem" style="list-style-type: disc">A <span class="strong"><strong>reduce</strong></span> step<a id="id1161" class="indexterm"/> in which the results of the small chunks of work are collected and synthesized into a final solution to the original problem</li></ul></div><p>A popular open source alternative to the proprietary MapReduce framework is <span class="strong"><strong>Apache Hadoop</strong></span>. The<a id="id1162" class="indexterm"/> Hadoop software comprises of the MapReduce concept, plus a distributed filesystem capable of storing large amounts of data across a cluster of computers.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note66"/>Note</h3><p>Packt Publishing has published a large number of books on Hadoop. To search current<a id="id1163" class="indexterm"/> offerings, visit <a class="ulink" href="https://www.packtpub.com/all/?search=hadoop">https://www.packtpub.com/all/?search=hadoop</a>.</p></div></div><p>Several R projects that provide an R interface to Hadoop are in development. The RHadoop project by Revolution Analytics provides an R interface to Hadoop. The project provides a <a id="id1164" class="indexterm"/>package, <code class="literal">rmr</code>, intended to be an easy <a id="id1165" class="indexterm"/>way for R developers to write MapReduce <a id="id1166" class="indexterm"/>programs. Another companion package, <code class="literal">plyrmr</code>, provides functionality <a id="id1167" class="indexterm"/>similar to the <code class="literal">dplyr</code> package to process large datasets. Additional RHadoop packages provide R functions to access Hadoop's distributed data stores.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note67"/>Note</h3><p>For more information<a id="id1168" class="indexterm"/> on the RHadoop project, see <a class="ulink" href="https://github.com/RevolutionAnalytics/RHadoop/wiki">https://github.com/RevolutionAnalytics/RHadoop/wiki</a>.</p></div></div><p>Another similar project is RHIPE by Saptarshi Guha, which attempts to bring Hadoop's divide and recombine philosophy into R by managing the communication between R and Hadoop.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note68"/>Note</h3><p>The <code class="literal">RHIPE</code> package<a id="id1169" class="indexterm"/> is not yet available at CRAN, but it can be built from the source available on the Web at <a class="ulink" href="http://www.datadr.org">http://www.datadr.org</a>.</p></div></div></div></div><div class="section" title="GPU computing"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec127"/>GPU computing</h2></div></div></div><p>An alternative to parallel processing uses a computer's <span class="strong"><strong>Graphics Processing Unit</strong></span> (<span class="strong"><strong>GPU</strong></span>) to increase the<a id="id1170" class="indexterm"/> speed of mathematical <a id="id1171" class="indexterm"/>calculations. A GPU is a specialized processor that is optimized to rapidly display images on a computer <a id="id1172" class="indexterm"/>screen. Because a computer often needs to display complex 3D graphics (particularly for video games), many GPUs use hardware designed for parallel processing and extremely efficient matrix and vector calculations. A side benefit is that they can be used to efficiently solve certain types of mathematical problems. Where a computer processor may have 16 cores, a GPU may have thousands.</p><div class="mediaobject"><img src="graphics/B03905_12_06.jpg" alt="GPU computing"/></div><p>The downside of<a id="id1173" class="indexterm"/> GPU computing is that it requires specific hardware that is not included in many computers. In most cases, a GPU from the manufacturer Nvidia is required, as they provide a proprietary framework called <span class="strong"><strong>Complete Unified Device Architecture</strong></span> (<span class="strong"><strong>CUDA</strong></span>) that makes the GPU programmable using<a id="id1174" class="indexterm"/> common languages such as C++.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note69"/>Note</h3><p>For more<a id="id1175" class="indexterm"/> information on Nvidia's role in GPU computing, go to <a class="ulink" href="http://www.nvidia.com/object/what-is-gpu-computing.html">http://www.nvidia.com/object/what-is-gpu-computing.html</a>.</p></div></div><p>The <code class="literal">gputools</code> package by Josh Buckner, Mark Seligman, and Justin Wilson implements several R <a id="id1176" class="indexterm"/>functions, such as matrix operations, clustering, and regression modeling using the Nvidia CUDA toolkit. The package requires a CUDA 1.3 or higher GPU and the installation of the Nvidia CUDA toolkit.</p></div><div class="section" title="Deploying optimized learning algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec128"/>Deploying optimized learning algorithms</h2></div></div></div><p>Some of the<a id="id1177" class="indexterm"/> machine learning algorithms covered in this book are able to work on extremely large<a id="id1178" class="indexterm"/> datasets with relatively minor modifications. For instance, it would be fairly straightforward to implement Naive Bayes or the Apriori algorithm using one of the data structures for big datasets described in the previous sections. Some types of learners, such as ensembles, lend themselves well to parallelization, because the work of each model can be distributed across processors or computers in a cluster. On the other hand, some require larger changes to the data or algorithm, or<a id="id1179" class="indexterm"/> need to be rethought altogether, before<a id="id1180" class="indexterm"/> they can be used with massive datasets.</p><p>The following sections examine packages that provide optimized versions of the learning algorithms we've worked with so far.</p><div class="section" title="Building bigger regression models with biglm"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec77"/>Building bigger regression models with biglm</h3></div></div></div><p>The <code class="literal">biglm</code> package by Thomas Lumley provides functions to train regression models on datasets<a id="id1181" class="indexterm"/> that<a id="id1182" class="indexterm"/> may be too large to fit into<a id="id1183" class="indexterm"/> memory. It works by using an iterative process in which the model is updated little by little using small chunks of data. In spite of it being a different approach, the results will be nearly identical to what would be obtained by running the conventional <code class="literal">lm()</code> function on the entire dataset.</p><p>For convenience while working with the largest datasets, the <code class="literal">biglm()</code> function allows the use of a SQL database in place of a data frame. The model can also be trained with chunks obtained from data objects created by the <code class="literal">ff</code> package described previously.</p></div><div class="section" title="Growing bigger and faster random forests with bigrf"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec78"/>Growing bigger and faster random forests with bigrf</h3></div></div></div><p>The <code class="literal">bigrf</code> package <a id="id1184" class="indexterm"/>by<a id="id1185" class="indexterm"/> Aloysius Lim implements the<a id="id1186" class="indexterm"/> training of random forests for classification and regression on datasets that are too large to fit into memory. It uses the <code class="literal">bigmemory</code> objects as described earlier in this chapter. For speedier forest growth, the package can be used with the <code class="literal">foreach</code> and <code class="literal">doParallel</code> packages described previously to grow trees in parallel.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note70"/>Note</h3><p>For more<a id="id1187" class="indexterm"/> information, including examples and Windows installation instructions, see the package's wiki, which is hosted on GitHub at <a class="ulink" href="https://github.com/aloysius-lim/bigrf">https://github.com/aloysius-lim/bigrf</a>.</p></div></div></div><div class="section" title="Training and evaluating models in parallel with caret"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec79"/>Training and evaluating models in parallel with caret</h3></div></div></div><p>The <code class="literal">caret</code> package<a id="id1188" class="indexterm"/> by <a id="id1189" class="indexterm"/>Max Kuhn (covered extensively in <a class="link" href="ch10.html" title="Chapter 10. Evaluating Model Performance">Chapter 10</a>, <span class="emphasis"><em>Evaluating Model Performance</em></span> and <a class="link" href="ch11.html" title="Chapter 11. Improving Model Performance">Chapter 11</a>, <span class="emphasis"><em>Improving Model Performance</em></span>) will transparently utilize a parallel backend if one has been registered with R using the <code class="literal">foreach</code> package described previously.</p><p>Let's take a look at a simple example in which we attempt to train a random forest model on the credit dataset. Without parallelization, the model takes about 109 seconds to be trained:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(caret)</strong></span>
<span class="strong"><strong>&gt; credit &lt;- read.csv("credit.csv")</strong></span>
<span class="strong"><strong>&gt; system.time(train(default ~ ., data = credit, method = "rf"))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>107.862   0.990 108.873</strong></span>
</pre></div><p>On the <a id="id1190" class="indexterm"/>other <a id="id1191" class="indexterm"/>hand, if we use the <code class="literal">doParallel</code> package to register the four cores to be used in parallel, the model takes under 32 seconds to build—less than a third of the time—and we didn't need to change even a single line of the <code class="literal">caret</code> code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; library(doParallel)</strong></span>
<span class="strong"><strong>&gt; registerDoParallel(cores = 4)</strong></span>
<span class="strong"><strong>&gt; system.time(train(default ~ ., data = credit, method = "rf"))</strong></span>
<span class="strong"><strong>   user  system elapsed </strong></span>
<span class="strong"><strong>114.578   2.037  31.362</strong></span>
</pre></div><p>Many of the tasks involved in training and evaluating models, such as creating random samples and repeatedly testing predictions for 10-fold cross-validation are embarrassingly parallel and ripe for performance improvements. With this in mind, it is wise to always register multiple cores before beginning a <code class="literal">caret</code> project.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note71"/>Note</h3><p>Configuration instructions and a case study of the performance improvements needed to enable <a id="id1192" class="indexterm"/>parallel processing in <code class="literal">caret</code> are available on the project's website at <a class="ulink" href="http://topepo.github.io/caret/parallel.html">http://topepo.github.io/caret/parallel.html</a>.</p></div></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec54"/>Summary</h1></div></div></div><p>It is certainly an exciting time to be studying machine learning. Ongoing work on the relatively uncharted frontiers of parallel and distributed computing offers great potential for tapping the knowledge found in the deluge of big data. The burgeoning data science community is facilitated by the free and open source R programming language, which provides a very low barrier for entry—you simply need to be willing to learn.</p><p>The topics you have learned, both in this chapter and in the previous chapters, provide the foundation to understand more advanced machine learning methods. It is now your responsibility to keep learning and adding tools to your arsenal. Along the way, be sure to keep in mind the <span class="emphasis"><em>No Free Lunch</em></span> theorem—no learning algorithm can rule them all, and they all have varying strengths and weaknesses. For this reason, there will always be a human element to machine learning, adding subject-specific knowledge and the ability to match the appropriate algorithm to the task at hand.</p><p>In the coming years, it will be interesting to see how the human side changes as the line between machine learning and human learning is blurred. Services such as Amazon's Mechanical Turk provide crowd-sourced intelligence, offering a cluster of human minds ready to perform simple tasks at a moment's notice. Perhaps one day, just as we have used computers to perform tasks that human beings cannot do easily, computers will employ human beings to do the reverse. What interesting food for thought!</p></div></body></html>
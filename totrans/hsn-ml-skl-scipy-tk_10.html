<html><head></head><body>
    <section>
      <header class="header-title chapter-title">
                    Ensembles – When One Model Is Not Enough
                </header>
      <article>
        <p>In the previous three chapters, we saw how <strong>neural networks</strong> help directly and indirectly in solving natural language understanding and image processing problems. This is because neural networks are proven to work well with <strong>homogeneous data</strong>; that is, if all the input features are of the same breed—pixels, words, characters, and so on. On the other hand, when it comes to <strong>heterogeneous</strong><strong>data</strong>, it is the <strong>ensemble</strong><strong>methods</strong> that are known to shine. They are well suited to deal with heterogeneous data—for example, where one column contains users' ages, the other has their incomes, and a third has their city of residence. </p>
        <p>You can view ensemble estimators as meta-estimators; they are made up of multiple instances of other estimators. The way they combine their underlying estimators is what differentiates between the different ensemble methods—for example, the <strong>bagging</strong> versus the <strong>boosting</strong> methods. In this chapter, we are going to look at these methods in detail and understand their underlying theory. We will also learn how to diagnose our own models and understand why they make certain decisions. </p>
        <p>As always, I would also like to seize the opportunity to shed light on general machine learning concepts while dissecting each individual algorithm. In this chapter, we will see how to handle the estimators' uncertainties using the classifiers' probabilities and the regression ranges. </p>
        <p>The following topics will be discussed in this chapter:</p>
        <ul>
          <li>The motivation behind ensembles</li>
          <li>Averaging/bagging ensembles</li>
          <li>Boosting ensembles</li>
          <li>Regression ranges</li>
          <li>The ROC curve</li>
          <li>Area under the curve</li>
          <li>Voting and stacking ensembles</li>
        </ul>
        <h1 id="uuid-1b8a728b-2533-4794-b5f6-03aad19137b5">Answering the question why ensembles? </h1>
        <p class="mce-root">The main idea behind ensembles is to combine multiple estimators so that they make better predictions than a single estimator. However, you should not expect the mere combination of multiple estimators to just lead to better results. The combined predictions of multiple estimators who make the exact same mistakes will be as wrong as each individual estimator in the group. Therefore, it is helpful to think of the possible ways to mitigate the mistakes that individual estimators make. To do so, we have to revisit our old friend the bias and variance dichotomy. We will meet few machine learning teachers better than this pair. </p>
        <p>If you recall from <a href="66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml">Chapter 2</a>,<em> Making Decisions with Trees</em>, when we allowed our decision trees to grow as much as they can, they tended to fit the training data like a glove but failed to generalize to newer data points. We referred to this as overfitting, and we have seen the same behavior with unregularized linear models and with a small number of nearest neighbors. Conversely, aggressively restricting the growth of trees, limiting the number of the features in linear models, and asking too many neighbors to vote caused the models to become biased and underfit the data at hand. So, we had to tread a thin line between trying to find the optimum balance between the bias-variance and the underfitting-overfitting dichotomies.</p>
        <p>In the following sections, we are going to follow a different approach. We will deal with the bias-variance dichotomy as a continuous scale, starting from one side of this scale and using the concept of <em>ensemble</em> to move toward the other side. In the next section, we are going to start by looking at high-variance estimators and averaging their results to reduce their variance. Later on, we will start from the other side and use the concept of boosting to reduce the estimators' biases.   </p>
        <h2 id="uuid-245131af-3e95-4194-8912-28f7cfed5280">Combining multiple estimators via averaging</h2>
        <div class="packt_quote">"To derive the most useful information from multiple sources of evidence, you should always try to make these sources independent of each other."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– <span class="authorOrTitle">Daniel Kahneman</span></div>
        <p>If a single fully grown decision tree overfits, and if having many voters in the nearest neighbors algorithm has an opposite effect, then why not combine the two concepts? Rather than having a single tree, let's have a forest that combines the predictions of each tree in it. Nevertheless, we do not want all the trees in our forest to be identical; we would love them to be as diverse as possible. The <strong>bagging</strong> and random forest meta-estimators are the most common examples here. To achieve diversity, they make sure that each one of the individual estimators they use is trained on a random subset of the training data—hence the <em>random</em> prefix in random forest. Each time a random sample is drawn, it can be done with replacement (<strong>bootstrapping</strong>) or without replacement (<strong>pasting</strong>). The term bagging stands for <strong>bootstrap aggregation</strong> as the estimators draw their samples with replacement. Furthermore, for even more diversity, the ensembles can assure that each tree sees a random subset of the training features.</p>
        <p>Both ensembles use decision tree estimators by default, but the <strong>bagging </strong>ensemble can be reconfigured to use any other estimator. Ideally, we would like to use high-variance estimators. The decisions made by the individual estimators are combined via voting or averaging. </p>
        <h2 id="uuid-413f17d9-ae27-4578-9327-cd64ba230be0">Boosting multiple biased estimators </h2>
        <div class="b-qt qt_135885 packt_quote">"If I have seen further than others, it is by standing upon the shoulders of giants."</div>
        <div class="b-qt qt_135885 packt_quote CDPAlignRight CDPAlign">–Isaac Newton</div>
        <p>In contrast to fully grown trees, a shallow tree tends to be biased. Boosting a biased estimator is commonly performed via <strong>AdaBoost</strong> or <strong>gradient boosting</strong>. The AdaBoost meta-estimator starts with a weak or biased estimator, then each consequent estimator learns from the mistakes made by its predecessors. We saw in <a href="66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml">Chapter 2</a>,<em> Making Decisions with Trees</em>, that we can give each individual training sample a different weight so that the estimators can give more emphasis to some samples versus others. In <strong>AdaBoost</strong>, erroneous predictions made by the preceding estimators are given more weight for their successors to pay more attention to. </p>
        <p>The <strong>gradient boosting</strong> meta-estimator follows a slightly different approach. It starts with a biased estimator, computes its loss function, then builds each consequent estimator to minimize the loss function of its predecessors. As we saw earlier, gradient descent always comes in handy when iteratively minimizing loss functions, hence the <em>gradient</em> prefix in the name of the gradient boosting algorithm.</p>
        <p>Due to the iterative nature of the two ensembles, they both have a learning rate to control their learning speed and to make sure they don't miss the local minima when converging. Like the <strong>bagging</strong> algorithm, <strong>AdaBoost</strong> is not limited to decision trees as its base estimator. </p>
        <p>Now that we have a good idea about the different ensemble methods, we can use real-life data to demonstrate how they work in practice. Each of the ensemble methods described here can be used for classification and regression. The classifier and regressor hyperparameters are almost identical for each ensemble. Therefore, I will pick a regression problem to demonstrate each algorithm and briefly show the classification capabilities of the random forest and gradient boosting algorithms since they are the most commonly used ensembles.</p>
        <p>In the next section, we are going to download a dataset prepared by the<strong>University of California, Irvine</strong> (<strong>UCI</strong>). It contains 201 samples for different cars, along with their prices. We will be using this dataset in a later section to predict the car prices via regression. </p>
        <h1 id="uuid-399214d4-4d69-4a36-a191-56b4434131c6">Downloading the UCI Automobile dataset</h1>
        <p>The Automobile dataset was created by Jeffrey C. Schlimmer and published in UCI's machine learning repository. It contains information about 201 automobiles, along with their prices. The names of the features are missing. Nevertheless, I could get them from the dataset's description (<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names">http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names</a>). So, we can start by seeing the URL and the feature names, as follows:</p>
        <pre>url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'<br/><br/>header = [<br/>    'symboling',<br/>    'normalized-losses',<br/>    'make',<br/>    # ... some list items are omitted for brevity <br/>    'highway-mpg',<br/>    'price',<br/><br/>]</pre>
        <p>Then, we use the following code to download our data.</p>
        <pre>df = pd.read_csv(url, names=header, na_values='?')</pre>
        <p>It is mentioned in the dataset's description that missing values are replaced with a question mark. To make things more Pythonic, we set <kbd>na_values</kbd> to <kbd>'?'</kbd> to replace these question marks with NumPy's <strong>Not a Number</strong>(<strong>NaN</strong>). </p>
        <p>Next, we can perform our <strong>Exploratory Data Analysis</strong> (<strong>EDA</strong>), check the percentages of the missing values, and see how to deal with them.</p>
        <h2 id="uuid-60a9c623-4fea-4231-99ba-15e8b939c5d0">Dealing with missing values</h2>
        <p>Now, we can check which columns have the most missing values:</p>
        <pre>cols_with_missing = df.isnull().sum()<br/>cols_with_missing[<br/>    cols_with_missing &gt; 0<br/>]</pre>
        <p>This gives us the following list:</p>
        <pre>normalized-losses    41
num-of-doors          2
bore                  4
stroke                4
horsepower            2
peak-rpm              2
price                 4</pre>
        <p>Since the price is our target value, we can just ignore the four records where the prices are unknown:</p>
        <pre>df = df[~df['price'].isnull()]</pre>
        <p class="mce-root">As for the remaining features, I'd say let's drop the<kbd>normalized-losses</kbd> column since 41 of its values are missing. Later on, we will use the data imputation techniques to deal with the other columns with fewer missing values. You can drop the <kbd>normalized-losses</kbd> column using the following code:</p>
        <pre>df.drop(labels=['normalized-losses'], axis=1, inplace=True)</pre>
        <p>At this point, we have a data frame with all the required features and their names. Next, we want to split the data into training and test sets, and then prepare our features. The different feature types require different preparations. You may need to separately scale the numerical features and encode the categorical ones. So, it is good practice to be able to differentiate between the numerical and the categorical features. </p>
        <h2 id="uuid-17346904-a77b-41b9-a4cd-60362daa036c">Differentiating between numerical features and categorical ones</h2>
        <p>Here, we are going to create a dictionary to separately list the numerical and categorical features. We will also make a combined list of the two, and provide the name of the target column, as in the following code:</p>
        <pre>features = {<br/>    'categorical': [<br/>        'make', 'fuel-type', 'aspiration', 'num-of-doors', <br/>        'body-style', 'drive-wheels', 'engine-location', <br/>        'engine-type', 'num-of-cylinders', 'fuel-system',<br/><br/>    ],<br/>    'numerical': [<br/>        'symboling', 'wheel-base', 'length', 'width', 'height', <br/>        'curb-weight', 'engine-size', 'bore', 'stroke', <br/>        'compression-ratio', 'horsepower', 'peak-rpm', <br/>        'city-mpg', 'highway-mpg', <br/>    ],<br/>}<br/><br/>features['all'] = features['categorical'] + features['numerical']<br/><br/>target = 'price'</pre>
        <p>By doing so, you can deal with the columns differently. Furthermore, just for my own sanity and to notprint too many zeros in the future, I rescaled the prices to be in thousands, as follows:</p>
        <pre>df[target] = df[target].astype(np.float64) / 1000</pre>
        <p>You can also display certain features separately. Here, we print a random sample, where just the categorical features are shown: </p>
        <pre> df[features['categorical']].sample(n=3, random_state=42)</pre>
        <p>Here are the resulting rows. I set <kbd>random_state</kbd> to <kbd>42</kbd> to make sure we all get the same random rows:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/806e2ee8-cfdf-4819-b53d-a58a4a38dd61.png" style="width:82.75em;"/>
        </p>
        <p>All other transformations, such as scaling, imputing, and encoding, should be done after splitting the data into training and test sets. That way, we can ensure that no information is leaked from the test set into the training samples.</p>
        <h2 id="uuid-b4af7df0-c0cc-4831-971d-72107d018179">Splitting the data into training and test sets</h2>
        <p>Here, we keep 25% of the data for testing and use the rest for training:</p>
        <pre>from sklearn.model_selection import train_test_split<br/>df_train, df_test = train_test_split(df, test_size=0.25, random_state=22)</pre>
        <p>Then, we can use the information from the previous section to create our <kbd>x</kbd> and <kbd>y</kbd> values:</p>
        <pre>x_train = df_train[features['all']]<br/>x_test = df_test[features['all']]<br/><br/>y_train = df_train[target]<br/>y_test = df_test[target]</pre>
        <p>As usual, with regression tasks, it is handy to understand the distribution of the target values:</p>
        <pre>y_train.plot(<br/>    title="Distribution of Car Prices (in 1000's)",<br/>    kind='hist', <br/>)</pre>
        <p>A histogram is usually a good choice for understanding distributions, as seen in the following graph:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/7e1e4eaa-47ca-4d98-a27d-299eb324a237.png"/>
        </p>
        <p>We may come back to this distribution later to put our regressor's mean error in perspective. Additionally, you can use this range for sanity checks. For example, if you know that all the prices you have seen fall in the range of 5,000 to 45,000, you may decide when to put your model in production to fire an alert any time it returns prices far from this range. </p>
        <h2 id="uuid-13e4987b-350a-4432-9b19-faa1e3d70425">Imputing the missing values and encoding the categorical features</h2>
        <p>Before bringing our ensembles to action, we need to make sure we do not have null values in our data. We will replace the missing values with the most frequent value in each column using the <kbd>SimpleImputer</kbd> function from<a href="f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml">Chapter 4</a>,<em> Preparing Your Data</em>:</p>
        <pre>from sklearn.impute import SimpleImputer<br/>imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')<br/><br/>x_train = imp.fit_transform(x_train)<br/>x_test = imp.transform(x_test)</pre>
        <p>You may have already seen me complain many times about the scikit-learn transformers, which do not respect the column names and insist on converting the input data frames into NumPy arrays. To stop myself from complaining again, let me solve my itch by using the following <kbd>ColumnNamesKeeper</kbd> class. Whenever I wrap it around a transformer, it will make sure all the data frames are kept unharmed: </p>
        <pre>class ColumnNamesKeeper:<br/><br/>    def __init__(self, transformer):<br/>        self._columns = None<br/>        self.transformer = transformer<br/><br/>    def fit(self, x, y=None):<br/>        self._columns = x.columns<br/>        self.transformer.fit(x)<br/><br/>    def transform(self, x, y=None):<br/>        x = self.transformer.transform(x)<br/>        return pd.DataFrame(x, columns=self._columns)<br/><br/>    def fit_transform(self, x, y=None):<br/>        self.fit(x, y)<br/>        return self.transform(x)</pre>
        <p>As you can see, it mainly saves the column name when the <kbd>fit</kbd> method is called. Then, we can use the saved names to recreate the data frames after the transformation steps. </p>
        <div class="packt_tip">The code for <kbd>ColumnNamesKeeper</kbd> can be simplified further by inheriting from <kbd>sklearn.base.BaseEstimator</kbd> and <kbd>sklearn.base.TransformerMixin</kbd>. You can check the source code of any of the library's built-in transformers if you are willing to write more scikit-learn-friendly transformers.</div>
        <p>Now, I can call <kbd>SimpleImputer</kbd> again while preserving <kbd>x_train</kbd> and <kbd>x_test</kbd> as data frames:</p>
        <pre>from sklearn.impute import SimpleImputer<br/><br/>imp = ColumnNamesKeeper(<br/>    SimpleImputer(missing_values=np.nan, strategy='most_frequent')<br/>)<br/><br/>x_train = imp.fit_transform(x_train)<br/>x_test = imp.transform(x_test)</pre>
        <p>We learned in <a href="f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml">Chapter 4</a>,<em> Preparing Your Data</em>, that <kbd>OrdinalEncoder</kbd><em><strong/></em>is recommended for tree-based algorithms, in addition to any other non-linear algorithms. The <kbd>category_encoders</kbd> library doesn't mess with the column names, and so we can use <kbd>OrdinalEncoder</kbd> without the need for<kbd>ColumnNamesKeeper</kbd> this time. In the following code snippet, we also specify which columns to encode (the categorical columns) and which to keep unchanged (the remaining ones):</p>
        <pre>from category_encoders.ordinal import OrdinalEncoder<br/>enc = OrdinalEncoder(<br/>    cols=features['categorical'],<br/>    handle_unknown='value'<br/>)<br/>x_train = enc.fit_transform(x_train)<br/>x_test = enc.transform(x_test)</pre>
        <p>In addition to <kbd>OrdinalEncoder</kbd>, you can also test the encoders mentioned in the target encoding in <a href="f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml">Chapter 4</a><em>, Preparing Your Data</em>. They, too, are meant to be used with the algorithms explained in this chapter. In the next section, we are going to use the random forest algorithm with the data we have just prepared. </p>
        <h1 id="uuid-507fd601-bbde-4743-8186-409e59e2f6e5">Using random forest for regression</h1>
        <p>The random forest algorithm is going to be the first ensemble to deal with here. It's an easy-to-grasp algorithm with straightforward hyperparameters. Nevertheless, as we usually do, we will start by training the algorithm using its default values, as follows, then explain its hyperparameters after that:</p>
        <pre>from sklearn.ensemble import RandomForestRegressor<br/>rgr = RandomForestRegressor(n_jobs=-1)<br/>rgr.fit(x_train, y_train)<br/>y_test_pred = rgr.predict(x_test)</pre>
        <p>Since each tree is independent of the others, I set <kbd>n_jobs</kbd> to <kbd>-1</kbd> to use my multiple processors to train the trees in parallel. Once they are trained and the predictions are obtained, we can print the following accuracy metrics:</p>
        <pre>from sklearn.metrics import (<br/>    mean_squared_error, mean_absolute_error, median_absolute_error, r2_score<br/>)<br/><br/>print(<br/>    'R2: {:.2f}, MSE: {:.2f}, RMSE: {:.2f}, MAE {:.2f}'.format(<br/>        r2_score(y_test, y_test_pred),<br/>        mean_squared_error(y_test, y_test_pred),<br/>        np.sqrt(mean_squared_error(y_test, y_test_pred)),<br/>        mean_absolute_error(y_test, y_test_pred),<br/>    )<br/>)</pre>
        <p>This will print the following scores:</p>
        <pre># R2: 0.90, MSE: 4.54, RMSE: 2.13, MAE 1.35</pre>
        <p>The average car price is 13,400. So, a <strong>Mean Absolute Error</strong> (<strong>MAE</strong><em>)</em> of <kbd>1.35</kbd> seems reasonable. As for the <strong>Mean Squared Error </strong>(<strong>MSE</strong>), it makes sense to use its square root to keep it in the same units as the MAE. In brief, given the high R<sup>2</sup> score and the low errors, the algorithm seems to perform well with its default values. Furthermore, you can plot the errors to get a better understanding of the model's performance:  </p>
        <pre>df_pred = pd.DataFrame(<br/>    {<br/>        'actuals': y_test,<br/>        'predictions': y_test_pred,<br/>    }<br/>)<br/><br/>df_pred['error'] = np.abs(y_test - y_test_pred)<br/><br/>fig, axs = plt.subplots(1, 2, figsize=(16, 5), sharey=False)<br/><br/>df_pred.plot(<br/>    title='Actuals vs Predictions',<br/>    kind='scatter',<br/>    x='actuals',<br/>    y='predictions',<br/>    ax=axs[0],<br/>)<br/><br/>df_pred['error'].plot(<br/>    title='Distribution of Error',<br/>    kind='hist',<br/>    ax=axs[1],<br/>)<br/><br/>fig.show()</pre>
        <p>I've excluded some of the formatting lines to keep the code concise. In the end, we get the following graphs:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/8d5c005a-a4d0-47be-b420-cdd3984f4ac7.png" style="width:73.58em;"/>
        </p>
        <p>By plotting the predictions versus the actuals, we can make sure that the models don't systematically overestimate or underestimate. This is shown via the 45<sup>o</sup> slope of the scattered points on the left. A lower slope for the scattered points would have systematically reflected an underestimation. Having the scattered points aligned on a straight line assures us that there aren't non-linearities that the model couldn't capture. The histogram to the right shows that most of the errors are below 2,000. It is good to understand what mean and maximum errors you can expect to get in the future.  </p>
        <h2 id="uuid-80ad38ff-d952-40e4-b4a0-5149ea79e2e3">Checking the effect of the number of trees</h2>
        <p>By default, each tree is trained on a random sample from the training data. This is achieved by setting the <kbd>bootstrap</kbd> hyperparameter to <kbd>True</kbd>. In bootstrap sampling, a sample may be used during training more than once, while another sample may not be used at all.</p>
        <p>When <kbd>max_samples</kbd> is kept as <kbd>None</kbd>, each tree is trained on a random sample of a size that is equal to the entire training data size. You can set <kbd>max_samples</kbd> to a fraction that is less than 1, then each tree is trained on a smaller random sub-sample. Similarly, we can set <kbd>max_features</kbd> to a fraction that is less than 1 to make sure each tree uses a random subset of the available features. These parameters help each tree to have its own personality and to ensure the diversity of the forest. To put it more formally, these parameters increase the variance of each individual tree. So, it is advised to have as many trees as possible to reduce the variance we have just introduced.</p>
        <p>Here, we compare three forests, with a different number of trees in each:</p>
        <pre>mae = []<br/>n_estimators_options = [5, 500, 5000]<br/><br/>for n_estimators in n_estimators_options:<br/><br/>    rgr = RandomForestRegressor(<br/>        n_estimators=n_estimators,<br/>        bootstrap=True,<br/>        max_features=0.75,<br/>        max_samples=0.75,<br/>        n_jobs=-1,<br/>    )<br/><br/>    rgr.fit(x_train, y_train)<br/>    y_test_pred = rgr.predict(x_test)<br/>    mae.append(mean_absolute_error(y_test, y_test_pred))</pre>
        <p>Then, we can plot the MAE for each forest to see the merits of having more trees: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/476a677f-6df2-4dea-8fa5-d6a7545117d0.png" style="width:67.08em;"/>
        </p>
        <p>Clearly, we have just encountered a new set of hyperparameters to tune <kbd>bootstrap</kbd>, <kbd>max_features</kbd>, and <kbd>max_samples</kbd>. So, it makes sense to apply cross-validation for hyperparameter tuning. </p>
        <h2 id="uuid-8eeee25e-3068-4558-ade8-84375ec3f057">Understanding the effect of each training feature</h2>
        <p>Once a random forest is trained, we can list the training features, along with their importance. As usual, we put the outcome in a data frame by using the column names and the <kbd>feature_importances_</kbd> attribute, as shown:</p>
        <pre>df_feature_importances = pd.DataFrame(<br/>    {<br/>        'Feature': x_train.columns,<br/>        'Importance': rgr.feature_importances_,<br/>    }<br/>).sort_values(<br/>    'Importance', ascending=False<br/>)</pre>
        <p>Here is the resulting data frame:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/5c526a3c-1ceb-4c52-87a5-4a83419a4181.png" style="width:13.83em;"/>
        </p>
        <p>Unlike with linear models, all the values here are positive. This is because these values only show the importance of each feature, regardless of whether it is positively or negatively correlated with the target. This is common for decision trees, as well as for tree-based ensembles. Thus, we can use <strong>Partial Dependence Plots</strong>(<strong>PDPs</strong>)to show the relationship between the target and the different features. Here, we only plot it for the top six features according to their importance: </p>
        <pre>from sklearn.inspection import plot_partial_dependence<br/><br/>fig, ax = plt.subplots(1, 1, figsize=(15, 7), sharey=False)<br/><br/>top_features = df_feature_importances['Feature'].head(6)<br/><br/>plot_partial_dependence(<br/>    rgr, x_train, <br/>    features=top_features,<br/>    n_cols=3, <br/>    n_jobs=-1,<br/>    line_kw={'color': 'k'},<br/>    ax=ax<br/>) <br/><br/>ax.set_title('Partial Dependence')<br/><br/>fig.show()</pre>
        <p class="CDPAlignLeft CDPAlign">The resulting graphs are easier to read, especially when the relationship between the target and the features is non-linear:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/b3eadf45-3663-4c33-8c33-b1c380f5ddf6.png" style="width:68.92em;"/>
        </p>
        <p>We can now tell that cars with bigger engines, more horsepower, and less mileage per gallon tend to be more expensive. </p>
        <div class="packt_infobox">PDPs are not just useful for ensemble methods, but also for any other complex non-linear model. Despite the fact the neural networks have coefficients for each layer, the PDP is essential in understanding the network as a whole. Furthermore, you can also understand the interaction between the different feature pairs by passing the list of features as a list tuples, with a pair of features in each tuple. </div>
        <h1 id="uuid-b6ac9146-80b7-42d9-bb2e-c91d415f569c">Using random forest for classification</h1>
        <p>To demonstrate the random forest classifier, we are going to use a synthetic dataset. We first create the dataset using the built-in <kbd>make_hastie_10_2</kbd> class:</p>
        <pre>from sklearn.datasets import make_hastie_10_2<br/>x, y = make_hastie_10_2(n_samples=6000, random_state=42)</pre>
        <p>This previous code snippet creates a random dataset. I set <kbd>random_state</kbd> to a fixed number to make sure we both get the same random data. Now, we can split the resulting data into training and test sets:</p>
        <pre>from sklearn.model_selection import train_test_split<br/>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)</pre>
        <p>Then, to evaluate the classifier, we are going to introduce a new concept called the <strong>Receiver Operating Characteristic</strong> (<strong>ROC</strong>) curve in the next section.</p>
        <h2 id="uuid-bdfc7a3f-b2ad-4afb-9d2b-9fc6c5bed9a7">The ROC curve</h2>
        <div class="packt_quote">"Probability is expectation founded upon partial knowledge. A perfect acquaintance with all the circumstances affecting the occurrence of an event would change expectation into certainty, and leave neither room nor demand for a theory of probabilities."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">– George Boole (Boolean data types are named after him)</div>
        <p>In a classification problem, the classifier assigns probabilities to each sample to reflect how likely it is that each sample belongs to a certain class. We get these probabilities via the classifier's <kbd>predict_proba()</kbd> method. The <kbd>predict()</kbd> method is typically a wrapper on top of the <kbd>predict_proba()</kbd> method. In a binary-classification problem, it assigns each sample to a specific class if the probability of it belonging to the class is above 50%. In practice, we may not always want to stick to this 50% threshold, especially as different thresholds usually change the <strong>T</strong><strong>rue Positive Rates</strong> (<strong>TPRs</strong>) and <strong>False Positive Rates</strong> (<strong>FPRs</strong>) for each class. So, you can choose a different threshold to optimize for a desired TPR. </p>
        <p>The best way to decide which threshold suits your needs is to use a ROC curve. This helps us see the TPR and FPR for each threshold. To create this curve, we will train our random forest classifier on the synthetic dataset we have just created, but get the classifier's probabilities this time: </p>
        <pre>from sklearn.ensemble import RandomForestClassifier<br/><br/>clf = RandomForestClassifier(<br/>    n_estimators=100,<br/>    oob_score=True,<br/>    n_jobs=-1,<br/>)<br/><br/>clf.fit(x_train, y_train)<br/>y_pred_proba = clf.predict_proba(x_test)[:,1]</pre>
        <p>Then, we can calculate the TPR and FPR for each threshold, as follows:</p>
        <pre>from sklearn.metrics import roc_curve<br/>fpr, tpr, thr = roc_curve(y_test, y_pred_proba)</pre>
        <p>Let's stop for a moment to explain what TPR and FPR mean: </p>
        <ul>
          <li>The<strong>TPR</strong>, also known as <strong>recall</strong> or <strong>sensitivity</strong>, is calculated as the number of <strong>True Positive</strong> (<strong>TP</strong>) cases divided by all the positive cases; that is, <img class="fm-editor-equation" src="assets/d14315f8-a807-4550-9467-be22f09a947b.png" style="width:7.50em;"/>, where <em>FN</em> is the positive cases falsely classified as negative (false negatives).</li>
          <li>The <strong>True Negative Rates</strong> (<strong>TNR</strong>), also known as <strong>specificity</strong>, is calculated as the number of <strong>True Negative</strong> (<strong>TN</strong>) cases divided by all the negative cases; that is, <img class="fm-editor-equation" src="assets/edcaf90b-1764-47ed-84bc-e42e16bfc779.png" style="width:8.25em;"/>, where <em>FP</em> is the negative cases falsely classified as positive (false positives).</li>
          <li>The <strong>FPR</strong> is defined as 1 minus TNR; that is, <img style="font-size: 1em;color: #333333;width:8.00em;" class="fm-editor-equation" src="assets/a73fab77-1996-4d38-88fc-669a544b5249.png"/>.</li>
          <li>The <strong>False Negative Rate</strong> (<strong>FNR</strong>) is defined as 1 minus TPR; that is, <img style="font-size: 1em;color: #333333;width:8.00em;" class="fm-editor-equation" src="assets/1d8f94e7-78b4-4197-8feb-678071889616.png"/>.</li>
        </ul>
        <p>Now, we can put the calculated TPR and FPR for our dataset into the following table:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/0abc76af-774e-4e2d-99b5-294fb69ab66c.png" style="width:26.00em;"/>
        </p>
        <p>Even better than a table, we can plot them into a graph using the following code:</p>
        <pre>pd.DataFrame(<br/>    {'FPR': fpr, 'TPR': tpr}<br/>).set_index('FPR')['TPR'].plot(<br/>    title=f'Receiver Operating Characteristic (ROC)',<br/>    label='Random Forest Classifier',<br/>    kind='line',<br/>)</pre>
        <p>I've omitted the graph's styling code for the sake of brevity. I also added a 45<sup>o</sup> line and the <strong>Area Under the Curve</strong> (<strong>AUC</strong>), which I will explain in a bit:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/c989db9d-5d8a-4d47-a9a1-8d3b031676ed.png" style="width:35.00em;"/>
        </p>
        <p>A classifier that randomly assigns each sample to a certain class will have a ROC curve that looks like the dashed 45<sup>o</sup> line. Any improvement over this will make the curve more convex upward. Obviously, random forest's ROC curve is better than chance. An optimum classifier will touch the upper-left corner of the graph. Therefore, the AUC can be used to reflect how good the classifier is. An area above <kbd>0.5</kbd> is better than chance, and an area of <kbd>1.0</kbd> is the best possible value. We typically expect values between <kbd>0.5</kbd> and <kbd>1.0</kbd>. Here, we got an AUC of <kbd>0.94</kbd>. The AUC can be calculated using the following code:</p>
        <pre>from sklearn.metrics import auc<br/>auc_values = auc(fpr, tpr)</pre>
        <p>We can also use the ROC and AUC to compare two classifiers. Here, I trained the random forest classifier with the <kbd>bootstrap</kbd> hyperparameter set to <kbd>True</kbd> and compared it to the same classifier when <kbd>bootstrap</kbd> was set to <kbd>False</kbd>:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/0457044f-36f5-4ba0-a1ab-4ae6c68f4a9b.png" style="width:32.17em;"/>
        </p>
        <p>No wonder the <kbd>bootstrap</kbd> hyperparameter is set to <kbd>True</kbd> by default—it gives better results. Now, you have seen how to use random forest algorithms to solve classification and regression problems. In the next section, we are going to explain a similar ensemble: the bagging ensemble.</p>
        <h1 id="uuid-4f0580e0-8ea5-4e52-b96c-2db531eee4fe">Using bagging regressors</h1>
        <p>We will go back to the Automobile dataset as we are going to use the <strong>bagging regressor</strong> this time. The bagging meta-estimator is very similar to random forest. It is built of multiple estimators, each one trained on a random subset of the data using a bootstrap sampling method. The key difference here is that although decision trees are used as the base estimators by default, any other estimator can be used as well. Out of curiosity, let's use the <strong>K-Nearest Neighbors</strong> (<strong>KNN</strong>) regressor as our base estimator this time. However, we need to prepare the data to suit the new regressor's needs. </p>
        <h2 id="uuid-bbac6aa5-bc7d-4186-b139-2db7b08f1a8a">Preparing a mixture of numerical and categorical features</h2>
        <p>It is recommended to put all features on the same scale when using distance-based algorithms such as KNN<em>.</em> Otherwise, the effect of the features with higher magnitudes on the distance metric will overshadow the other features. As we have a mixture of numerical and categorical features here, we need to create two parallel pipelines to prepare each feature set separately.</p>
        <p>Here is a top-level view of our pipeline:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/306f62b0-7a03-40b8-bbd2-30a85bba31f4.png" style="width:47.42em;"/>
        </p>
        <p>Here, we start by building the four transformers in our pipelines: <kbd>Imputer</kbd>, <kbd>Scaler</kbd><strong>,</strong> and <kbd>OneHotEncoder</kbd>. We also wrap them in <kbd>ColumnNamesKeeper</kbd>, which we created earlier in this chapter:</p>
        <pre class="mce-root">from sklearn.impute import SimpleImputer<br/>from category_encoders.one_hot import OneHotEncoder<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.pipeline import Pipeline<br/><br/>numerical_mputer = ColumnNamesKeeper(<br/>    SimpleImputer(<br/>        missing_values=np.nan, <br/>        strategy='median'<br/>    )<br/>)<br/><br/>categorical_mputer = ColumnNamesKeeper(<br/>    SimpleImputer(<br/>        missing_values=np.nan, <br/>        strategy='most_frequent'<br/>    )<br/>)<br/><br/>minmax_scaler = ColumnNamesKeeper(<br/>    MinMaxScaler()<br/>) <br/><br/>onehot_encoder = OneHotEncoder(<br/>    cols=features['categorical'],<br/>    handle_unknown='value'<br/>)</pre>
        <p>Then, we put them into two parallel pipelines:</p>
        <pre class="mce-root">numerical_pipeline = Pipeline(<br/>    [<br/>        ('numerical_mputer', numerical_mputer), <br/>        ('minmax_scaler', minmax_scaler)<br/>    ]<br/>)<br/><br/>categorical_pipeline = Pipeline(<br/>    [<br/>        ('categorical_mputer', categorical_mputer), <br/>        ('onehot_encoder', onehot_encoder)<br/>    ]<br/>)</pre>
        <p>Finally, we concatenate the outputs of the pipelines for both the training and the test sets:</p>
        <pre>x_train_knn = pd.concat(<br/>    [<br/>        numerical_pipeline.fit_transform(df_train[features['numerical']]), <br/>        categorical_pipeline.fit_transform(df_train[features['categorical']]),<br/>    ],<br/>    axis=1<br/>)<br/><br/>x_test_knn = pd.concat(<br/>    [<br/>        numerical_pipeline.transform(df_test[features['numerical']]), <br/>        categorical_pipeline.transform(df_test[features['categorical']]),<br/>    ],<br/>    axis=1<br/>)</pre>
        <p>At this point, we are ready to build our bagged KNNs.</p>
        <h2 id="uuid-18a4c6dc-dd7d-443a-a4a1-872ec1307540">Combining KNN estimators using a bagging meta-estimator</h2>
        <p><kbd>BaggingRegressor</kbd> has a <kbd>base_estimator</kbd> hyperparameter, where you can set the estimators you want to use. Here, <kbd>KNeighborsRegressor</kbd> is used with a single neighbor. Since we are aggregating multiple estimators to reduce their variance, it makes sense to have a high variance estimator in the first place, hence the small number of neighbors here:</p>
        <pre class="mce-root">from sklearn.ensemble import BaggingRegressor<br/>from sklearn.neighbors import KNeighborsRegressor<br/><br/>rgr = BaggingRegressor(<br/>    base_estimator=KNeighborsRegressor(<br/>        n_neighbors=1<br/>    ),<br/>    n_estimators=400,<br/>)<br/><br/>rgr.fit(x_train_knn, df_train[target])<br/>y_test_pred = rgr.predict(x_test_knn)</pre>
        <p>This new setup gives us an MAE of <kbd>1.8</kbd>. We can stop here, or we may decide to improve the ensemble's performance by tuning its big array of hyperparameters.</p>
        <p>First of all, we can try different estimators other than KNN, each with its own hyperparameters. Then, the bagging ensemble also has its own hyperparameters. We can change the number of estimators via <kbd>n_estimators</kbd>. Then, we can decide whether to use the entire training set or a random subset of it for each estimator via <kbd>max_samples</kbd>. Similarly, we can also pick a random subset of the columns to use for each estimator to use via <kbd>max_features</kbd>. The choice of whether to use bootstrapping for the rows and the columns can be made via the <kbd>bootstrap</kbd> and <kbd>bootstrap_features</kbd>hyperparameters, respectively.</p>
        <p>Finally, since each estimator is trained separately, we can use a machine with a high number of CPUs and parallelize the training process by setting <kbd>n_jobs</kbd> to <kbd>-1</kbd>. </p>
        <p>Now that we have experienced two versions of the averaging ensembles, it is time to check their boosting counterparts. We will start with the gradient boosting ensemble, then move to the AdaBoost ensemble.</p>
        <h1 id="uuid-8178b9c0-9e45-4f75-91d5-713553efbc58">Using gradient boosting to predict automobile prices</h1>
        <p>If I were ever stranded on a desert island and had to pick one algorithm to take with me, I'd definitely chose the gradient boosting ensemble! It has proven to work very well on many classification and regression problems. We are going to use it with the same automobile data from the previous sections. The classifier and the regressor versions of this ensemble share the exact same hyperparameters, except for the loss functions they use. This means that everything we are going to learn here will be useful to us whenever we decide to use gradient boosting ensembles for classification. </p>
        <p>Unlike the averaging ensembles we have seen so far, the boosting ensembles build their estimators iteratively. The knowledge learned from the initial ensemble is used to build its successors. This is the main downside of boosting ensembles, where parallelism is unfeasible. Putting parallelism aside, this iterative nature of the ensemble calls for a learning rate to be set. This helps the gradient descent algorithm reach the loss function's minima easily. Here, we use 500 trees, each with a maximum of 3 nodes, and a learning rate of <kbd>0.01</kbd>. Furthermore, the <strong>Least Squares</strong> (<strong>LS</strong>) loss is used here; think MSE. More on the available loss functions in a moment: </p>
        <pre>from sklearn.ensemble import GradientBoostingRegressor<br/><br/>rgr = GradientBoostingRegressor(<br/>    n_estimators=1000, learning_rate=0.01, max_depth=3, loss='ls'<br/>)<br/><br/>rgr.fit(x_train, y_train)<br/>y_test_pred = rgr.predict(x_test)</pre>
        <p>This new algorithm gives us the following performance on the test set:</p>
        <pre># R2: 0.92, MSE: 3.93, RMSE: 1.98, MAE: 1.42</pre>
        <p>As you can see, this setting gave a lower MSE compared to random forest, while random forest had a better MAE. Another loss function that the gradient boosting<em><strong/></em>regressor can use is <strong>Least Absolute Deviation</strong> (<strong>LAD</strong>); think MAE, this time. LAD may help when dealing with outliers, and it can sometimes reduce the model's MAE performance on the test set. Nevertheless, it did not improve the MAE for the dataset at hand. We also have a percentile (quantile) loss, but before going deeper into the supported loss functions, we need to learn how to diagnose the learning process. </p>
        <p>The main hyperparameters to set here are the number of trees, the depth of the trees, the learning rate, and the loss function. As a rule of thumb, you should aim for a higher number of trees and a low learning rate. As we will see in a bit, these two hyperparameters are inversely proportional to each other. Controlling the depth of your trees is purely dependent on your data. In general, we need to have shallow trees and let boosting empower them. Nevertheless, the depth of the tree controls the number of feature interactions we want to capture. In a stub (a tree with a single split), only one feature can be learned at a time. A deeper tree resembles a nested <kbd>if</kbd> condition where a few more features are at play each time. I usually start with<kbd>max_depth</kbd> set to around <kbd>3</kbd> and <kbd>5</kbd> and tune it along the way. </p>
        <h2 id="uuid-a73b0d44-e758-467d-9b8f-25391dc2019d">Plotting the learning deviance</h2>
        <p>With each additional estimator, we expect the algorithm to learn more and the loss to decrease. Yet, at some point, the additional estimators will keep overfitting on the training data while not offering much improvement for the test data.</p>
        <p>To have a clear picture, we need to plot the calculated loss with each additional estimator for both the training and test sets. As for the training loss, it is saved by the gradient boosting meta-estimator into its <kbd>loss_</kbd> attribute. For the test loss, we can use the meta-estimator's <kbd>staged_predict()</kbd> methods. This method can be used for a given dataset to make predictions for each intermediate iteration.</p>
        <p>Since we have multiple loss functions to choose from, gradient boosting also provides a<kbd>loss_()</kbd> method, which calculates the loss for us based on the loss function used. Here, we create a new function to calculate the training and test errors for each iteration and put them into a data frame:</p>
        <pre>def calculate_deviance(estimator, x_test, y_test):<br/><br/>    train_errors = estimator.train_score_<br/>    test_errors = [<br/>        estimator.loss_(y_test, y_pred_staged) <br/>        for y_pred_staged in estimator.staged_predict(x_test)<br/>    ]<br/><br/>    return pd.DataFrame(<br/>        {<br/>            'n_estimators': range(1, estimator.estimators_.shape[0]+1),<br/>            'train_error': train_errors,<br/>            'test_error': test_errors,<br/>        }<br/>    ).set_index('n_estimators')</pre>
        <p>Since we are going to use an LS loss here, you can simply replace the <kbd>estimator.loss_()</kbd> method with <kbd>mean_squared_error()</kbd> and get the exact same results. But let's keep the <kbd>estimator.loss_()</kbd> function for a more versatile and reusable code. </p>
        <p>Next, we train our gradient boosting regressor, as usual:</p>
        <pre>from sklearn.ensemble import GradientBoostingRegressor<br/><br/>rgr = GradientBoostingRegressor(n_estimators=250, learning_rate=0.02, loss='ls')<br/>rgr.fit(x_train, y_train)</pre>
        <p>Then, we use the trained model, along with the test set, to plot the training and test learning deviance: </p>
        <pre>fig, ax = plt.subplots(1, 1, figsize=(16, 5), sharey=False)<br/><br/>df_deviance = calculate_deviance(rgr, x_test, y_test)<br/><br/>df_deviance['train_error'].plot(<br/>    kind='line', color='k', linestyle=':', ax=ax<br/>)<br/><br/>df_deviance['test_error'].plot(<br/>    kind='line', color='k', linestyle='-', ax=ax<br/>)<br/><br/>fig.show()</pre>
        <p>Running the code gives us the following graph:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/b3779f19-026c-41c1-b736-d59e91380aa9.png" style="width:56.17em;"/>
        </p>
        <p>The beauty of this graph is that it tells us that the improvements on the test set stopped after <kbd>120</kbd> estimators or so, despite the continuous improvement in the training set; that is, it started to overfit. Furthermore, we can use this graph to understand the effect of a chosen learning rate, as we did in <em><a href="7559b34f-080c-485d-b4bc-5f22580fc1d1.xhtml">Chapter 7</a></em>, <em>Neural Networks - Here comes the Deep Learning</em>. </p>
        <h2 id="uuid-18584ff7-d1c1-4aee-b915-7944fd0cb556">Comparing the learning rate settings</h2>
        <p>Rather than training one model, we will train three gradient boosting regressors this time, each with a different learning rate. Then, we will plot the deviance graph for each one side by side, as shown: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/6e63c903-27fd-44ab-a4d1-315f8b478d9a.png" style="width:55.83em;"/>
        </p>
        <p>As with other gradient descent-based models, a high learning rate causes the estimator to overshoot and miss the local minima. We can see this in the first graph where no improvements are seen despite the consecutive iterations. The learning rates in the second and third graphs seem reasonable. In comparison, the learning rate in the third graph seems to be too slow for the model to converge in 500 iterations. You may then decide to increase the number of estimators for the third model to allow it to converge. </p>
        <p>We have learned from the bagging ensembles that using a random training sample with each estimator may help with overfitting. In the next section, we are going to see whether the same approach can also help the boosting ensembles. </p>
        <h2 id="uuid-df629ee6-7c09-4dac-b5de-39e4e0a538a1">Using different sample sizes</h2>
        <p>We have been using the entire training set for each iteration. This time, we are going to train three gradient boosting regressors, each with a different subsample size, and plot their deviance graphs as before. We will use a fixed learning rate of <kbd>0.01</kbd> and the LAD<em><strong/></em>as our loss function, as shown:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/0b4eacd5-f8e0-41a7-9a38-895ae96e13a6.png" style="width:63.08em;"/>
        </p>
        <p>In the first graph, the entire training sample is used for each iteration. So, the training loss did not fluctuate as much as in the other two graphs. Nevertheless, the sampling used in the second model allowed it to reach a better test score, despite its noisy loss graph. This was similarly the case for the third model, but with a slightly larger final error.   </p>
        <h2 id="uuid-4d099fa2-e901-4c65-ba6b-5f9b17d888d6">Stopping earlier and adapting the learning rate</h2>
        <p>The <kbd>n_iter_no_change</kbd> hyperparameter is used to stop the training process after a certain number of iterations if the validation score is not improving enough. The subset set aside for validation, <kbd>validation_fraction</kbd>, is used to calculate the validation score. The <kbd>tol</kbd><strong/>hyperparameter is used to decide how much improvement we must consider as being enough. </p>
        <p>The <kbd>fit</kbd> method in the gradient boosting algorithm accepts a callback function that is called after each iteration. It can also be used to set a custom condition to stop the training process based on it. Furthermore, It can be used for monitoring or for any other customizations you need. This callback function is called with three parameters: the order of the current iteration (<kbd>n</kbd>), an instance of gradient boosting (<kbd>estimator</kbd>), and its settings (<kbd>params</kbd>). To demonstrate how this callback function works, let's build a function to change the learning rate to <kbd>0.01</kbd> for one iteration at every <kbd>10</kbd> iterations, and keep it at <kbd>0.1</kbd> for the remaining iterations, as shown:</p>
        <pre>def lr_changer(n, estimator, params):<br/>    if n % 10:<br/>        estimator.learning_rate = 0.01<br/>    else:<br/>        estimator.learning_rate = 0.1<br/>    return False</pre>
        <p>Then, we use our <kbd>lr_changer</kbd> function, as follows: </p>
        <pre>from sklearn.ensemble import GradientBoostingRegressor<br/>rgr = GradientBoostingRegressor(n_estimators=50, learning_rate=0.01, loss='ls')<br/>rgr.fit(x_train, y_train, monitor=lr_changer)</pre>
        <p>Now, if we print the deviance as we usually do, we will see how after every 10<sup>th</sup> iteration, the calculated loss jumps due to the learning rate changes:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/b94a9c30-3e42-4298-8ec8-63bdeb95e55e.png" style="width:51.00em;"/>
        </p>
        <p>What I've just done is pretty useless, but it demonstrates the possibilities you have at hand. For example, you can borrow ideas such as the adaptive learning rate and the momentum from the solvers used in the neural networks and incorporate them here using this callback function. </p>
        <h2 id="uuid-d8067977-49cc-440d-8f40-439e0d116242">Regression ranges </h2>
        <div class="packt_quote">"I try to be a realist and not a pessimist or an optimist."</div>
        <div class="packt_quote CDPAlignRight CDPAlign">–Yuval Noah Harari</div>
        <p>One last gem that gradient boosting regression offers to us is regression ranges. These are very useful in quantifying the uncertainty of your predictions.</p>
        <p> We try our best to make our predictions exactly the same as the actual data. Nevertheless, our data can still be noisy, or the features used may not capture the whole truth. Take the following example:</p>
        <table style="border-collapse: collapse;width: 429px;height: 168px" border="1">
          <tbody>
            <tr>
              <td style="width: 128px">
                <strong>x<sub>1</sub></strong>
              </td>
              <td style="width: 142px">
                <strong>x<sub>2</sub></strong>
              </td>
              <td style="width: 149px">
                <strong>y</strong>
              </td>
            </tr>
            <tr>
              <td style="width: 128px">0</td>
              <td style="width: 142px">0</td>
              <td style="width: 149px">10</td>
            </tr>
            <tr>
              <td style="width: 128px">1</td>
              <td style="width: 142px">1</td>
              <td style="width: 149px">50</td>
            </tr>
            <tr>
              <td style="width: 128px">0</td>
              <td style="width: 142px">0</td>
              <td style="width: 149px">20</td>
            </tr>
            <tr>
              <td style="width: 128px">0</td>
              <td style="width: 142px">0</td>
              <td style="width: 149px">22</td>
            </tr>
          </tbody>
        </table>
        <p class="mce-root"/>
        <p>Consider a new sample with <em>x<sub>1 </sub></em>= 0 and <em>x<sub>2 </sub></em>= 0. We already have three training examples with the exact same features, so what would the predicted <em>y</em> value be for this new sample? If a squared loss function is used during the training, then the predicted target will be close to <kbd>17.3</kbd>, which is the mean of the three corresponding targets<em/>(<kbd>10</kbd>, <kbd>20</kbd>, and <kbd>22</kbd>). Now, if MAE is used, then the predicted target will be closer to <kbd>22</kbd>, which is the median (50<sup>th</sup> percentile) of the three corresponding targets. Rather than the 50<sup>th</sup> percentile, we can use any other percentiles when a <strong>quantile </strong>loss function is used. So, to achieve regression ranges, we can use two regressors with two different quantiles as the upper and lower bounds of our range. </p>
        <p>Although the regression ranges work regardless of the dimensionality of the data at hand, the format of the page has forced us to come up with a two-dimensional example for more clarity. The following code creates 400samples to play with:</p>
        <pre>x_sample = np.arange(-10, 10, 0.05)<br/>y_sample = np.random.normal(loc=0, scale=25, size=x_sample.shape[0]) <br/>y_sample *= x_sample <br/><br/>pd_random_samples = pd.DataFrame(<br/>    {<br/>        'x': x_sample,<br/>        'y': y_sample<br/>    }<br/>)</pre>
        <p>Here is a scatter plot of the generated <em>y</em> versus <em>x</em> values: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/f3c2d69c-05d6-413b-af26-4bef741ba82b.png" style="width:55.42em;"/>
        </p>
        <p>Now, we can train two regressors with the 10<sup>th</sup> and 90<sup>th</sup> percentiles as our range boundaries and plot those regression boundaries, along with our scattered data points: </p>
        <pre>from sklearn.ensemble import GradientBoostingRegressor<br/><br/>fig, ax = plt.subplots(1, 1, figsize=(12, 8), sharey=False)<br/><br/>pd_random_samples.plot(<br/>    title='Regression Ranges [10th &amp; 90th Quantiles]', <br/>    kind='scatter', x='x', y='y', color='k', alpha=0.95, ax=ax<br/>)<br/><br/>for quantile in [0.1, 0.9]:<br/><br/>    rgr = GradientBoostingRegressor(n_estimators=10, loss='quantile', alpha=quantile)<br/>    rgr.fit(pd_random_samples[['x']], pd_random_samples['y'])<br/>    pd_random_samples[f'pred_q{quantile}'] = rgr.predict(pd_random_samples[['x']])<br/><br/>    pd_random_samples.plot(<br/>        kind='line', x='x', y=f'pred_q{quantile}', <br/>        linestyle='-', alpha=0.75, color='k', ax=ax<br/>    )<br/><br/>ax.legend(ncol=1, fontsize='x-large', shadow=True)<br/><br/>fig.show()</pre>
        <p>We can see that the majority of the points fall within the range. Ideally, we would expect 80% of the points to fall in the <strong>90</strong>-<strong>100</strong> range: </p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/6f7f891c-eae2-48f9-a5cd-33ab625fe6ae.png" style="width:53.00em;"/>
        </p>
        <p>We can now use the same strategy to predict automobile prices:</p>
        <pre>from sklearn.ensemble import GradientBoostingRegressor<br/><br/>rgr_min = GradientBoostingRegressor(n_estimators=50, loss='quantile', alpha=0.25)<br/>rgr_max = GradientBoostingRegressor(n_estimators=50, loss='quantile', alpha=0.75)<br/><br/>rgr_min.fit(x_train, y_train, monitor=lr_changer)<br/>rgr_max.fit(x_train, y_train, monitor=lr_changer)<br/><br/>y_test_pred_min = rgr_min.predict(x_test)<br/>y_test_pred_max = rgr_max.predict(x_test)<br/><br/>df_pred_range = pd.DataFrame(<br/>    {<br/>        'Actuals': y_test,<br/>        'Pred_min': y_test_pred_min,<br/>        'Pred_max': y_test_pred_max,<br/>    }<br/>)</pre>
        <p>Then, we can check what percentage of our test set falls within the regression range: </p>
        <pre>df_pred_range['Actuals in Range?'] = df_pred_range.apply(<br/>    lambda row: 1 if row['Actuals'] &gt;= row['Pred_min'] and row['Actuals'] &lt;= row['Pred_max'] else 0, axis=1<br/>)</pre>
        <p>Calculating the mean of <kbd>df_pred_range['Actuals in Range?']</kbd> gives us <kbd>0.49</kbd>, which is very close to the <kbd>0.5</kbd> value we expected. Obviously, we can use wider or narrower ranges, depending on our use case. If your model is going to be used to help car owners sell their cars, you may need to give them reasonable ranges, since telling someone that they can sell their car for any price between $5 and $30,000 is pretty accurate yet useless advice. Sometimes, a less accurate yet useful model is better than an accurate and useless one. </p>
        <p>Another boosting algorithm that is not used as much nowadays is the AdaBoost algorithm. We will briefly explore it in the next section for the sake of completeness.</p>
        <h1 id="uuid-78f681ca-ff48-4329-b161-7540a04ed679">Using AdaBoost ensembles </h1>
        <p>In an AdaBoost ensemble, the mistakes made in each iteration are used to alter the weights of the training samples for the following iterations. As in the boosting meta-estimator, this method can also use any other estimators instead of the decision trees used by default. Here, we have used it with its default estimators on the Automobile dataset: </p>
        <pre>from sklearn.ensemble import AdaBoostRegressor<br/><br/>rgr = AdaBoostRegressor(n_estimators=100)<br/>rgr.fit(x_train, y_train)<br/>y_test_pred = rgr.predict(x_test)</pre>
        <p>The AdaBoost meta-estimator also has a <kbd>staged_predict</kbd> method, which allows us to plot the improvement in the training or test loss after each iteration. Here is the code for plotting the test error: </p>
        <pre>pd.DataFrame(<br/>    [<br/>        (n, mean_squared_error(y_test, y_pred_staged))<br/>        for n, y_pred_staged in enumerate(rgr.staged_predict(x_test), 1)<br/>    ],<br/>    columns=['n', 'Test Error']<br/>).set_index('n').plot()<br/><br/>fig.show()</pre>
        <p>Here is a plot for the calculated loss after each iteration:</p>
        <p class="CDPAlignCenter CDPAlign">
          <img src="assets/3b048dd9-613c-4fe3-94e2-649a975e5d87.png" style="width:64.58em;"/>
        </p>
        <p>As in the other ensembles, the more estimators we add, the more accurate it becomes. Once we start to overfit, we should be able to stop. That's why having a validation sample is essential in knowing when to stop. I used the test set for demonstration here, but in practice, the test sample should be kept aside and a validation set should be used instead. </p>
        <h1 id="uuid-4dd5a31c-890e-4304-8ffe-f3bb096c9468">Exploring more ensembles</h1>
        <p>The main ensemble techniques are the ones we have seen so far. The following ones are also good to know about and can be useful for some peculiar cases.</p>
        <h2 id="uuid-8ef1116b-3dac-462f-b033-40db81d35a06">Voting ensembles </h2>
        <p>Sometimes, we have a number of good estimators, each with its own mistakes. Our objective is not to mitigate their bias or variance, but to combine their predictions in the hope that they don't all make the same mistakes. In these cases, <kbd>VotingClassifier</kbd> and <kbd>VotingRegressor</kbd> could be used. You can give a higher preference to some estimators versus the others by adjusting the <kbd>weights</kbd> hyperparameter. <kbd>VotingClassifier</kbd> has different voting strategies, depending on whether the predicted class labels are to be used or whether the predicted probabilities should be used instead.</p>
        <h2 id="uuid-bd492598-ffc5-4d3b-a8a6-93686442cadb">Stacking ensembles </h2>
        <p>Rather than voting, you can combine the predictions of multiple estimators by adding an extra one that uses their predictions as input. This strategy is known as <strong>stacking</strong>. The inputs of the final estimator can be limited to the predictions of the previous estimators, or it can be a combination of their predictions and the original training data. To avoid overfitting, the final estimators are usually trained using cross-validation. </p>
        <h2 id="uuid-1db3de7b-2660-4b4f-9fe6-b6afa0d221d1">Random tree embedding</h2>
        <p>We have seen how the trees are capable of capturing the non-linearities in the data. So, if we still want to use a simpler algorithm, we can just use the trees to transform the data and leave the prediction for the simple algorithm to do. When building a tree, each data point falls into one of its leaves. Therefore, the IDs of the leaves can be used to represent the different data points. If we build multiple trees, then each data point can be represented by the ID of the leaf it fell on in each tree. These leaves, IDs can be used as our new features and can be fed into a simpler estimator. This kind of embedding is useful for feature compression and allows linear models to capture the non-linearities in the data. </p>
        <p>Here, we use an unsupervised <kbd>RandomTreesEmbedding</kbd> method to transform our automobile features, and then use the transformed features in a <kbd>Ridge</kbd> regression: </p>
        <pre>from sklearn.ensemble import RandomTreesEmbedding<br/>from sklearn.linear_model import Ridge<br/>from sklearn.pipeline import make_pipeline<br/><br/>rgr = make_pipeline(RandomTreesEmbedding(), Ridge())<br/>rgr.fit(x_train, y_train)<br/>y_test_pred = rgr.predict(x_test)<br/><br/>print(f'MSE: {mean_squared_error(y_test, y_test_pred)}')</pre>
        <p>From the preceding block of code, we can observe the following:</p>
        <ul>
          <li>This approach is not limited to <kbd>RandomTreesEmbedding</kbd>.</li>
        </ul>
        <ul>
          <li>Gradient-boosted trees can also be used to transform the data for a downstream estimator to use.</li>
          <li>Both <kbd>GradientBoostingRegressor</kbd> and <kbd>GradientBoostingClassifier</kbd> have an <kbd>apply</kbd> function, which can be used for feature transformation.</li>
        </ul>
        <h1 id="uuid-51acb467-bdf5-41e7-b153-f03ff3605361">Summary</h1>
        <p>In this chapter, we saw how algorithms benefit from being assembled in the form of ensembles. We learned how these ensembles can mitigate the bias versus variance trade-off.</p>
        <p>When dealing with heterogeneous data, the gradient boosting and random forest algorithms are my first two choices for classification and regression. They do not require any sophisticated data preparation, thanks to their dependence on trees. They are able to deal with non-linear data and capture feature interactions. Above all, the tuning of their hyperparameters is straightforward.</p>
        <p>The more estimators in each method, the better, and you should not worry so much about them overfitting. As for gradient boosting, you can pick a lower learning rate if you can afford to have more trees. In addition to these hyperparameters, the depth of the trees in each of the two algorithms should be tuned via trail and error and cross-validation. Since the two algorithms come from different sides of the bias-variance spectrum, you may initially aim for forests with big trees that you can prune later on. Conversely, you can start with shallow trees and rely on your gradient-boosting meta-estimator to boost them. </p>
        <p>So far in this book, we have predicted a single target at a time. Here, we predicted the prices of automobiles and that's it. In the next chapter, we will see how to predict multiple targets at a time. Furthermore, when our aim is to use the probabilities given by a classifier, having a calibrated classifier is paramount. We can have a better estimation of our risks if we have probabilities that we trust. Thus, calibrating a classifier is going to be another topic covered in the next chapter.  </p>
      </article>
    </section>
  </body></html>
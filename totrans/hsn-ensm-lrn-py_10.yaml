- en: Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging is generally used to reduce variance of a model. It achieves it by creating
    an ensemble of base learners, each one trained on a unique bootstrap sample of
    the original train set. This forces diversity between the base learners. Random
    Forests expand on bagging by inducing randomness not only on each base learner's
    train samples, but in the features as well. Furthermore, their performance is
    similar to boosting techniques, although they do not require as much fine-tuning
    as boosting methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will provide the basic background of random forests, as
    well as discuss the strengths and weaknesses of the method. Finally, we will present
    usage examples, using the scikit-learn implementation. The main topics covered
    in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: How Random Forests build their base learners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How randomness can be utilized in order to build better random forest ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strengths and weaknesses of Random Forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing scikit-learn's implementation for regression and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07)'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/2LY5OJR](http://bit.ly/2LY5OJR).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding random forest trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will go over the methodology of building a basic random
    forest tree. There are other methods that can be employed, but they all strive
    to achieve the same goal: diverse trees that serve as the ensemble''s base learners.'
  prefs: []
  type: TYPE_NORMAL
- en: Building trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 1](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml), *A
    Machine Learning Refresher*, create a tree by selecting at each node a single
    feature and split point, such that the train set is best split. When an ensemble
    is created, we wish the base learners to be as uncorrelated (diverse) as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging is able to produce reasonably uncorrelated trees by diversifying each
    tree''s train set through bootstrapping. But bagging only diversifies the trees
    by acting on one axis: each set''s instances. There is still a second axis on
    which we can introduce diversity, the features. By selecting a subset of the available
    features during training, the generated base learners can be even more diverse.
    In random forests, for each tree and at each node, only a subset of the available
    features is considered when choosing the best feature/split point combination.
    The number of features that will be selected can be optimized by hand, but one-third
    of all features for regression problems and the square root of all features are
    considered to be a good starting point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm''s steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the number of features *m* that will be considered at each node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each base learner, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a bootstrap train sample
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the node to split
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select *m* features randomly
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick the best feature and split point from *m*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the node into two nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat from step 2-2 until a stopping criterion is met, such as maximum tree
    depth
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Illustrative example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to better illustrate the process, let''s consider the following dataset,
    indicating whether a second shoulder dislocation has occurred after the first
    (recurrence):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** | **Operated** | **Sex** | **Recurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | y | m | y |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | n | f | n |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | y | m | y |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | n | m | n |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | n | f | y |'
  prefs: []
  type: TYPE_TB
- en: Shoulder dislocation recurrence dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to build a Random Forest tree, we must first decide the number of
    features that will be considered in each split. As we have three features, we
    will use the square root of 3, which is approximately 1.7\. Usually, we use the
    floor of this number (we round it down to the closest integer), but as we want
    to illustrate the process, we will use two features in order to better demonstrate
    it. For the first tree, we generate a bootstrap sample. The second row is an instance
    that was chosen twice from the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** | **Operated** | **Sex** | **Recurrence** |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | y | m | y |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | y | m | y |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | y | m | y |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | n | m | n |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | n | f | y |'
  prefs: []
  type: TYPE_TB
- en: The bootstrap sample
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the root node. First, we randomly select two features to consider.
    We choose **operated** and **sex**. The best split is given for **operated**,
    as we get a leaf with 100% accuracy and one node with 50% accuracy. The resulting
    tree is depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2585e8c5-1dc8-4d1d-842b-0ca00748fc81.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree after the first split
  prefs: []
  type: TYPE_NORMAL
- en: Next, we again select two features at random and the one that offers the best
    split. We now choose **operated** and **age**. As both misclassified instances
    were not operated, the best split is offered through the age feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the final tree is a tree with three leaves, where if someone is operated
    they have a recurrence, while if they are not operated and are over the age of
    18 they do not:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that medical research indicates that young males have the highest chance
    for shoulder dislocation recurrence. The dataset here is a toy example that does
    not reflect reality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcf0023c-0cb0-499e-9972-8eb6714ef467.png)'
  prefs: []
  type: TYPE_IMG
- en: The final decision tree
  prefs: []
  type: TYPE_NORMAL
- en: Extra trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another method to create trees in a Random Forest ensemble is Extra Trees (extremely randomized
    trees). The main difference with the previous method is that the feature and split
    point combination does not have to be the optimal. Instead, a number of split
    points are randomly generated, one for each available feature. The best split
    point of those generated is selected. The algorithm constructs a tree as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the number of features *m* that will be considered at each node and the
    minimum number of samples *n* in order to split a node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each base learner, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a bootstrap train sample
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the node to split (the node must have at least *n* samples)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select *m* features randomly
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly generate *m* split points, with values between the minimum and maximum
    value of each feature
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the best of these split points
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the node into two nodes and repeat from step 2-2 until there are no available
    nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By creating a number of trees using any valid randomization method, we have
    essentially created a forest, hence the algorithm''s name. After generating the
    ensemble''s trees, their predictions must be combined in order to have a functional
    ensemble. This is usually achieved through majority voting for classification
    problems and through averaging for regression problems. There are a number of
    hyperparameters associated with Random Forests, such as the number of features
    to consider at each node split, the number of trees in the forest, and the individual
    tree''s size. As mentioned earlier, a good starting point for the number of features
    to consider is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The square root of the number of total features for classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-third of the number of total features for regression problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of trees can be fine-tuned by hand, as the ensemble's error
    converges to a limit when this number increases. Out-of-bag errors can be utilized
    to find an optimal value. Finally, the size of each tree can be a deciding factor
    in overfitting. Thus, if overfitting is observed, the tree size should be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random Forests provide information about the underlying dataset that most of
    other methods cannot easily provide. A prominent example is the importance of
    each individual feature in the dataset. One method to estimate feature importance
    is to use the Gini index for each node of each tree and compare each feature's
    cumulative value. Another method uses the out-of-bag samples. First, the out-of-bag
    accuracy is recorded for all base learners. Then, a single feature is chosen and
    its values are shuffled in the out-of-bag samples. This results in out-of-bag
    sample sets with the same statistical properties as the original sets, but any
    predictive power that the chosen feature might have is removed (as there is now
    zero correlation between the selected feature's values and the target). The difference
    in accuracy between the original and the partially random dataset is used as measure
    for the selected feature's importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning bias and variance, although random forests seem to cope well with
    both, they are certainly not immune. Bias can appear when the available features
    are great in number, but only few are correlated to the target. When using the
    recommended number of features to consider at each split (for example, the square
    root of the number of total features), the probability that a relevant feature
    will be selected can be small. The following graph shows the probability that
    at least one relevant feature will be selected, as a function of relevant and
    irrelevant features (when the square root of the number of total features is considered
    at each split):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89e05ebd-3f52-4666-9694-b76675e7d70b.png)'
  prefs: []
  type: TYPE_IMG
- en: Probability to select at least one relevant feature as a function of the number
    of relevant and irrelevant features
  prefs: []
  type: TYPE_NORMAL
- en: The Gini index measures the frequency of incorrect classifications, assuming
    that a randomly sampled instance would be classified according to the label distribution
    dictated by a specific node.
  prefs: []
  type: TYPE_NORMAL
- en: Variance can also appear in Random Forests, although the method is sufficiently
    resistant to it. Variance usually appears when the individual trees are allowed
    to grow fully. We have previously mentioned that as the number of trees increases,
    the error approximates a certain limit. Although this claim still holds true,
    it is possible that the limit itself overfits the data. Restricting the tree size
    (by increasing the minimum number of samples per leaf or reducing the maximum
    depth) can potentially help in such circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Strengths and weaknesses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random Forests are a very robust ensemble learning method, able to reduce both
    bias and variance, similar to boosting. Furthermore, the algorithm's nature allows
    it to be fully parallelized, both during training, as well as during prediction.
    This is a considerable advantage over boosting methods, especially when large
    datasets are concerned. Furthermore, they require less hyperparameter fine-tuning,
    compared to boosting techniques, especially XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: The main weaknesses of random forests are their sensitivity to class imbalances,
    as well as the problem we mentioned earlier, which involves a low ratio of relevant
    to irrelevant features in the train set. Furthermore, when the data contains low-level
    non-linear patterns (such as in raw, high-resolution image recognition), Random
    Forests usually are outperformed by deep neural networks. Finally, Random Forests
    can be computationally expensive when very large datasets are used combined with
    unrestricted tree depth.
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn implements both conventional Random Forest trees, as well as Extra
    Trees. In this section, we will provide basic regression and classification examples
    with both algorithms, using the scikit-learn implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Random Forests classification class is implemented in `RandomForestClassifier`,
    under the `sklearn.ensemble` package. It has a number of parameters, such as the
    ensemble's size, the maximum tree depth, the number of samples required to make
    or split a node, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will try to classify the hand-written digits dataset, using
    the Random Forest classification ensemble. As usual, we load the required classes
    and data and set the seed for our random number generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Following this, we create the ensemble, by setting the `n_estimators` and `n_jobs` parameters.
    These parameters dictate the number of trees that will be generated and the number
    of parallel jobs that will be run. We train the ensemble using the `fit` function
    and evaluate it on the test set by measuring its achieved accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The classifier is able to achieve an accuracy of 93%, which is even higher
    than the previously best-performing method, XGBoost ([Chapter 6](a1a92022-31ce-4c9b-9712-6b8282fac1af.xhtml), *Boosting*).
    We can visualize the approximation of the error limit we mentioned earlier, by
    plotting validation curves (from [Chapter 2](d7921006-351e-4c21-ab54-f1dc834557dc.xhtml), *Getting
    Started with Ensemble Learning*) for a number of ensemble sizes. We test for sizes
    of 10, 50, 100, 150, 200, 250, 300, 350, and 400 trees. The curves are depicted
    in the following graph. We can see that the ensemble approaches a 10-fold cross-validation
    error of 96%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fcce0cc8-1a54-41c7-8ebc-200301ca4071.png)'
  prefs: []
  type: TYPE_IMG
- en: Validation curves for a number of ensemble sizes
  prefs: []
  type: TYPE_NORMAL
- en: Random forests for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn also implements random forests for regression purposes in the
    `RandomForestRegressor` class. It is also highly parameterizable, with hyper-parameters
    concerning both the ensemble as a whole, as well as the individual trees. Here,
    we will generate an ensemble in order to model the diabetes regression dataset.
    The code follows the standard procedure of loading libraries and data, creating
    the ensemble and calling the `fit` and predict methods, along with calculating
    the MSE and R-squared values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The ensemble is able to produce an R-squared of 0.51 and an MSE of 2722.67 on
    the test set. As the R-squared and MSE on the train set are 0.92 and 468.13 respectively,
    it is safe to assume that the ensemble overfits. This is a case where the error
    limit overfits, and thus we need to regulate the individual trees in order to
    achieve better results. By reducing the minimum number of samples required to
    be at each leaf node (increased to 20, from the default value of 2) through `min_samples_leaf=20`,
    we are able to increase R-squared to 0.6 and reduce MSE to 2206.6\. Furthermore,
    by increasing the ensemble size to 1000, R-squared is further increased to 0.61
    and MSE is further decreased to 2158.73.
  prefs: []
  type: TYPE_NORMAL
- en: Extra trees for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from conventional Random Forests, scikit-learn also implements Extra
    Trees. The classification implementation lies in the `ExtraTreesClassifier`, in
    the `sklearn.ensemble` package. Here, we repeat the hand-written digit recognition
    example, using the Extra Trees classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may notice, the only difference with the previous example is the switch
    from `RandomForestClassifier` to `ExtraTreesClassifier`. Nonetheless, the ensemble
    achieves an even higher test accuracy score of 94%. Once again, we create validation
    curves for a number of ensemble sizes, depicted as follows. The 10-fold cross
    validation error limit for this ensemble is approximately at 97%, which further
    confirms that it outperforms the conventional Random Forest approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbe8dc8b-dd5a-49f5-b170-43944c0d5d51.png)'
  prefs: []
  type: TYPE_IMG
- en: Extra Trees validation curves for a number of ensemble sizes
  prefs: []
  type: TYPE_NORMAL
- en: Extra trees regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, we present the regression implementation of Extra Trees, implemented
    in `ExtraTreesRegressor`. In the following code, we repeat the previously presented
    example of modeling the diabetes dataset, using the regression version of Extra
    Trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the classification examples, Extra Trees outperform conventional
    random forests by achieving a test R-squared of 0.55 (0.04 better than Random
    Forests) and an MSE of 2479.18 (a difference of 243.49). Still, the ensemble seems
    to overfit, as it perfectly predicts in-sample data. By setting `min_samples_leaf=10` and
    the ensemble size to 1000, we are able to produce an R-squared of 0.62 and an
    MSE of 2114.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed Random Forests, an ensemble method utilizing
    decision trees as its base learners. We presented two basic methods of constructing
    the trees: the conventional Random Forests approach, where a subset of features
    is considered at each split, as well as Extra Trees, where the split points are
    chosen almost randomly. We discussed the basic characteristics of the ensemble
    method. Furthermore, we presented regression and classification examples using
    the scikit-learn implementations of Random Forests and Extra Trees. The key points
    of this chapter that summarize its contents are provided below.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random Forests** use bagging in order to create train sets for their base
    learners. At each node, each tree considers only a subset of the available features
    and computes the optimal feature/split point combination. The number of features
    to consider at each point is a hyper-parameter that must be tuned. Good starting
    points are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The square root of the total number of parameters for classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-third of the total number of parameters for regression problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extra trees** and random forests use the **whole dataset** for each base
    learner. In extra trees and random forests, instead of calculating the optimal
    feature/split-point combination of the feature subset at each node, a random split
    point is generated for each feature in the subset and the best is selected. Random
    forests can give information regarding the importance of each feature. Although
    relatively resistant to overfitting, random forests are not immune to it. Random
    forests can exhibit high bias when the ratio of relevant to irrelevant features
    is low. Random forests can exhibit high variance, although the ensemble size does
    not contribute to the problem. In the next chapter, we will present ensemble learning
    techniques that can be applied to unsupervised learning methods (clustering).'
  prefs: []
  type: TYPE_NORMAL

- en: Chapter 8. The Perceptron
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters we discussed generalized linear models that relate a linear
    combination of explanatory variables and model parameters to a response variable
    using a link function. In this chapter, we will discuss another linear model called
    the perceptron. The perceptron is a binary classifier that can learn from individual
    training instances, which can be useful for training from large datasets. More
    importantly, the perceptron and its limitations inspire the models that we will
    discuss in the final chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Invented by Frank Rosenblatt at the Cornell Aeronautical Laboratory in the late
    1950's, the development of the perceptron was originally motivated by efforts
    to simulate the human brain. A brain is composed of cells called **neurons** that
    process information and connections between neurons called **synapses** through
    which information is transmitted. It is estimated that human brain is composed
    of as many as 100 billion neurons and 100 trillion synapses. As shown in the following
    image, the main components of a neuron are dendrites, a body, and an axon. The
    dendrites receive electrical signals from other neurons. The signals are processed
    in the neuron's body, which then sends a signal through the axon to another neuron.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![The Perceptron](img/8365OS_08_01.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: 'An individual neuron can be thought of as a computational unit that processes
    one or more inputs to produce an output. A perceptron functions analogously to
    a neuron; it accepts one or more inputs, processes them, and returns an output.
    It may seem that a model of just one of the hundreds of billions of neurons in
    the human brain will be of limited use. To an extent that is true; the perceptron
    cannot approximate some basic functions. However, we will still discuss perceptrons
    for two reasons. First, perceptrons are capable of online, error-driven learning;
    the learning algorithm can update the model''s parameters using a single training
    instance rather than the entire batch of training instances. Online learning is
    useful for learning from training sets that are too large to be represented in
    memory. Second, understanding how the perceptron works is necessary to understand
    some of the more powerful models that we will discuss in subsequent chapters,
    including support vector machines and artificial neural networks. Perceptrons
    are commonly visualized using a diagram like the following one:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '![The Perceptron](img/8365OS_08_02.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
- en: The circles labeled ![The Perceptron](img/8365OS_08_18.jpg), ![The Perceptron](img/8365OS_08_19.jpg),
    and ![The Perceptron](img/8365OS_08_20.jpg) are inputs units. Each input unit
    represents one feature. Perceptrons frequently use an additional input unit that
    represents a constant bias term, but this input unit is usually omitted from diagrams.
    The circle in the center is a computational unit or the neuron's body. The edges
    connecting the input units to the computational unit are analogous to dendrites.
    Each edge is **weighted**, or associated with a parameter. The parameters can
    be interpreted easily; an explanatory variable that is correlated with the positive
    class will have a positive weight, and an explanatory variable that is correlated
    with the negative class will have a negative weight. The edge directed away from
    the computational unit returns the output and can be thought of as the axon.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron classifies instances by processing a linear combination of the
    explanatory variables and the model parameters using an **activation function**
    as shown in the following equation. The linear combination of the parameters and
    inputs is sometimes called the perceptron's **preactivation**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![Activation functions](img/8365OS_08_03.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Activation functions](img/8365OS_08_21.jpg) are the model''s parameters,
    ![Activation functions](img/8365OS_08_22.jpg) is a constant bias term, and ![Activation
    functions](img/8365OS_08_23.jpg) is the activation function. Several different
    activation functions are commonly used. Rosenblatt''s original perceptron used
    the **Heaviside step** function. Also called the unit step function, the Heaviside
    step function is shown in the following equation, where ![Activation functions](img/8365OS_08_24.jpg)
    is the weighted combination of the features:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![Activation functions](img/8365OS_08_04.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: 'If the weighted sum of the explanatory variables and the bias term is greater
    than zero, the activation function returns one and the perceptron predicts that
    the instance is the positive class. Otherwise, the function returns zero and the
    perceptron predicts that the instance is the negative class. The Heaviside step
    activation function is plotted in the following figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Activation functions](img/8365OS_08_05.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: 'Another common activation function is the **logistic sigmoid** activation function.
    The gradients for this activation function can be calculated efficiently, which
    will be important in later chapters when we construct artificial neural networks.
    The logistic sigmoid activation function is given by the following equation, where
    ![Activation functions](img/8365OS_08_24.jpg) is the sum of the weighted inputs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Activation functions](img/8365OS_08_06.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: This model should seem familiar; it is a linear combination of the values of
    the explanatory variables and the model parameters processed through the logistic
    function. That is, this is identical to the model for logistic regression. While
    a perceptron with a logistic sigmoid activation function has the same model as
    logistic regression, it learns its parameters differently.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron learning algorithm
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perceptron learning algorithm begins by setting the weights to zero or
    to small random values. It then predicts the class for a training instance. The
    perceptron is an **error-driven** learning algorithm; if the prediction is correct,
    the algorithm continues to the next instance. If the prediction is incorrect,
    the algorithm updates the weights. More formally, the update rule is given by
    the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![The perceptron learning algorithm](img/8365OS_08_07.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: For each training instance, the value of the parameter for each explanatory
    variable is incremented by ![The perceptron learning algorithm](img/8365OS_08_25.jpg),
    where ![The perceptron learning algorithm](img/8365OS_08_26.jpg) is the true class
    for instance ![The perceptron learning algorithm](img/8365OS_08_27.jpg), ![The
    perceptron learning algorithm](img/8365OS_08_28.jpg) is the predicted class for
    instance ![The perceptron learning algorithm](img/8365OS_08_27.jpg), ![The perceptron
    learning algorithm](img/8365OS_08_29.jpg) is the value of the ![The perceptron
    learning algorithm](img/8365OS_08_30.jpg) explanatory variable for instance ![The
    perceptron learning algorithm](img/8365OS_08_27.jpg), and ![The perceptron learning
    algorithm](img/8365OS_08_31.jpg) is a hyperparameter that controls the learning
    rate. If the prediction is correct, ![The perceptron learning algorithm](img/8365OS_08_32.jpg)
    equals zero, and the ![The perceptron learning algorithm](img/8365OS_08_25.jpg)
    term equals zero. So, if the prediction is correct, the weight is not updated.
    If the prediction is incorrect, the weight is incremented by the product of the
    learning rate, ![The perceptron learning algorithm](img/8365OS_08_32.jpg), and
    the value of the feature.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: This update rule is similar to the update rule for gradient descent in that
    the weights are adjusted towards classifying the instance correctly and the size
    of the update is controlled by a learning rate. Each pass through the training
    instances is called an **epoch.** The learning algorithm has converged when it
    completes an epoch without misclassifying any of the instances. The learning algorithm
    is not guaranteed to converge; later in this chapter, we will discuss linearly
    inseparable datasets for which convergence is impossible. For this reason, the
    learning algorithm also requires a hyperparameter that specifies the maximum number
    of epochs that can be completed before the algorithm terminates.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification with the perceptron
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s work through a toy classification problem. Suppose that you wish to
    separate adult cats from kittens. Only two explanatory variables are available
    in your dataset: the proportion of the day that the animal was asleep and the
    proportion of the day that the animal was grumpy. Our training data consists of
    the following four instances:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '| Instance | Proportion of the day spent sleeping | Proportion of the day spent
    being grumpy | Kitten or Adult? |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.2 | 0.1 | Kitten |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.4 | 0.6 | Kitten |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.5 | 0.2 | Kitten |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.7 | 0.9 | Adult |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: 'The following scatter plot of the instances confirms that they are linearly
    separable:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary classification with the perceptron](img/8365OS_08_10.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Our goal is to train a perceptron that can classify animals using the two real-valued
    explanatory variables. We will represent kittens with the positive class and adult
    cats with the negative class. The preceding network diagram describes the perceptron
    that we will train.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Our perceptron has three input units. ![Binary classification with the perceptron](img/8365OS_08_34.jpg)
    is the input unit for the bias term. ![Binary classification with the perceptron](img/8365OS_08_35.jpg)
    and ![Binary classification with the perceptron](img/8365OS_08_36.jpg) are input
    units for the two features. Our perceptron''s computational unit uses a Heaviside
    activation function. In this example, we will set the maximum number of training
    epochs to ten; if the algorithm does not converge within 10 epochs, it will stop
    and return the current values of the weights. For simplicity, we will set the
    learning rate to one. Initially, we will set all of the weights to zero. Let''s
    examine the first training epoch, which is shown in the following table:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '| Epoch 1 |   |   |   |   |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated weights** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0, 0, 0;1.0, 0.2, 0.1;1.0*0 + 0.2*0 + 0.1*0 = 0.0; | 0, 1 | False | 1.0,
    0.2, 0.1 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1.0, 0.2, 0.1;1.0, 0.4, 0.6;1.0*1.0 + 0.4*0.2 + 0.6*0.1 = 1.14; | 1,
    1 | True | 1.0, 0.2, 0.1 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1.0, 0.2, 0.1;1.0, 0.5, 0.2;1.0*1.0 + 0.5*0.2 + 0.2*0.1 = 1.12; | 1,
    1 | True | 1.0, 0.2, 0.1 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.0, 0.2, 0.1;1.0, 0.7, 0.9;1.0*1.0 + 0.7*0.2 + 0.9*0.1 = 1.23; | 1,
    0 | False | 0, -0.5, -0.8 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: Initially, all of the weights are equal to zero. The weighted sum of the explanatory
    variables for the first instance is zero, the activation function outputs zero,
    and the perceptron incorrectly predicts that the kitten is an adult cat. As the
    prediction was incorrect, we update the weights according to the update rule.
    We increment each of the weights by the product of the learning rate, the difference
    between the true and predicted labels and the value of the corresponding feature.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'We then continue to the second training instance and calculate the weighted
    sum of its features using the updated weights. This sum equals 1.14, so the activation
    function outputs one. This prediction is correct, so we continue to the third
    training instance without updating the weights. The prediction for the third instance
    is also correct, so we continue to the fourth training instance. The weighted
    sum of the features for the fourth instance is 1.23\. The activation function
    outputs one, incorrectly predicting that this adult cat is a kitten. Since this
    prediction is incorrect, we increment each weight by the product of the learning
    rate, the difference between the true and predicted labels, and its corresponding
    feature. We completed the first epoch by classifying all of the instances in the
    training set. The perceptron did not converge; it classified half of the training
    instances incorrectly. The following figure depicts the decision boundary after
    the first epoch:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary classification with the perceptron](img/8365OS_08_11.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'Note that the decision boundary moved throughout the epoch; the decision boundary
    formed by the weights at the end of the epoch would not necessarily have produced
    the same predictions seen earlier in the epoch. Since we have not exceeded the
    maximum number of training epochs, we will iterate through the instances again.
    The second training epoch is shown in the following table:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '| Epoch 2 |   |   |   |   |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated weights** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0, -0.5, -0.81.0, 0.2, 0.11.0*0 + 0.2*-0.5 + 0.1*-0.8 = -0.18 | 0, 1
    | False | 1, -0.3, -0.7 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1, -0.3, -0.71.0, 0.4, 0.61.0*1.0 + 0.4*-0.3 + 0.6*-0.7 = 0.46 | 1, 1
    | True | 1, -0.3, -0.7 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1, -0.3, -0.71.0, 0.5, 0.21.0*1.0 + 0.5*-0.3 + 0.2*-0.7 = 0.71 | 1, 1
    | True | 1, -0.3, -0.7 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1, -0.3, -0.71.0, 0.7, 0.91.0*1.0 + 0.7*-0.3 + 0.9*-0.7 = 0.16 | 1, 0
    | False | 0, -1, -1.6 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: The second epoch begins using the values of the weights from the first epoch.
    Two training instances are classified incorrectly during this epoch. The weights
    are updated twice, but the decision boundary at the end of the second epoch is
    similar the decision boundary at the end of the first epoch.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary classification with the perceptron](img/8365OS_08_12.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'The algorithm failed to converge during this epoch, so we will continue training.
    The following table describes the third training epoch:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '| Epoch 3 |   |   |   |   |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated Weights** |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0, -1, -1.61.0, 0.2, 0.11.0*0 + 0.2*-1.0 + 0.1*-1.6 = -0.36 | 0, 1 |
    `False` | 1,-0.8, -1.5 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1,-0.8, -1.51.0, 0.4, 0.61.0*1.0 + 0.4*-0.8 + 0.6*-1.5 = -0.22 | 0, 1
    | False | 2, -0.4, -0.9 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2, -0.4, -0.91.0, 0.5, 0.21.0*2.0 + 0.5*-0.4 + 0.2*-0.9 = 1.62 | 1, 1
    | True | 2, -0.4, -0.9 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2, -0.4, -0.91.0, 0.7, 0.91.0*2.0 + 0.7*-0.4 + 0.9*-0.9 = 0.91 | 1, 0
    | False | 1, -1.1, -1.8 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: 'The perceptron classified more instances incorrectly during this epoch than
    during previous epochs. The following figure depicts the decision boundary at
    the end of the third epoch:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary classification with the perceptron](img/8365OS_08_13.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'The perceptron continues to update its weights throughout the fourth and fifth
    training epochs, and it continues to classify training instances incorrectly.
    During the sixth epoch the perceptron classified all of the instances correctly;
    it converged on a set of weights that separates the two classes. The following
    table describes the sixth training epoch:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '| Epoch 6 |   |   |   |   |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| **Instance** | **Initial Weights****x****Activation** | **Prediction, Target**
    | **Correct** | **Updated weights** |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2, -1, -1.51.0, 0.2, 0.11.0*2 + 0.2*-1 + 0.1*-1.5 = 1.65 | 1, 1 | True
    | 2, -1, -1.5 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2, -1, -1.51.0, 0.4, 0.61.0*2 + 0.4*-1 + 0.6*-1.5 = 0.70 | 1, 1 | True
    | 2, -1, -1.5 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2, -1, -1.51.0, 0.5, 0.21.0*2 + 0.5*-1 + 0.2*-1.5 = 1.2 | 1, 1 | True
    | 2, -1, -1.5 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2, -1, -1.51.0, 0.7, 0.91.0*2 + 0.7*-1 + 0.9*-1.5 = -0.05 | 0, 0 | True
    | 2, -1, -1.5 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: 'The decision boundary at the end of the sixth training epoch is shown in the
    following figure:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary classification with the perceptron](img/8365OS_08_16.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: The following figure shows the decision boundary throughout all the training
    epochs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Binary classification with the perceptron](img/8365OS_08_17.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Document classification with the perceptron
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn provides an implementation of the perceptron. As with the other
    implementations that we used, the constructor for the `Perceptron` class accepts
    keyword arguments that set the algorithm's hyperparameters. `Perceptron` similarly
    exposes the `fit_transform()` and `predict()` methods. `Perceptron` also provides
    a `partial_fit()` method, which allows the classifier to train and make predictions
    for streaming data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we train a perceptron to classify documents from the 20 newsgroups
    dataset. The dataset consists of approximately 20,000 documents sampled from 20
    Usenet newsgroups. The dataset is commonly used in document classification and
    clustering experiments; scikit-learn provides a convenience function to download
    and read the dataset. We will train a perceptron to classify documents from three
    newsgroups: `rec.sports.hockey`, `rec.sports.baseball`, and `rec.auto`. scikit-learn''s
    `Perceptron` natively supports multiclass classification; it will use the one
    versus all strategy to train a classifier for each of the classes in the training
    data. We will represent the documents as TF-IDF-weighted bags of words. The `partial_fit()`
    method could be used in conjunction with `HashingVectorizer` to train from large
    or streaming data in a memory-constrained setting:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们训练一个感知器来分类20个新闻组数据集中的文档。该数据集由约20,000个文档组成，采样自20个Usenet新闻组。该数据集通常用于文档分类和聚类实验；scikit-learn提供了一个便利的函数来下载和读取数据集。我们将训练一个感知器来分类来自三个新闻组的文档：`rec.sports.hockey`、`rec.sports.baseball`和`rec.auto`。scikit-learn的`Perceptron`原生支持多类分类；它将使用“一对多”策略为训练数据中的每个类别训练一个分类器。我们将文档表示为TF-IDF加权的词袋。`partial_fit()`方法可以与`HashingVectorizer`结合使用，在内存受限的环境中对大量或流数据进行训练：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following is the output of the script:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是脚本的输出：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, we download and read the dataset using the `fetch_20newsgroups()` function.
    Consistent with other built-in datasets, the function returns an object with `data`,
    `target`, and `target_names` fields. We also specify that the documents' headers,
    footers, and quotes should be removed. Each of the newsgroups used different conventions
    in the headers and footers; retaining these explanatory variables makes classifying
    the documents artificially easy. We produce TF-IDF vectors using `TfifdVectorizer`,
    train the perceptron, and evaluate it on the test set. Without hyperparameter
    optimization, the perceptron's average precision, recall, and F1 score are 0.85.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`fetch_20newsgroups()`函数下载并读取数据集。与其他内置数据集一致，函数返回一个包含`data`、`target`和`target_names`字段的对象。我们还指定删除文档的标题、页脚和引用。每个新闻组在标题和页脚中使用不同的约定；保留这些解释性变量会使得文档分类变得过于简单。我们使用`TfifdVectorizer`生成TF-IDF向量，训练感知器，并在测试集上进行评估。未经超参数优化，感知器的平均精度、召回率和F1分数为0.85。
- en: Limitations of the perceptron
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器的局限性
- en: While the perceptron classified the instances in our example well, the model
    has limitations. Linear models like the perceptron with a Heaviside activation
    function are not **universal function approximators**; they cannot represent some
    functions. Specifically, linear models can only learn to approximate the functions
    for **linearly separable** datasets. The linear classifiers that we have examined
    find a hyperplane that separates the positive classes from the negative classes;
    if no hyperplane exists that can separate the classes, the problem is not linearly
    separable.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器在我们的例子中很好地分类了实例，但该模型存在局限性。像感知器这种使用Heaviside激活函数的线性模型并不是**通用函数逼近器**；它们无法表示某些函数。具体来说，线性模型只能学习逼近**线性可分**数据集的函数。我们所检查的线性分类器找到一个超平面，将正类与负类分开；如果没有一个超平面能够分开这些类别，那么问题就不是线性可分的。
- en: 'A simple example of a function that is linearly inseparable is the logical
    operation **XOR**, or exclusive disjunction. The output of XOR is one when one
    of its inputs is equal to one and the other is equal to zero. The inputs and outputs
    of XOR are plotted in two dimensions in the following graph. When XOR outputs
    **1**, the instance is marked with a circle; when XOR outputs **0**, the instance
    is marked with a diamond, as shown in the following figure:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的线性不可分的函数示例是逻辑运算**XOR**，即排他或运算。XOR的输出为1，当其输入之一为1而另一个为0时。XOR的输入和输出在以下图中以二维形式绘制。当XOR输出**1**时，实例用圆圈标记；当XOR输出**0**时，实例用菱形标记，如下图所示：
- en: '![Limitations of the perceptron](img/8365OS_08_09.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![感知器的局限性](img/8365OS_08_09.jpg)'
- en: It is impossible to separate the circles from the diamonds using a single straight
    line. Suppose that the instances are pegs on a board. If you were to stretch a
    rubber band around both of the positive instances, and stretch a second rubber
    band around both of the negative instances, the bands would intersect in the middle
    of the board. The rubber bands represent **convex** **hulls**, or the envelope
    that contains all of the points within the set and all of the points along any
    line connecting a pair points within the set. Feature representations are more
    likely to be linearly separable in higher dimensional spaces than lower dimensional
    spaces. For instance, text classification problems tend to be linearly separable
    when high-dimensional representations like the bag-of-words are used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: In the next two chapters, we will discuss techniques that can be used to model
    linearly inseparable data. The first technique, called **kernelization**, projects
    linearly inseparable data to a higher dimensional space in which it is linearly
    separable. Kernelization can be used in many models, including perceptrons, but
    it is particularly associated with support vector machines, which we will discuss
    in the next chapter. Support vector machines also support techniques that can
    find the hyperplane that separates linearly inseparable classes with the fewest
    errors. The second technique creates a directed graph of perceptrons. The resulting
    model, called an **artificial neural network**, is a universal function approximator;
    we will discuss artificial neural networks in [Chapter 10](ch10.html "Chapter 10. From
    the Perceptron to Artificial Neural Networks"), *From the Perceptron to Artificial
    Neural Networks*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the perceptron. Inspired by neurons, the perceptron
    is a linear model for binary classification. The perceptron classifies instances
    by processing a linear combination of the explanatory variables and weights with
    an activation function. While a perceptron with a logistic sigmoid activation
    function is the same model as logistic regression, the perceptron learns its weights
    using an online, error-driven algorithm. The perceptron can be used effectively
    in some problems. Like the other linear classifiers that we have discussed, the
    perceptron is not a universal function approximator; it can only separate the
    instances of one class from the instances of the other using a hyperplane. Some
    datasets are not linearly separable; that is, no possible hyperplane can classify
    all of the instances correctly. In the following chapters, we will discuss two
    models that can be used with linearly inseparable data: the artificial neural
    network, which creates a universal function approximator from a graph of perceptrons
    and the support vector machine, which projects the data onto a higher dimensional
    space in which it is linearly separable.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL

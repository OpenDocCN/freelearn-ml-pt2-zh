["```py\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> X_dict = [{'interest': 'tech', 'occupation': 'professional'},\n...           {'interest': 'fashion', 'occupation': 'student'},\n...           {'interest': 'fashion','occupation':'professional'},\n...           {'interest': 'sports', 'occupation': 'student'},\n...           {'interest': 'tech', 'occupation': 'student'},\n...           {'interest': 'tech', 'occupation': 'retired'},\n...           {'interest': 'sports','occupation': 'professional'}]\n>>> dict_one_hot_encoder = DictVectorizer(sparse=False)\n>>> X_encoded = dict_one_hot_encoder.fit_transform(X_dict)\n>>> print(X_encoded)\n[[ 0\\.  0\\. 1\\. 1\\.  0\\. 0.]\n [ 1\\.  0\\. 0\\. 0\\.  0\\. 1.]\n [ 1\\.  0\\. 0\\. 1\\.  0\\. 0.]\n [ 0\\.  1\\. 0\\. 0\\.  0\\. 1.]\n [ 0\\.  0\\. 1\\. 0\\.  0\\. 1.]\n [ 0\\.  0\\. 1\\. 0\\.  1\\. 0.]\n [ 0\\.  1\\. 0\\. 1\\.  0\\. 0.]] \n```", "```py\n>>> print(dict_one_hot_encoder.vocabulary_)\n{'interest=fashion': 0, 'interest=sports': 1,\n'occupation=professional': 3, 'interest=tech': 2,\n'occupation=retired': 4, 'occupation=student': 5} \n```", "```py\n>>> new_dict = [{'interest': 'sports', 'occupation': 'retired'}]\n>>> new_encoded = dict_one_hot_encoder.transform(new_dict)\n>>> print(new_encoded)\n[[ 0\\. 1\\. 0\\. 0\\. 1\\. 0.]] \n```", "```py\n>>> print(dict_one_hot_encoder.inverse_transform(new_encoded))\n[{'interest=sports': 1.0, 'occupation=retired': 1.0}] \n```", "```py\n>>> new_dict = [{'interest': 'unknown_interest',\n               'occupation': 'retired'},\n...             {'interest': 'tech', 'occupation':\n               'unseen_occupation'}]\n>>> new_encoded = dict_one_hot_encoder.transform(new_dict)\n>>> print(new_encoded)\n[[ 0\\.  0\\. 0\\. 0\\.  1\\. 0.]\n [ 0\\.  0\\. 1\\. 0\\.  0\\. 0.]] \n```", "```py\n>>> import pandas as pd\n>>> df = pd.DataFrame({'score': ['low',\n...                              'high',\n...                              'medium',\n...                              'medium',\n...                              'low']})\n>>> print(df)\n    score\n0     low\n1    high\n2  medium\n3  medium\n4     low\n>>> mapping = {'low':1, 'medium':2, 'high':3}\n>>> df['score'] = df['score'].replace(mapping)\n>>> print(df)\n   score\n0      1\n1      3\n2      2\n3      2\n4      1 \n```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> def sigmoid(input):\n...     return 1.0 / (1 + np.exp(-input)) \n```", "```py\n>>> z = np.linspace(-8, 8, 1000)\n>>> y = sigmoid(z)\n>>> plt.plot(z, y)\n>>> plt.axhline(y=0, ls='dotted', color='k')\n>>> plt.axhline(y=0.5, ls='dotted', color='k')\n>>> plt.axhline(y=1, ls='dotted', color='k')\n>>> plt.yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n>>> plt.xlabel('z')\n>>> plt.ylabel('y(z)')\n>>> plt.show() \n```", "```py\n>>> y_hat = np.linspace(0.001, 0.999, 1000)\n>>> cost = -np.log(y_hat)\n>>> plt.plot(y_hat, cost)\n>>> plt.xlabel('Prediction')\n>>> plt.ylabel('Cost')\n>>> plt.xlim(0, 1)\n>>> plt.ylim(0, 7)\n>>> plt.show() \n```", "```py\n>>> y_hat = np.linspace(0.001, 0.999, 1000)\n>>> cost = -np.log(1 - y_hat)\n>>> plt.plot(y_hat, cost)\n>>> plt.xlabel('Prediction')\n>>> plt.ylabel('Cost')\n>>> plt.xlim(0, 1)\n>>> plt.ylim(0, 7)\n>>> plt.show() \n```", "```py\n    >>> def compute_prediction(X, weights):\n    ...     \"\"\"\n    ...     Compute the prediction y_hat based on current weights\n    ...     \"\"\"\n    ...     z = np.dot(X, weights)\n    ...     return sigmoid(z) \n    ```", "```py\n>>> def update_weights_gd(X_train, y_train, weights,\n                                           learning_rate):\n...     \"\"\"\n...     Update weights by one step\n...     \"\"\"\n...     predictions = compute_prediction(X_train, weights)\n...     weights_delta = np.dot(X_train.T, y_train - predictions)\n...     m = y_train.shape[0]\n...     weights += learning_rate / float(m) * weights_delta\n...     return weights \n```", "```py\n    >>> def compute_cost(X, y, weights):\n    ...     \"\"\"\n    ...     Compute the cost J(w)\n    ...     \"\"\"\n    ...     predictions = compute_prediction(X, weights)\n    ...     cost = np.mean(-y * np.log(predictions)\n                          - (1 - y) * np.log(1 - predictions))\n    ...     return cost \n    ```", "```py\n>>> def train_logistic_regression(X_train, y_train, max_iter,\n                                  learning_rate, fit_intercept=False):\n...     \"\"\" Train a logistic regression model\n...     Args:\n...         X_train, y_train (numpy.ndarray, training data set)\n...         max_iter (int, number of iterations)\n...         learning_rate (float)\n...         fit_intercept (bool, with an intercept w0 or not)\n...     Returns:\n...         numpy.ndarray, learned weights\n...     \"\"\"\n...     if fit_intercept:\n...         intercept = np.ones((X_train.shape[0], 1))\n...         X_train = np.hstack((intercept, X_train))\n...     weights = np.zeros(X_train.shape[1])\n...     for iteration in range(max_iter):\n...         weights = update_weights_gd(X_train, y_train,\n                                       weights, learning_rate)\n...         # Check the cost for every 100 (for example)      \n             iterations\n...         if iteration % 100 == 0:\n...             print(compute_cost(X_train, y_train, weights))\n...     return weights \n```", "```py\n    >>> def predict(X, weights):\n    ...     if X.shape[1] == weights.shape[0] - 1:\n    ...         intercept = np.ones((X.shape[0], 1))\n    ...         X = np.hstack((intercept, X))\n    ...     return compute_prediction(X, weights) \n    ```", "```py\n>>> X_train = np.array([[6, 7],\n...                     [2, 4],\n...                     [3, 6],\n...                     [4, 7],\n...                     [1, 6],\n...                     [5, 2],\n...                     [2, 0],\n...                     [6, 3],\n...                     [4, 1],\n...                     [7, 2]])\n>>> y_train = np.array([0,\n...                     0,\n...                     0,\n...                     0,\n...                     0,\n...                     1,\n...                     1,\n...                     1,\n...                     1,\n...                     1]) \n```", "```py\n>>> weights = train_logistic_regression(X_train, y_train,\n             max_iter=1000, learning_rate=0.1, fit_intercept=True)\n0.574404237166\n0.0344602233925\n0.0182655727085\n0.012493458388\n0.00951532913855\n0.00769338806065\n0.00646209433351\n0.00557351184683\n0.00490163225453\n0.00437556774067 \n```", "```py\n>>> X_test = np.array([[6, 1],\n...                    [1, 3],\n...                    [3, 1],\n...                    [4, 5]])\n>>> predictions = predict(X_test, weights)\n>>> print(predictions)\narray([ 0.9999478 , 0.00743991, 0.9808652 , 0.02080847]) \n```", "```py\n>>> plt.scatter(X_train[:5,0], X_train[:5,1], c='b', marker='x')\n>>> plt.scatter(X_train[5:,0], X_train[5:,1], c='k', marker='.')\n>>> for i, prediction in enumerate(predictions):\n        marker = 'X' if prediction < 0.5 else 'o'\n        c = 'b' if prediction < 0.5 else 'k'\n        plt.scatter(X_test[i,0], X_test[i,1], c=c, marker=marker) \n```", "```py\n>>> plt.show() \n```", "```py\n>>> import pandas as pd\n>>> n_rows = 300000\n>>> df = pd.read_csv(\"train.csv\", nrows=n_rows)\n>>> X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'],\n                                                     axis=1).values\n>>> Y = df['click'].values\n>>> n_train = 10000\n>>> X_train = X[:n_train]\n>>> Y_train = Y[:n_train]\n>>> X_test = X[n_train:]\n>>> Y_test = Y[n_train:]\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> enc = OneHotEncoder(handle_unknown='ignore')\n>>> X_train_enc = enc.fit_transform(X_train)\n>>> X_test_enc = enc.transform(X_test) \n```", "```py\n>>> import timeit\n>>> start_time = timeit.default_timer()\n>>> weights = train_logistic_regression(X_train_enc.toarray(),\n              Y_train, max_iter=10000, learning_rate=0.01,\n              fit_intercept=True)\n0.6820019456743648\n0.4608619713011896\n0.4503715555130051\n…\n…\n…\n0.41485094023829017\n0.41477416506724385\n0.41469802145452467\n>>> print(f\"--- {(timeit.default_timer() - start_time :.3f} seconds ---\")\n--- 183.840 seconds --- \n```", "```py\n>>> pred = predict(X_test_enc.toarray(), weights)\n>>> from sklearn.metrics import roc_auc_score\n>>> print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')\nTraining samples: 10000, AUC on testing set: 0.703 \n```", "```py\n>>> def update_weights_sgd(X_train, y_train, weights,\n                                           learning_rate):\n...     \"\"\" One weight update iteration: moving weights by one\n            step based on each individual sample\n...     Args:\n...     X_train, y_train (numpy.ndarray, training data set)\n...     weights (numpy.ndarray)\n...     learning_rate (float)\n...     Returns:\n...     numpy.ndarray, updated weights\n...     \"\"\"\n...     for X_each, y_each in zip(X_train, y_train):\n...         prediction = compute_prediction(X_each, weights)\n...         weights_delta = X_each.T * (y_each - prediction)\n...         weights += learning_rate * weights_delta\n...     return weights \n```", "```py\n>>> def train_logistic_regression_sgd(X_train, y_train, max_iter,\n                              learning_rate, fit_intercept=False):\n...     \"\"\" Train a logistic regression model via SGD\n...     Args:\n...     X_train, y_train (numpy.ndarray, training data set)\n...     max_iter (int, number of iterations)\n...     learning_rate (float)\n...     fit_intercept (bool, with an intercept w0 or not)\n...     Returns:\n...     numpy.ndarray, learned weights\n...     \"\"\"\n...     if fit_intercept:\n...         intercept = np.ones((X_train.shape[0], 1))\n...         X_train = np.hstack((intercept, X_train))\n...     weights = np.zeros(X_train.shape[1])\n...     for iteration in range(max_iter):\n...         weights = update_weights_sgd(X_train, y_train, weights,\n                                                     learning_rate)\n...         # Check the cost for every 2 (for example) iterations\n...         if iteration % 2 == 0:\n...             print(compute_cost(X_train, y_train, weights))\n...     return weights \n```", "```py\n>>> start_time = timeit.default_timer()\n>>> weights = train_logistic_regression_sgd(X_train_enc.toarray(),\n        Y_train, max_iter=10, learning_rate=0.01, fit_intercept=True)\n0.4127864859625796\n0.4078504597223988\n0.40545733114863264\n0.403811787845451\n0.4025431351250833\n>>> print(f\"--- {(timeit.default_timer() - start_time)}.3fs seconds ---\")\n--- 25.122 seconds ---\n>>> pred = predict(X_test_enc.toarray(), weights)\n>>> print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')\nTraining samples: 100000, AUC on testing set: 0.732 \n```", "```py\n>>> from sklearn.linear_model import SGDClassifier\n>>> sgd_lr = SGDClassifier(loss='log_loss', penalty=None,\n             fit_intercept=True, max_iter=20,\n             learning_rate='constant', eta0=0.01) \n```", "```py\n>>> sgd_lr.fit(X_train_enc.toarray(), Y_train)\n>>> pred = sgd_lr.predict_proba(X_test_enc.toarray())[:, 1]\n>>> print(f'Training samples: {n_train}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')\nTraining samples: 100000, AUC on testing set: 0.732 \n```", "```py\n>>> sgd_lr_l1 = SGDClassifier(loss='log_loss',\n                          penalty='l1',\n                          alpha=0.0001,\n                          fit_intercept=True,\n                          max_iter=10,\n                          learning_rate='constant',\n                          eta0=0.01,\n                          random_state=42)\n>>> sgd_lr_l1.fit(X_train_enc.toarray(), Y_train) \n```", "```py\n>>> coef_abs = np.abs(sgd_lr_l1.coef_)\n>>> print(coef_abs)\n[[0\\. 0.16654682 0\\. ... 0\\. 0\\. 0.12803394]] \n```", "```py\n>>> print(np.sort(coef_abs)[0][:10])\n[0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]\n>>> bottom_10 = np.argsort(coef_abs)[0][:10] \n```", "```py\n>>> feature_names = enc.get_feature_names_out()\n>>> print('10 least important features are:\\n', feature_names[bottom_10])\n10 least important features are:\n ['x0_1001' 'x8_84c2f017' 'x8_84ace234'  'x8_84a9d4ba' 'x8_84915a27'\n'x8_8441e1f3' 'x8_840161a0' 'x8_83fbdb80' 'x8_83fb63cd' 'x8_83ed0b87'] \n```", "```py\n>>> print(np.sort(coef_abs)[0][-10:])\n[0.67912376 0.70885933 0.75157162 0.81783177 0.94672827 1.00864062\n 1.08152137 1.130848   1.14859459 1.37750805]\n>>> top_10 = np.argsort(coef_abs)[0][-10:]\n>>> print('10 most important features are:\\n', feature_names[top_10])\n10 most important features are:\n ['x4_28905ebd' 'x3_7687a86e' 'x18_61' 'x18_15' 'x5_5e3f096f' 'x5_9c13b419' 'x2_763a42b5' 'x3_27e3c518' 'x2_d9750ee7' 'x5_1779deee'] \n```", "```py\n>>> from sklearn.ensemble import RandomForestClassifier\n>>> random_forest = RandomForestClassifier(n_estimators=100,\n                 criterion='gini', min_samples_split=30, n_jobs=-1)\n>>> random_forest.fit(X_train_enc.toarray(), Y_train) \n```", "```py\n>>> feature_imp = random_forest.feature_importances_\n>>> print(feature_imp)\n[1.22776093e-05 1.42544940e-03 8.11601536e-04 ... 7.51812083e-04 8.79340746e-04 8.49537255e-03] \n```", "```py\n>>> feature_names = enc.get_feature_names()\n>>> print(np.sort(feature_imp)[:10])\n[0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]\n>>> bottom_10 = np.argsort(feature_imp)[:10]\n>>> print('10 least important features are:\\n', feature_names[bottom_10])\n10 least important features are:\n ['x5_f0222e42' 'x8_7d196936' 'x2_ba8f6070' 'x2_300ede9d' 'x5_72c55d0b' 'x2_4390d4c5' 'x5_69e5a5ec' 'x8_023a5294' 'x11_15541' 'x6_2022d54e'] \n```", "```py\n>>> print(np.sort(feature_imp)[-10:])\n[0.00849437 0.00849537 0.00872154 0.01010324 0.0109653  0.01099363 0.01319093 0.01471638 0.01802233 0.01889752]\n>>> top_10 = np.argsort(feature_imp)[-10:]\n>>> print('10 most important features are:\\n', feature_names[top_10])\n10 most important features are:\n ['x3_7687a86e' 'x18_157' 'x17_-1' 'x14_1993' 'x8_8a4875bd' 'x2_d9750ee7' 'x3_98572c79' 'x16_1063' 'x15_2' 'x18_33'] \n```", "```py\n>>> n_rows = 100000 * 11\n>>> df = pd.read_csv(\"train.csv\", nrows=n_rows)\n>>> X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'],\n                                                      axis=1).values\n>>> Y = df['click'].values\n>>> n_train = 100000 * 10\n>>> X_train = X[:n_train]\n>>> Y_train = Y[:n_train]\n>>> X_test = X[n_train:]\n>>> Y_test = Y[n_train:] \n```", "```py\n>>> enc = OneHotEncoder(handle_unknown='ignore')\n>>> enc.fit(X_train) \n```", "```py\n>>> sgd_lr_online = SGDClassifier(loss='log_loss',\n                              penalty=None,\n                              fit_intercept=True,\n                              max_iter=1,\n                              learning_rate='constant',\n                              eta0=0.01,\n                              random_state=42) \n```", "```py\n>>> start_time = timeit.default_timer()\n>>> for i in range(10):\n...     x_train = X_train[i*100000:(i+1)*100000]\n...     y_train = Y_train[i*100000:(i+1)*100000]\n...     x_train_enc = enc.transform(x_train)\n...     sgd_lr_online.partial_fit(x_train_enc.toarray(), y_train,\n                                                    classes=[0, 1]) \n```", "```py\n>>> print(f\"--- {(timeit.default_timer() - start_time):.3f} seconds ---\")\n--- 87.399s seconds --- \n```", "```py\n>>> x_test_enc = enc.transform(X_test)\n>>> pred = sgd_lr_online.predict_proba(x_test_enc.toarray())[:, 1]\n>>> print(f'Training samples: {n_train * 10}, AUC on testing set: {roc_auc_score(Y_test, pred):.3f}')\nTraining samples: 10000000, AUC on testing set: 0.762 \n```", "```py\n>>> from sklearn import datasets\n>>> digits = datasets.load_digits()\n>>> n_samples = len(digits.images) \n```", "```py\n>>> X = digits.images.reshape((n_samples, -1))\n>>> Y = digits.target \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n                                    test_size=0.2, random_state=42) \n```", "```py\n>>> from sklearn.model_selection import GridSearchCV\n>>> parameters = {'penalty': ['l2', None],\n...               'alpha': [1e-07, 1e-06, 1e-05, 1e-04],\n...               'eta0': [0.01, 0.1, 1, 10]}\n>>> sgd_lr = SGDClassifier(loss='log_loss',\n                       learning_rate='constant',\n                       fit_intercept=True,\n                       max_iter=50,\n                       random_state=42)\n>>> grid_search = GridSearchCV(sgd_lr, parameters,\n                               n_jobs=-1, cv=5)\n>>> grid_search.fit(X_train, Y_train)\n>>> print(grid_search.best_params_)\n{'alpha': 1e-05, 'eta0': 0.01, 'penalty': 'l2' } \n```", "```py\n>>> sgd_lr_best = grid_search.best_estimator_\n>>> accuracy = sgd_lr_best.score(X_test, Y_test)\n>>> print(f'The accuracy on testing set is: {accuracy*100:.1f}%')\nThe accuracy on testing set is: 94.7% \n```", "```py\n    >>> import tensorflow as tf\n    >>> X_train_enc = enc.fit_transform(X_train).toarray().astype('float32')\n    >>> X_test_enc = enc.transform(X_test).toarray().astype('float32')\n    >>> Y_train = Y_train.astype('float32')\n    >>> Y_test = Y_test.astype('float32') \n    ```", "```py\n    >>> batch_size = 1000\n    >>> train_data = tf.data.Dataset.from_tensor_slices((X_train_enc, Y_train))\n    >>> train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1) \n    ```", "```py\n    >>> n_features = X_train_enc.shape[1]\n    >>> W = tf.Variable(tf.zeros([n_features, 1]))\n    >>> b = tf.Variable(tf.zeros([1])) \n    ```", "```py\n    >>> learning_rate = 0.001\n    >>> optimizer = tf.optimizers.Adam(learning_rate) \n    ```", "```py\n    >>> def run_optimization(x, y):\n    ...     with tf.GradientTape() as tape:\n    ...         logits = tf.add(tf.matmul(x, W), b)[:, 0]\n    ...         loss = tf.reduce_mean(\n                         tf.nn.sigmoid_cross_entropy_with_logits(\n                                             labels=y, logits=logits))\n            # Update the parameters with respect to the gradient calculations\n    ...     gradients = tape.gradient(loss, [W, b])\n    ...     optimizer.apply_gradients(zip(gradients, [W, b])) \n    ```", "```py\n    >>> training_steps = 5000\n    >>> for step, (batch_x, batch_y) in\n                  enumerate(train_data.take(training_steps), 1):\n    ...     run_optimization(batch_x, batch_y)\n    ...     if step % 500 == 0:\n    ...         logits = tf.add(tf.matmul(batch_x, W), b)[:, 0]\n    ...         loss = tf.reduce_mean(\n                           tf.nn.sigmoid_cross_entropy_with_logits(\n                                 labels=batch_y, logits=logits))\n    ...         print(\"step: %i, loss: %f\" % (step, loss))\n    step: 500, loss: 0.448672\n    step: 1000, loss: 0.389186\n    step: 1500, loss: 0.413012\n    step: 2000, loss: 0.445663\n    step: 2500, loss: 0.361000\n    step: 3000, loss: 0.417154\n    step: 3500, loss: 0.359435\n    step: 4000, loss: 0.393363\n    step: 4500, loss: 0.402097\n    step: 5000, loss: 0.376734 \n    ```", "```py\n    >>> logits = tf.add(tf.matmul(X_test_enc, W), b)[:, 0]\n    >>> pred = tf.nn.sigmoid(logits)\n    >>> auc_metric = tf.keras.metrics.AUC()\n    >>> auc_metric.update_state(Y_test, pred)\n    >>> print(f'AUC on testing set: {auc_metric.result().numpy():.3f}')\n    AUC on testing set: 0.736 \n    ```"]